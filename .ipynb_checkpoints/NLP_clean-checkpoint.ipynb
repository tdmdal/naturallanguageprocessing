{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We will be using the venturebeat data that we have scrapped and stored. We will begin with loading the data, inspecting it and then convert text into numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('venturebeat.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task/problem here is to build a natural language processing model that can take the information of the article and determine the topic it belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing\n",
    "\n",
    "We can extract date, month and day from the url using regular expression and datatime functionalities. We can also add a length and nwords column that represent the number of characters and the number of words in the article text, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(string):\n",
    "    match = re.search(r'\\d{4}/\\d{1,2}/\\d{1,2}', str(string))\n",
    "    return match.group() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['url'].apply(extract_date))\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "\n",
    "data['length'] = data['text'].str.len()\n",
    "data['nwords'] = data['text'].str.split().str.len()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lexical diversity__ is one aspect of 'lexical richness' and refers to the ratio of different unique words to the total number of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len( set(text) ) / len( text.split() )\n",
    "\n",
    "data['text'] = data['text'].astype(str)\n",
    "data['lex_div'] = data['text'].apply(lexical_diversity)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['text'].values.tolist()\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into meaningul elements called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular', quiet=True)\n",
    "from nltk import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I haven't watched the show at the theatre.\"\n",
    "tokenized = nltk.word_tokenize(example)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( wordpunct_tokenize(example) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each word as it appears in the document. \n",
    "\n",
    "### 1b. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(token):\n",
    "    stops  = set(stopwords.words('english'))\n",
    "    return token.lower() in stops\n",
    "\n",
    "print(tokenized)\n",
    "print( [ is_stopword(i) for i in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example.split() )\n",
    "print( [ is_stopword(i) for i in example.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def is_punct(token):\n",
    "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "print(tokenized)\n",
    "print( [ is_punct(i) for i in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( wordpunct_tokenize(example) ) \n",
    "print( [ is_punct(i) for i in wordpunct_tokenize(example)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmed = [ stemmer.stem(token) for token in tokenized ]\n",
    "print( [example] )\n",
    "print(tokenized)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokenized = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        tokenized.append(stem.stem(token))\n",
    "    \n",
    "    tokenized = [token for token in tokenized \n",
    "                 if not is_punct(token)            # remove tokens that are punctuations\n",
    "                 and not is_stopword(token)        # remove stopwords\n",
    "                 and token.isascii()               # remove non-english characters\n",
    "               ]\n",
    "            \n",
    "    return ' '.join(tokenized)                     # join b/c we are inputting a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( example )\n",
    "print( '---> ' + normalizer(example) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = [ normalizer(i) for i in corpus ]\n",
    "print(corpus[0][:999])\n",
    "norm_corpus[0][:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_punct(token):\n",
    "    return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "\n",
    "def lemmatizer(token, postag):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tag= {\n",
    "        'N':wn.NOUN,\n",
    "        'V':wn.VERB,\n",
    "        'R':wn.ADV,\n",
    "        'J':wn.ADJ\n",
    "    }.get(postag[0], wn.NOUN)\n",
    "    \n",
    "    return lemm.lemmatize(token, tag)\n",
    "\n",
    "def normalizer_lemm(text):\n",
    "    \n",
    "    tagged_tokenized = pos_tag(wordpunct_tokenize(text))\n",
    "    \n",
    "    tokenized = [ lemmatizer(token, tag).lower() \n",
    "                 for (token, tag) in tagged_tokenized\n",
    "                 if not is_punct(token) \n",
    "                 and token.isascii()\n",
    "                ]\n",
    "    \n",
    "    # remove extended stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['game', 'compani'])\n",
    "    stops = set(stop_words)\n",
    "    tokenized = [token for token in tokenized if not token in stops]\n",
    "    \n",
    "    return ' '.join(tokenized)                     # join b/c we are inputting a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normlemm_corpus = [ normalizer_lemm(i) for i in corpus ]\n",
    "normlemm_corpus[0][:999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction: Vectorization\n",
    "\n",
    "The simplest vector encoding model is to simply fill in the vector with the frequency of each word as it appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = defaultdict(int)\n",
    "for token in word_tokenize(example):\n",
    "    words[token] += 1\n",
    "words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2a. Count Vectorizer \n",
    " \n",
    " Scikit-Learn has a CountVectorizer transformer which does this for us easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(norm_corpus)\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "nfeatures = len(features)\n",
    "print(nfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text.freqdist import FreqDistVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "visualizer = FreqDistVisualizer(features=features, n=30, ax=ax )\n",
    "visualizer.fit(vector)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15,5))\n",
    "lists_asc = sorted(vocab.items())\n",
    "x = [i for (i,j) in lists_asc]\n",
    "y = [j for (i,j) in lists_asc]\n",
    "\n",
    "n=30\n",
    "plt.bar(x[:n], y[:n])\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokenized = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        tokenized.append(stem.stem(token))\n",
    "    \n",
    "    tokenized = [token for token in tokenized \n",
    "                 if not is_punct(token)            # remove tokens that are punctuations\n",
    "                 and token.isascii()               # remove non-english characters\n",
    "               ]\n",
    "    \n",
    "    # remove extended stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['data','compani'])\n",
    "    stops = set(stop_words)\n",
    "    tokenized = [token for token in tokenized if not token in stops]\n",
    "    \n",
    "    return ' '.join(tokenized)                     # join b/c we are inputting a list\n",
    "\n",
    "normalizer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = [ normalizer(i) for i in corpus ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. TFIDF Vectorizer\n",
    "\n",
    "Again, Scikit-learn has provided an easy to work with functin for this. There is also a \"ngram_range\" parameter, which will help to create vocabulary with one or phrases of two words or both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word')\n",
    "tfidf_vector = tfidf.fit_transform(norm_corpus)\n",
    "tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15,8))\n",
    "visualizer = FreqDistVisualizer(features=tfidf.get_feature_names(), n=30, ax=ax )\n",
    "visualizer.fit(tfidf_vector)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(vector.toarray(), columns=features)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['month','day','length','nwords','lex_div']\n",
    "X = pd.concat([data[cols], X], axis=1)\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html # in class notes 1 is the best cluster. It is a highly dense. Higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "clusterer = KMeans(n_clusters=k, random_state=10)\n",
    "cluster_labels = clusterer.fit_predict(X)\n",
    "inertia = clusterer.inertia_\n",
    "avg_silhouette = silhouette_score(X, cluster_labels)\n",
    "print(k, inertia, avg_silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = range(3,12)\n",
    "silhouette_scores = []\n",
    "inertia_scores = []\n",
    "for k in clusters:\n",
    "    clstr = MiniBatchKMeans(n_clusters=k, random_state=24) # create an instance of Kmeans\n",
    "    pred = clstr.fit_predict(X)\n",
    "    inertia_scores.append(clstr.inertia_) \n",
    "    avg_silhouette = silhouette_score(X, pred)\n",
    "    silhouette_scores.append((k, avg_silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(clusters, inertia_scores, 'bx-')\n",
    "for i, txt in enumerate(silhouette_scores):\n",
    "    ax.annotate(round(txt[1],2), (clusters[i], inertia_scores[i]))\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method showing the optimal k\\n (Silhouette score annotated for each k)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(KMeans(), k=(3,12))\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher alpha assumes documents to be made up of more topics and result in more specific topic distribution per document.\n",
    "# with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
    "number_topics = 3\n",
    "number_words = 20\n",
    "\n",
    "def topics(model, count_vectorizer, n_top_words):\n",
    "\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda_fit = lda.fit(X)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "topics(lda, vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer='word')\n",
    "tfidf_vector = tfidf.fit_transform(normlemm_corpus)\n",
    "\n",
    "tfidf_features = tfidf.get_feature_names()\n",
    "X = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "number_topics = 6\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda_fit = lda.fit(X)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "topics(lda, tfidf, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.sklearn.prepare(lda, tfidf_vector, tfidf)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
