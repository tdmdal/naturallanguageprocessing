url,title,text
https://venturebeat.com/2021/05/13/google-cloud-ceo-predicts-boom-in-business-process-as-a-service/,Google Cloud CEO predicts boom in business-process-as-a-service,"Google Cloud CEO Thomas Kurian is focusing on automation and business process improvement as the company seeks to gain ground on cloud computing competitors. Speaking at the virtual Automation Anywhere Imagine Digital conference, Kurian said a shift in focus to business processes as a service will define enterprises’ future cloud migrations. The traditional on-ramp to the cloud was about technical integration. That meant migration of enterprise apps to virtual machines, data to cloud databases, and refactoring apps to scale more efficiently with microservices, Kurian said. Now, business executives’ traditional focus on practical business problems — the search for new opportunities, improving customer experiences, and reducing the costs — will come more into play. “By abstracting the underlying technology, and making it easier and more efficient to get the business process, you can accelerate time to value,” Kurian said. “People really want to find places where they can unlock value. And the places where they unlock value are in every touchpoint with the customer.” At the conference, Kurian weighed in on how ongoing efforts will strategically shift the on-ramp to the cloud. He said enterprises will work to streamline the way that they can define, execute, and operate a core business process, whether that is a loan origination process in a financial institution, an accounts receivable process in a traditional company, or an order to cash process in a manufacturing institution. One of the daunting things about moving to the cloud is that businesses discover they are putting an “internal mess” on display for the world to see, Kurian said. Companies capable of cleaning up a business process mess will improve their time to market, predictability, and customer experiences. Kurian expanded on the three-step processes involved in migrating business processes to the cloud. The first step is to use data to understand what the most valuable processes might be to automate. This may be driven by cost reduction or seeking competitive advantage by speeding up, for example, the process of originating a loan compared to competitors. The second step lies in finding the best way to automate the business process. Process mining and process discovery can find ways to reduce certain steps from the process. These can be implemented with RPA and low-code tools. The third step lies in using analytics on the processes that have been automated to understand how the company could get even better, said Kurian. Process analytics tools can assess the value of automations, prioritize them, and calibrate the estimates with actual results. He said the combination of better process automation and analytics tools using Google’s AI will allow executives to track the efficiency of the processes they have instrumented for constant improvement. Kurian argued the combination of better process tools with AI will support the next level of abstraction for natural language processing (NLP), one of Google’s strong suits. Google started off by enabling NLP to understand and translate words, Kurian told the conference attendees. Then the company’s engineers saw there was more value in translating sentences. When they started applying these tools to customer service, they realized it was important to be able to interpret the ongoing conversation people were having about an issue, so that customers did not have to repeat themselves each time. Kurian said that bringing business processes into NLP will extend the boundary of what AI can do even further. For example, an expert involved in approving loans may have focused on understanding and comparing certain fields in the application in making the loan. Once this process is digitized, the company can scale that particular person by capturing their process and decisions into an RPA bot. Kurian positioned this as “the core to streamlining how organizations function, and how organizations can improve the efficiency and speed of their core processes.” For now, Google appears intent on pursuing alliances and partnerships in such key technology areas as RPA. That has occurred as players including Microsoft, IBM, and ServiceNow have acquired RPA-oriented startups. Both Google Cloud and Automation Anywhere have made significant pivots recently to meld business services with cloud infrastructure. Just this year, these pivots aligned around a new partnership between the two to accelerate the adoption of RPA on a global scale. This included technology integration, joint solution development, and aligning sales and marketing efforts. Google hired Kurian, a seasoned Oracle exec, to infuse a pragmatic business focus into its technology-centric commercial cloud undertaking. Meanwhile, Automation Anywhere took an extended pause on product development to migrate its core robotic process automation (RPA) platform to a native cloud architecture. “Digitization requires complete front-to-back automation, not just for efficiency, but for competitive advantage, and that’s the vision that both our companies share,” Kurian said."
https://venturebeat.com/2021/05/13/linkedin-open-sources-greykite-a-library-for-time-series-forecasting/,"LinkedIn open-sources Greykite, a library for time series forecasting","LinkedIn today open-sourced Greykite, a Python library for long- and short-term predictive analytics. Greykite’s main algorithm, Silverkite, delivers automated forecasting, which LinkedIn says it uses for resource planning, performance management, optimization, and ecosystem insight generation. For enterprises using predictive models to forecast consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the pandemic. This being the case, accurate knowledge about the future remains helpful to any business. Automation, which enables reproducibility, may improve accuracy and can be consumed by algorithms downstream to make decisions. For example, LinkedIn says that Silverkite improved revenue forecasts for 1-day ahead and 7-day ahead, as well as Weekly Active User forecasts for 2-week ahead. Median absolute percent error for revenue and Weekly Active User forecasts grew by more than 50% and 30%, respectively. Greykite provides time series tools for trends, seasonality, holidays, and more so that users can fit the AI models of their choice. The library provides exploratory plots and templates for tuning, which define regressors based on data characteristics and forecast requirements like hourly short-term forecast and daily long-term forecast. Tuning knobs provided by the templates reduce the search to find a satisfactory forecast. And the Greykite library has flexibility to customize a model template for algorithms, letting users label (and specify whether to ignore or adjust) known anomalies. Greykite, which provides outlier detection, can also select the optimal model from multiple candidates using past performance data. Instead of tuning each forecast separately, users can define a set of candidate forecast configurations that capture different types of patterns. Lastly, the library provides a summary that can be used to assess the effect of individual data points. For example, Greykite can check the magnitude of a holiday, see how much a changepoint affected the trend, or show how a certain feature might be beneficial to a model.  With Greykite, a “next 7-day” forecast trained on over 8 years of daily data takes only a few seconds to produce forecasts. LinkedIn says that its whole pipeline, including automatic changepoint detection, cross-validation, backtest, and evaluation, completes in under 45 seconds. “The Greykite library provides a fast, accurate, and highly customizable algorithm — Silverkite — for forecasting. Greykite also provides intuitive tuning options and diagnostics for model interpretation. It is extensible to multiple algorithms, and facilitates benchmarking them through a single interface,” the LinkedIn research team wrote in a blog post. “We have successfully applied Greykite at LinkedIn for multiple business and infrastructure metrics use cases.” The Greykite library is available on GitHub and PyPI, and it joins the many other tools LinkedIn has open-sourced to date. They include Iris, for managing website outages; PalDB, a low-key value store for handling side data; Ambry, an object store for media files; GDMix, a framework for training AI personalization models; LiFT, a toolkit to measure AI model fairness; and Dagli, a machine learning library for Java."
https://venturebeat.com/2021/05/13/framework-opens-preorders-for-1000-repairable-and-upgradeable-laptop/,"Framework opens preorders for $1,000 repairable and upgradeable laptop","Framework has begun taking preorders in the U.S. for its upgradeable, customizable, and repairable laptop. The company designed the laptop in response to big companies like Apple making it expensive to repair or upgrade devices. The company acknowledged that the worldwide supply chain constraints mean the number of Framework laptops available at launch will be limited. Founder Nirav Patel said in an interview with VentureBeat that the time has come for consumer electronics products that are designed to last. He founded the company in 2019 with the aim of empowering consumers to take care of their own products, increasing longevity and reducing electronic waste in the process. Patel showed me in a video demo how he could pull out little modules that changed the functionality of various ports, changing them from standard USB ports to USB-C ports and so on. It was very easy to swap out parts. “We are building repairable, upgradeable, customizable consumer electronics products,” Patel said. “The goal here is to give consumers back the power to be able to use the products for as long as they would like to. If you go back into the history of computing, that’s what the default was. You bought a machine with the expectation that it was not a sealed box, and it was an item that you could modify, update, and upgrade. And the expectation in the PC space, in general, has always been that you have the power and the flexibility to customize the machine to do the things that you’d like it to.” The Framework Laptop is a thin, lightweight, high-performance productivity notebook with a 13.5-inch screen. It can be upgraded, customized, and repaired in ways no other notebook can, Patel said. “Over the past couple of decades, both PCs and other categories of consumer electronics have just continued to trend towards this path of being sealed up and locked down and fixed into place in a way that when a consumer buys that product, they know that they’re really only going to get a few years of use out of it before some part of it wears out, or goes out of date or breaks,” Patel said. “And they’re forced to buy another one and repeat that over and over again. It’s a pretty poor experience for consumers. But it’s also pretty bad for the environment, and that we’re just generating this enormous quantity of e-waste that continues to grow each year. And so Framework was a way to solve this problem in a way that actually makes sense as a business and as a product model.” It’s a big challenge to take on the likes of Apple, and Framework has just 14 employees. But there are forces on its side. The Federal Trade Commission recently sent a report to Congress detailing repair practices in the industry, and it wasn’t happy with the status quo. Along with socketed storage, Wi-Fi, and two slots of memory, the entire mainboard can be swapped to boost performance as Framework launches updated versions with new CPU generations. High-use parts like the battery, screen, keyboard, and color-customizable magnetic-attach bezel are easy to replace, with spares available directly through the web store. In addition to releasing new upgrade modules regularly and ensuring replacement parts are available, the company is opening up the ecosystem to enable a community of partners to build and sell compatible modules through the Framework Marketplace. “What we’ve done is built a product that is as thin and light and powerful and performant as the notebooks that consumers are buying today,” Patel said. “But every part of it is replaceable by the end user, whether that’s to repair something or replace a battery, which will just inherently wear out after years of use, or even upgrade and replace the entire mainboard and move to new CPU generation to get more performance.” Most consumer electronics devices are disposable one-offs by design. But the single best way to reduce the environmental impact of electronics is to make them last longer, Patel said. In addition to enabling longevity, the company is focused on improving sustainability across the life of its products. The Framework Laptop has both 50% post-consumer recycled (PCR) aluminum and 30% PCR plastic. The packaging is 100% PCR material with no single-use plastics, and the company says all of the product shipments are fully carbon offset. The Framework Laptop is available in a range of preconfigured models running Windows 10 Home or Pro. For people who love to tinker, Framework also created the Framework Laptop DIY Edition, a laptop available as a kit of modules you can customize and assemble yourself, with the ability to choose Windows or install your preferred Linux distribution. The device will ship as the company completes its production models this year. Regarding the semiconductor shortage, Patel said, “It’s definitely a big challenge. We have a great supply chain team, we have a great set of partners, and we’ve anticipated this for months. And so we’ve gone and placed our long-term orders and forecasts, and gotten almost everything that we need for the foreseeable future. There is a single chip, actually, that we are constrained on.  And that’s the reason we’re doing preorder batches, to give people visibility.” The Framework Laptop uses the latest Intel 11th Gen Core i5 and i7 processors and it is upgradeable to Wi-Fi 6E, up to 64GB of DDR4 memory, and 4TB or more of Gen4 NVMe storage. The entire mainboard is replaceable too, enabling upgrades to future CPU generations. On top of that, the Framework Laptop is deeply customizable. The expansion card system lets you choose the ports you want and which side you want them on, selecting from four at a time of USB-C, USB-A, HDMI, DisplayPort, MicroSD, ultra-fast 250GB and 1TB storage, and more. Magnetic-attach bezels are color-customizable to match your style, and the keyboard language can be swapped too. Framework made upgrades and repairs straightforward. The only tool you need is the Framework Screwdriver included in the box, Patel said. Every module has a QR code on it that you can scan for step-by-step instructions, support information, and a link to order a replacement from the Framework Marketplace. The aim is to serve consumer better than companies that believe they know what’s better for consumers. “That attitude by existing companies just gives us the opportunity as a new brand to come in and go to those consumers and go to those repair shops and say we hear you,” he said. “We’ve built the product that answers your needs.” Patel believes that the industry should just make its practices on repair and upgrading transparent to consumers, so people can choose what they want.  To make preordering easy, the company will only be taking a fully refundable $100 deposit at the time of order. It will give you a heads-up when it is preparing to ship the system and then collect the balance of the order. It also offers a 30-day return guarantee. The company is opening preorders in the U.S. today, with Canada coming in the next few weeks. It will take orders in additional countries in Asia and Europe before the end of the year, and you can sign up to get notified when it is available in your region. The company was founded in January 2020, after Patel had spent time at places such as Oculus, Apple, and Facebook."
https://venturebeat.com/2021/05/14/github-now-lets-all-developers-upload-videos-to-demo-bugs-and-features/,GitHub now lets all developers upload videos to demo bugs and features,"GitHub has officially opened video uploads five months after launching the feature in beta and now allows all developers to include .mp4 or .mov files directly in pull requests, discussions, issues, comments, and more. Video uploads are designed to help developers visually demonstrate to project maintainers the steps they went through when they encountered a bug, for example, or illustrate what a major new code change achieves in terms of functionality. So rather than having to follow detailed step-by-step textual instructions that may be ambiguous or unclear, the next person to come along can now see exactly what’s happening at the other end. This could also be used in conjunction with a voice track, with a narrator explaining the on-screen actions. It’s worth noting that with this launch, GitHub also now fully supports video uploads from within its mobile app. Native video upload support helps bypass the cumbersome alternative of recording and uploading a video to a third-party platform and then sharing a link. On that note, GitHub doesn’t yet support video unfurling from shared links, but the company said it’s working on this feature, alongside enabling video annotations for specific pieces of code. As the world has had to rapidly adapt to remote work and collaboration, learning to embrace asynchronous communication is one of the factors essential to distributed teams’ success, and recorded video could play a big part in enabling this process."
https://venturebeat.com/2021/05/13/google-cloud-ceo-predicts-boom-in-business-process-as-a-service/,Google Cloud CEO predicts boom in business-process-as-a-service,"Google Cloud CEO Thomas Kurian is focusing on automation and business process improvement as the company seeks to gain ground on cloud computing competitors. Speaking at the virtual Automation Anywhere Imagine Digital conference, Kurian said a shift in focus to business processes as a service will define enterprises’ future cloud migrations. The traditional on-ramp to the cloud was about technical integration. That meant migration of enterprise apps to virtual machines, data to cloud databases, and refactoring apps to scale more efficiently with microservices, Kurian said. Now, business executives’ traditional focus on practical business problems — the search for new opportunities, improving customer experiences, and reducing the costs — will come more into play. “By abstracting the underlying technology, and making it easier and more efficient to get the business process, you can accelerate time to value,” Kurian said. “People really want to find places where they can unlock value. And the places where they unlock value are in every touchpoint with the customer.” At the conference, Kurian weighed in on how ongoing efforts will strategically shift the on-ramp to the cloud. He said enterprises will work to streamline the way that they can define, execute, and operate a core business process, whether that is a loan origination process in a financial institution, an accounts receivable process in a traditional company, or an order to cash process in a manufacturing institution. One of the daunting things about moving to the cloud is that businesses discover they are putting an “internal mess” on display for the world to see, Kurian said. Companies capable of cleaning up a business process mess will improve their time to market, predictability, and customer experiences. Kurian expanded on the three-step processes involved in migrating business processes to the cloud. The first step is to use data to understand what the most valuable processes might be to automate. This may be driven by cost reduction or seeking competitive advantage by speeding up, for example, the process of originating a loan compared to competitors. The second step lies in finding the best way to automate the business process. Process mining and process discovery can find ways to reduce certain steps from the process. These can be implemented with RPA and low-code tools. The third step lies in using analytics on the processes that have been automated to understand how the company could get even better, said Kurian. Process analytics tools can assess the value of automations, prioritize them, and calibrate the estimates with actual results. He said the combination of better process automation and analytics tools using Google’s AI will allow executives to track the efficiency of the processes they have instrumented for constant improvement. Kurian argued the combination of better process tools with AI will support the next level of abstraction for natural language processing (NLP), one of Google’s strong suits. Google started off by enabling NLP to understand and translate words, Kurian told the conference attendees. Then the company’s engineers saw there was more value in translating sentences. When they started applying these tools to customer service, they realized it was important to be able to interpret the ongoing conversation people were having about an issue, so that customers did not have to repeat themselves each time. Kurian said that bringing business processes into NLP will extend the boundary of what AI can do even further. For example, an expert involved in approving loans may have focused on understanding and comparing certain fields in the application in making the loan. Once this process is digitized, the company can scale that particular person by capturing their process and decisions into an RPA bot. Kurian positioned this as “the core to streamlining how organizations function, and how organizations can improve the efficiency and speed of their core processes.” For now, Google appears intent on pursuing alliances and partnerships in such key technology areas as RPA. That has occurred as players including Microsoft, IBM, and ServiceNow have acquired RPA-oriented startups. Both Google Cloud and Automation Anywhere have made significant pivots recently to meld business services with cloud infrastructure. Just this year, these pivots aligned around a new partnership between the two to accelerate the adoption of RPA on a global scale. This included technology integration, joint solution development, and aligning sales and marketing efforts. Google hired Kurian, a seasoned Oracle exec, to infuse a pragmatic business focus into its technology-centric commercial cloud undertaking. Meanwhile, Automation Anywhere took an extended pause on product development to migrate its core robotic process automation (RPA) platform to a native cloud architecture. “Digitization requires complete front-to-back automation, not just for efficiency, but for competitive advantage, and that’s the vision that both our companies share,” Kurian said."
https://venturebeat.com/2021/05/13/hybrid-multi-clouds-promise-easier-upgrades-but-threaten-data-risk/,"Hybrid multiclouds promise easier upgrades, but threaten data risk","Enterprises see hybrid multicloud as a promising path to new customers and digital transformation — and as a quick on-ramp to rejuvenating IT and driving new revenue models. But many enterprises err badly as they migrate decades-old legacy systems to public, private, and community clouds, accidentally allowing bad actors access to their company’s most valuable data. Marketing claims promise enterprises they can continue to get security and value out of datacenters if they choose hybrid cloud as their future. For many enterprises, the opposite is true. Hybrid multicloud brings greater risk to data in transit and at rest, opening enterprises to more cyber threats and malicious activity from bad actors than they ever encountered before. By definition, a hybrid cloud is an IT architecture comprising legacy IT systems integrated with public, private, and community-based cloud platforms and services. Gartner defines hybrid cloud computing as policy-based and coordinated service provisioning, use, and management across a mixture of internal and external cloud services. Hybrid clouds’ simple definition conflicts with the complexity of making them work securely and at scale. What makes hybrid multicloud so challenging to get right from a security standpoint is how dependent it is on training people and keeping them current on new integration and security techniques. The more manual the hybrid cloud integration process, the easier it is to make an error and expose applications, network segments, storage, and applications. How pervasive are human-based errors in configuring multiclouds? Research group Gartner predicts this year that 50 percent of enterprises will unknowingly and mistakenly expose some applications, network segments, storage, and APIs directly to the public, up from 25% in 2018. By 2023, nearly all (99%) of cloud security failures will be tracked back to manual controls not being set correctly. The promises of hybrid multiclouds need to come with a disclaimer: Your results may vary depending on how deep your team’s expertise is on multiple platforms extending into compliance and governance. Hybrid multiclouds promise to provide the following under ideal conditions that are rarely achieved in organizations today: Enterprises need to work their way through the dark side of hybrid multiclouds to see any benefits. While the challenges are unique to the specific enterprise’s legacy systems, previous results in public, private, and hybrid cloud pilots and proofs-of-concept are a reliable predictor of future results. In reality, hybrid multicloud platforms are among the riskiest and most challenging to get right of any IT infrastructure. According to Bain’s Technology Report 2020:Taming the Flux, the average organization relies on 53 different cloud platform services that go beyond basic computing and storage. Bain’s study found that CIOs say the greater the complexity of multicloud configurations, the greater the security and downtime risks their entire IT infrastructures are exposed to. CIOs also told Bain their organizations are struggling to develop, hire, and retain the talent needed to securely operate one cloud infrastructure at scale, let alone several. That heads a list of indicators that innovative enterprises are seeing as they work to improve their hybrid multicloud security. The indicators include: Lack of compliance and governance are the most costly errors enterprises are making today when it comes to hybrid multicloud deployments. Not only are they paying the fines for lack of compliance, but they’re also losing customers forever when their data is compromised in a breach. Gaps between legacy systems and public, private, and community clouds that provide bad actors an open door to exfiltrate customer data violate the California CCPA laws and the EU’s GDPR laws. Enterprises can achieve more real-time visibility and control across all cloud instances by standardizing on a small series of monitoring tools. That means trimming back, to better ensure assorted tools don’t conflict with each other. How quickly any given business can keep reinventing itself and digitally transform how it serves customers depends on how quickly IT can adapt. Leaders must understand that hybrid multicloud is an important strategy, but the hype doesn’t match the reality. Too many organizations are leaving wide gaps between cloud platforms. The recent high-profile SolarWinds breach exposed hybrid multicloud’s weaknesses and showed the need for Zero Trust frameworks. In the next article in this series, I’ll look at the lessons learned from the SolarWinds hack and how greater understanding can help strengthen compliance and governance of any hybrid cloud initiative. Machine learning and terrain analytics show promising potential to identify and troubleshoot hybrid multicloud security gaps as well, and this too will be explored in the upcoming series."
https://venturebeat.com/2021/05/13/for-language-models-analogies-are-a-tough-nut-to-crack-study-shows/,"For language models, analogies are a tough nut to crack, study shows","Analogies play a crucial role in commonsense reasoning. The ability to recognize analogies like “eye is to seeing what ear is to hearing,” sometimes referred to as analogical proportions, shape how humans structure knowledge and understand language. In a new study that looks at whether AI models can understand analogies, researchers at Cardiff University used benchmarks from education as well as more common datasets.  They found that while off-the-shelf models can identify some analogies, they sometimes struggle with complex relationships, raising questions about to what extent models capture knowledge. Large language models learn to write humanlike text by internalizing billions of examples from the public web. Drawing on sources like ebooks, Wikipedia, and social media platforms like Reddit, they make inferences to complete sentences and even whole paragraphs. But studies demonstrate the pitfall of this training approach. Even sophisticated language models such as OpenAI’s GPT-3 struggle with nuanced topics like morality, history, and law and often memorize answers found in the data on which they’re trained. Memorization isn’t the only challenge large language models struggle with. Recent research shows that even state-of-the-art models struggle to answer the bulk of math problems correctly. For example, a paper published by researchers at the University of California, Berkeley finds that large language models including GPT-3 can only complete 2.9% to 6.9% of problems from a dataset of over 12,500. The Cardiff University researchers used a test dataset from an educational resource that included analogy problems from assessments of linguistic and cognitive abilities. One subset of problems was designed to be equivalent to analogy problems on the Scholastic Aptitude Test (SAT), the U.S. college admission test, while the other set was similar in difficulty to problems on the Graduate Record Examinations (GRE). In the interest of thoroughness, the coauthors combined the dataset with an analogy corpus from Google and BATS, which includes a larger number of concepts and relations split into four categories: lexicographic, encyclopedic, derivational morphology, and inflectional morphology. The word analogy problems are designed to be challenging. Solving them requires identifying nuanced differences between word pairs that belong to the same relation. In experiments, the researchers tested three language models based on the transformer architecture, including Google’s BERT, Facebook’s RoBERTa, and GPT-2, the predecessor of GPT-3. The results show that difficult analogy problems, which are generally more abstract or contain obscure words (e.g., grouch, cantankerous, palace, ornate), present a major barrier. While the models could understand analogies, not all of the models achieved “meaningful improvement.” The researchers leave open the possibility that language models can learn to solve analogy tasks when given the appropriate training data, however. “[Our] findings suggest that while transformer-based language models learn relational knowledge to a meaningful extent, more work is needed to understand how such knowledge is encoded, and how it can be exploited,” the coauthors wrote. “[W]hen carefully tuned, some language models are able to achieve state-of-the-art results.”"
https://venturebeat.com/2021/05/13/sosivio-nabs-4m-for-container-monitoring-and-observability/,Sosivio nabs $4M for container monitoring and observability,"Container observability startup Sosivio today announced that it closed a $4 million seed round led by Seamans Holdings, with participation from Superposition Venture Partners and Side Door Ventures. Cofounder and CEO Nuri Golan says that the proceeds will be used to support product launches and allow the company to scale over the next few years. Containers and Kubernetes technology are entering the mainstream, with more than 35% of IT teams reporting having adopted them in 2019, according to a study from Kubernetes management firm Diamanti. Containers have the advantage of allowing developers to package apps and their dependencies together. But containers also present a monitoring and telemetry challenge. In environments with a large number of containers and microservices, over 50% of IT managers say that a single app request touches more than 25 different technologies, according to a VMWare Tanzu report. Sosivio’s platform provides visibility into cloud-native environments as well as proactive failure prediction, automated resolution, and contextual analysis of signals. It observes data across layers of a cloud environment, facilitating process monitoring on every server. Sosivio can perform app profiling and observability without the need for agents or data-offloading. And the company’s AI engine correlates data, detecting malfunction sequences as they evolve. “Liran Cohen, Sosivio’s cofounder and CTO, was the lead Kubernetes architect for Redhat Europe, working with some of the largest cloud environments for very sensitive and regulated companies, organizations, and governments,” Golan told VentureBeat via email. “During his time working with these customers and fixing critical failures after the fact — with hours and hours of human intervention from several teams in the org — he started to conceptualize a way to do this proactively with the help of AI. Adam Weiner, Avi Stokholm-Cohen, and I joined forces to help take this brilliant technology out of the ‘garage.'” Twenty-one-employee Sosivio competes with a number of container monitoring and observability startups, including Sysdig, which last year raised $70 million in a series E funding round. But Sosivio’s solution runs as a set of microservices without permanent installation, allowing even organizations in regulated sectors such as financial services to benefit, Golan claims. “It only takes a few minutes of research to realize what a big opportunity the observability world offers and how big of a disconnect there is between the tools that are out there and what the customers really want,” Golan said. “When customers see our AI engines in action, predicting their crashes or even minor problems ahead of time, they are shocked. We’re pushing the bounds of what you can do with AI. This is not just tracing or anomaly detection, we built a platform that can actually predict and diagnose your issues before they ever materialize. Sosivio’s platform makes DevOps teams infinitely more effective.” Prior to its formal launch in a couple of weeks, Golan says that Sosivio has been working with 10 customers including a software and IT company, a military unit, an online gambling and gaming company, a startup in the music industry, and an AI company. “Our design partners are using our platform in production environments which run digital applications and services we use every day,” Golan continued. “Imagine you run a large online sports betting company and your users experience delays when placing bets. Or even worse, imagine you are trying to transfer funds from your bank account and the app crashes mid transaction. Did it go through? Did it not? Both are real stories which we helped identify and solve in real time, prior to the predictive capability of our product being live.” Sosivio’s latest financing brings its total raised to over $5 million."
https://venturebeat.com/2021/05/13/linkedin-open-sources-greykite-a-library-for-time-series-forecasting/,"LinkedIn open-sources Greykite, a library for time series forecasting","LinkedIn today open-sourced Greykite, a Python library for long- and short-term predictive analytics. Greykite’s main algorithm, Silverkite, delivers automated forecasting, which LinkedIn says it uses for resource planning, performance management, optimization, and ecosystem insight generation. For enterprises using predictive models to forecast consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the pandemic. This being the case, accurate knowledge about the future remains helpful to any business. Automation, which enables reproducibility, may improve accuracy and can be consumed by algorithms downstream to make decisions. For example, LinkedIn says that Silverkite improved revenue forecasts for 1-day ahead and 7-day ahead, as well as Weekly Active User forecasts for 2-week ahead. Median absolute percent error for revenue and Weekly Active User forecasts grew by more than 50% and 30%, respectively. Greykite provides time series tools for trends, seasonality, holidays, and more so that users can fit the AI models of their choice. The library provides exploratory plots and templates for tuning, which define regressors based on data characteristics and forecast requirements like hourly short-term forecast and daily long-term forecast. Tuning knobs provided by the templates reduce the search to find a satisfactory forecast. And the Greykite library has flexibility to customize a model template for algorithms, letting users label (and specify whether to ignore or adjust) known anomalies. Greykite, which provides outlier detection, can also select the optimal model from multiple candidates using past performance data. Instead of tuning each forecast separately, users can define a set of candidate forecast configurations that capture different types of patterns. Lastly, the library provides a summary that can be used to assess the effect of individual data points. For example, Greykite can check the magnitude of a holiday, see how much a changepoint affected the trend, or show how a certain feature might be beneficial to a model.  With Greykite, a “next 7-day” forecast trained on over 8 years of daily data takes only a few seconds to produce forecasts. LinkedIn says that its whole pipeline, including automatic changepoint detection, cross-validation, backtest, and evaluation, completes in under 45 seconds. “The Greykite library provides a fast, accurate, and highly customizable algorithm — Silverkite — for forecasting. Greykite also provides intuitive tuning options and diagnostics for model interpretation. It is extensible to multiple algorithms, and facilitates benchmarking them through a single interface,” the LinkedIn research team wrote in a blog post. “We have successfully applied Greykite at LinkedIn for multiple business and infrastructure metrics use cases.” The Greykite library is available on GitHub and PyPI, and it joins the many other tools LinkedIn has open-sourced to date. They include Iris, for managing website outages; PalDB, a low-key value store for handling side data; Ambry, an object store for media files; GDMix, a framework for training AI personalization models; LiFT, a toolkit to measure AI model fairness; and Dagli, a machine learning library for Java."
https://venturebeat.com/2021/05/13/framework-opens-preorders-for-1000-repairable-and-upgradeable-laptop/,"Framework opens preorders for $1,000 repairable and upgradeable laptop","Framework has begun taking preorders in the U.S. for its upgradeable, customizable, and repairable laptop. The company designed the laptop in response to big companies like Apple making it expensive to repair or upgrade devices. The company acknowledged that the worldwide supply chain constraints mean the number of Framework laptops available at launch will be limited. Founder Nirav Patel said in an interview with VentureBeat that the time has come for consumer electronics products that are designed to last. He founded the company in 2019 with the aim of empowering consumers to take care of their own products, increasing longevity and reducing electronic waste in the process. Patel showed me in a video demo how he could pull out little modules that changed the functionality of various ports, changing them from standard USB ports to USB-C ports and so on. It was very easy to swap out parts. “We are building repairable, upgradeable, customizable consumer electronics products,” Patel said. “The goal here is to give consumers back the power to be able to use the products for as long as they would like to. If you go back into the history of computing, that’s what the default was. You bought a machine with the expectation that it was not a sealed box, and it was an item that you could modify, update, and upgrade. And the expectation in the PC space, in general, has always been that you have the power and the flexibility to customize the machine to do the things that you’d like it to.” The Framework Laptop is a thin, lightweight, high-performance productivity notebook with a 13.5-inch screen. It can be upgraded, customized, and repaired in ways no other notebook can, Patel said. “Over the past couple of decades, both PCs and other categories of consumer electronics have just continued to trend towards this path of being sealed up and locked down and fixed into place in a way that when a consumer buys that product, they know that they’re really only going to get a few years of use out of it before some part of it wears out, or goes out of date or breaks,” Patel said. “And they’re forced to buy another one and repeat that over and over again. It’s a pretty poor experience for consumers. But it’s also pretty bad for the environment, and that we’re just generating this enormous quantity of e-waste that continues to grow each year. And so Framework was a way to solve this problem in a way that actually makes sense as a business and as a product model.” It’s a big challenge to take on the likes of Apple, and Framework has just 14 employees. But there are forces on its side. The Federal Trade Commission recently sent a report to Congress detailing repair practices in the industry, and it wasn’t happy with the status quo. Along with socketed storage, Wi-Fi, and two slots of memory, the entire mainboard can be swapped to boost performance as Framework launches updated versions with new CPU generations. High-use parts like the battery, screen, keyboard, and color-customizable magnetic-attach bezel are easy to replace, with spares available directly through the web store. In addition to releasing new upgrade modules regularly and ensuring replacement parts are available, the company is opening up the ecosystem to enable a community of partners to build and sell compatible modules through the Framework Marketplace. “What we’ve done is built a product that is as thin and light and powerful and performant as the notebooks that consumers are buying today,” Patel said. “But every part of it is replaceable by the end user, whether that’s to repair something or replace a battery, which will just inherently wear out after years of use, or even upgrade and replace the entire mainboard and move to new CPU generation to get more performance.” Most consumer electronics devices are disposable one-offs by design. But the single best way to reduce the environmental impact of electronics is to make them last longer, Patel said. In addition to enabling longevity, the company is focused on improving sustainability across the life of its products. The Framework Laptop has both 50% post-consumer recycled (PCR) aluminum and 30% PCR plastic. The packaging is 100% PCR material with no single-use plastics, and the company says all of the product shipments are fully carbon offset. The Framework Laptop is available in a range of preconfigured models running Windows 10 Home or Pro. For people who love to tinker, Framework also created the Framework Laptop DIY Edition, a laptop available as a kit of modules you can customize and assemble yourself, with the ability to choose Windows or install your preferred Linux distribution. The device will ship as the company completes its production models this year. Regarding the semiconductor shortage, Patel said, “It’s definitely a big challenge. We have a great supply chain team, we have a great set of partners, and we’ve anticipated this for months. And so we’ve gone and placed our long-term orders and forecasts, and gotten almost everything that we need for the foreseeable future. There is a single chip, actually, that we are constrained on.  And that’s the reason we’re doing preorder batches, to give people visibility.” The Framework Laptop uses the latest Intel 11th Gen Core i5 and i7 processors and it is upgradeable to Wi-Fi 6E, up to 64GB of DDR4 memory, and 4TB or more of Gen4 NVMe storage. The entire mainboard is replaceable too, enabling upgrades to future CPU generations. On top of that, the Framework Laptop is deeply customizable. The expansion card system lets you choose the ports you want and which side you want them on, selecting from four at a time of USB-C, USB-A, HDMI, DisplayPort, MicroSD, ultra-fast 250GB and 1TB storage, and more. Magnetic-attach bezels are color-customizable to match your style, and the keyboard language can be swapped too. Framework made upgrades and repairs straightforward. The only tool you need is the Framework Screwdriver included in the box, Patel said. Every module has a QR code on it that you can scan for step-by-step instructions, support information, and a link to order a replacement from the Framework Marketplace. The aim is to serve consumer better than companies that believe they know what’s better for consumers. “That attitude by existing companies just gives us the opportunity as a new brand to come in and go to those consumers and go to those repair shops and say we hear you,” he said. “We’ve built the product that answers your needs.” Patel believes that the industry should just make its practices on repair and upgrading transparent to consumers, so people can choose what they want.  To make preordering easy, the company will only be taking a fully refundable $100 deposit at the time of order. It will give you a heads-up when it is preparing to ship the system and then collect the balance of the order. It also offers a 30-day return guarantee. The company is opening preorders in the U.S. today, with Canada coming in the next few weeks. It will take orders in additional countries in Asia and Europe before the end of the year, and you can sign up to get notified when it is available in your region. The company was founded in January 2020, after Patel had spent time at places such as Oculus, Apple, and Facebook."
https://venturebeat.com/2021/05/13/google-will-bring-starlink-satellite-connectivity-to-enterprises-in-late-2021/,Google will bring Starlink satellite connectivity to enterprises in late 2021,"SpaceX has unveiled its second major cloud infrastructure deal to bring satellite-powered internet and cloud services to Earth’s farthest reaches. The Elon Musk-led company is partnering with Google Cloud, a deal that will see SpaceX’s Starlink satellites connected to ground stations located at Google datacenter sites. The news follows a similar announcement last October involving Microsoft, which launched a new Azure Space initiative designed to make its cloud service “the platform and ecosystem of choice for the mission needs of the space community,” according to a statement at the time. SpaceX had already launched hundreds of its low earth orbit (LEO) satellites capable of delivering internet to remote locations, and through Microsoft’s new Azure Modular Datacenters (MDCs) the duo are able to bring high-speed broadband to “extreme environments” that had hithero lacked the necessary infrastructure. While the Google partnership follows a similar ethos, the deal appears to go far beyond the Microsoft tie-up by weaving SpaceX technologies more tightly into Google’s core datacenter infrastructure. Moreover, the duo said they plan to start offering the satellite service to Google’s enterprise customers sometime in the second half of 2021, combining their respective competencies to deliver “data, cloud services, and applications to customers at the network edge,” according to a press release. There has been a flurry of public and private space initiatives in recent years, many of which have involved companies launching constellations of micro-satellites into orbit. SpaceX alone has launched more than 1,500 Starlink satellites to date. But harnessing satellite data requires antennas and other infrastructure, such as storage, networks, and servers on the ground, which is effectively what the SpaceX and Google deal is all about. Locating its ground stations next to Google’s datacenters provides easy access to an array of Google Cloud services with minimal latency, including data analytics, AI, and machine learning. Another obvious contender for this deal would have been Amazon Web Services (AWS). However, Amazon is also planning a network of satellites via Project Kuiper, and it already has a ground station service in operation to enable AWS customers to manage data from satellites. Plus, Musk has had a feud with Amazon founder and CEO Jeff Bezos that goes back years, with Bezos launching a space company called Blue Origin in 2000 —  two years before Musk launched SpaceX. Furthermore, SpaceX recently won a $2.9 billion NASA contract to build a spacecraft capable of taking U.S. astronauts to the moon from 2024. That contact was then put on hold, following challenges from Blue Origin over the procurement process. Thus, Google Cloud was always a more likely partnership option for SpaceX than AWS."
https://venturebeat.com/2021/05/13/soniox-taps-unsupervised-learning-to-build-speech-recognition-systems/,Soniox taps unsupervised learning to build speech recognition systems,"AI-powered speech transcription platforms are a dime a dozen in a market estimated to be worth over $1.6 billion. Deepgram and Otter.ai build voice recognition models for cloud-based real-time processing, while Verbit offers tech not unlike that of Oto, which combines intonation with acoustic data to bolster speech understanding. Amazon, Google, Facebook, and Microsoft offer their own speech transcription services. But a new entrant launching out of beta this week claims its approach yields superior accuracy. Called Soniox, the company leverages vast amounts of unlabeled audio and text to teach its algorithms to recognize speech with accents, background noises, and “fairfield” recording. In practice, Soniox says its system correctly transcribes 24% more words compared with other speech-to-text systems, achieving “super-human” recognition on “most domains of human knowledge.” Those are bold claims, but Soniox founder and CEO Klemen Simonic says the accuracy improvements arise from the platform’s unsupervised learning techniques. With unsupervised learning, an algorithm — in Soniox’s case, a speech recognition algorithm — is fed “unknown” data for which no previously defined labels exist. The system must teach itself to classify the data, processing it to learn from its structure. At the advent of the modern AI era, when people realized powerful hardware and datasets could yield strong predictive results, the dominant form of machine learning fell into a category known as supervised learning. Supervised learning is defined by its use of labeled datasets to train algorithms to classify data, predict outcomes, and more. Simonic, a former Facebook researcher and engineer who helped to build the speech team at the social network, notes that supervised learning in text-to-speech is both time-consuming and expensive. Companies have to obtain tens of thousands of hours of audio and recruit human teams to manually transcribe the data. And this same process has to be repeated for each language. “Google and Facebook have more than 50,000 hours of transcribed audio. One has to invest millions — more like tens of millions — of dollars into collecting transcribed data,” Simonic told VentureBeat via email. “Only then one can train a speech recognition AI on the transcribed data.” A technique known as semi-supervised learning offers a potential solution. It can accept partially labeled data, and Google recently used it to obtain state-of-the-art results in speech recognition. In the absence of labels, however, unsupervised learning — also known as self-supervised learning — is the only way to fill gaps in knowledge. According to Simonic, Soniox’s self-supervised learning pipeline sources audio and text from the internet. In the first iteration of training, the company used the Librispeech dataset, which contains 960 hours of transcribed audiobooks. Soniox’s iterative approach continuously refines the platform’s algorithms, enabling them to recognize more words as the system gains access to additional data. Currently, Soniox’s vocabulary spans different people, places, and geography to domains including education, technology, engineering, medicine, health, law, science, art, history, food, sports, and more. “To do fine-tuning of a particular model on a particular dataset, you would need an actual transcribed audio dataset. We do not require transcribed audio data to train our speech AI. We do not do fine-tuning,” Simonic said. Soniox claims to have a proprietary dataset containing over 88,000 hours of audio and 6.6 billion words of preprocessed text. By comparison, the latest speech recognition works from Facebook and Microsoft used between 13,100 and 65,000 hours of labeled and transcribed speech data. And Mozilla’s Common Voice, one of the largest public annotated voice corpora, has 9,000 hours of recordings. While relatively underexplored in the speech domain, a growing body of research demonstrates the potential of learning from unlabeled data. Microsoft is using unsupervised learning to extract knowledge about disruptions to its cloud services. More recently, Facebook announced SEER, an unsupervised model trained on a billion images that ostensibly achieves state-of-the-art results on a range of computer vision benchmarks. Soniox collects more data on a weekly basis, with the goal of expanding the range of vocabulary the platform can transcribe. However, Simonic points out that more audio and text isn’t necessarily required to improve word accuracy. Soniox’s algorithms can “extract” more about familiar words with multiple iterations, essentially learning to recognize particular words better than before. AI has a well-known bias problem, and unsupervised learning doesn’t eliminate the potential for bias in a system’s predictions. For example, unsupervised computer vision systems can pick up racial and gender stereotypes present in training datasets. Simonic says Soniox has taken care to ensure its audio data is “extremely diverse,” with speakers from most countries and accents around the world represented. He admits that the data distribution across accents isn’t balanced but claims the system still manages to perform “extremely well” with different speakers. Soniox also built its own training hardware infrastructure, which it stores across multiple servers located in a collocation datacenter facility. Simonic says the company’s engineering team installed and optimized the system and machine learning frameworks and wrote the inference engine from scratch. “It is utterly important to have under control every single bit of transfer and computation when you are training AI models at large scale. You need a rather large amount of computation to do just one iteration over a dataset of more than 88,000 hours,” Simonic said. “[The inferencing engine] is highly optimized and can potentially run on any hardware. This is super important for production deployment because speech recognition is computationally expensive to run compared to most other AI models and saving every bit of computation on a large volume amounts to large sums in savings — think of millions of hours of audio and video per month.” After launching in beta earlier this year, Soniox is making its platform generally available. New users get five hours per month of free speech recognition, which can be used in Soniox’s web or iOS app to record live audio from a microphone or upload and transcribe files. Soniox offers an unlimited number of free recognition sessions for up to 30 seconds per session, and developers can use the hours to transcribe audio through the Soniox API. It’s early days, but Soniox says it recently signed its first customer in DeepScribe, a transcription startup targeting health care. DeepScribe switched from a Google speech-to-text model because Soniox’s transcriptions of doctor-patient conversations were more accurate, Simonic claims. “To make a business, developing novel technology is not enough. Thus we developed services and products around our new speech recognition technology,” Simonic said. “I expect there will be a lot more customers like DeepScribe once the word about Soniox gets around.”"
https://venturebeat.com/2021/05/13/moogsoft-rolls-out-new-aiops-features/,Moogsoft rolls out new AIOps features,"AIOps startup Moogsoft this week announced the launch of new product capabilities and updates, including extended integration capabilities, incident workflow automations, and automatic event enrichment. Moogsoft says the features are designed to optimize productivity while enabling developers to focus their time on more involved tasks. AI for IT operations (AIOps) is a category of products that enhance IT by leveraging AI to analyze data from tools and devices. Research and Markets anticipates AIOps will become a $14.3 billion market by 2025. That might be a conservative projection, in light of the pandemic, which is increasingly forcing IT teams to conduct their work remotely. In lieu of access to infrastructure, AIOps solutions could help prevent major outages, which a study from Aberdeen Research estimates cost $260,000 per hour. Moogsoft upgraded its Create Your Own Integration service, which lets customers generate custom API endpoints they can send event and metric data to from observability and monitoring tools. Now users have the ability to integrate payloads that include multiple events, ostensibly saving time and resources. Also new in beta is Microsoft Azure App Insights integration, a cloud-to-cloud integration that provides admins the ability to ingest metric data configured in Azure. Moogsoft analyzes the data to establish normal operating behaviors with an upper and lower threshold. If the metric deviates, an anomaly event occurs, which allows users to understand the difference between normal behavior and a potential issue. Auto Classify, another new feature in beta, taps machine learning to analyze data and determine infrastructure elements and types of failure. Meanwhile, the similarly AI-powered Auto Close extends workflow automation to automatically close any alerts or incidents when they meet the criteria users set. For example, when a metric falls back into normal behavior, the alert severity moves to “clear” and the status to “resolved.” Dovetailing with Auto Classify and Auto Close is automated incident context with tags, which aggregates and deduplicates tags from alerts, including custom tags. New personalization capabilities are also in tow and allow users to change the order of columns, which columns are displayed, and the refresh rate of their UI. “In a period of immense growth, our team is working harder than ever to allow development, ITOps, and SRE teams to work more efficiently,” Moogsoft VP Adam Frank told VentureBeat. “Simplicity within the user experience continues to be at the forefront of everything we release — from new and improved navigation to the ease-of-use of our industry-leading open and transparent integrations — making it even easier for our customers to take advantage of observability with AIOps and allowing them to develop more and operate less.”"
https://venturebeat.com/2021/05/13/how-3d-location-technology-is-completing-the-picture-in-location-based-apps-vb-live/,How 3D location technology is completing the picture in location-based apps (VB Live),"Presented by NextNav For the first time, geolocation technology enables accurate vertical location at scale – so accurately that it can pinpoint what floor you’re on in a skyscraper. Join VB’s Dean Takahashi and others to learn about the opportunities 3D geolocation is unlocking, from mobile apps to gaming and more in this VB Live event. Register here for free. “2D geolocation has been critical for generally locating people and things in spaces, but it has key limitations,” says Chris Gates, SVP of corporate development and strategy at NextNav. “As you go up levels in structures and begin to work in more complicated environments with vertical separation — even a bridge — significant location ambiguity emerges.  2D location is a starting point, but it’s incomplete and we no longer have to live with the limitations imposed by ‘flat’ technologies.” About 84% of the U.S. population lives in an urbanized area, according to the U.S. census, Gates says. In those urbanized areas where altitude dimension has been lacking, people have had incomplete location experiences. Despite the vast value that 2D has brought, it’s only the tip of the iceberg. 3D gives you the full location of the user, and that information has critical use cases, beyond just mapping applications. It’s providing the complete physical context for a user, as opposed to the first approximation, which is a game changer in the public safety sector, for instance. When a user calls 911 from a 30-story hotel in an urban environment, how do you locate that person quickly, without 3D geolocation? In higher-density aviation, keeping vertical separation among drones is important. It requires reliable altitude service, so the drones stay at their assigned altitude. For urban air mobility, 3D provides the complete operational picture that’s necessary for safe air traffic control, as well as safe navigation. In hospitality, 3D location information can provide increased safety for the room cleaners who are alone in relatively large structures. It can also offer a full picture of a worker’s environment in multilevel industrial facilities, which is especially important for jobs that include safety concerns and employees need to call for help, or be alerted before they wander into a less safe location. If you want to track employees seamlessly, and at lowest cost, you could add a vertical location capability to your productivity. Response for FirstNet has NextNav technology embedded for altitude determination. When a first responder exits their vehicle, they can communicate their location to their partners and other team members, including their vertical location. First responders often work in small teams, and knowing where the members of your team are in a situation that’s often high-stress is of great value. For delivery and pickup services, from food and retail to traditional package delivery, vertical altitude provides a complete verification, closing the loop on interaction with those services. There’s tremendous value for retail, also, Gates adds, where consumers opt in to service applications in malls, for example. Users expect ease of experience with these applications, and that includes being directed accurately on maps. Anonymized tracking and analysis for consumers is relevant in a high-pressure retail environment as well, where a business is looking to maximize consumer experience. “It completes the picture for a host of applications that have started to emerge that aren’t completely usable, precisely because you can’t determine what level someone is on in a structure,” he says. 3D in location-based games offers huge benefits, Gates adds. It can impact both content and gameplay mechanics, adding an exciting potential domain for new experiences. Any location-based game or AR experience with game play, or content tailored to a specific geographic location, can now take altitude into account. “For example,” says Gates, “collecting extra points if you go up to a higher floor, or showing a pterodactyl instead of a tyrannosaurus outside on a specific floor to be captured. For developers, the technical effort to integrate is very low, but from a game experience point of view, it opens up a new dimension in development opportunities for geo and AR content.” Indeed, one third of apps in the Google Play store use location in one way or another, he adds. These apps, both entertainment and productivity, are dependent on 2D location, and 3D has a pivotal role in their future. Technology like NextNav’s offers developers access to very accurate 3D location via an SDK that’s lightweight and easy to implement. They offer plugins for game engines, and will be making an API available later this year for people that aren’t operating in the Android or iOS ecosystems, moving into IOT and some of those more enterprise-oriented domains. “As you go to 3D, user experience and user interface is important,” Gates says. “Anything that uses GPS for 2D location today could benefit from 3D location. We have some guides out there that we hope will spur a lot of creativity in the community — starting points to help spur people’s imagination.” Don’t miss out! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/05/13/chaossearch-brings-sql-support-to-log-analytics/,ChaosSearch brings SQL support to log analytics,"ChaosSearch today announced it has added support for SQL to a log analytics platform that employs an index engine to make data stored in an object-based cloud storage service available to tools without having to convert it into another format. That SQL capability complements existing support for an application programming interface (API) that is compatible with the open source ElasticSearch engine. The goal is to make any type of log data available to any type of analytics tool, including analytics applications that employ machine learning algorithms via the ChaosSearch Data Platform, CEO Ed Walsh told VentureBeat. Rather than requiring organizations to acquire and move all their log data into a cloud data warehouse, the ChaosSearch Data Platform indexes log data stored in Amazon Web Services (AWS) S3 and Google Cloud Platform (GCP) without asking an IT organization to normalize it. This capability makes it possible for organizations to analyze the data using applications from, for example, Tableau, Grafana, or Looker, in addition to employing an ElasticSearch engine. Delivered as a managed service, ChaosSearch Data Platform doesn’t require an internal IT department to set it up or maintain it. This approach also eliminates the need to hire a data engineer to prep a cloud data warehouse for a new use case or worry about how long data might need to be available via that cloud data warehouse, Walsh said. “All the end users need is to have access,” he added. With the rise of new cloud-native platforms, the amount of log data organizations might need to analyze has increased by several orders of magnitude. The bulk of those platforms are running in cloud computing environments that make it simple to store log data in, for example, an S3 bucket. Much of that log data is being generated by applications driving digital business transformation initiatives that are mission-critical. Insights into everything from application performance to how those applications are being employed can be surfaced by analyzing log data. Much of that data isn’t analyzed today, either because it’s too difficult to access or too costly to move into a data warehouse, Walsh said. The ChaosSearch Data Platform makes it economical to store and analyze months’ or years’ worth of data, Walsh noted. The ability to access that data via artificial intelligence (AI) tools such as TensorFlow will only make that log data more valuable, he added. It’s not likely organizations are going to abandon cloud data warehouses, but they may be more selective about the type of data they opt to store in them, rather than relying on object-storage systems that can be accessed directly using a simple SQL query. Those object storage systems can be accessed on multiple clouds without requiring a data engineer to move data from GCP to AWS so it can be stored in a data warehouse. The object storage systems are for all intents and purposes a virtual data lake. Less clear is the degree to which organizations may be able to employ additional platforms to directly access data stored in an object-storage system. However, it’s becoming clear that there are multiple ways to centralize data management. The critical thing for most organizations is to start moving toward a solution as the volume of data that needs to be stored and analyzed continues growing. Much of that data, including logs, may not need to be stored for more than a few days. The signal-to-noise ratio when it comes to the amount of data being generated is high, but the insights that data enables can be invaluable."
https://venturebeat.com/2021/05/13/deepscribe-raises-5-3m-to-transcribe-medical-notes-with-ai/,DeepScribe raises $5.2M to transcribe medical notes with AI,"DeepScribe, an AI-driven platform for medical record-taking, today announced it has raised $5.2 million in a seed round led by Bee Partners. The company says the funding will be put toward product research and development, as well as hiring efforts. Physicians spend on average 16 minutes updating electronic health records (EHRs) for each patient visit, a 2020 study in the Annals of Internal Medicine found. Typically, chart review (33%), documentation (24%), and ordering (17%) account for most note-taking activity. But the compressed time frame tends to promote errors. According to research published in the Journal of the American Medical Association, just 18% of a typical medical note is manually entered by the doctor, leaving 46% copied and pasted from other parts of the medical record and 36% imported by the electronic medical software. DeepScribe, launched in 2019 by Akilesh Bapu, Matthew Ko, and Kairui Zeng and based on technology developed at the University of California, Berkeley, is designed to transcribe patient notes even when dealing with accents, interruptions, multiple speakers, and more. The platform combines AI with rules-based natural language processing, as well as the output of three speech recognition engines — including its own — each with distinct advantages. For example, DeepScribe leverages IBM’s speech recognition technology to recognize medications and conditions and Google’s tech for conversational speech. “Though difficult to pull off, this approach makes DeepScribe the only AI widely accepted by medical professionals that can draft medical notes from the natural patient conversation,” Bapu told VentureBeat via email. “On average, physicians only have to make one correction or less after 20 days of using the product. The rules-based natural language processing portion of DeepScribe’s approach also allows providers to custom-tailor their notes without having to retrain deep learning models, which can be unpredictable at times.” DeepScribe’s platform captures both sides of doctor-patient interactions and distills them to their essence without the need for the doctor’s intervention. Even in the absence of wake words and instructions, DeepScribe can listen to an appointment in the background via an iOS or web-based app, take notes, and automatically summarize the discussion into an entry in a doctor’s EHR system. The transcribed summaries are customizable, allowing doctors to create tailored notes including long sentences or short, a conversational or official tone, and a summary of everything discussed or just what’s medically relevant. DeepScribe interfaces with major EHR platforms, including AdvancedMD, AthenaHealth, Claimpower, Elation, DrChrono, and PracticeFusion. The startup also builds custom integrations that can transfer notes into discrete fields within EHR systems. “Many doctors are turning to scribes — human and mechanical — to reduce the strain of EHR paperwork. The problem is, human scribes are too expensive for many practices, and AI scribes aren’t accurate enough to rely on. DeepScribe is the best of both worlds: quality equal to or better than a human scribe at a cost any physician can afford,” Bapu continued. “Unlike other AI scribes, which need wake words or explicit direction, DeepScribe is an ‘ambient AI’ that [simply] listens in.” The closure of DeepScribe’s financing comes after Microsoft acquired Nuance Communications, an AI and speech technology company with a large health provider client base, for $19.7 billion. The global medical transcription market is anticipated to reach $4.89 billion by 2027, up from $1.32 billion in 2019, according to Fortune Business Insights. Beyond DeepScribe, Nvidia, Google, and Amazon are among those who’ve built health note transcription solutions. The increasing investment in the segment is in part motivated by research showing that EHR responsibilities can have a negative impact on patient care. A Stanford Medicine-commissioned survey found that 7 out of 10 physicians believe EHRs “greatly” contribute to physician burnout. The same study suggests 6 out of 10 doctors think EHRs need a “complete” overall. DeepScribe claims to have over 300 customers who collectively see 10,000 patients a week across 18 different specialties. “The pandemic helped accelerate our growth as the dire need of frontline providers put a magnifying glass on the inefficiencies leading providers to see fewer patients,” Bapu said. “It also became unsanitary and a friction point to have in-person scribes, given the spread of COVID, along with the increased frequency of virtual visits. These two factors increased the demand for our product and exploded our growth.” Industry Ventures and Stage II Capital participated in the seed round, along with existing investors Tsingyuan Ventures, 1984 Ventures, Wavemaker 360, Supernode Ventures, Skydeck, Plug and Play, and Sequoia Scout Fund. DeepScribe has 26 employees."
https://venturebeat.com/2021/05/13/optimal-dynamics-nabs-22m-for-ai-powered-freight-logistics/,Optimal Dynamics nabs $22M for AI-powered freight logistics,"Optimal Dynamics, a New York-based startup applying AI to shipping logistics, today announced it has closed an $18.4 million round led by Bessemer Venture Partners. Optimal Dynamics says the funds will be used to more than triple its 25-person team and support engineering efforts, as well as bolstering sales and marketing departments. Last-mile delivery logistics tend to be the most expensive and time-consuming part of the shipping process. According to one estimate, last-mile costs account for 53% of total shipping costs and 41% of total supply chain costs. With the rise of ecommerce in the U.S., retail providers are increasingly focusing on fulfilment and distribution at the lowest cost. Particularly in the construction industry, the pandemic continues to disrupt wholesalers — a 2020 Statista survey found that 73% of buyers and users of freight transportation and logistics services experienced an impact on their operations. Founded in 2016, Optimal Dynamics offers a platform that taps AI to generate shipment plans likely to be profitable — and on time. The fruit of nearly 40 years of R&D at Princeton University, the company’s product generates simulations for freight transportation, enabling logistics companies to answer questions about what equipment they should buy, how many drivers they need, daily dispatching, load acceptance, and more. Roughly 80% of all cargo in the U.S. is transported by the 7.1 million people who drive flatbed trailers, dry vans, and other heavy lifters for the country’s 1.3 million trucking companies. The trucking industry generates $726 billion in revenue annually and is forecast to grow 75% by 2026. Even before the pandemic, last-mile delivery was fast becoming the most profitable part of the supply chain, with research firm Capgemini pegging its share of the pie at 41%. Optimal Dynamics’ platform can perform strategic, tactical, and real-time freight planning, forecasting shipment events as far as two weeks in advance. CEO Daniel Powell — who cofounded the company with his father, Warren Powell, a professor of operations research and financial engineering — says the underlying technology was deployed, tested, and iterated with trucking companies, railroads, and energy companies, along with projects in health, ecommerce, finance, and materials science. “Use of something called ‘high-dimensional AI’ allows us to take in exponentially greater detail while planning under uncertainty. We also leverage clever methods that allow us to deploy robust AI systems even when we have very little training data, a common issue in the logistics industry,” Daniel Powell told VentureBeat via email. “The results are … a dramatic increase in companies’ abilities to plan into the future.” The global logistics market was worth $10.32 billion in 2017 and is estimated to grow to $12.68 billion by 2023, according to Research and Markets. Optimal Dynamics competes with Uber, which offers a logistics service called Uber Freight. San Francisco-based startup KeepTruckin recently secured $149 million to further develop its shipment marketplace, while Next Trucking closed a $97 million investment. And Convoy raised $400 million at a $2.75 billion valuation to make freight trucking more efficient. But Optimal Dynamics investor Mike Droesch, a partner at BVP, says demand for the company’s products remains strong. “Logistics operators need to consider a staggering number of variables, making this an ideal application for a software-as-a-service product that can help operators make more informed decisions by leveraging Optimal Dynamics’ industry-leading technology. We were really impressed with the combination of their deep technology and the commercial impact that Optimal Dynamics is already delivering to their customers,” he said in a statement. Including this latest series A, Optimal Dynamics has raised over $22 million. Fusion Fund, the Westly Group, TenOneTen Ventures, Embark Ventures, FitzGate Ventures, and John Larkin, and John Hess also contributed to the round."
https://venturebeat.com/2021/05/13/code-scanning-platform-blubracket-nabs-12m-for-enterprise-security/,Code-scanning platform BluBracket nabs $12M for enterprise security,"Code security startup BluBracket today announced it has raised $12 million in a series A round led by Evolution Equity Partners. The capital will be used to further develop BluBracket’s products and grow its sales team. Detecting exploits in source code can be a pain point for enterprises, especially with the onset of containerization, infrastructure as code, and microservices. According to a recent Flexera report, the number of vulnerabilities remotely exploitable in apps reached more than 13,300 from 249 vendors in 2020. In 2019, Barracuda Networks found that 13% of security pros hadn’t patched their web apps over the past 12 months. And in a 2020 survey from Edgescan, organizations said it took them an average of just over 50 days to address critical vulnerabilities in internet-facing apps. BluBracket, which was founded in 2019 and is headquartered in Palo Alto, California, scans codebases for secrets and blocks future commits from introducing new risks. The platform can monitor real-time risk scores across codebases, Git configurations, infrastructure as code, code copies, and code access and resolve issues, detecting passwords and over 50 different types of tokens, keys, and IDs. Coralogix estimates that developers create 70 bugs per 1,000 lines of code and that fixing a bug takes 30 times longer than writing a line of code. In the U.S., $113 billion is spent annually on identifying and fixing product defects. BluBracket attempts to prevent this by proactively monitoring public repositories with the highest risk factors, generating reports for dev teams. It prioritizes commits based on their risk scores, minimizing duplicates using a tracking hash for every secret. A rules engine reduces false positives and scans for regular expressions, as well as sensitive words. And BluBracket sanitizes commit history both locally and remotely, supporting the exporting of reports via download or email. BluBracket offers a free product in its Community Edition. Both it and the company’s paid products, Teams and Enterprise, work with GitHub, BitBucket, and Gitlab and offer CI/CD integration with Jenkins, GitHub Actions, and Azure Pipelines. “Since our introduction early last year, the industry has seen through Solar Winds how big of an attack surface code is. Hackers are exploiting credentials and secrets in code, and valuable code is available in the public domain for virtually every company we engage with,” CEO Prakash Linga, who cofounded BluBracket with Ajay Arora, told VentureBeat via email. BluBracket competes on some fronts with Sourcegraph, a “universal code search” platform that enables developer teams to manage and glean insights from their codebase. It has another rival in Amazon’s CodeGuru, an AI-powered developer tool that provides recommendations for improving code quality. There’s also cloud monitoring platform Datadog, codebase coverage tester Codecov, and feature-piloting solution LaunchDarkly, to name a few. But BluBracket, which has about 30 employees, says demand for its code security solutions has increased “dramatically” since 2020. Its security products are being used in “dozens” of companies with “thousands” of users, according to Linga. “DevSecOps and AppSec teams are scrambling, as we all know, to address this growing threat. By enabling their developers to keep these secrets out of code in the first place, our solutions make everyone’s life easier,” Linga continued. “We are excited to work with Evolution on this next stage of our company’s growth.” Unusual Ventures, Point72 Ventures, SignalFire, and Firebolt Ventures also participated in BluBracket’s latest funding round. The startup had previously raised $6.5 million in a seed round led by Unusual Ventures."
https://venturebeat.com/2021/05/13/data-governance-and-security-startup-cyral-raises-26m/,Data governance and security startup Cyral raises $26M,"Data security and governance startup Cyral today announced it has raised $26 million, bringing its total to date to $41.1 million. The company plans to put the funds toward expanding its platform and global workforce. Managing and securing data remains a challenge for enterprises. Just 29% of IT executives give their employees an “A” grade for following procedures to keep files and documents secure, according to Egnyte’s most recent survey. A separate report from KPMG found only 35% of C-suite leaders highly trust their organization’s use of data and analytics, with 92% saying they were concerned about the reputational risk of machine-assisted decisions. Redwood City, California-based Cyral, which was founded in 2018 by Manav Mital and Srini Vadlamani, uses stateless interception technology to deliver enterprise data governance across platforms, including Amazon S3, Snowflake, Kafka, MongoDB, and Oracle. Cyral monitors activity across popular databases, pipelines, and data warehouses — whether on-premises, hosted, or software-as-service-based. And it traces data flows and requests, sending output logs, traces, and metrics to third-party infrastructure and management dashboards. Cyral can prevent unauthorized access from users, apps, and tools and provide dynamic attribute-based access control, as well as ephemeral access with “just-enough” privileges. The platform supports both alerting and blocking of disallowed accesses and continuously monitors privileges across clouds, tracking and enforcing just-in-time and just-enough privileges for all users and apps. Beyond this, Cyral can identify users behind shared roles and service accounts to tag all activity with the actual user identity, enabling policies to be specified against them. And it can perform baselining and anomaly detection, analyzing aggregated activity across data endpoints and generating policies for normal activity, which can be set to alert or block anomalous access. “Cyral is built on a high-performance stateless interception technology that monitors all data endpoint activity in real time and enables unified visibility, identity federation, and granular access controls. [The platform] automates workflows and enables collaboration between DevOps and Security teams to automate assurance and prevent data leakage,” the spokesperson said.  Existing investors, including Redpoint, Costanoa Ventures, A.Capital, and strategic investor Silicon Valley CISO Investments, participated in Cyral’s latest funding round. Since launching in Q2 2020, Cyral — which has 40 employees and occupies a market estimated to be worth $5.7 billion by 2025, according to Markets and Markets — says it has nearly doubled the size of its team and close to quadrupled its valuation. “This is an emerging market with no entrenched solutions … We’re now working with customers across a variety of industries — finance, health care, insurance, supply chain, technology, and more. They include some of the world’s largest organizations with complex environments and some of the fastest-growing tech companies,” the spokesperson said. “With Cyral, our company was built during the pandemic. We have grown the majority of our company during this time, and it has allowed us to start our company with a remote-first business model.”"
https://venturebeat.com/2021/05/13/starburst-galaxy-brings-fully-managed-trino-deployment-to-aws/,Starburst Galaxy brings fully managed Trino deployment to AWS,"Starburst, a company that targets enterprises with a commercial version of the open source Presto-based SQL query engine Trino, has launched a new fully managed cloud product for Amazon Web Services (AWS). Trino can be traced back to Facebook in 2012, when a trio of engineers developed Presto to help the company’s data scientists and analysts run faster queries on huge swathes of data. The social networking giant open-sourced Presto the following year, and in 2019 the original Presto developers left Facebook and launched a fork called PrestoSQL, which was rebranded as Trino a few months back. Amid all this hullabaloo, the PrestoSQL/Trino creators also launched a new commercial entity called Starburst Data (now just called “Starburst”) for enterprises. Starburst Enterprise has so far been a self-managed tool that can be hosted on-premises or on public clouds, but a few months back Starburst announced a new software-as-a-service (SaaS) offering called Starburst Galaxy, which this week launched in general availability after three months in public beta. While Starburst Enterprise is a fully supported product that can be deployed across clouds, the key difference is it’s managed and maintained by the customer. As a fully managed SaaS offering, Starburst Galaxy sports an integrated SQL editor for querying data out of the box and is designed to be set up in just a few clicks. It requires minimal technical support to connect new data sources or perform any kind of configuration or upgrades. Ultimately, it’s all about reducing the infrastructure management resources required for data teams. The launch comes shortly after Starburst raised $100 million in a round of funding from notable investors, including Andreessen Horowitz, Salesforce Ventures, Index Partners, and Coatue. Starburst Galaxy is currently available exclusively for AWS, meaning users can query data from Amazon S3 and AWS Glue Data Catalog. But as with the original Starburst Enterprise platform, it will eventually be made available for Microsoft Azure and Google Cloud Platform."
https://venturebeat.com/2021/05/13/major-hacks-drive-push-for-new-u-s-government-software-standards/,Major hacks drive push for new U.S. government software standards,"(Reuters) — President Joe Biden on Wednesday ordered the creation of an air accident-style cyber review board and the imposition of new software standards for government agencies following a spate of digital intrusions that have rattled the United States. The executive order’s initiatives include the creation of a organization that would investigate major hacks along the lines of National Transportation Safety Board inquiries that are launched after plane crashes. They also include the imposition of new security standards for software bought by government agencies — a requirement first reported by Reuters in March. The order follows a digital extortion attempt against major fuel transport company Colonial Pipeline that triggered panic buying and fuel shortages in the southeastern United States. Some recommendations were clearly aimed at avoiding a repeat of the hack of Texas software company SolarWinds, whose software was hijacked to break into government agencies and steal thousands of officials’ emails. The software rules — which are due to be drawn up by the U.S. National Institute of Standards and Technology — were among the most important parts of the order, said Kiersten Todt, the managing director of the Cyber Readiness Institute, which is geared toward helping protect small- and medium-sized businesses. “It’s using the government’s buying power to improve the security of software,” Todt said, saying that if drafted correctly, the rules “will be a game changer in security.” Other rules imposed by the order mandate the use of multi-factor authentication — effectively a second failsafe password — and the use of encryption both for stored data and communications. The order follows an series of dramatic or damaging hacks against American interests. Beyond the digital ransom demand imposed on Colonial — which sources told Reuters the company did not intend to pay — and the SolarWinds-linked compromises, foreign hackers have also used vulnerabilities in software made by Microsoft and Ivanti to extract data from U.S. government targets. Senator Mark Warner, a Democrat who chairs the Senate Intelligence Committee, said the executive order is a good first step but the United States “is simply not prepared to fend off state-sponsored or criminal hackers intent on compromising our systems for profit or espionage.” “Congress is going to have to step up and do more to address our cyber vulnerabilities,” he said."
https://venturebeat.com/2021/05/13/ai-powered-identity-access-management-platform-authomize-raises-22m/,AI-powered identity access management platform Authomize raises $16M,"Cloud-based authorization startup Authomize today announced that it raised $16 million in series A funding led by Innovation Endeavors, bringing the startup’s total raised to $22 million to date. CEO and cofounder Dotan Bar Noy says that the capital will be used to support Authomize’s R&D and hiring efforts this year, as expansion ramps up. One study found that companies consider implementing adequate identity governance and administration (IGA) practices to be among the least urgent tasks when it comes to securing the cloud. That’s despite the fact that, according to LastPass, 82% of IT professionals at small and mid-size businesses say identity challenges and poor practices pose risks to their employers. Authomize, which emerged from stealth in June 2020, aims to address IGA challenges by delivering a complete view of apps across cloud environments. The company’s platform is designed to reduce the burden on IT teams by providing prescriptive, corrective suggestions and securing identities, revealing the right level of permissions and managing risk to ensure compliance. “As security has evolved from endpoints and networks, attention has increasingly moved to identity and access management, and specifically the authorization space. Many of the CISOs and CIOs we spoke with expressed the need for a system that would secure and manage permissions from a single platform. They took access decisions based on hunches, not data, and when they tried to take data-driven decisions, they found out that the data was outdated. Additionally, most, if not all, of the process has been manually managed, making the IT and security teams the bottleneck for growth,” Noy told VentureBeat in an interview via email. Authomize’s secret sauce is a technology called Smart Groups that aggregates data from enterprise systems in real time and infers the right-sized permissions. Using this data in tandem with graph neural networks, unsupervised learning methods, evolutionary systems, and quantum-inspired algorithms, the platform offers action and process automation recommendations. Using AI, Authomize detects relationships between identities and company assets throughout an organization’s clouds. The platform offers an inventory of access policies, blocking unintended access with guardrails and alerting on anomalies and risks. In practice, Authomize constructs a set of policies for each identity-asset relationship. It performs continuous access modeling, self-correcting as it incorporates new inputs like actual usage, activities, and decisions. Of course, Authomize isn’t the only company in the market claiming to automate away IGA. ForgeRock, for instance, recently raised $93.5 million to further develop its products that tap AI and machine learning to streamline activities like approving access requests, performing certifications, and predicting what access should be provisioned to users. But Authomize has the backing of notable investor M12 (Microsoft’s venture fund), Entrée Capital, and Blumberg Capital, along with acting and former CIOs, CISOs, and advisers from Okta, Splunk, ServiceNow, Fidelity, and Rubrik. Several undisclosed partners use the company’s product in production, Authomize claims — including an organization with 5,000 employees that tapped Smart Groups to cut its roughly 50,000 Microsoft Office 365 entitlements by 95%. And annual recurring revenue growth is expected to hit 600% during 2021. Authomize recently launched an integration with the Microsoft Graph API to provide explainable, prescriptive recommendations for Microsoft services permissions. Via the API, Authomize can evaluate customers’ organization structure and authorization details, including role assignments, group security settings, SharePoint sites, OneDrive files access details, calendar sharing information, applications, and service principal access scopes and settings. “Our technology is allowing teams to make authorization decisions based on accurate and updated data, and we also automate day-to-day processes to reduce IT burden … Authomize currently secures more than 7 million identities and hundreds of millions of assets, and our solution is deployed across dozens of customers,” Noy said. “Using our proprietary [platform], organizations can now strike a balance between security and IT, ensuring human and machine identity have only the permission they need. Our technology is built to connect easily to the entire organization stack and help solve the increasing complexity security, and IT teams face while reducing the overall operational burden.” Authomize, which is based in Tel Aviv, Israel, has 22 full-time employees. It expects to have more than 55 by the end of the year as it expands its R&D teams to develop new entitlement eligibility engine and automation capabilities and increases its sales and marketing operations in North America."
https://venturebeat.com/2021/05/12/how-bias-helped-office-depot-save-big-bucks-on-cloud-adoption/,How Office Depot slashed operational costs with a “cloud first” strategy,"Enterprises want to move their IT operations to the cloud more urgently than ever before. How they get there is the trillion-dollar question. With Gartner predicting that public cloud spending will reach $332 billion this year and IDC projecting a $1 trillion market for all cloud-related services by 2024, the race is on to provide attractive cloud-migration solutions in a competitive marketplace. What does a large-scale cloud migration look like for a Fortune 500 company and the IT services provider assisting with the transformation? Bias, an award-winning managed service provider (MSP) based in Roswell, Georgia, recently shared details with VentureBeat about its implementation of a cloud solution for Office Depot. The upshot: Bias helped Office Depot conduct a cross-platform migration to Oracle Cloud Infrastructure (OCI) that shifted all of Office Depot’s applications to OCI in an effort to reduce capital expenditures and improve overall system stability and performance. VentureBeat spoke with Bias cofounder and executive vice president John Ezzell about the Office Depot project. This interview has been edited for brevity and clarity. VentureBeat: What were the specific goals with this solution, in terms of a target for reducing operational costs? Have you hit that target? John Ezzell: Office Depot wanted to modernize and simplify their IT infrastructure, reduce operational costs, and enhance the performance of their Oracle E-Business Suite (EBS) Financial, Supply Chain, and Human Capital Management systems. Bias proposed a modernization solution that aligned with Office Depot’s “cloud first” strategy to move all their applications to Oracle Cloud Infrastructure. We were able to achieve all of our goals, both in terms of reducing operational costs and improving IT system performance throughout the organization. The efficiencies we introduced not only lowered costs for Office Depot, but they also dramatically improved the user experience for business users and the IT staff supporting them. Here are some of the highlights of the implementation: VentureBeat: As a solution provider, you promise customers that you’re able to do your work with “minimal downtime” while migrating to OCI. What was that network downtime for Office Depot while the solution was being implemented? Ezzell: Bias was able to migrate 27+ applications to Oracle OCI with under 48 hours of total downtime. This was a complex cross-platform migration from Solaris to Linux that was actually accomplished with record downtime. VentureBeat: In your opinion, why did Office Depot select Bias over other solutions? Ezzell: Bias provided a solution that was better than the alternatives in the marketplace and also aligned with Office Depot’s “cloud-first” strategy. Here’s how: The Bias Digital Platform (BDP) lets us develop solutions in a collaborative automation environment. And for Office Depot, we were able to implement extensive automation utilizing Terraform, Jenkins, and Ansible products. Bias was able to make the process of deploying infrastructure and configuring the applications repeatable. This resulted in an average fivefold decrease in overall time to configure the various environments while also reducing errors associated with manual provisioning. VentureBeat: What capex (capital expenditure) improvements have you seen (or do you expect to see) for Office Depot as a result of the migration to OCT? Ezzell: Office Depot anticipates realizing a $5 million return on investment (ROI) on the implementation over the next five years, with most of the savings coming from the elimination of the need to purchase replacement hardware and the canceling of Oracle licenses."
https://venturebeat.com/2021/05/12/apply-to-join-transforms-annual-tech-showcase/,Apply to join Transform’s annual Tech Showcase,"VentureBeat’s annual Tech Showcase returns at Transform 2021: Accelerating Enterprise Transformation with AI and Data, hosted July 12-16. VentureBeat will be selecting 5 companies to present the latest and greatest AI and data products on the main stage during the first day of Transform 2021. We are looking for companies that have built the coolest products and solutions leveraging bleeding edge technologies to help businesses achieve real and tangible results using AI and data. Whether you are a stealth startup or Fortune 500, or anywhere in between, we welcome your submission. If you have a story to tell, and an AI or Data product/solution with tangible business results and demonstrative use cases, please submit your application here before 5pm PST on Tuesday, June 1. Selected companies will present on Transform’s main stage in front of hundreds of industry decision makers and will be featured in our on-demand video-hub following the event. Attendees from across the globe will join online to hear from top industry experts on strategy and technology in the main application areas of AI/ML automation technology, data, analytics, intelligent automation, conversational AI, intelligent AI assistants, AI at the edge, IoT, & computer vision. Executives across industries are invited to join Transform 2021, so register today to join VentureBeat for 5-days of AI and data. QUESTIONS Contact events@venturebeat.com with any questions regarding your application."
https://venturebeat.com/2021/05/12/former-blizzard-and-epic-veterans-raise-5m-for-lightforge-games/,Former Blizzard and Epic veterans raise $5M for Lightforge Games,"When game companies become successful, they tend to breed offspring. That’s the case with veterans of Blizzard Entertainment and Epic Games, who have raised $5 million to open a new studio called Lightforge Games. The new studio is based near Epic Games in Raleigh, North Carolina. It wants to redefine how developers make role-playing games. The team is working on a cross-platform social video game where players have the power to create worlds and tell stories with freedom. CEO Matt Schembari said in an interview with GamesBeat that the company is hiring people for remote jobs. “Our blended DNA from both Blizzard and Epic extends to the entire studio at this point,” Schembari said. “About a year ago, a bunch of us got together and have been operating quietly, building up the company and our early game prototypes. We’re testing and validating crazy game ideas that we have been coming up with.” While he isn’t talking about the game yet, Schembari said the it will be highly social and creative, and it will run across multiple platforms. “We love experiences where players can come together and build worlds together, create stories together, tell stories together, where they’re able to have this kind of emergent gameplay. Telling stories together is really the part that we’re most focused on,” he said. “We really believe that there’s no barrier between creation and play. It’s not user-generated content in the classic sense of you create something and then you publish it and people can download it. It’s a different kind of model than just UGC.” The funding came from Galaxy Interactive, NetEase Games, Dreamhaven, Maveron, 1UP Ventures, and angel investors from the gaming and tech industries. One of the surprises is that Dreamhaven is another game startup itself, started by former Blizzard president Mike and Amy Morhaime. In a statement, Mike Morhaime said that Lightforge is creating a game in a space with a lot of potential and he is excited about the team’s vision. Schembari has 20 years of experience and he shipped games played by millions as former lead engineer at Blizzard and director of user interface at Epic Games, where he led the Fortnite platform team. Other founders include Dan Hertzka, Nathan Fairbanks, Glenn Rane, and Marc Hutcheson. Hertzka is engineering director and he led a team at Fortnite that added the client social layer to the battle royale game. Fairbanks has been games for 13 years and has worked on titles such as Fortnite, Star Wars: The Old Republic, and The Elder Scrolls Online. He is serving as studio director. Rane is art director and he has worked on World of Warcraft, Hearthstone, and Diablo Immortal. Hutcheson is product director and he has 18 years of experience in marketing and publishing games such as World of Warcraft, StarCraft II, Overwatch, Diablo III, Fortnite, and Hearthstone. Lightforge has a total of 11 people and is on the verge of hiring three more. The team brings decades of experience from Epic, Blizzard, Riot, Bioware, and Zenimax Online and have shipped top games such as Fortnite, World of Warcraft, Diablo 3, Star Wars: The Old Republic, Hearthstone, the StarCraft 2 trilogy, Overwatch, Elder Scrolls Online, and more. Lightforge is an all-remote studio where employees can work and live nearly anywhere. Schembari said that his startup received multiple offers and went with Galaxy Interactive as the lead investor because of its understanding about games and online communities. “We are all remote and have been since the very beginning and this is something that was really important to us,” Schembari said. “One of our values is to really think about embracing empathy with everything we do. And, in particular, in the case of being all remote. We’ve all lived the experience that one of the most disruptive things you can do to someone’s life is to ask them to relocate for a job. And that was something that we really just strongly didn’t want to do. We are now at a point both technologically and culturally where you can totally work remotely.”"
https://venturebeat.com/2021/05/12/amazons-saas-boost-tool-addresses-dev-challenges/,Amazon’s SaaS Boost tool addresses dev challenges,"Amazon today open-sourced Amazon Web Services (AWS) SaaS Boost, an open source tool that helps software developers migrate their existing solutions to software-as-a-service (SaaS) delivery models. Amazon says that SaaS Boost — which launched in preview at AWS Re:Invent 2020 — has the potential to offload development efforts by supporting app transformations to SaaS, freeing up developers to focus on other aspects. SaaS apps are constantly evolving. Many of them use industry-standard protocols and interface with other products, but they all need certain foundational capabilities to onboard users, provision infrastructure, and surface key metrics. These functions are critical for enabling SaaS providers to scale. However, if every company invested in building these capabilities, it’d take resources — slowing down the time to market. To address this challenge, AWS SaaS Boost provides functionality including tenant isolation, data partitioning, monitoring, metering, and billing. According to Amazon, the focus is on creating an environment that brings together all the elements of a ready-to-use SaaS architecture, removing much of the heavy lifting commonly associated with migrating a solution to a SaaS model. Unifying data across disparate sources is one key feature in AWS SaaS Boost. Between 60% and 73% of all data within corporations is never analyzed for insights or larger trends, a Forrester survey found. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why organizations have taken an interest in technologies like AWS SaaS Boost that help to ingest, understand, organize, share, and act on data from multiple environments. According to Gartner, creating a‌n architecture‌ ‌that helps‌ ‌operationalize data‌ ‌pipelines‌ ‌is one‌ ‌of‌ ‌the‌ ‌major‌ ‌trends‌ ‌for‌ ‌2021. Organizations want to make better use of their data, but most lack a mature strategy. Indeed, surveys show that data’s business impact is limited by challenges in lifecycle management. Recognizing this, Amazon designed AWS SaaS Boost to be adaptable to the needs of individual projects and organizations. The management and core services of SaaS Boost were built using a serverless application model, with a dashboard where users can configure the ports, domains, compute settings, databases, file systems, and billing options unique to their apps. New tenants are introduced to the AWS SaaS Boost environment through an onboarding process that collects a tenant’s configuration options and launches an automation. From there, AWS SaaS Boost provisions tenants with separate subdomains that are used to route them to their architectures. The specific resources that apps will need are set up automatically, so that when new versions of the apps are uploaded, SaaS Boost can deploy the updates to all tenants. On the analytics side, SaaS Boost includes a collection of tenant-focused graphs that can be used to analyze trends. Beyond this, the tool enables integration with preprovisioned infrastructure that can aggregate and surface custom metrics views. In a blog post, AWS worldwide partner solution architecture Adrian De Luca said that the goal is to “build a vibrant community of developers using AWS SaaS Boost” for production workloads. “We’d like to [encourage] contributors [to donate] code to enhance and optimize … features. As the project matures, we plan to invite other maintainers to take active roles in determining the project’s direction,” he wrote. “Throughout the preview period with developers all over the world, we received interest from large industry-leading software companies who want to offer their traditional products in an easier way, startups who want to build new products with it, and systems integrators modernizing enterprise software on behalf of customers.”"
https://venturebeat.com/2021/05/12/zenlayer-raises-50mm-in-series-c-financing-to-boost-its-lead-in-edge-cloud-services/,Zenlayer Raises $50MM in Series C Financing to Boost Its Lead in Edge Cloud Services," Edge cloud services provider announces its third round of funding  LOS ANGELES–(BUSINESS WIRE)–May 12, 2021– Zenlayer, a leading global edge cloud service provider, announced the closing of its $50 million Series C financing today. The round was led by a group including Anatole Investment and Prospect Avenue Capital, and included existing investor Volcanics Venture. These investors join F&G Venture, NSFOCUS, and Forebright Capital to bring Zenlayer’s total financing to $90 million since inception. Zenlayer will allocate the new financing toward enhancing its edge cloud technology and expanding global network coverage. “I am excited to announce that we have successfully raised $50 million in Series C,” said Joe Zhu, Zenlayer’s Founder & CEO. “We’ll accelerate the development and adoption of our PaaS solutions, and continue to focus on emerging markets, helping our customers to capture the explosive growth of regions like Southeast Asia and South America. This capital will bring us one step closer to realizing our mission of improving digital experiences for every organization and person, anywhere in the world.” With its unique edge cloud offerings, Zenlayer enables organizations to instantly deploy compute closer to end users and accelerate their networks to deliver the best digital experience possible. Today, Zenlayer can help organizations reach over 85% of the world’s internet population in just 25 milliseconds or less. By using Zenlayer’s PaaS solutions, organizations can achieve this without deploying any infrastructure. Zenlayer Highlights include: George Yang, Chief Investment Officer, Anatole Investment: “Zenlayer is an edge cloud technologies leader uniquely positioned to accelerate digital transformation across the world. IT Infrastructure is a critical need for digital enterprises and Zenlayer has demonstrated the immense value companies gain by improving their users’ digital experience. We’re excited to help Zenlayer accelerate their rapid growth and expansion, and continue to innovate edge cloud computing to new heights.” Ming Liao, Founding Partner, Prospect Avenue Capital: “Zenlayer has a remarkable competitive edge. Its exponential growth has made Zenlayer an ideal investment target for Prospect Avenue Capital. Zenlayer is positioned to meet the demands of emerging markets in Southeast Asia, South America, and Northern Africa with its global coverage, low latency edge cloud solutions, and strong infrastructure.” Suyang Zhang, Managing Partner, Volcanics Venture: “We began our relationship with Zenlayer in 2019 and have observed the strength and contributions of their team and growth potential first-hand. We are excited to participate in this latest round of financing to continue Zenlayer’s mission of improving digital experiences for everyone in the world through edge cloud services, instantly.” For more information about Zenlayer, please visit www.zenlayer.com. About Zenlayer Zenlayer (www.zenlayer.com) offers on-demand edge cloud services in over 180 PoPs around the world, with expertise in fast-growing emerging markets like Southeast Asia, India, China, and South America. Businesses utilize Zenlayer’s global edge cloud platform to instantly improve digital experiences for their users with ultra-low latency and worldwide connectivity on demand. About Anatole Investment Anatole Investment Management Limited is an international investment management firm that manages long-term capital for highly sophisticated professional investors and clients globally. About Prospect Avenue Capital Prospect Avenue Capital is a growth equity firm with a focus on IT, financial services, technology, and AI-related sectors. About Volcanics Venture Volcanics Venture (www.volcanics.com) is committed to identifying, investing in, and serving the most promising companies and outstanding entrepreneurs in internet innovation, intelligent technology, and healthcare industries. Volcanics Venture brings a powerful combination of global perspective and local experience to investment management, striving to provide sustainable value-added services to portfolio companies. About F&G Venture F&G Venture (www.fgventure.com/en/index.jsp) is a venture capital fund focused on companies with exponential growth in IT industries, such as IT infrastructure, cloud computing, IoT, SaaS, big data, etc. It also targets high-end manufacturing businesses, including intelligence devices, robots, and drones. About Forebright Capital Forebright Capital (www.forebrightcapital.com) is a differentiated institutional-quality multi-stage growth equity fund manager investing in selected sectors including advanced manufacturing, healthcare, and business services. It is committed to partnering with visionary business leaders, providing value added services, and contributing to the long-term growth of outstanding enterprises. About NSFOCUS NSFOCUS (www.nsfocus.com) is a network and cyber security provider for telecom carriers, BFSI, enterprises, healthcare, retail, as well as government agencies. It has a proven track record of protecting over 20% of the Fortune 500 companies, including four of the five largest banks, and six of the world’s top ten telecommunications companies.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005036/en/ David Purdie(626) 412-0004"
https://venturebeat.com/2021/05/12/mvl-raises-8m-and-announces-startup-studio-innovation-assembles-whos-who-of-successful-operators-as-investors-co-builders/,MVL Raises $8M and Announces Startup Studio Innovation; Assembles Who’s Who of Successful Operators as Investors & Co-Builders,"SEATTLE–(BUSINESS WIRE)–May 12, 2021– Madrona Venture Labs (MVL), a Seattle-based startup studio, lands $8M in fresh funding and innovates on venture studio model–adding successful operators as investors and co-founders of their spinout companies. This is a unique opportunity for founders working with MVL to co-found companies with the creators of Zillow, DocuSign, Tableau, Concur, Qualtrics, Tinder, and more. MVL is excited to partner with operating executive investors, including Spencer Rascoff, former Zillow CEO; Elie Seidman, former Tinder CEO; Court Lorenzini, co-founder DocuSign; Elena Donio, former Axiom CEO; Elissa Fink, former Tableau CMO; Julie Larson-Green, former Qualtrics CXO; Erik Blachford, former Expedia CEO; Mike McSherry, Xealth CEO; Jon Gelsey, former Auth0 CEO; Chad Cohen, Adaptive Biotech CFO; and Ralph Pascualy, former Swedish Medical Group CEO. MVL will continue and build upon it’s close partnership with the investment team at Madrona. For example, last year MVL worked closely with Hope Cochran, former King Digital CFO, who partnered to co-build Strike Graph and Steve Singh, former Concur CEO, who is the co-founder and executive chairman of Stratify, a case study for this innovative new model. This group of experienced investors and operators will be co-founding board members and advisors, collaborating on idea validation, leveraging their deep talent networks for recruiting, participating in customer development and outreach, and providing fundraising support and coaching to the founding teams. “I’m excited about this opportunity because we are never done innovating. MVL gives us a chance to think critically about what’s next and to partner with fantastic leaders, entrepreneurs, and investors to make vision reality,” said Elena Donio, former Axiom CEO. Data shows that studio-built startups (compared to traditional, non-studio) have 3x faster path to seed, 2x faster path to Series A, and 30% better likelihood of achieving Series A*. For founders, by working with exceptional executive chairpersons, board members, and advisors at the formation stage, MVL further stacks the deck in their favor–lowering their real and opportunity costs and de-risking their path to an outsized outcome. “Our obsessive focus on building founder value has made MVL the obvious choice for founders building a company in the PNW. Our next group of founders will benefit from a 100% operator-led team with a proven company building process; successful operators as co-founders and partners; and funding and support from Madrona, the leading VC in the PNW, ” said Mike Fridgen, MVL CEO and Managing Director. A recent proof point is Strike Graph, a Geekwire Startup-of-the-Year nominee, and MVL spinout. “As a founder you’re challenged to be an expert in almost everything. That is impossible. While I have strengths, the team at Madrona Venture Labs filled in my gaps. Strike Graph’s successful launch is due to their expertise, their support and their camaraderie,” said Justin Beals, Strike Graph CEO. Hear more about his journey in a recent podcast with Hope Cochran, Madrona Managing Director. MVL investment themes are AI-driven applications, automation, and the future of work, building off the MVL team’s operating experience, Madrona’s themes, and Seattle’s strengths. MVL is a team of life-long PNW operators and are deeply committed to founders’ and the community’s long-term growth and success. If you are a founder interested in building a world-changing company, contact MVL. About Madrona Venture Labs Madrona Venture Labs is the incubation arm of Madrona Venture Group–the leading venture capital firm in the PNW, which recently raised a $345M fund focused on seed and early-stage investing. Together, they partner with founders from day one to build venture-scale companies. A 100% operator-led studio, MVL represents an integral part in Madrona’s continuum of Day One for the long-run commitment to founders. Madrona has a 25-year history of supporting PNW founders, including early investors in Amazon, Smartsheet, Rover, and many more. * Global Startup Studio Network (GSSN), Disrupting the Venture Landscape, 2020  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005169/en/ Maria HessMadrona Venture Labsmaria@madronavl.com 206-909-9118"
https://venturebeat.com/2021/05/12/yelp-built-an-ai-system-to-identify-spam-and-inappropriate-photos/,Yelp built an AI system to identify spam and inappropriate photos,"Malicious actors are constantly finding ways to circumvent platforms’ policies and game their systems — and 2020 was no exception. According to online harassment tracker L1ght, in the first few weeks of the pandemic, there was a 40% increase in toxicity on popular gaming services including Discord. Anti-fraud experts saw a rise in various types of fraud last year across online platforms, including bank and insurance fraud. And from March 2020 to April 2020, IBM observed a more than 6,000% increase in COVID-19-related spam. Yelp wasn’t immune from the uptick in problematic digital content.  With a rise in travel cancellations, the company noticed an increase of images being uploaded with text to promote fake customer support numbers and other promotional spam. To mitigate the issue and automate a solution that relies relied on manual content reporting from its community of users, Yelp says its engineers built a custom, in-house system using machine learning algorithms to analyze hundreds of thousands of photo uploads per day — detecting inappropriate and spammy photos at scale. Yelp’s use of AI and machine learning runs the gamut from advertising to restaurant, salon, and hotel recommendations. The app’s Collections feature leverages a combination of machine learning, algorithmic sorting, and manual curation to put local hotspots at users’ fingertips. (Deep learning-powered image analysis automatically identifies the color, texture, and shape of objects in user-submitted photos, allowing Yelp to predict attributes like “good for kids” and “ambiance is classy.”) Yelp optimizes photos on businesses’ listings to serve up the most relevant image for browsing potential customers. And advertisers can opt to have an AI system recommend photos and review content to use in banner ads based on their “impactfulness” with users. There’s also Popular Dishes, Yelp’s feature that highlights the name, photos, and reviews of most-ordered restaurant menu items. More recently, the platform added tools to help reopening businesses indicate whether they’re taking steps like enforcing distancing and sanitization, employing a combination of human moderation and machine learning to update sections with information businesses have posted elsewhere. Building the new content moderation system was more challenging than previous AI projects because Yelp engineers had a limited dataset to work with, the company told VentureBeat. Most machine learning algorithms are trained on input data annotated for a particular output until they can detect the underlying relationships between the inputs and output results. During the training phase, the system is fed with labeled datasets, which tell it which output is related to each specific input value. Yelp’s annotated corpora of spam was limited prior to the pandemic and had to be augmented over time. “Ultimately, our engineers developed a multi-stage, multimodel approach for promotional spam and inappropriate content,” a spokesperson said. In this context, “inappropriate” refers to spam that runs afoul of Yelp’s Content Guidelines, including suggestive or explicit nudity (e.g., revealing clothes, sexual activity), violence (weapons, offensive gestures, hate symbols), and substances like drugs, tobacco, and alcohol. Yelp also had to ensure that the system understood the context of uploaded content. Unlike most AI systems, humans understand the meaning of text, videos, audio, and images together in context. For example, given text and an image that seem innocuous when considered apart (e.g., “Look how many people love you” and a picture of a barren desert), people recognize that these elements take on potentially hurtful connotations when they’re paired or juxtaposed. Yelp’s anti-spam solution is a two-part framework that first identifies photos most likely to contain spam. During the second stage, flagged content is run through machine learning models tuned for precision, which send only a small amount of photos to be reviewed by human moderators. A set of heuristics play alongside the models to speed up the pipeline and react quickly to new potential spam and inappropriate content. “We used a custom dataset of tens of thousands of Yelp photos and applied transfer learning to tune pre-trained large-scale models,” Vivek Raman, Yelp’s VP of engineering for trust and safety, told VentureBeat via email. “The models were trained on GPU-accelerated instances, which made the transfer-learning process training very efficient — compared to training a deep neural network from scratch. The performance of the models in production is monitored to catch any drift and allow us to react quickly to any evolving threats.” In the case of promotional spam, the system searches for simple graphics that are text- or logo-heavy. Inappropriate content is a bit more complex, so the framework leverages a residual neural network to identify photos that violate Yelp’s policies as well as a convolutional neural network model to spot photos containing people. Residual neural networks build on constructs known from pyramidal cells in the cerebral cortex, which transform inputs into outputs of action potentials. Convolutional neural networks, which are similarly inspired by biological processes, are adept at analyzing visual imagery. When the system detects promotional spam, it extracts the text from the photos using another deep learning neural network and performs classification via a regular expression and a natural language processing service. For inappropriate content, a deep learning model is used to help the framework calibrate for precision based on confidence scores and a set of context heuristics, like business category, that take into account where the content is being displayed. Yelp’s heuristics help combat repeat spammers. Photos flagged as spam are tracked by a fuzzy matching service so that if users try to reupload spam, it’s automatically discarded by the system. If there’s no similar spam match, it could end up in the content moderation team queue. While awaiting moderation, images are hidden from users so that they’re not exposed to potentially unsafe content. And the content moderation team has the ability to act on user profiles instead of single pieces of content. For example, if a user is found to be generating spam, its user profile is closed and all associated content is removed. AI is by no means a silver bullet when it comes to content moderation. Researchers have documented instances in which automated content moderation tools on platforms such as YouTube mistakenly categorized videos posted by nongovernmental organizations documenting human rights abuses by ISIS in Syria as extremist content and removed them. A New York University study estimates that Facebook’s AI systems alone make about 300,000 content moderation mistakes per day, and that problematic posts continue to slip through Facebook’s filters. Raman acknowledges that AI moderation systems are susceptible to bias, but says that Yelp’s engineers have taken steps to mitigate it. “[Bias] can come from the conscious or unconscious biases of their designers, or from the datasets themselves … When designing this system, we used sophisticated sampling techniques specifically to produce balanced training sets with the explicit goal of reducing bias in the system. We also train the model for precision to minimize mistakes or the likelihood of removing false positives.” Raman also asserts that Yelp’s new system augments, not replaces, its team of human moderators. The goal is to prioritize the items that moderation teams — who have the power to restore falsely flagged content — review rather than take down spam proactively. “While it’s important to leverage technology to create more efficient processes and manage content at scale, it’s even more important to create checks and balances through human moderation,” Raman said. “Business pages that receive less traffic are less likely to have a consumer or business owner catch and report the content to our moderators — so, our photo moderation workflow helps weed out suspicious content in a more scalable way.”"
https://venturebeat.com/2021/05/12/inx-becomes-spice-vcs-third-portfolio-company-to-go-public/,INX Becomes SPiCE VC’s Third Portfolio Company to Go Public," The First Fully Tokenized VC Fund Affirms its Winning Investment Strategy and Plans for a Significant Investor Payout in 2021  SAN FRANCISCO & ZURICH & TEL AVIV, Israel–(BUSINESS WIRE)–May 12, 2021– SPiCE VC, the leading venture capital (VC) fund in the Blockchain/Tokenization ecosystem, which has revolutionized the VC asset class, becomes the first venture capital firm in this growing space to experience three portfolio companies successfully going public. The latest one, INX Limited, the blockchain-based platform for trading digital securities and cryptocurrencies and one of SPiCE VC’s portfolio companies, is officially the first to complete an SEC-registered token IPO. In addition to INX’s IPO, SPiCE portfolio companies Lottery.com and Bakkt have both gone public at healthy valuations – contributing to the firm’s early and ongoing success. As a result, the firm plans to implement its first investor payout later this year. “SPiCE’s objective has always been, and continues to be, to provide our investors exposure to the massive growth of the Blockchain and tokenization industry. This is why we’ve maintained a singular focus on investing in companies like INX, Securitize and others that are developing the world’s tokenization ecosystem, standards, and processes,” said Tal Elyashiv, founder and managing partner of SPiCE VC. “What excites us the most is our ability to generate remarkable return for our investors, while also continuing to fund, impact and influence the formation of this accelerating sector.” INX is one of over 15 strategic investments Tal Elyashiv and the SPiCE VC management team have made within the Digital Finance sector – an ecosystem which includes digital securities, cryptocurrencies, lending, real estate, Non-Fungible Tokens (NFTs), etc. INX’s historic IPO achieved an estimated $93 Million in gross proceeds from token sales, and a pending CA$39.6 Million private placement ahead of its Toronto Stock Exchange (TSXv) listing. In total, INX expects to have raised over $125 Million for the launch and operation of the INX fully regulated trading platform for cryptocurrencies and digital securities. “The entire industry is experiencing success because of partnerships like SPiCE and INX’s. SPiCE understands the ecosystem and the driving forces behind it, and this has been very helpful to us,” said Shy Datika, founder & president of INX. “I am excited about INX’s success, about the partnership with SPiCE, its continued support for INX’s vision, and its investment in INX that has impacted our ability to succeed.” SPiCE attributes its early and ongoing success to a number of factors, including explosive growth and persistent headwinds in the Digital Finance sector, as well as its long-term investment strategy, which has been the greatest catalyst of growth for the firm. In fact, SPiCE reported a Q1 2021 NAV Report up more than 15%, and over 91% unrealized gains in the first three years of operation for its early investors. Since the fund’s first closing mid 2018, SPiCE has focused on portfolio companies aiming to change the digital tokenization landscape – providing its investors wide exposure to the massive growth of the blockchain and tokenization ecosystem. As one of the pioneers of the industry, SPiCE has also played a significant role in the creation of the legal and regulatory framework that is used today. To learn more about SPiCE VC and its SPICE digital security, visit https://spicevc.com/ ABOUT SPiCE VC: SPiCE VC is a Venture Capital fund providing investors exposure to the massive growth of the blockchain/tokenization ecosystem. SPiCE invests globally in platforms and ecosystem providers enabling access to capital markets, banking, real estate, and other industries enhanced through Blockchain technologies. The fund focuses on companies who stand to benefit the most from the massive growth of the industry. Combining institutional know-how, hands-on management, entrepreneurial innovation and professional investment experience SPiCE’s management team has been involved in hundreds of tech funding rounds totaling billions of dollars; as entrepreneurs, investors, and executives. SPiCE is located in the US, Switzerland, Singapore and Israel. To learn more about SPiCE VC visit www.spicevc.com or email Tal Elyashiv, Founder and Managing Partner, at tal@spicevc.com. About INX: INX aims to provide a regulated trading platform for digital securities and cryptocurrencies. With the combination of traditional markets expertise and a disruptive fintech approach, INX provides state-of-the-art solutions to modern financial problems. INX is led by an experienced and dedicated team of business, finance, and technology veterans with the shared vision of redefining the world of capital markets via blockchain technology and innovative regulatory approach.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005353/en/ Liz Whelanliz@lwprconsulting.com (312) 315-0160"
https://venturebeat.com/2021/05/12/games-industry-experts-weigh-in-on-the-global-game-industry/,Games industry experts weigh in on the global game industry,"Presented by Google for Games The global gaming market reached $162.32 billion in 2020. By 2026, that number is expected to rise to $295.63 billion. In other words, the industry is booming — and incredibly competitive. To keep up and get ahead, it’s essential to stay on top of the latest insights from around the globe. To that end, Google has stepped in, with the 2021 Google for Games Global Insights Report. It covers key questions game developers are asking about the world of gaming, around the globe, as we move into the next decade: What are gamers searching online? What’s keeping them engaged? What are they purchasing? And what can gamers expect next? The report covers these essential questions, some surprising statistics, and a look at some major global trends, including the boom of women gamers and competitive games in some of the world’s biggest markets, shifts in how players are paying, and how the very nature of gaming is changing with the evolution of cutting-edge technology. This comes with more in-depth coverage of some of the biggest topics in the industry from a panel of Google for Games industry experts. Here’s a closer look at how some of the leaders at Google are weighing in. Staying at home has meant that the number of people playing games has increased dramatically, as well as the kind of games that people are playing. Global game downloads increased by 75%, from 2019 to 2020, and watch time for gaming live stream increased by 45%. “As part of the pandemic, we saw that games continue to be how people create and maintain connections,” says Meagan Timney, Head of UX, Stadia at Google. “Gaming grew as a socially connected experience, whether that was watching streams or using games as a way to connect with family and friends while sheltering at home.” As COVID-19 altered the day-to-day, the game industry had to get a bit more creative, with a potential market shift toward more digital and cloud-based gaming platforms. This meant a seamless experience became paramount — games that were easy and fast to start, easy to put down and playable anywhere — on a variety of devices, switching seamlessly during the span of a day. “The big opportunity here is for developers to consider ways to make it easier for players to hop in and out of the action — and cloud gaming platforms can be a great way to help support this,” says Timney. There’s also an enormous pool of potential gamers out there that remains largely untapped, she says. Google estimates 15 to 30% of the population has one or more disabilities, which is about 300 to 400 million potential players with visual, hearing, motor, or cognitive disabilities around the globe. “The bar should be set high as an industry to enable experiences for people with those visual, auditory, or motor impairments,” she says. “What that means for  developers is continuing to place more emphasis on accessibility and ease of use in their games, whether that’s through the hardware we provide or the platforms and solutions we build,” she says. That means personalizing games to make them more equitable, and really digging into how players interact with a game, not just in traditional ways, but through various devices. It also means thinking about that seamless cross-device experience, and what those narrative moments look like for people who hop in and out of games, no matter how they’re showing up to that game or interacting with it. “I’d encourage developers to spend time deeply understanding their gaming populations and areas where their players could be better served, whether that’s different genres, accessibility features, really allowing and thinking through how to invest in games that allow that connection and immersion,” she says. “Focusing on connection, personalization, and accessibility will be keys to success in the digital gaming world going forward.” For growing game developers, the key to success is understanding what success actually looks like. Even if your game never reaches Call of Duty or Candy Crush levels of revenue, studios are enjoying long-term success with the right strategies. In part, that means moving toward a hybrid monetization strategy, as single streams of revenue just won’t cut it anymore, says Francis Ma, product director for Firebase, Google’s platform for mobile apps and game developers. “A hybrid monetization strategy is about being able to take advantage of the best of both worlds, because there are different types of players that respond differently to types of monetization schemes,” Ma says. “The idea of hybrid monetization is where game developers take advantage of these multiple ways of monetizing their player base.” A challenge many developers face is recognizing there is no one-size-fits-all solution, meaning you can’t merely replicate one game’s strategy in another. That means developers often have to create their own path — there isn’t an existing playbook. Add to that finding the right tools and infrastructure to execute on monetization plans, especially for smaller development shops and studios that don’t have the same amount of resources as the well-established studios. “That’s where products like Firebase come in and help solve some of these problems,” Ma says. “We provide tools to help small developers, or even developers that are well-resourced and would rather channel their resources elsewhere.” When it comes to hybrid monetization, it’s especially important to focus on the gaming experience and ensure that your monetization strategy is tied in deeply with the gaming experience, as opposed to a bolt-on or an afterthought, Ma says. From a developer’s point of view, you want to combine both behavioral analytics and also technical performance analytics, making sure that the core experience of the game works well for both. For example, Google Analytics helps uncover user behavior, while tools like Crashlytics and Performance Monitoring offer data around performance for players’ specific devices. “We often talk a lot about the behavioral side and less about the technical side, or sometimes these discussions happen in silos where the game designer may focus on the user, the player behavior, whereas the core engineering team is focusing on the technical side,” he says. “Ultimately, to provide an amazing player experience, you need to marry both sets of data when you look at them.” APAC is a gaming powerhouse in the wake of lower-cost smartphones and more affordable access to the Internet. The mobile apps user population is booming and the audience and developer community for connected games in the region is surging, says Paula Wang, Director of App Developer sales, APAC.  It’s not just about expanding successfully in the APAC market with the right strategies — though that’s a solid road to profit these days; they can also leverage the most important lessons APAC studios now offer. “APAC is not one homogeneous market; it is a rich and diverse collection of countries and sub-regions,” Wang says. “When entering the region, developers should be careful not to oversimplify the market needs into one entry strategy. Instead, they should focus on understanding each market’s specific nuances in order to be successful.” It’s an area ripe for expansion, but also inspiration for success wherever a studio is positioned in its strategy. APAC studios have demonstrated that building a locally relevant product or product portfolio is key, followed by having an effective user acquisition strategy. “We’re seeing that many developers in Asia tend to invest a higher percentage of their revenue into user acquisition compared to other developers in the world, and it seems to be working well for them,” she says. “And we’re now seeing more and more games developed by Asian studios ranking in the top global positions.” The number of women players are also growing rapidly in the APAC region — as of 2019, China is home to one of the highest populations of female gamers in the world, making up 45% of total Chinese gamers. “Female players are a core component of the gaming population, both for today and tomorrow,” Wang says. “The growth rate for new female players is much faster than the one for males, so, in a future where females could be the majority of the player population in many markets, it’s important we think about what they enjoy most, and how they’re different. This will influence the types of genres that are created and launched. It’s all about understanding the audience, what they enjoy and care about.” Dig deeper: For more on the global state of gaming today, more insight from Google’s in-house gaming experts, and how Google’s solutions can help you across the entire lifecycle — today and tomorrow — read the full 2021 Google for Games Global Insight report. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/05/12/hidden-leaf-games-raises-3-2m-for-3v3-moba-game-called-fangs/,Hidden Leaf Games raises $3.2 million on a MOBA gambit called Fangs,"Hidden Leaf Games is making a 3-on-3 multiplayer online battle arena (MOBA) game. And the team has raised $3.2 million for its new game studio. CEO David Li and president Bao Lam founded this L.A.-based startup. Both are veterans from Riot Games, PUBG, and other studios. The design team’s led is Steve “Guinsoo” Feak, whose custom Warcraft III map DotA: All Stars helped define the MOBA genre and inspired smash hits such as League of Legends and Dota 2. They’re working on a new action MOBA called Fangs, which will have strategic real-time gameplay, a diverse roster of characters, and session-based rounds set in an arena. While that sounds like a lot of MOBAs, the company aims to tackle the genre’s biggest frustrations for players. That includes a long time before having actual fun in a match, long game times, no cross-platform play, and few incentives to play with friends. Lightspeed Venture Partners, a global tech venture capital firm that has backed companies such as Snap, Affirm, Epic Games, and more, led the round. Investors Vermillion Ventures, Logan Margulies (early Riot Games leader), Eden Chen (CEO of Pragma) and Alex Paley (vice president of product at Scopely) joined the round. Amy Wu, a partner at Lightspeed, said in an interview with GamesBeat that she was impressed that Lam and Li were able to hire leaders like Feak and others in a highly competitive environment for game talent. “We’ve honed in on finding special founders in every category, including gaming, and so at a high level our thesis is around founders who have great pedigrees in gaming,” Wu said. “David and Bao are part of a new generation of free-to-play game makers and they have completely bootstrapped their game. They were able to convince great people to work with them. We really love to see that hustle.” While Lightspeed hasn’t made many game investments in its history, Wu said she and partner Jeremy Liew have been investigating opportunities in games for the last five months. “We’ve been a lot more aggressive, and we have made four investments year-to-date,” Wu said. “Hidden Leaf Games is the first early-stage, prelaunch studio that we are announcing.” Wu said the attraction of games is that it has become the largest entertainment category. It’s also converging with social and the free-to-play business model, resulting in an acceleration of adoption. “In our perspective, this is just going to continue to drive forward the gaming industry,” she said. “We aren’t delusional about the risks elements. But with the right investment approach, you can mitigate those risks. Money is coming into the industry. I don’t see that changing. We’re still on the rise in gaming. Game developers are able to raise funding from a wide variety of sources.” Li said that he and Lam have been friends for eight years and they’re always competitive, doing things like betting a dollar on who will make a better move in a game. Li bet Lam that he could make diamond rank in League of Legends, and he poured hundreds of hours into that task. Their friends have always followed the big competitive gaming trends, such as MOBA and battle royale, and that kept them connected even if they were physically far apart. Those hundreds of hours spent online competing together on voice chat defined their friendship. Li said the team built its game on three design principles so far. It is competitive, it has a lot of progression, and it’s social. “We’re super-competitive and like games that test our skills,” Li said. “We married competitive play plus progression.” Lam did multiple startups in addition to Riot Games. Some of them didn’t work out, but he learned from them. “I’ve been playing competitive games since I was 5,” he said. “Both of my brothers are esports players involved in the esports scene. So we have a very competitive gaming family.” Li got his start at Riot Games and met Lam there. Li left to join Zynga and also worked on a mobile MOBA, Vainglory, at Super Evil Megacorp. He joined a startup focused on interactive storytelling games, but that company shut down after it lost its publishing partner, NBCUniversal, which dabbled in games and shut down its efforts. “Bao and I had stayed friends this entire time. We play games every week. And I knew that I was a serial entrepreneur,” Li said. Then they finally decided to make a game together with a focus on an accessible MOBA. The team is focused on making a PC game first, and then it will extend it to mobile devices as well, with the ultimate goal being a cross-platform game. They chose 3-on-3 instead of 5-on-5 in League of Legends because it makes for a less chaotic game and it’s more accessible, with a chance for shorter matches, Lam said. They also wanted to have clear roles for every player as well as maps that weren’t too large, Li said. “Three was the number that was the best fit for the map size we were going for,” Li said. Lam and Li saw a game market rich with opportunities to brawl against anonymous opponents, but often the grind of competitive play and myopic focus on the solo competitive experience left their group divided. With that goal in mind the founding team started building their dream game, spending nights, weekends, and their own resources to get to a playable proof of concept. And that concept and vision got the attention of one of the biggest names in game design, Feak. The founders say Feak has done a great job leading the team and mentoring them. The team has 32 people, all working remotely in the U.S., United Kingdom, Brazil, Argentina, Slovenia, India, and Spain. Chad Wavell-Jimenez will serve as vice president of marketing and publishing. “It’s crazy that we haven’t met 28 of our people in person yet,” Lam said. Li said, “We look for passion, dedication, and talent, rather than industry experience. Because of the pandemic, it’s been double-edged. It was tough meeting people in person. On the other hand, the potential for talking to anyone is higher because everyone is working at home.” The team has been working for 18 months and Lam said the team is entering its alpha testing now and he wants it to go into beta testing later this year and launch. That’s a pretty fast timetable. “We’re executing,” Li said. “We’re growing, we’re getting faster and getting better at developing. That’s what Lightspeed liked about us. We’re very scrappy.”"
https://venturebeat.com/2021/05/12/cybersaint-unveils-automated-crosswalking-functionality-providing-cisos-immediate-visibility-into-their-posture-across-any-framework-or-standard/,CyberSaint Unveils Automated Crosswalking Functionality Providing CISOs Immediate Visibility into Their Posture Across Any Framework or Standard," Company continues to lead industry in innovation with the mission to automate as much of the cyber and IT risk management function as possible  BOSTON–(BUSINESS WIRE)–May 12, 2021– CyberSaint, the developer of the leading platform delivering cyber risk automation, announced the addition of patented, Natural Language Processing (NLP)-backed crosswalking functionality to its CyberStrong platform. Organizations, regardless of industry, face a barrage of regulatory change as well as a stark increase in vendor security questionnaires. Coupled with increased executive reporting expectations and audit requests, CISOs need real-time visibility into their compliance posture across frameworks. With CyberStrong’s new functionality, customers can project security posture data across regulatory frameworks, industry standards, or custom control sets in seconds with unparalleled accuracy. “Getting risk and compliance visibility, with precision, is still one of the most difficult tasks CISOs and teams face, and then there’s the issue of duplicate efforts across assessments,” said Padraic O’Reilly, Co-founder & CPO at CyberSaint. “Industry standard mappings only go so far, and it’s time a solution allowed organizations to confidently translate their current compliance activities across frameworks with accuracy and speed. Our customers can simply select any standard or custom control set they wish to map to or from, and see their posture light up in seconds pulling from data they already have.” Security teams have historically struggled to meet compliance across the myriad of requirements necessary to operate in highly regulated industries today. The need to assess against multiple frameworks such as NIST CSF, CMMC and NIST SP 800-171 (DFARS), ISO27001, NERC-CIP, CIS 20, and others with similar outcomes is one of the greatest inefficiencies facing security teams today. These organizations are often forced to deviate their resources to crosswalk frameworks in-house or, alternatively, outsource these mapping projects. Despite lackluster outcomes that achieve a fraction of the desired efficiency, enterprises are quoted hundreds of thousands of dollars to complete these projects with timelines that span weeks, even months. The resulting mappings are historically subjective and based on a word search for matching keywords in the control language rather than the intent of the control or the actions within the control itself. This cycle is repeated as new requirements come in from vendors and regulators, increasing the complexity and the duplicated efforts. By leveraging Natural Language Processing, CyberSaint’s CyberStrong platform processes the control action language and intent in seconds, enabling a much more accurate map across frameworks and control sets. Users select any framework to map their current compliance posture against and see results instantly, allowing CISOs and their teams to get credit for their work across requirements without spending time or resources on the tedious mapping exercises of the past. Want to see this new feature in action? Register for the Live Demo on May 19th at 12 pm EDT or watch after on-demand. For more information about CyberSaint’s CyberStrong platform, please visit: www.cybersaint.io/ About CyberSaint CyberSaint’s mission is to empower today’s organizations to build a cybersecurity program that is as clear, actionable, and measurable as any other business function. CyberSaint’s CyberStrong platform empowers teams, CISOs, and Boards to measure, mitigate, and communicate risk with agility and alignment.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005530/en/ Media Contact: Danielle OstrovskyHi-Touch PR410-302-9459Ostrovsky@hi-touchpr.com"
https://venturebeat.com/2021/05/12/datarobots-zepl-acquisition-bridges-the-ai-divide/,DataRobot’s Zepl acquisition bridges the AI divide,"DataRobot revealed yesterday during an online AI Experience Worldwide conference that it has acquired Zepl as part of a larger effort to enable data scientists to customize AI models developed on its platform. Zepl created an open source Apache Zepyl notebook that enables data scientists to collaboratively develop and analyze code written in Python, R, or Scala. DataRobot also unveiled a slew of updates, including an ability to clone, edit, and reconfigure machine learning blueprints created using the AutoML framework at the core of its platform for constructing AI models. That Composable ML feature enables organizations to integrate their own custom training code with the AutoML framework employed by DataRobot, senior VP Nenshad Bardoliwalla said. “Data scientists can now tweak our AI models,” he added. DataRobot is also adding a Continuous AI feature that enables organizations to implement a policy that defines when an AI model should be retrained based on the level of drift detected by the MLOps governance framework embedded in the DataRobot platform. Alternatively, organizations can simply decide to schedule an AI model to be retrained at a given interval. There is also now a No Code AI App Builder that automatically converts any model into an AI application without requiring coding. Widgets, data visualizations, and prebuilt templates enable AI applications to be constructed in a few minutes. Finally, DataRobot has added a grading tool to generate automatic scorecard grading based on an evaluation of data quality, robustness, accuracy, and fairness. The company is making available a Bias & Fairness Production Monitoring tool that monitors models for bias after they have been deployed in a production environment. Previously, bias detection could only be applied to AI models as they were being developed. DataRobot is driving an effort to democratize AI using a framework that automates most of the routine tasks associated with aggregating data and then training an AI model. So far, business analysts and executives have been the primary users of a DataRobot platform accessed via graphical tools. A challenge  DataRobot has encountered is that many AI models today are built by data scientists using notebooks and other types of open source toolkits. The acquisition of Zepl sets the stage for enabling data scientists to employ the same platform as end users and business analysts to automate the training of an AI model, Zepl CEO Dan Maloney said. Zepl claims its notebook has been downloaded more than 500,000 times. This provides the foundation for AI model training to become a more collaborative effort involving end users, business analysts, data scientists, and developers, Maloney added. “They can use any open source toolkit,” he said. Developers can invoke AI models running on the DataRobot platform via a REST application programming interface (API). Alternatively, they can embed the server running an AI model as a Java Archive (JAR) file or in a Docker container within their application. The pace at which organizations want to employ AI to automate a wide range of digital processes is far outstripping the available supply of data scientists. As a result, there is a chronic need to enable end users and business analysts to create AI models without the aid of a data scientist. However, there are still complex AI models that need to be custom-built by a team of data scientists. The challenge organizations now face is recognizing the difference between the level of skills required to build one AI model versus another."
https://venturebeat.com/2021/05/12/phishing-attacks-exploit-cognitive-biases-research-finds/,"Phishing attacks exploit cognitive biases, research finds","Cybercriminals are crafting personalized social engineering attacks that exploit cognitive bias, according to a new report from Security Advisor, which uses machine learning to customize security awareness training for individual employees. Cognitive bias refers to mental shortcuts humans subconsciously take when processing and interpreting information prior to making decisions. Bias is an attempt to simplify information processing to speed up decision-making and can be effectively exploited in phishing attacks, SecurityAdvisor CEO Sai Venkataraman told VentureBeat. Cybercriminals manipulate a recipient’s thoughts and actions to convince that person to engage in risky behavior — such as clicking on a link they normally wouldn’t click on or entering sensitive information on a website. Enterprise security teams usually rely on security awareness programs to train employees to recognize attacks so that they won’t be tricked. However, traditional security awareness programs rarely take into account the role cognitive biases play in these situations, nor do they typically consider people’s roles or past behavior. The training concepts weren’t sticking, and the data showed 5% of users accounted for 90% of security incidents, Venkataraman said. SecurityAdvisor isn’t the only one saying traditional security awareness training and phishing simulations have a limited ability to protect organizations. A recent Cyentia Institute study found that security training resulted in slightly lower phishing simulation click rates among users, but it had no significant effect at the organizational level or in real-world attacks. The report, commissioned by Elevate Security, examined malware, phishing, email security, and other real-world attack data and found that increasing simulations and training can be counterproductive and result in people clicking malicious links more often than individuals with little or no training. Just 11% of users with only one training session clicked on a phishing link, but 14% of users with five training sessions clicked on the link, according to Cyentia’s analysis. Phishing works because people filter what they see through their experiences and preferences, and these influence the choices they make. Cognitive biases take many forms, but SecurityAdvisor’s research identified five major types used in phishing attacks: halo effect, hyperbolic discounting, curiosity effect, recency effect, and authority bias. Halo effect, which refers to the individual having a positive impression of a person, brand, or product, is the type most commonly used by cybercriminals, appearing in 29% of phishing attacks. In this type of attack, a cybercriminal pretends to be a trusted entity to gain access. Cybercriminals targeting C-suite executives may send fake speaking invitations from reputable universities and organizations, for example. Hyperbolic discounting, or the inclination to choose a reward that gives immediate results rather than a long-term equivalent, appeared in 28% of phishing attacks analyzed by SecurityAdvisor. This can take the form of clicking on a link to get $100 off a MacBook Air, for example. Spammers have long used this tactic to lure victims with promises of free or exclusive deals. Curiosity effect, the desire to resolve uncertainty, rounded out the top three by appearing in 17% of phishing attacks. In this kind of attack, the C-suite executive may receive information about exclusive access to unnamed golf events, and a desire to know more about the event could make the executive more susceptible. IT teams may see phishing emails focused on things they are concerned about, such as securing the remote workforce and top trends in data analytics. The recency effect takes advantage of the tendency to remember recent events, such as using information about COVID-19 vaccinations in the subject lines of phishing emails. And finally, the authority bias is based on people’s willingness to defer to the opinions of an authority figure. An attacker using authority bias may impersonate a senior manager or even the CEO. For example, in organizations with “control-based cultures,” the authority bias means people will be less likely to question email messages that appear to be sent by the CFO instructing them to pay an invoice, for example, Venkataraman said. SecurityAdvisor found that C-suite executives are targeted 50 times more than regular employees, followed by people on IT security teams, who are targeted 43.5 times more than regular employees. The biases used are also different. Cybercriminals targeting C-suite executives tend to employ the halo effect or the curiosity bias, while the majority of scams against IT security teams employed the curiosity bias. There were also industry-specific differences. People in the health care industry were more likely to see scams employing authority bias, recency effect, and loss aversion, while retail employees are more likely to be targeted by the halo effect, curiosity bias, and hyperbolic discounting. Financial services employees were likely to see phishing messages employing the halo effect to appear as if they came from regulators and vendors, or authority bias to appear as if they were sent by the CEO or tax authorities. Technology can go only so far when it comes to filtering out these attack messages because they are designed to look legitimate. But training employees to simply not fall for these attacks is not the answer either. Instead, the goal is to help mitigate risky behaviors. One way to counter the effects of cognitive biases is to help employees recognize tricks when they are being used. Machine learning can help facilitate individual changes in employee behavior by providing constant reminders to apply this knowledge at the exact moment of risk, Venkataraman said. SecurityAdvisor’s platform fortifies people against these biases with “just-in-time” nudges, such as showing a quick refresher video when the platform detects the user had been targeted in an attack. The key message with these nudges is to remind employees they are part of the organization’s security infrastructure, Venkataraman said. Instead of saying that humans are the weakest link when it comes to corporate security, “we wanted to say humans are the strongest part of the security community.”"
https://venturebeat.com/2021/05/12/perceptive-advisors-closes-515-million-perceptive-xontogeny-venture-fund-ii-lp-%e2%80%8b/,"Perceptive Advisors Closes $515 Million Perceptive Xontogeny Venture Fund II, LP ​"," – PXV Fund II plans to lead Series A financings in 10 to 12 early-stage life sciences companies – – PXV Funds total $725 million of AUMs since launching strategy less than 18 months ago –  NEW YORK & BOSTON–(BUSINESS WIRE)–May 12, 2021– Perceptive Advisors, an investment management firm focused on the life sciences sector, today announced the first and final closing of Perceptive Xontogeny Venture Fund II, LP (“PXV Fund II”) with $515 million in capital commitments in an oversubscribed fund to further the firm’s early-stage life sciences platform. The close of PXV Fund II more than triples assets under management to $725 million in less than two years since the closing of the inaugural PXV Fund I, which launched Perceptive’s early-stage venture fund strategy. PXV Fund II includes existing investors from the inaugural fund, new investors including top-tier asset managers, endowments, foundations, family offices, and notable individual investors, and all of the PXV Fund II Partners. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210512005295/en/ The PXV Fund strategy complements Perceptive Advisors’ Life Sciences Fund (hedge fund) and Credit Opportunities Funds as it will be solely focused on early-stage venture capital and expects to be the sole or predominant lead investor in Series A financings in the range of $20-40 million with capacity to participate in subsequent Series B financings. The PXV Funds are led by Chris Garabedian, Portfolio Manager, who also serves as Chairman and CEO of Xontogeny. Xontogeny is a Boston-based accelerator that provides seed investments along with strategic and operational support to early-stage life sciences companies through its professional staff and network of consultants and service providers. The PXV Funds are focused exclusively on life sciences technologies across a broad range of therapeutic areas with a greater emphasis on drug technologies and a lesser focus on early drug and discovery platforms. The primary goal of PXV Funds’ Series A and subsequent Series B financings will be advancing products and technologies from preclinical stages through early clinical development and the generation of proof-of-concept data in patients. “With PXV Fund I, we were able to quickly identify and fund a number of opportunities that were high quality with tremendous potential for addressing important disease areas and the potential for generating outsized value creation. I’m excited that we will continue our funding of promising early-stage life science companies through clinical proof-of-concept with the PXV Fund II,” said Chris Garabedian. “The portfolio of opportunities in which we invested from PXV Fund I was quite diverse across modality and therapeutic area and we are pleased with the progress in advancing almost all of the portfolio companies from preclinical stages into clinical studies in less than 18 months from inception. We are grateful for the trust and enthusiasm that our limited partners, both new and existing, have shown in this strategy of product-focused, data-driven value creation.” “We have been impressed with the execution of the PXV Fund strategy and how quickly we found quality opportunities for investment,” said Adam Stone, Chief Investment Officer of Perceptive Advisors. “To further strengthen our efforts, Chris has successfully expanded the Xontogeny team to serve as Perceptive’s early-stage venture arm, which is fully integrated with the Perceptive investment team. Beyond evaluating companies seeking Series A investments, the Xontogeny team provides a robust pipeline of future opportunities for the PXV Fund through their sourcing, evaluation and incubation of startups with seed financing and active operational management.” The PXV Fund II Investment Committee consists of Joe Edelman, CEO of Perceptive Advisors, Adam Stone, Chief Investment Officer of Perceptive Advisors and Chris Garabedian, Portfolio Manager. Fred Callori, Xontogeny’s SVP of Corporate Development and Ben Askew, PhD, Xontogeny’s Head of R&D, are also Partners in PXV Fund II. About Perceptive Advisors Founded in 1999 and based in New York, NY, Perceptive Advisors is an investment management firm focused on supporting the progress of the life sciences industry by identifying opportunities and directing financial resources to the most promising technologies in healthcare. For more information about Perceptive, visit www.perceptivelife.com. About Perceptive Xontogeny Venture Funds The Perceptive Xontogeny Venture Funds are Perceptive Advisors’ investment vehicles focused purely on early-stage, private venture investments in life sciences companies. Primary investments for the Fund include companies that are seeking a lead investor for Series A financings, which include both companies that are seeded and incubated at Xontogeny, as well as companies that are seeded and incubated by other organizations, accelerators and seed investors. The PXV Funds are also open to participating in syndicated Series A financings as a co-lead or passive investor with other venture capital firms. For more information, visit www.perceptivelife.com. About Xontogeny Based in Boston, MA, Xontogeny seeks to accelerate the development of life science technologies by providing entrepreneurs with funding options as well as the leadership, strategic guidance and operational support necessary to increase the probability of success in early drug and technology development. Through a differentiated collaboration model, the Xontogeny team partners with founding scientists and entrepreneurs to support their vision while allowing a more efficient development model that benefits company founders and early equity holders. For more information, please visit www.xontogeny.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005295/en/ Media Contact: Katie GallagherLaVoieHealthSciencekgallagher@lavoiehealthscience.com +1 617-792-3937 Investor Contact: Patrick Morrow, Director of Investor Relations & MarketingPerceptive Advisorspatrick@perceptivelife.com +1 646-205-5344"
https://venturebeat.com/2021/05/12/juno-diagnostics-announces-closing-of-25-million-series-a-financing-led-by-perceptive-xontogeny-venture-fund/,Juno Diagnostics Announces Closing of $25 Million Series A Financing Led by Perceptive Xontogeny Venture Fund," – Reproductive health pioneers developing genetic testing solutions to dramatically improve access to prenatal care for all women in all geographies – – Experienced leadership team developed and launched the industry’s first noninvasive prenatal testing product – – Proceeds to support clinical validation studies, product development, and commercial launch of reproductive health offerings –  SAN DIEGO–(BUSINESS WIRE)–May 12, 2021– Juno Diagnostics (JunoDx), a company focused on improving access to vital genetic health information and led by industry experts, announced today the closing of an expanded Series A financing round of $25 million led by the Perceptive Xontogeny Venture (PXV) Fund. The capital will be used to support clinical validation studies, product development, and commercial launch of JunoDx’s product offerings for noninvasive prenatal testing (NIPT). “We are excited to partner with investors who share our vision of enabling access to highly accurate, cost-effective prenatal genetic testing solutions,” said Dirk van den Boom, Founder and Chief Executive Officer of JunoDx. “This funding allows us to advance the development of our clinical technology platform and rapidly bring our lead product to market. We are dedicated to bringing the most advanced and cost-effective NIPT solution to market in support of millions of families along their pregnancy journey.” With its lead product, JunoDx is ushering in the next generation of noninvasive prenatal tests. The Company’s NIPT solution simplifies sample collection and improves access to high quality genetic testing results without the high cost, long lead times, and phlebotomy requirements of traditional NIPT. Recent endorsements by professional medical societies have recognized the use of cell-free DNA (cfDNA) as the most sensitive and specific screening test for common fetal aneuploidies for all pregnant women, which drastically expands the addressable market. “With a highly experienced and motivated team, JunoDx is positioned to become the cfDNA testing market leader driven by a sea change in reimbursement and democratizing access via technology innovation,” said Chris Garabedian, Chief Executive Officer of Xontogeny and Manager of the PXV Fund for Perceptive Advisors. “We are excited to announce this investment to support the JunoDx team in their mission to address the unmet need for cost-effective genetic testing solutions. The team at Perceptive Advisors is confident in the abilities of the JunoDx team to validate and commercialize best-in-class products to provide customers with access to highly accurate genetic testing.” JunoDx was founded by an experienced team who developed and launched the industry’s first NIPT product while at Sequenom. The Series A financing was closed following the generation of a differentiated product offering and collection of promising clinical data. JunoDx is led by Dirk van den Boom Ph.D., Chief Executive Officer, Mathias Ehrich M.D., Chief Scientific Officer, and Jim Chauvapun, Chief Operations Officer. The Board of Directors comprises Chris Garabedian, Fred Callori, SVP Corporate Development at Xontogeny and Partner in PXV Fund, Dirk van den Boom, and Jim Chauvapun. About Juno DiagnosticsJuno Diagnostics is developing genetic testing solutions to improve access to vital health information. The Company’s lead product is an NIPT solution that simplifies sample collection and improves access to high quality genetic testing results without the high cost, long lead times, and phlebotomy requirements of traditional NIPT. The founding team worked together at Sequenom, Inc., pioneering the development and commercialization of the first noninvasive cell-free DNA based prenatal test in the United States. For more information, visit www.junodx.com. The Perceptive Xontogeny Venture FundsThe Perceptive Xontogeny Venture Funds are Perceptive Advisors investment vehicles focused purely on early-stage, private venture investments in life sciences companies. Primary investments for the Funds include companies that are seeking a lead investor for Series A financings, which include both companies that are seeded and incubated at Xontogeny and companies that are seeded and incubated by other organizations, accelerators and seed investors. The PXV Funds are also open to participating in syndicated Series A financings as a co-lead or passive investor with other venture capital firms. For more information, visit www.perceptivelife.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005283/en/ Juno Diagnostics: Jim Chauvapunjim@junodx.com Media: Anna ArmyLaVoie Health Scienceaarmy@lavoiehealthscience.com +1 508-471-8570 Katie GallagherLaVoie Health Sciencekgallagher@lavoiehealthscience.com +1 617-792-3937 Investors: Patrick Morrow, Director of Investor Relations & MarketingPerceptive Advisorspatrick@perceptivelife.com +1 646-205-5344"
https://venturebeat.com/2021/05/12/new-kpmg-survey-says-ai-adoption-is-picking-up-speed-but-is-it-moving-too-fast/,"New KPMG survey says AI adoption is picking up speed, but is it moving too fast?","Presented by KPMG COVID-19 left business leaders no choice. Across industries, organizations went all in on digital transformation. And along with the fundamental changes these companies made to their operating models, came an explosion in AI adoption. Business leaders seized the opportunity to level up their technology, and were forced to take a bold leap forward in their AI capabilities, says Dr. Sreekar Krishna, national leader for AI and head of data engineering at KPMG. “Some leaders saw that AI was on their road map perhaps two or three years down the line and made the important and necessary decision to accelerate its adoption now,” Krishna says. “The digital hurdles that they initially faced to AI adoption are now being removed very fast.” The recent survey from KPMG, “Thriving in an AI World: Unlocking the Value of AI across 7 Industries,” bears that out. Seventy-nine percent of executives say that they have a moderately functional AI strategy while 43% say it’s fully functional at scale. In the industrial manufacturing sector, 93% of executives say AI is at least moderately functional, in the financial services industry it’s 84%, and in technology, it’s 83%. Driving the adoption of AI is the fact that many institutions, especially financial services, retail, and the like, have been experimenting with AI technologies for the past few years, Krishna says. Now, with the dire need to productionalize their technologies, they are focusing on the biggest questions: Where can AI deliver value, and where is it getting ahead of us? Yet some leaders are worrying that acceleration is happening too quickly. The KPMG survey, which looks at how business leaders across seven industries (technology, financial services, industrial manufacturing, healthcare, life sciences, retail, and government) perceive AI’s value, as well as AI-related pain points, risks, and challenges, found that despite the positive perception of AI, more than one-third (44%) of the executives surveyed believe that their industry is moving too fast with AI technologies. Why the contradiction? “We think the belief some of our survey respondents have, that AI is moving too fast, can be bucketed into four categories,” Krishna says — the extraordinary pace of growth in AI capabilities, the ‘black box,’ lack of skills internally and externally, and overall digital readiness of the organization. First and foremost, leaders have seen what AI is capable of doing, Krishna says — use cases that demonstrate the power of the AI technologies to accomplish things that had previously only been seen in science fiction. This rapid pace of growth means that institutions are not able to keep up with their internal processes and skills to handle the societal demands, especially if something goes wrong with their AI-backed offerings. He points to very complex AI technologies like the GPT-3 model, which produces remarkably human-like text in a range of different styles and has been shown to develop everything from self-help blogs to short stories, songs, technical manuals, and far more. However, these technologies are already out there in the open-source world. And as Krishna points out, they could soon be embedded into ubiquitous tool chains in a matter of years, if not months — and the ramifications have not yet been considered. Much has been written (by humans) about the risk of the technology being used by bad actors — whether that’s in the service of fake news, or phishing scams. Krishna also cited the recent deep fake concerns about AI image and video manipulation. These concerns stem from the fact that it is becoming harder and harder to distinguish what was produced by a real human, to what was generated by a machine. AI-backed business tools, which are becoming incredibly common across enterprises, are finding their way into critical decision pipelines. This means black box algorithms could now become at least partially responsible for handing down sensitive verdicts, from loan and credit decisions to physical security solutions, and more. These decisions, based on potentially biased data and assumptions, can profoundly impact businesses and lives. Those black box algorithms are already giving decision makers headaches, if not nightmares. As AI becomes more ubiquitous in the tools their organizations use, concern is growing about lack of explainability. A decade back, as these algorithms were being developed and evolved, the decisioning use cases for them didn’t have serious consequences. For example, Krishna explains, “What’s really the worst thing that can happen on Netflix if they give you a bad recommendation? You watch a bad movie. On Amazon, it may recommend a bad product. The decision is still with the human, with the user. That’s the origin of a lot of these AI algorithms.” Now many of the CXOs Krishna interacts with regularly are asking questions. If an AI algorithm provides a negative score for a user, say at a financial institution, or an insurance company, they may not know how the algorithm arrived at that conclusion. “That’s the fear,” says Krishna. “As long as an algorithm is not able to explain itself, it’s hard for decision-makers to say, we’ll make a decision based on that number. And this is something I constantly talk to my clients about. In fact, this is one of the driving factors for many of our clients asking for some form of regulatory need to help explain what’s going on under the hood.” They want to know they can control outcome from the AI — if it’s controllable at all. If it’s embedded into the tool, how much compliance does the tool maker need to give the decision makers so that they can understand what that AI is doing, and be able to take responsibility for its decisions? Tomorrow, if one of their customers has a material impact, will they have to explain that it’s not because of a company policy, but because this tool that made a cryptic ruling? “I believe all of these are leading to that perception that our survey respondents shared where AI seems to be moving very fast,” says Krishna. C-suite and business leaders are realizing very fast that they lack the skilled resources that are necessary to leverage, productionalize and control the rapidly evolving AI technologies. Some of it comes from institutions not budgeting the necessary resources to reskill and upskill their own internal talent. There is also a general AI and Digital talent shortage in the market, including AI developers and engineers, AI researchers and data scientists. Further, most AI skills do not yet fit nicely into existing enterprise’s business operating models. “Too often my clients complain how they keep losing their AI talent within 6 months of hiring,” says Krishna. This is driven a lot by the fact that these skills are in high demand in the market, and if the institution hiring the talent has not changed their operating model to adopt the new digital and data skills, it will be hard to retain this talent pool. “Preparing the organization for the new talent first starts by upskilling the existing business talent to understand and appreciate the need for AI,” adds Krishna. “Too many times, organizations find themselves in situations where the newly hired talent does not understand the business need, while the existing business talent does not understand the need for a data scientist amongst them.” The growth of AI technologies has certainly been driven by the digital transformation trend of the past decade. Unfortunately, not enough institutions have spent the time and resources to accelerate towards a digital future. From business processes, to tool integrations, to work force shaping, organizations don’t feel ready for the digital future, especially one that is powered by AI. The C-suite execs and business leaders alike feel like they haven’t invested nearly enough in digital readiness. They increasingly feel as if their institution is falling behind, while the pace of AI innovation continues to accelerate. The fear is that they may not be able to catch up, finding that scaling even their smaller projects across the organization is far more challenging than they expected — especially if they haven’t yet set the groundwork for AI, from data mastery to cultural transformation. The chasm between traditional organizations, which may have spent their money doing business as usual, versus a startup where the VC money has gone exclusively into digital and AI initiatives — from the technology, to the data preparation, to attracting and training a workforce with the right skillset — is particularly wide. “A traditional financial organization compared to a fintech today — from a resource perspective, from a skill perspective, from a process perspective — they are literally worlds apart,” Krishna says. “Leaders know that. That’s another reason they feel like AI is moving too fast for them to keep pace with.” But despite those concerns, artificial intelligence is gaining a foothold in organizations of every size, as its promise becomes real every day, and the path to realizing AI’s promise becomes more clear. The survey found that nearly all executives surveyed are confident AI can help tackle their industry’s most pressing issues — such as detecting fraud (93%), and in their own organizations it’s adding even more value than was anticipated (60%). Most organizations are prioritizing AI education and skill set building for their employees, which is a fundamental need — if not the most foundational — for developing a successful AI strategy. “I’ve been talking to a lot of industry leaders today about digital training, digital education, AI education at different levels,” says Krishna. “You don’t need to teach linear algebra to everyone, but you can teach them how these systems work and interface.” It’s an issue so vital, as AI becomes embedded across businesses and lifestyles, that it needs to be tackled at the society level with increased investment in STEM education. “We don’t have the work force, the necessary skills,” explains Krishna. He emphasizes the importance of early education in programming and logic-based design, in preparing our next generation to understand the complex technologies growing up around them. It’s this investment that will truly realize the promise of AI. Dig Deeper: Read the entire 2021 KPMG study, “Thriving in an AI World.” Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/05/12/securityscorecard-taps-hackerone-to-bring-bug-bounty-data-to-security-ratings/,SecurityScorecard taps HackerOne to bring bug bounty data to security ratings,"HackerOne and SecurityScorecard have announced a platform integration that will showcase data from the ethical hacking community on a company’s digital scorecard. SecurityScorecard, for the uninitiated, is a cybersecurity rating and risk-monitoring platform major companies such as Nokia, AXA, and Liberty Mutual use to monitor and assess security throughout their supply chain, including weaknesses in third-party vendors. It’s kind of like a credit score rating for security. HackerOne, meanwhile, connects businesses with security researchers, or “white hat hackers,” who are financially incentivized to find software vulnerabilities before bad actors do. The HackerOne platform has powered bug bounty programs for major businesses, including Microsoft, Google, Intel, the U.S. Department of Defense, and Goldman Sachs. The San Francisco-based company recently touted major enterprise growth, with nearly half of its new sales stemming from businesses with over $1 billion in revenue. SecurityScorecard uses 10 broad risk categories as part of its rating system, including endpoint security, network security, DNS health, and patching cadence. It also uses a risk category it calls “hacker chatter,” which automatically collects and analyzes conversations from popular public hacker community channels, such as private forums, social networks, and internet relay chat (IRC). It’s all about finding mentions of a business and its associated digital properties to assess whether any potential undisclosed exploits are being discussed. This latest partnership with HackerOne builds on that basic concept, though it instead surfaces official bug bounty and vulnerability disclosure data gleaned from HackerOne’s API. For SecurityScorecard customers, a “hacker report” signal will appear on scorecards for companies that use HackerOne, though this is on an entirely opt-in basis. Enterprises will be able to see recent security issues involving companies in their supply chain and take appropriate action — with the ability to download a CSV file containing all of HackerOne’s findings. Perhaps more importantly, this goes some way toward helping interested companies become more transparent about their vulnerability disclosure activities and the current status of any identified flaws."
https://venturebeat.com/2021/05/12/xontogeny-announces-launch-of-several-new-early-stage-portfolio-companies/,Xontogeny Announces Launch of Several New Early-Stage Portfolio Companies," – Five new drug therapeutic companies supported with strategic and operational guidance and seed capital – – Programs include a diverse portfolio of therapeutic modalities across a wide range of disease areas –  BOSTON–(BUSINESS WIRE)–May 12, 2021– Xontogeny LLC, a Boston-based accelerator that provides seed-stage investments along with strategic and operational support to early-stage life science companies, unveiled five of its latest portfolio companies. These early-stage startups, developing promising therapeutics for a wide range of diseases, have received seed investment and collaboration from Xontogeny to advance their preclinical programs. Since inception in 2016, Xontogeny has supported more than 10 companies through incubation, including two Xontogeny portfolio companies that had liquidity events in Q1 2021. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210512005343/en/ “We’re helping founders build companies around life-saving technologies while showing that capital can be deployed more efficiently to benefit both founders and investors,” said Chris Garabedian, Chairman and CEO of Xontogeny. “Through smartly deploying seed-stage investments, providing early development expertise, and active collaboration on program development, we help entrepreneurs, founding scientists, and first-time CEOs create value for their life science startups. Additionally, Xontogeny’s team is fully integrated with Perceptive Advisors, serving as Perceptive’s early-stage venture arm. This unique partnership gives our companies access to capital from the Perceptive Xontogeny Venture Funds (the “PXV Funds”) and enables us to stay committed to our mission to support and advance life science technologies from early preclinical development through clinical proof-of-concept.” Xontogeny provides access to seed capital and strategic support to advance companies in preclinical stages through important de-risking activities and key milestones in preparation for the next stage of financing. Once achieved, Xontogeny-supported companies will be considered for Series A financings through Xontogeny’s affiliation with Perceptive Advisors and the PXV Funds. These complementary investment strategies enable the Xontogeny and Perceptive teams to support companies through the most challenging development stages: from early preclinical development to clinical proof-of-concept studies in patients. While the PXV Funds are focused on Series A and subsequent Series B financings, they are just some of Perceptive Advisors’ various investment vehicles that can support life sciences companies across all stages of their investment life cycle, including late-stage private/mezzanine/crossover financings, debt financings, SPACs, and IPO/follow-on public offerings. Perceptive Advisors recently announced the final close of an oversubscribed $515 million Perceptive Xontogeny Venture Fund II, LP (“PXV Fund II”), the raise coming less than 18 months after closing of Perceptive Xontogeny Venture Fund I, LP (“PXV Fund I”, collectively with PXV Fund II, the “PXV Funds”). Companies that are seeded and incubated by Xontogeny will be one source of prospective investments for PXV Fund II, along with direct Series A investments in companies that may have received investment capital and support from other seed funds, incubators, and accelerators in the life sciences sector. Xontogeny and the PXV Funds support opportunities across all therapeutic areas and drug modalities, including small molecules, peptides, antibodies, genetic technologies, and cell therapy. Additionally, the team can support company creation and provide financing for technologies beyond drug therapeutics, such as medical devices, diagnostics, and emerging healthtech opportunities. Two companies managed and supported by Xontogeny and PXV Fund I have already achieved public liquidity events: Landos Biopharma (NASDAQ:LABP) successfully completed its IPO in February 2021, and Quellis Biosciences was acquired by Catabasis Pharmaceuticals (NASDAQ:CATB) through a reverse merger transaction in January 2021. The recent additions to the Xontogeny seed-funded portfolio of companies are: About Xontogeny, LLC Based in Boston, MA, Xontogeny seeks to accelerate the development of life science technologies by providing entrepreneurs with funding options as well as the leadership, strategic guidance and operational support necessary to increase the probability of success in early drug and technology development. Through a differentiated approach, the Xontogeny team partners with founding scientists and entrepreneurs to support their vision while allowing a more efficient development model that benefits company founders and early equity holders. For more information visit www.xontogeny.com. About Perceptive Xontogeny Venture Funds The Perceptive Xontogeny Venture Funds are Perceptive Advisors investment vehicles focused purely on early-stage, private venture investments in life sciences companies. Primary investments for the Funds include companies that are seeking a lead investor for Series A financings, which include both companies that are seeded and incubated at Xontogeny and companies that are seeded and incubated by other organizations, accelerators and seed investors. The PXV Funds are also open to participating in syndicated Series A financings as a co-lead or passive investor with other venture capital firms. For more information, visit www.perceptivelife.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210512005343/en/ Media Contact: Katie GallagherLaVoie Health Sciencekgallagher@lavoiehealthscience.com +1 617-792-3937 Investor Contact: Patrick Morrow, Director of Investor Relations & MarketingPerceptive Advisorspatrick@perceptivelife.com +1 646-205-5344"
https://venturebeat.com/2021/05/12/cyber-risk-management-startup-visiblerisk-secures-25m/,Cyber risk management startup VisibleRisk secures $25M,"VisibleRisk, a joint venture between Moody’s Investors Service and VC firm Team8, today announced that it raised $25 million for its cyber risk evaluation platform. The proceeds — which come as VisibleRisk launches a new risk assessment service, Cyber Rating — will be used to expand the company’s workforce well into this year. It’s estimated that over 65% of security breaches are attributable to third-party failures. The pandemic has heightened the concern among legal and compliance leaders, 52% of whom worry about the risks posed by remote work. VisibleRisk claims its cyber ratings are based on “cyber risk quantification,” which allows companies to benchmark their risks against those of peers. Combining economic, cybersecurity, and industry data, VisibleRisk aims to arrive at a holistic, validated set of factors affecting a firm’s security posture — and to quantify those risks in economic terms. VisibleRisk offers real-time monitoring, custom reporting, and analysis, as well as transparency into the variables that determine a cyber rating. The platform takes into account multiple factors, including an organization’s susceptibility to attack given their business profile, the overall attractiveness to adversaries, and the strength of their security controls and mitigation efforts. Beyond this, VisibleRisk looks at a business’ ability to reduce and absorb the financial impact of a cyber attack through the use of mitigation activities. Insurance and cash reserves also affect cyber risk scores, as well as investments in activities like third-party oversight, security culture, and crisis response. VisibleRisk starts by understanding a company’s technology environment. After onboarding, it creates a project plan aligned with the customer’s requirements. Then VisibleRisk collects and analyzes internal and external data leveraging its toolset, applying algorithms and statistical models to generate a cyber risk rating. VisibleRisk also maps the results to commonly used frameworks so that security teams can align them with existing controls. And VisibleRisk reviews its findings with stakeholders to confirm the accuracy of the data, in collaboration with a customer’s board and executives. For example, VisibleRisk says it worked with Arvest Bank, a U.S. regional bank, to support the organization’s shift to cyber risk quantification (CRQ). Using CRQ, Arvest collaborated with VisibleRisk to measure the frequency and impact of cyber events in the context of its governance, defensive capabilities, and threat intelligence. Arvest then began the process to catalog, prioritize, and implement controls for reducing the impact of specific scenarios.  “VisibleRisk cyber rating and monitoring platform enables business leaders to better understand and manage cyber risk as they would financial risk, quantifying it in economic terms. This allows business leaders to track their cyber risk exposure and monitor the impact of changes to technology, policies, procedures, and the broader cyber risk landscape on their overall risk profile,” VisibleRisk cofounder and CEO Derek Vadala told VentureBeat via email. “At a time of intense digital transformation and a rising tide of threats combined with hybrid working globally, confidence in a business’ digital infrastructure is paramount.” The global cybersecurity market size is anticipated to reach $199.98 billion by 2025, according to the latest report by Market Research Future. VisibleRisk has competition in Viso Trust, which assesses third-party cybersecurity risk with AI. There’s also cybersecurity ratings platform SecurityScorecard. Another cyber risk management startup, RiskLens, recently raised $20.55 million. For Moody’s, the investment in New York-based, 44-employee VisibleRisk, which has around 10 customers, was funded with cash on hand. Team8 also participated."
https://venturebeat.com/2021/05/12/u-s-fcc-asks-what-impact-chips-shortage-has-on-communications-sector/,U.S. FCC asks what impact chips shortage has on communications sector,"(Reuters) — The Federal Communications Commission (FCC) on Tuesday said it was seeking comments on the impact the continuing global shortage of semiconductors is having on the U.S. communications sector. The supply shortage has especially hit the U.S. auto industry hard, the FCC noted, as automakers have sharply cut production because of a lack of chips. Ford Motor Co has warned the shortage could halve second-quarter production. An industry group warned in April that broadband providers were facing chip delays, resulting in delays delivering some cable TV boxes and seeing delayed “network switches, routers, and servers.” The group said semiconductors shortages “will result in hundreds of millions of dollars in impact to the broadband and cable television industry this year.” The FCC asks “what steps can be taken to prevent similar challenges in the future, particularly those challenges related to unanticipated, catastrophic, global events?” and to what extent shortages are driving “changes to the communications industry more broadly.” “We are pursuing a proactive strategy to help build a more secure, resilient, and next-generation communications supply chain,” said Acting FCC Chairwoman Jessica Rosenworcel in a statement. On Tuesday, some of the world’s biggest chip buyers, including Apple, Microsoft and Alphabet’s Google, joined top chip-makers such as Intel to create a new lobbying group to press for government chip manufacturing subsidies. The new group asked U.S. lawmakers to provide funding to boost U.S. chips production, for which President Joe Biden has asked Congress to provide $50 billion. Automakers want the Biden administration to secure chip supply for car factories but tech companies oppose favoring a single industry. Tech companies such as Apple are also being hit by the chip shortage, but less severely. The iPhone maker said last month it will lose $3 billion to $4 billion in sales in the current quarter ending in June because of the chip shortage, but that equates to just a fraction of the $72.9 billion in sales analyst expect for Apple’s fiscal third quarter, according to Refinitiv revenue estimates."
https://venturebeat.com/2021/05/12/snyk-bolsters-open-source-software-security-with-fossid-acquisition/,Snyk bolsters open source software security with FossID acquisition,"Open source software vulnerability scanning platform Snyk has acquired FossID, a Swedish startup that develops a software composition analysis tool for open source code. Though the two companies operate in the same space, bringing FossID under its wing will give Snyk greater coverage for open source license compliance issues and more extensive support for software written in C and C++. Snyk, which was founded out of London in 2015, helps developer teams find and address vulnerabilities and license violations in their open source codebases, containers, and Kubernetes applications via a giant internal database. The company counts high-profile companies such as Google, Twilio, Atlassian, and Salesforce among its customer base. Snyk currently supports dozens of languages, such as Java, JavaScript, Golang, Python, Ruby, and Scala. And although it already offered some support for C/C++, FossID will allow it to go deeper. C and C++ are popular with millions of developers and used partly or wholly in major applications ranging from Amazon and YouTube to Photoshop, as well as in a wide range of open source software, such as database management system MySQL, Firefox, Google’s Chromium browser, and myriad legacy applications. “It’s a broad ecosystem,” Snyk cofounder and president Guy Podjarny told VentureBeat. “This acquisition helps us reach all 6.3 million C/C++ developers and bring them the combined depth of analysis FossID offers with the great developer experience Snyk is known for.” Founded out of Stockholm in 2016, FossID has amassed a decent roster of customers, including Bosch, Ericsson, and companies from across the automotive, finance, and manufacturing spheres.  FossID claims to be adept at identifying vulnerabilities in “all forms” of open source, including small snippets that have been copied from an open source software package. Traditionally, this has been difficult to achieve at scale. “This acquisition will help Snyk identify ‘messier’ uses of open source,” Podjarny explained. “This includes binaries downloaded from the internet; snippets of code copy-pasted from StackOverflow into a commercial code base; or source code that was downloaded, modified, and then used.” FossID tracks 2 petabytes of open source code from its internal data warehouse and leverages AI to match code between that database and the customer’s own codebase. “This helps you find those pieces of open source, which in turn helps find and address vulnerabilities in them and track license issues to stay compliant,” Podjarny added. “This will be especially useful when securing embedded, gaming, trading, and legacy enterprise applications.” Put simply, bolstering its own data pool and diving deeper into C and C++ broadens Snyk’s horizons significantly. As a result of the acquisition, FossID will be integrated into Snyk Open Source, Snyk’s software composition analysis (SCA) product. It also follows a flurry of activity across the open source security and compliance landscape. Just last month, WhiteSource raised $75 million from prominent investors such as Microsoft’s M12, shortly after Snyk itself secured a fresh $300 million cash injection at a valuation of $4.7 billion. Earlier this week, cybersecurity giant Trend Micro announced a new partnership with Snyk to offer its own customers a product that gives security teams (rather than developers) insights into vulnerabilities and compliance risks across their open source code."
https://venturebeat.com/2021/05/11/onestream-data-analysis-ai-tools-usage-increased-in-2021/,"OneStream: Data analysis, AI tools usage increased in 2021","CFOs and other finance executives are optimistic that economic recovery is on the horizon: three-quarters (73 percent) expect that they will return to normal growth by the end of 2021, according to the latest Enterprise Financial Decision-Making Report from OneStream, a provider of corporate performance management solutions for mid-sized and large enterprises. Companies have significantly increased their data analysis tool investments and usage over the past year, the report found. OneStream’s study targeted finance leaders across North America and identified the factors driving their priorities, budgets and technology adoption plans for 2021. The survey found that the COVID-19 pandemic created a heightened need for agile forecasting, predictive planning and digital transformation. The ability to quickly reforecast budgets and shift workflows has become essential. The 2021 report found that finance executives have significantly increased their data analysis tool investments and usage. Companies commonly invested in artificial intelligence (59 percent) and increased their use of cloud-based planning and reporting solutions (65 percent). Most companies already use (69 percent) or plan to use (18 percent) low-code development platforms, which enable business users and citizen developers to take on new roles while circumventing complicated coding requirements. For return-to-office budgets, data privacy tools are the most common priority (18%), followed by hybrid cloud technologies. Compare the results with OneStream’s 2020 Enterprise Financial Decision-Making Report where less than half (46 percent) of the finance executives reported using cloud-based solutions regularly, while less than a quarter used machine learning (21 percent) and artificial intelligence (20 percent) solutions. Many finance executives are evaluating their workforce, technology and supply chain needs for a post-pandemic reality. However, the political and social landscape have also heavily impacted investment decisions, leading executives to prioritize sustainability and diversity initiatives as well. The commissioned study, conducted by Hanover Research in April of 2021, sourced insights from 340 finance decision makers in the United States, Canada and Mexico. All individuals hold management position (C-level executive (CFO), VP, Director, Controller) in finance. Respondents work at companies across numerous industries and varying revenues, with 24 percent employed by companies with over $1 billion in annual revenue. Read the full Onestream report Enterprise Financial Decision-Making Report 2021 — North America."
https://venturebeat.com/2021/05/11/digital-transformation-will-spur-economic-boom-in-2021-ceos-tell-gartner/,"Digital transformation will spur economic boom in 2021, CEOs tell Gartner","Chief executives around the world expect a return to strong economic growth over the next two years and are betting on digital transformation, AI technology, and corporate activism to help make it happen. Some 60% of CEOs polled for Gartner’s 2021 CEO Survey said they anticipate a return to economic growth this year and in 2022. That follows pandemic-ravaged global economic performance in 2020, the research firm said. Gartner on Tuesday released its annual survey, which over six months last year polled 465 CEOs and other senior business executives employed at companies of varying size, revenue, and industries located in North America, EMEA, and APAC. “CEOs’ top priorities for 2021 show confidence,” said Mark Raskino, research vice president at Gartner. “Over half report growth as their primary focus and see opportunity on the other side of the crisis, followed by technology change and corporate action.” “This year, all leaders will be working hard to decode what the post-pandemic world looks like, and redeveloping mid- to long-range business strategy accordingly. In most cases, that will uncover a round of new structural changes to capability, location, products, and business models,” Raskino said in a statement. Respondents cited business growth, technology change, and corporate actions such as mergers and acquisitions as the top three priorities for their companies over the next two years. Technology is a particularly strategic concern for CEOs — digital capabilities were the only area where a majority of respondents said they planned to increase investment in 2021. Gartner found that more CEOs than ever are citing digital change and investment as a priority for their organizations. When they gave answers about top strategic business priorities in their own words, 20% of CEOs used the word “digital,” up from 17% in 2020 and 15% in 2019. The unprompted citation of digitization as a priority has been steadily increasing in Gartner’s survey over the past several years, growing from just 2% of citations in 2012. Drilling down to specific technological areas where CEOs expect to invest, respondents cited AI as the “most industry-impactful technology” over the coming years, Gartner said. Some 30% of respondents said quantum computing would be “highly relevant” to their companies’ long-term plans, but a majority weren’t certain how that would look. Respondents also cited blockchain and 5G as technologies they were focused on. While a majority of CEOs polled did not have designated data officers such as chief digital officers or chief data officers, 83% of respondents said they employed chief information officers. A majority of CEOs surveyed by Gartner said their “top ask” of their CIOs is digitalization. The United States-China economic rivalry and trade relations between the countries was another area of concern for Gartner respondents. One-third of surveyed CEOs said that “evolving trade disputes between the two nations” over core technologies like AI and 5G were “a significant concern for their businesses.” Global CEOs also cited M&As and other corporate actions, social and environmental issues, and new workplace conditions resulting from the pandemic as primary areas of focus. Interestingly, fewer respondents than in previous surveys cited “sales revenue” as a growth priority, while more mentioned “new markets.” Gartner’s Raskino suggested that this shift, plus the increased emphasis on M&A opportunities, “shows that CEOs and senior executives seeking advantage from a cyclical downturn are going shopping for structural inorganic growth” rather than counting on incremental sales growth “using the strategies that have served them well in the past.” “‘Techquisitions’ can bolster digital business progress, while also providing access to potential fast-growth market sectors,” Raskino said. Meanwhile, more than 80% of CEOs believe “societal behavior change” taking place during the pandemic to become more or less the “new normal.” Most expect hybrid work-from-home arrangements to become permanent for many workers, while expenditures on travel-related activities will remain lower than before the pandemic. These developments, as well as nearly half of surveyed companies’ prioritization of sustainability to mitigate climate change, will further increase companies’ reliance on digital technology and digital channel flexibility in the coming years, said Kristin Moyer, Gartner research vice president. “This suggests that continuing to improve the way customers are served digitally will be vital,” Moyer said."
https://venturebeat.com/2021/05/11/appian-debuts-new-low-code-features-for-enterprise/,Appian debuts new low-code features for enterprise,"Software company Appian this morning unveiled the latest version of its low-code automation platform. The new release introduces AI-driven intelligent document processing (IDP) and developer collaboration features, as well as enhanced DevSecOps capabilities and what Appian calls “low-code data,” a code-free approach to unifying enterprise data. Research firm Gartner estimates the market for hyperautomation-enabling technologies will reach $596 billion in 2022, up nearly 24% from the $481.6 billion in 2020. As organizations look for ways to accelerate the digitization and structuring of data and content, technologies like document ingestion and natural language processing will remain in high demand.  Low-code data headlines the enhancements launched today. Using it, Appian customers can source data without needing to migrate and visually combine, extend, and model relationships between data. This lets them automatically optimize datasets for performance without coding or database programming. “Appian strips away the complexities of working with the most advanced automation technologies so we can focus on making our member experience the best it can be,” Matt Richard, CIO at Laborers International Union of North America (LiUNA) and an Appian customer, said in a press release. “We were able to build our first Appian robotic process automation process in just four days, integrated with our people processes and Appian AI. Simplifying data design is going to have a huge impact for us so I am looking forward to the new release with low-code data.” On the document processing side, Appian says its updated platform is capable of straight-through processing of large volumes of unstructured data. IDP features optical character recognition to extract data from documents without third-party software or services, and it ships alongside new low-code robotic process automation Windows actions and libraries of actions that can be downloaded directly from Appian’s marketplace. Studies like IBM’s Global AI Adoption Index survey support the notion that enterprise deployment of automation is increasing. Adoption is being driven by both pressures and opportunities, from the pandemic to technological advances that make AI more accessible. Indeed, a third of companies told IBM that they plan to invest in automation skills and solutions over the next 12 months. The new Appian platform release also lets organizations build and change apps and automations faster than before. Collaboration capabilities simplify the co-creation of apps while enhanced design guidance optimizes app performance, security, and testing. Meanwhile, new DevSecOps tools streamline the movement of software packages between development, test, and production environments. Appian says the latest version of its platform will be generally available in June 2021."
https://venturebeat.com/2021/05/11/widen-brands-struggle-to-use-product-data-to-its-fullest-potential/,Widen: Brands struggle to use product data to its fullest potential,"Although marketers consider product information critical to establishing trust and driving sales online, they struggle to use this information to its fullest potential, according to a new report from Widen, maker of digital asset management and product information management software. In the study, 36% of respondents claimed to have “very high” control over the product information their brand publishes on e-commerce sites, and 55% said they have “very high” control over the information presented on their own website. Product data establishes trust, while product marketing assets and content drives sales. Nearly 50% of respondents said product data is the information type that has the biggest impact on customer trust. However, 72% of respondents believe that digital assets like photography and videos, along with product marketing content like item descriptions, have the biggest impact on sales. These different types of product information appear to offer a superior buying experience when they are presented together rather than in isolation. With the ongoing challenges marketers and creatives face when trying to balance technology with human touch, digital asset management company Widen Enterprises aims to explore a new area on the topic of connectivity each year to connect the dots between marketing, information, and customer experience. This year, they delve into the topic of product information and its role in driving customer connection. Widen’s research team sought out to explore how brands have reached their audiences despite lockdowns, social distancing, and other challenges of the COVID-19 economy. A total of 155 leading marketers and creatives from the US and UK participated in the research study for this year’s Widen Connectivity Report. Findings are based on data gathered through a 21-question digital survey and a series of interviews conducted from August to September 2020. Read the rest of Widen’s research at 2021 Widen Connectivity Report: Connecting the dots between marketing, information, and customer experience."
https://venturebeat.com/2021/05/11/htc-vive-launches-2-vr-headsets-and-pro-tools-for-enterprises/,HTC Vive launches 2 VR headsets and pro tools for enterprises,"HTC Vive is aiming to take virtual reality for the enterprise to a new level with two VR headsets and other professional tools in a big product launch today. While consumer VR has proven to be a tougher market than expected, HTC has brought its Vive brand into businesses that need to cut training costs through immersive VR experiences. And that’s why it is introducing the HTC Vive Pro 2, a new VR headset that is connected to the PC, and the HTC Vive Focus 3, a standalone VR headset that is easier to take with you. With those two products, HTC is covering both the high end and the low end of the enterprise VR markets, which are humming along because enterprises don’t mind paying higher prices for VR gear that can save them millions of dollars a year in training costs. Consumers, meanwhile, have been reluctant to buy pricey headsets and instead are opting for the wireless Oculus Quest 2, which sells for as little as $300, while they’re waiting for true mainstream adoption. HTC teamed up with Valve, the maker of SteamVR, to make the first Vive headset in 2015. Since that time, HTC has seen VR grow in business markets such as aerospace, public safety, health care, education, and automotive. Lately, it’s been a struggle to keep the headsets in stock, the company acknowledged. The original Vive Pro and Vive Focus debuted for the enterprise in 2018. “In terms of driving into business and enterprise, this isn’t a pivot for HTC,” Dan O’Brien, global head of enterprise at HTC Vive, said in an interview with VentureBeat. “This is a continuation of something that we’ve been working on from day one.” Meanwhile, Talespin Reality Labs, a spatial computing company focused on workforce knowledge transfer and skills insights, announced the debut of CoPilot Designer, a no-code authoring tool for immersive soft skills learning content available on PC and Mac devices. In addition, Talespin came out with new features for its skills data platform and released the new Talespin App, enabling organizations and learners to easily access learning content across XR devices. These additions to Talespin’s product ecosystem integrate with its recently announced desktop streaming capability and expanded content library to offer an end-to-end platform for content creation, distribution, and skills data. Talespin argues that VR helps improve learning in the enterprise. HTC Vive unveiled the headsets at its online ViveCon 2021 event. Both headsets feature 5K resolution, a 120-degree field of view, and real RGB sub-pixels. The Vive Pro 2 will be available for preorder today and will go on sale on June 4, while the Vive Focus 3 will be on sale on June 27. Also at ViveCon, HTC unveiled Vive Business: a range of new tools designed specifically for businesses of all sizes to exploit VR. While HTC has focused on the enterprise, it also said that the Vive Pro 2 pushes the boundaries of PC VR for “incredible gaming, creating, and experiences,” while Vive Focus 3 redefines business VR with a purpose-built, all-in-one platform. “The first area that we wanted to drive into was fidelity and immersion,” O’Brien said. “So we’re introducing the new Focus 3 with a 5K with two 2.5K panels.” He said the OLED panel can push 36 million pixels. “It comes down to really strong graphics, and high field-of-view, for those simulation training sessions,” O’Brien said. “You can see everything in a more detailed layer.” It’s a big bet by Cher Wang, CEO of HTC. In a statement, she said it was a major milestone in the quest to create the best immersive experiences. The headsets were designed from the ground up to deal with professional challenges using the latest technology available, she said. She said the ViveCon event itself was a sign of how virtual the world has become in the past year due to the pandemic. The HTC Vive Pro 2 takes PC VR to the next level for high-end gaming. The 5K resolution display delivers 2.5K to each eye, with a 120-hertz refresh rate and fast-switching panel with real RGB sub-pixels. The field of view is now 120 degrees, which means you can see more of your peripheral vision while wearing the headset. And it has a dual stacked-lens design with minimal motion blur. The dreaded “screen door effect,” where it looks like you are looking through a screen door when wearing the headset, is virtually gone now. HTC worked closely with Nvidia and Advanced Micro Devices to use Display Stream Compression for the first time in a VR headset. Display Stream Compression ensures maximum visual quality and is also backwards compatible with DisplayPort 1.2, so even graphics cards that supported Vive Pro will see a benefit with Vive Pro 2. Vive Pro 2 has fine-adjustable inter-pupillary distance (IPD, or the space between your eyes), evenly distributed weight balance, adjustable head strap, and a quick-adjustable sizing dial. It also has 3D spatial sound with Hi-Res Audio Certified headphones and supports third-party headphones. All Vive SteamVR ecosystem accessories will work with Vive Pro 2 – Vive Trackers of any generation, the new Vive Facial Tracker, and more. There’s a facial tracker that captures about 38 different datapoints on your face. Vive Pro 2 will slot into an existing SteamVR setup, whether it’s Base Station 1.0 or Base Station 2.0, Vive Wireless Adapter, Vive controllers, or even controllers and gloves like Valve’s Index ‘knuckle’ controllers. For upgraders, Vive Pro 2 in the headset-only version is available for preorder from 10 a.m. Pacific time today. As a thank you for loyal users, Vive is running a promotion during the preorder period, offering a discount for those who want to upgrade, making Vive Pro 2 available for $749. The full-kit Vive Pro 2, which includes Base Station 2.0 and Vive Controllers, is available from June 4 for $1,400. The new all-in-one Vive Focus 3 is a business VR headset for those who don’t want to be tied down with wires and instead prefer the freedom of a wireless solution that doesn’t have to tap the power of a PC. The Vive Focus 3 has 5K resolution with dual 2.5K displays, a 90-hertz refresh rate, and a 120-degree field of view for better immersion. The fast-switching display panel uses real RGB subpixels, also eliminating the screen door effect. The new visuals mean fine details like writing and overall fidelity are dramatically clearer, allowing for software design and user interaction to be more natural. “About 90 hertz is the minimum bar to deliver solutions with the right level of comfort,” O’Brien said. Vive Focus 3 delivers comfort with a new strap design, balanced weight distribution, and a swappable curved battery pack. Vive Focus 3’s battery pack can be changed in seconds, allowing you to keep going. Quick charge gives you 50% battery from 30 minutes of charging, and an LED indicator lets you know how much power you have left. Durable and lightweight, the magnesium alloy frame of Vive Focus 3 is 20% lighter and 500% stronger than traditional plastics. Vive Focus 3 has a wide range and fine-adjustable IPD, as well as a quick-release button and easily removable magnetic front and rear face gaskets. That’s good if there are multiple users. Vive Focus 3 has new open-back speakers featuring a pair of dual drivers, delivering immersive and true-to-life audio. They are contact-free, which means users can still maintain environmental awareness while staying immersed in VR. For peace of mind in VR meetings, a special audio privacy setting dramatically reduces the risk of sound being overheard by people nearby. Vive Focus 3 uses an AI-powered inside-out tracking algorithm for precise tracking (this means you don’t have to set up sensors around the room), with privacy addressed by storing all tracking data in an encrypted format on the headset. The redesigned controllers are intuitive to use — one of the lightest six-degrees-of-freedom controllers on the market, and it lasts for 15 hours on a single charge. Hand tracking support will be released in the future. Powering Vive Focus 3 is a specially optimized Qualcomm Snapdragon XR2 platform, combined with a copper heat pipe and cooling fan. The all-new Vive Reality System 2.0 delivers a more streamlined and professional environment. Vive Focus 3 will be on sale June 27 for $1,300. Vive Business is a complete suite of software and services, supporting the needs of businesses looking to get the best out of XR. Vive Business is scalable and secure, making life easier for deployment, maintenance, remote support, and training needs, the company said. “We’ve been using it for business meetings, brainstorming, design reviews, and client interaction,” O’Brien said. “And we could also support press conferences.” More than 50 third-party software developers are signed up to work with HTC, and 20 applications will be ready at launch. The Vive Business Device Management System is an ISO-certified mobile device management (MDM) system that allows IT to quickly and easily see the status of each Vive Focus 3 on the network, remotely install new business apps, and update software. Vive Focus 3 is designed to work with Android Enterprise MDM, so it can also slot into pre-existing MDM solutions already active in your environment. Building on the consumer Viveport store, the Vive Business AppStore is a curated collection of apps and tools, covering diverse areas like training, communication, and visualization. Vive Business Training supports training sessions of any size. The training leader can observe the progress of each trainee via an Android device, highlighting the next steps needed and talking the trainee through it, even in a class size of hundreds. Vive Business Streaming supports connecting Vive Focus 3 to a PC via a cable, with fully wireless streaming support coming in the future. Vive Business will see new tools and features added in the future, delivering a continuously updated powerful suite of business solutions. The Vive Business platform has tools such as a digital whiteboard for a classroom setting, as well as ways to collaborate on a design together. Finally, Vive Sync lets people meet in VR with realistic avatars to discuss business, host presentations, examine 3D models, and teach a class. Originally built as an internal communications tool to support collaboration on product design, Vive Sync was launched in beta in June 2020. Now, having been used in almost 100 countries around the world, Vive Sync is launching in Pro and Enterprise versions, offering different new environments, tools, and ways to interact. You can bring everyday files into VR, from Microsoft PowerPoint presentations to 3D formats. Vive Sync is good for groups of up to 30 people; for larger groups, Vive’s XR Suite offers a range of different scenarios. You can see a bunch of partner testimonials from companies such as Qualcomm, Dassault Systemes, Autodesk, Epic Games, Adobe, Enscape, Twinmotion, Axon, Penumbra, and Zaha-Hadid Architects here."
https://venturebeat.com/2021/05/11/edge-ai-chipset-developer-sima-ai-raises-80m/,Edge AI chipset developer Sima.ai raises $80M,"Sima.ai, a company developing embedded edge hardware for machine learning applications, today announced that it raised $80 million in a series B round led by Fidelity Management & Research Company. The startup says that the funds will be used to commercialize its first-generation system-on-a-chip product, as well as to jumpstart development of its second-generation product’s architecture and supports Sima.ai’s go-to-market, customer success, and hiring initiatives globally. Edge computing is forecast to be a $6.72 billion market by 2022, according to Markets and Markets. Its growth will coincide with that of the deep learning chipset market, which some analysts predict will reach $66.3 billion by 2025. There’s a reason for these rosy projections — edge computing is expected to make up roughly three-quarters of the total global AI chipset business in the next six years. After emerging from stealth in late 2019, Sima.ai unveiled what it calls its “machine learning system-on-chip” platform: an AI accelerator chipset designed with low power requirements and support for fast inferencing. The company says that its hardware’s performance ranges between 50 TOPS (theoretical operations per second) to 200 TOPS at 5 watts to 20 watts, delivering what Sima.ai claims is an “industry first” of 10 TOPS per watt.  “[Our chip] combines traditional compute IP from Arm with our own machine learning accelerator and dedicated vision accelerator … By combining multiple machine learning accelerator mosaics via a proprietary interconnect, we can scale from 50 TOPs at 5 Watts up to [a theoretical] 400 TOPs at 40 Watts,” Kavitha Prasad, VP of business development and system applications at Sima.ai, explained in a blog post last year. “While it’s capable of a wide range of ML workloads such as natural language processing, SiMa.ai’s [chip] is initially optimized for computer vision applications.” Sima.ai aims to work with customers in robotics, smart cities, autonomous vehicles, medical imaging, and government. The company claims to have completed several early customer engagements and recently announced the opening of a design center in Bengaluru, India, which Sima.ai says will support engineering and operations while launching job opportunities for board development, operations, infrastructure, and system application roles. “The embedded edge is a multi-trillion dollar market and still using decades old technology. Sima.ai is poised to disrupt this massive market with our differentiated machine learning technology and approach,” founder and CEO Krishna Rangasayee said in a press release. It’s worth noting that Sima.ai has plenty in the way of competition. Startups AIStorm, Hailo, Esperanto Technologies, Quadric, Graphcore, Xnor, and Flex Logix are developing chips customized for AI workloads — and they’re far from the only ones. Mobileye, the Tel Aviv company Intel acquired for $15.3 billion in March 2017, offers a computer vision processing solution for AVs in its EyeQ product line. And Baidu last July unveiled Kunlun, a chip for edge computing on devices and in the cloud via datacenters. But Sima.ai appears to be well-capitalized, with $120 million in funding to date, having closed a $30 million financing round in May 2020 led by Dell Technologies Capital. The company plans to tape out its chipset early this year with the goal of delivering engineering samples and potentially customer samples toward the end of 2021. Besides Fidelity, Adage Capital Management, Amplify Partners, Dell, Wing Venture Capital, Alter Venture Partners, and +ND Capital participated in San Jose, California-based Sima.ai’s series B."
https://venturebeat.com/2021/05/11/continuous-integration-and-delivery-platform-circleci-raises-100-million/,Continuous software integration/delivery platform CircleCI nabs $100M,"CircleCI, a continuous integration and delivery (CI/CD) platform for developer teams, today announced it has raised $100 million at a $1.7 billion valuation. Alongside its funding, CircleCI announced it has acquired Vamp, a cloud-native release orchestration platform that automates facets of the software release and rollback process. Continuous integration is concerned with allowing multiple developers to frequently push out small changes to a shared code repository and test for product-readiness. Then there’s continuous deployment, which is all about automatically releasing the quality-checked code to the final product in small batches. Companies can adhere to a continuous integration ethos but not continuous deployment if they prefer a manual approach to shipping final code changes, which falls into a category known as continuous delivery. That, essentially, is what CircleCI is all about — it integrates with various tools and platforms such as GitHub, Bitbucket, and Slack to enable developers to automate many of their software engineering processes, monitor the quality of their code, and enable swift rollbacks if flaws are found. It helps developers move fast without breaking anything major. “We build for the builders of the digital age — developers,” CircleCI CEO Jim Rose told VentureBeat. “Our goal is to help developers deliver high-quality code quickly.” CircleCI’s latest raise comes at a time of heightened activity across the developer space, particularly in the CI/CD sphere. Indeed, Jfrog went public on the Nasdaq in September, while CircleCI rival Harness raised $115 million at a $1.7 billion valuation (the same as CircleCI) back in January. Other notable players in the space include CloudBees, which has created a commercial, enterprise-focused product built on top of the open source Jenkins project. Founded in 2011, San Francisco-based CircleCI claims a number of notable customers, including Facebook, Spotify, and GoPro. The company had previously raised $215 million, and with another $100 million in the bank it’s going to double down on its existing product and growth — specifically across three key areas, including “managing software complexity, continuous validation, and data and insights.” This builds on other new features CircleCI has recently introduced, including an insights dashboard that gives its cloud-based customers more data to help track the status of their projects and see which jobs are failing, which workflows take the longest, and more. CircleCI launched an ecosystem product called Orbs in 2018 that allowed developers to share reusable snippets of code that automate repeatable software development processes. It’s basically an open source approach to sharing solutions to common problems, freeing businesses to dedicate more resources to solving unique business-critical issues. In February, CircleCI launched private orbs, which allows developers to privately share configuration code internally across projects — this might be particularly useful in industries with high privacy or compliance standards, such as finance or health care.  For the future, Rose laid out a vision whereby CircleCI further capitalizes on its “network effect and shared ecosystem of builders” to generate insights and data for developers. This is part of what its Vamp acquisition will enable. “The way we build today is more interconnected than ever,” Rose said. “Sources of change no longer exist solely in a repository, making it impossible for a single developer to understand the entire process. The acquisition of Vamp will allow us to go farther into production and bring feedback and data from users into the CI/CD feedback cycle.” CircleCI already releases research based on its vast banks of data via its annual State of Software Delivery report, which establishes benchmarks for engineering team performances. This sheds light on some of the areas CircleCI may venture into in the future. One example Rose provided is a situation in which a business sets key performance indicators (KPIs) around software tweaks and changes — “Does this change decrease shopping cart abandonment rates?” — and can automatically roll back these changes if they don’t meet KPI stipulations. “Over time, we aim to capitalize on our knowledge of how the best teams build so that we can proactively help teams manage complexity and avoid pitfalls other teams have seen,” Rose said."
https://venturebeat.com/2021/05/11/open-source-api-gateway-krakend-lands-at-the-linux-foundation/,Open source API gateway KrakenD lands at the Linux Foundation,"The Linux Foundation has added another new entity to its growing arsenal of projects, today revealing that open source stateless API gateway KrakenD is now under its wing as the newly rebranded Lura Project. There, it will serve as the “only enterprise-grade API gateway hosted in a neutral, open forum,” according to a statement issued by the Linux Foundation. Application programming interfaces (APIs) are what enable different pieces of software components to connect to each other and share data. The economy around APIs is thriving, driven in part by organizations’ shift from tightly woven monolithic applications to “microservices.” By splitting applications into smaller segments that connect by APIs, businesses and developers have more agility in terms of maintaining and scaling their software. Moreover, it allows them to tap domain-specific expertise from third parties — rather than having to build the entire infrastructure to offer in-app video messaging, for example, they can use purpose-built APIs created by specialists instead. However, the fact that a single application may siphon from dozens of internal or external APIs has opened the door to a billion-dollar API management market, with myriad tools designed to help developers create and deploy APIs, control and monitor access, analyze data, and more. Some of the notable players in the space include Apigee, which Google acquired for $625 million back in 2016; Mulesoft, which Salesforce shelled out $6.5 billion for shortly after; and Kong, a venture-backed startup that recently raised $100 million at a $1.4 billion valuation. Nestled within the broader API management segment is something known as API gateways, which have emerged as essential tools for bridging cloud apps and services. They serve as a unified interface for managing incoming API requests and connecting them to the correct destination while supporting functionalities across security, authentication, load balancing, and more. While many of the popular API gateways are operated by commercial entities such as Google (Apigee), Salesforce (Mulesoft), and Amazon (it launched API Gateway back in 2015), KrakenD has been available under an open source license since 2016, allowing anyone to build robust API gateways. During that time, it has garnered a fairly impressive roster of users, including Oracle, eBay, and Nvidia, with 1.8 million servers leveraging the technology every month. KrakenD is the result of four core developers, plus some fifty other contributors. KrakenD cofounder and CEO Albert Lombarte also cofounded a separate company called Brutale, which markets a commercial enterprise version of KrakenD. The Linux Foundation, for the uninitiated, is a not-for-profit consortium founded two decades ago to support and promote the commercial growth of Linux and other open source technologies. The organization hosts various projects spanning just about every sector, including automotive, wireless networks, and security. Just last week, it launched an open source agriculture infrastructure project called the AgStack Foundation. Although all these various projects have different users and use cases, they share a common thread — they serve specific industries with open technologies that aren’t tied to a single commercial company. They also typically attract strong buy-in from companies that are keen to avoid proprietary software or trying to develope their own rival proprietary software. This in turn creates a healthier ecosystem, with competing platforms and services. KrakenD will continue to exist as it is today, but it’s now donating its core framework/library to the Linux Foundation — what will be known as the Lura Project. Its core mission is to offer an “extendable, simple, and stateless high-performance API gateway framework,” either for on-premises or cloud deployments, according to Lombarte. “Rather than solving a specific use case, Lura provides a library of components, a framework for assembling them into custom API gateway systems like a Lego set,” Lombarte told VentureBeat. The Lura Project also launches in partnership with a number of companies, including billion-dollar API development platform Postman; Ohio State University-backed 99P Labs; Hepsiburada; Open Room; and Stayforlong. KrakenD’s current maintainers will serve as the steering committee for the Lura Project."
https://venturebeat.com/2021/05/11/anvilogic-raises-10m-to-scale-no-code-cyberattack-detection-platform/,Anvilogic raises $10M to scale no-code cyberattack detection platform,"Cybersecurity detection automation company Anvilogic today announced a $10 million series A round led by Cervin Ventures. CEO Karthik Kannan says the capital will be put toward scaling and R&D. In a 2017 Deloitte survey, only 42% of respondents considered their institutions to be extremely or very effective at managing cybersecurity risk. The pandemic has certainly done nothing to alleviate these concerns. Despite increased IT security investments companies made in 2020 to deal with distributed IT and work-from-home challenges, nearly 80% of senior IT workers and IT security leaders believe their organizations lack sufficient defenses against cyberattacks, according to IDG. Anvilogic is a VC-funded cybersecurity startup based in Palo Alto, California and founded by former Splunk, Proofpoint, and Symantec data engineers. The company’s product, which launched in 2019, is a collaborative, no-code platform that streamlines detection engineering workflows by helping IT teams assess cloud, web, network, and endpoint environments and build and deploy attack-pattern detection code.  Anvilogic is designed to provide improved visibility, enrichment, and context across alerting datasets, enhancing the ability to aggregate, detect, and respond using existing data. The platform provides a continuous maturity scoring model and AI-assisted use case recommendations based on industry trends, threat priorities, and data sources. Using Anvilogic, security teams can visualize suspicious activity patterns and synchronize content metadata for detection and alerting. As Kannan explained to VentureBeat via email, the Anvilogic platform has four key functionality focus areas. The first is automated assessment of state of security, which spans the ability to automatically score a customer’s security readiness with a metric, along with a gap analysis. This capability provides AI-driven prioritization to guide the customer on where to start and when to go deeper, based on criteria such as their industry, the current landscape, peer behavior, available data sources, current gaps, and more. The platform’s next area is automation of detection engineering, which includes AI-based suggestions for security teams, data sources, a no-code build environment to construct detections, and an integrated workflow for task management and detection deployment. Then there’s automation of hunting and triage, where AI-based correlations of signals produce higher-order threat detection outcomes, which provide the entire story of an alert. Anvilogic auto-enriches alerts based on a hunting and triage framework. The final piece is ongoing learning across enterprises to learn new workflows, patterns, and actions and to provide the entire network better insights and recommendations for detections, hunting, and triage. “All use cases are connected and have a smooth handoff via a task management workspace, along with baked-in access controls such that the entire detection engineering and hunting/triage process is automated by the platform,” Kannan said. “The user experience is guided by our intuitive and domain-driven user interface, and the maturity score provides users guidance on what to build/deploy and also serves as a tracker of progress and gaps.” Reflecting the pace of adoption, the AI in cybersecurity market will reach $38.2 billion in value by 2026, Markets and Markets projects. That’s up from $8.8 billion in 2019, representing a compound annual growth rate of around 23.3%. Just last week, a study from MIT Technology Review Insights and Darktrace found that 96% of execs at large enterprises are considering adopting “defensive AI” against cyberattacks. “Our vision is to deliver complete automation to the security operation center (SOC) in the emerging cloud-first world and deliver what we call SOC neutrality. We believe that all logging will be on a distributed cloud warehouse in the future, and there will be even more silos of alerts and workflows (e.g., primary on-premises logging, traditional network workloads, and newer cloud workloads) in the SOC,” Kannan said. “Anvilogic will become the unified security fabric that delivers total end-to-end SOC automation across silos, successfully delivering detection and hunting capability by correlating across workloads, powered by AI, domain-specific frameworks and automation.” Beyond Cervin, Foundation Capital, Point 72 Ventures, and Dan Warmenhoven participated in 25-employee Anvilogic’s latest funding round. It brings the company’s total raised to date to over $15 million."
https://venturebeat.com/2021/05/11/cibc-innovation-banking-provides-vertu-capital-with-a-capital-call-line-of-credit/,CIBC Innovation Banking provides Vertu Capital with a Capital Call Line of Credit,"TORONTO–(BUSINESS WIRE)–May 11, 2021– CIBC Innovation Banking is pleased to announce it has provided financing solutions, including a Capital Call Line of Credit, to Vertu Partners Fund I L.P. (“Vertu Fund I”), a private equity fund managed by Vertu Capital (“Vertu”). The capital call facility provides Vertu Fund I with the flexibility to make investments in portfolio companies prior to calling capital from the fund’s limited partners. Founded in 2017 by veteran technology private equity investor Lisa Melchior, Vertu was established to fill a gap in Canada’s growing technology sector between early-stage growth and large buyouts. Vertu brings decades of global experience and deep sector-focused expertise to high-potential and scaling technology companies seeking transformational growth. “Vertu’s core vision is to assist Canadian technology companies in scaling and growth,” said Rob Rosen, Managing Director in CIBC Innovation Banking’s Toronto office. “It’s a vision shared at CIBC Innovation Banking as well, and we look forward to working with the Vertu team to help Canadian software businesses grow.” “Vertu has a long-standing relationship with CIBC Innovation Banking and we are very excited to continue working together,” said Kimberly Davis, Partner & COO. “The team understands our business and listened closely to our needs, ultimately offering a tailored facility that will allow Vertu to effectively manage its business.” About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Vertu Capital Founded in 2017 by veteran technology investor, Lisa Melchior, Vertu Capital is a private equity manager focused on Canadian technology companies with proven technology, scalable business models, and established management teams primed to further grow and scale their businesses. Vertu has decades of global private equity experience with deep domain expertise in software and seeks to actively collaborate with founders and management teams to achieve great things at a rapid pace. For more information, please visit www.vertucapital.ca.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210511005113/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609 "
https://venturebeat.com/2021/05/11/verbit-acquires-transcription-service-provider-vitac/,Verbit acquires transcription service provider VITAC,"AI-powered transcription platform Verbit today announced it has acquired VITAC, the largest provider of captioning products and solutions in North America. Verbit says the deal will position it as one of the top firms in the professional transcription and captioning market across legal, media, education, government, and corporate sectors. The voice and speech recognition tech market is anticipated to be worth $31.82 billion by 2025, driven by new applications in the banking, health care, and automotive industries. Indeed, the enterprise is experiencing an uptick in voice technology adoption during the pandemic. In a 2020 survey of 500 IT and business decision-makers in the U.S., France, Germany, and the U.K., 28% of respondents said they were using voice technologies and 84% expect to use them this year. Verbit says VITAC, which was founded in 1986, will enable it to offer a “deeper” portfolio of transcription solutions, including experts, AI-based tools, additional languages, and integrations with video cloud platforms. VITAC also provides access to a 1,700-organization customer base spanning every broadcast company, most cable networks, program producers, corporations, educational institutions, and government agencies. Canonsburg, Pennsylvania-based VITAC, which has over 700 employees, estimates that it captions over 550,000 live and 75,000 prerecorded hours of programming per year. “We’re delighted to join the Verbit family and bolster their leading position in the transcription industry globally,” VITAC CEO Chris Crowell said in a statement. “We’ve been incredibly impressed with Verbit’s rapid growth and technology advantages, and together we look forward to serving more of this dynamic industry with clients across all vertical segments.” Verbit’s voice transcription and captioning services aren’t novel — well-established players like Nuance, Cisco, Otter, Voicera, Microsoft, Amazon, and Google have offered rival products for years, including enterprise-focused platforms like Microsoft 365. But Verbit’s adaptive speech recognition tech can generate transcriptions it claims offer over 99.9% accuracy. Verbit customers first upload audio or video files to a dashboard for AI-guided processing. Then a team of over 33,000 human freelancers in over 120 countries edits and reviews the material, taking into account customer-supplied notes and guidelines. Finished transcriptions from Verbit are available for export to services like Blackboard, Vimeo, YouTube, Canvas, and BrightCode. A web frontend shows the progress of jobs and lets users edit and share files or define the access permissions for each, as well as adding inline comments, requesting reviews, or viewing usage reports. As of November 2020, Verbit had over 400 educational institutions and commercial clients, including Harvard, the NCAA, London Business School, Fashion Institute of Technology, Stanford, Coursera, and Udacity. Revenue is in the “millions,” and the company is cash flow positive, Verbit CEO Tom Livne told VentureBeat in a previous interview. Verbit recently closed a $60 million funding round led by Sapphire Ventures. The Tel Aviv- and New York-based company’s total raised stands at over $100 million. “We are thrilled to further strengthen our position as the market leader in the transcription and captioning industry, in partnership with VITAC. This opportunity allows us to expand our offerings for the media vertical and provide advanced transcription capabilities to our current education, legal, and corporate customers,” Livne said in a press release. “The combined company will harness decades of transcription and captioning expertise to offer customers a best-in-class solution based on our proven technology. We will continue to invest in our platform, top talent, and domain expertise to evolve and develop our solutions to meet our customers’ dynamic needs.”"
https://venturebeat.com/2021/05/11/cycode-raises-20-million-series-a-round-from-insight-partners-to-secure-devops-pipelines-and-prevent-code-tampering/,Cycode Raises $20 Million Series A Round From Insight Partners to Secure DevOps Pipelines and Prevent Code Tampering," Cycode also launches knowledge graph that correlates data across the software development lifecycle to protect against rising threats like software supply chain attacks, source code leakage, hardcoded secrets and Infrastructure as Code misconfigurations  SAN FRANCISCO–(BUSINESS WIRE)–May 11, 2021– Cycode, an innovator in securing DevOps pipelines, today announced a $20 million Series A round led by Insight Partners, with participation from seed investor, YL Ventures. The new funding brings total investment to $25 million and positions Cycode to accelerate growth into securing enterprise DevOps tools such as source control management systems, build systems and cloud infrastructure. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210511005231/en/ In addition to the Series A funding, Cycode also announced the signing of new customers including: Grubhub, Databricks, Flexport, Rapyd, Copart and Cobalt. Further, Cycode has hired Dor Atias as VP of R&D, Tom Kennedy as VP of Sales and Andrew Fife as VP of Marketing. As the Software Development Lifecycle (SDLC) has become faster and more automated, slow application security processes have often been deprioritized in favor of new feature velocity. Additionally, many of the new tools that drive the automation and efficiency in application development have opened up new attack surfaces and created new security challenges. The adoption of Everything as Code means attacks no longer have to start in production. In development, gaining access to source control management systems enables code tampering, finding passwords to critical systems and modifying cloud configurations (through code) to allow unauthorized access. “Modernizing the SDLC has created new security gaps that attackers are readily exploiting,” said Ronen Slavin, CTO and co-founder of Cycode. “Recent supply chain attacks like SolarWinds and Codecov, major source code leaks from Microsoft and Nissan, and attacks targeting developers like Sawfish and XcodeSpy demonstrate that the battlefield is already shifting.” Cycode protects DevOps tools such as source control management systems, build systems, registries and cloud infrastructure. The solution addresses multiple layers of security, including access and authorization, security configurations, compliance and scanning engines. This enables customers to identify code tampering, code leakage, hardcoded secrets, Infrastructure as Code (IaC) misconfigurations, excess privileges and more, all from a single platform. To ensure customers never have to choose between security and speed, Cycode provides workflows to automate remediation. Customers can also seamlessly integrate remediation into their developers’ workflows via pre-built integrations with pull requests, alerting and ticketing systems. “As the leading Pentest as a Service company, our internal security has always been paramount,” said Ray Espinoza, CISO at Cobalt. “Cycode has saved us a massive number of hours hardening our source control management system, enforcing security configurations and preventing secrets from entering our code. Plus, by plugging seamlessly into our developers’ workflows, our team adopted Cycode right away.” Today, Cycode launches its knowledge graph to derive security insights from the rapidly increasing volumes of data and alerts that are overwhelming security teams. Through an agentless architecture, Cycode collects asset information and user activity from DevOps tools, infrastructure and security scanners, which is then mapped in its knowledge graph. By correlating events across the SDLC, Cycode’s knowledge graph creates contextual insights, helps prioritize remediation, reduces false positives and ensures the integrity of the pipeline to prevent code tampering incidents, such as the breaches at SolarWinds and Codecov. “The problem of protecting CI/CD tools like GitHub, Jenkins and AWS is a gap for virtually every enterprise,” said Jon Rosenbaum, principal at Insight Partners, who will join Cycode’s board of directors. “Cycode secures CI/CD pipelines in an elegant, developer-centric manner. This positions the company to be a leader within the new breed of application security companies — those that are rapidly expanding the market with solutions which secure every release without sacrificing velocity.” “With these new funds, part of the focus will naturally be on expanding sales and marketing efforts,” said Lior Levy, CEO and co-founder of Cycode. “What I’m really excited about is expanding Cycode’s platform with even more integrations into CI/CD and security tools to increase the power of our knowledge graph. Furthermore, we’re releasing a low-code query engine and a knowledge-sharing community that will enable security teams without development expertise to leverage the full power of the graph.” About Cycode Cycode secures DevOps pipelines and provides full visibility into enterprise development infrastructure. Powered by its knowledge graph, Cycode’s advanced detection capabilities correlate event data and user activity across the SDLC to create contextual insights and automate remediation. Cycode delivers security, governance and pipeline integrity without disrupting developers’ velocity. About Insight Partners Insight Partners is a leading global venture capital and private equity firm investing in high-growth technology and software ScaleUp companies that are driving transformative change in their industries. Founded in 1995, Insight Partners has invested in more than 400 companies worldwide and has raised through a series of funds more than $30 billion in capital commitments. Insight’s mission is to find, fund and work successfully with visionary executives, providing them with practical, hands-on software expertise to foster long-term success. Across its people and its portfolio, Insight encourages a culture around a belief that ScaleUp companies and growth create opportunity for all. For more information on Insight and all its investments, visit insightpartners.com or follow us on Twitter @insightpartners. About YL Ventures YL Ventures funds and supports brilliant Israeli tech entrepreneurs from seed to lead. Based in Silicon Valley and Tel Aviv, YL Ventures manages over $300 million and specializes in cybersecurity. YL Ventures accelerates the evolution of portfolio companies via strategic advice and U.S.-based operational execution, leveraging a powerful network of Chief Information Security Officers and global industry leaders. The firm’s track record includes successful, high-profile portfolio company acquisitions by major corporations including Palo Alto Networks, Microsoft, CA and Proofpoint. Heading the fund is Silicon Valley-based Managing Partner Yoav Andrew Leitersdorf, a serial entrepreneur and early-stage investor for over 25 years. Yoav works alongside Partner John Brennan in the U.S., while Partner & Head of Israeli Office Ofer Schreiber leads the Tel Aviv office together with Chief Marketing Officer Sharon Seemann. With a multidisciplinary team of 15 spread across two offices, YL Ventures has engrained itself in both the U.S. and Israeli cybersecurity ecosystems. For more information, visit ylventures.com. ADDENDUM FOR JOURNALISTS AppSec’s Biggest Unmet Need 91 respondents | Poll Completed in April 2021 See “Figure 1: Poll findings” in multimedia resources Cycode Knowledge Graph Example Use Cases Uncover issues that require comparison across the SDLC: Answer complex questions in a simple manner: Prioritize vulnerabilities and reduce false positives: The Attack Surface Has Shifted Recent DevOps Pipeline Tools & Infrastructure Incidents  View source version on businesswire.com: https://www.businesswire.com/news/home/20210511005231/en/ Media: Deb Montner Montner Tech PR dmontner@montner.com"
https://venturebeat.com/2021/05/11/servicenow-extends-services-into-ot-for-manufacturing/,ServiceNow extends services into OT for manufacturing,"At the digital Knowledge 2021 conference, ServiceNow today announced it is extending its reach beyond traditional IT services into the realm of operations technology (OT) with the launch of a suite of service management offerings aimed at the manufacturing sector. This will be part of a larger effort as the company extends its Now platform into industry sectors where IT services are managed by individuals outside a traditional IT department, ServiceNow GM Achyut Jajoo said. “This is just the beginning of our journey,” he promised. ServiceNow also announced it is expanding an existing Return to Work initiative by adding to its Workplace Service Delivery platform. This offering provides a Neighborhoods capability that allows members of a team to reserve a physical space in an office. The reservation capability has also been integrated with Microsoft Outlook. And the company added a planning tool to its Safe Workplace Suite for tracking workplace occupancy levels. Finally, ServiceNow announced it has updated the set of tools it provides to organizations that need to manage the administration of COVID-19 vaccines. ServiceNow is continuing to expand the reach of a platform that initially focused almost exclusively on IT services management to include everything from employee management to internet of things (IoT) environments. The Now platform will be employed to automate workflow processes in OT environments in much the same way it is employed to increase the efficiency of IT teams today, Jajoo noted. As organizations continue to increase their investments in IoT, in part to drive digital business transformation initiatives, OT and IT management processes are starting to converge. As that line blurs, it’s becoming more common for IT and security professionals to work collaboratively with OT professionals to, for example, secure an IoT environment from ransomware attacks, Jajoo noted. That’s critical because when a manufacturing line goes down, an organization loses revenue, he added. Data collected by the Now platform will also help organizations optimize the allocation of personnel in manufacturing environments, Jajoo noted. Automation is more critical than ever because pandemic-related restrictions in many areas of the world mean it’s not possible to send a team to physically visit a manufacturing site, he added. Organizations have over the years employed a wide range of tools to manage OT processes. In some cases, those tools are provided via a packaged application. In others, a custom application has been created that an internal OT or IT team is required to build, deploy, secure, and maintain. ServiceNow is making a case for a software-as-a-service (SaaS) application platform that it will maintain and secure and that can also be extended using low-code tools it provides. Siemens Energy is among the first customers for its OT platform. It’s not clear to what degree the worlds of IT and OT will converge. But as the level of investment in IoT platforms continues to steadily increase, the OT environment is becoming more complex. The number of platforms connected to applications running outside of a factory floor increases as management teams, for example, apply analytics to streaming data. In fact, there’s almost no part of a modern manufacturing process that isn’t already heavily instrumented. But now individuals across the rest of a business are more likely to appreciate the value of that data."
https://venturebeat.com/2021/05/11/mythic-raises-70m-to-disrupt-ai-chips-with-analog-and-flash-components/,Mythic raises $70M to disrupt AI chips with analog and flash components,"Mythic has raised $70 million to fund the analog-based AI processors it plans to launch later this year. Mythic has spent nine years coming up with a patented, potentially disruptive way to compete with other makers of artificial intelligence chips. The company emphasizes energy efficiency and lower cost with its design that focuses on analog technology integrated with dense flash memory, CEO Mike Henry said in an interview with VentureBeat. Henry said that in AI computing, chips have to normally handle massive amounts of simple arithmetic, with trillions of adds and multiplies per second. His company figured out how to do that in analog circuits, rather than digital, using a smaller electrical current. It stored the results in flash memory, which is a dense storage medium. He believes this will work much more efficiently than graphics processing units (GPUs) or other ways of handling the same calculations, when it comes to chip size, cost, and power usage. “We had a high level of ambition on the technical side,” Henry said. “This was a milestone almost everyone thought was impossible. It’s been pursued since the 1960s. We have developed a  disruptive way of doing a computer AI that gives us the same compute as a GPU, but at a much lower cost and power.” The funding was led by BlackRock and Hewlett Packard Pathfinder. Mythic has raised $165.2 million since its founding in 2012. The fresh funds will help expand the team and ready it for the launch of its chip. Mythic currently has 125 employees in Redwood City, California and Austin, Texas. Mythic wants to accelerate plans for mass production of its solutions and better support its growing customer base, which spans Asia Pacific, Europe, and the U.S. Mythic will also use the money to develop its next-generation hardware platform and build out its software portfolio. In November 2020, Mythic unveiled the first Analog Matrix Processor for AI applications, which combines high performance with good power efficiency in a cost-efficient solution. While expensive and power-hungry hardware has limited the broad deployment of AI applications, Mythic’s integrated hardware and software platform is making it easier and more affordable for companies to deploy powerful AI applications for the smart home, AR/VR, drones, video surveillance, smart cities, manufacturing markets, and more. A single drone can contain as many as six cameras to help it learn to avoid collisions. “Really, it comes down to cost and power. If it’s too expensive and too power-hungry, they just don’t put the features in,” Henry said. “We think there’s a lot of pent-up demand at the edge. There are a lot of people wanting to do a lot of really interesting things, and they just can’t do it with the last-generation hardware.” Mythic’s M1108 Mythic AMP makes this possible by using analog computing and integrated flash memory to deliver significantly faster results at much lower power consumption compared to typical digital processors. This kind of computing is needed at the edge of the network, alongside sensors such as cameras, lidar, radar, and security. Those sensors produce so much data that it’s hard to fit such large data models into a chip. So Mythic handles the computing with a small chip and packs a lot of flash memory into the chip, eliminating excess parts in the system. The Mythic chip should fit into an area the size of a postage stamp, Henry said. By contrast, GPUs and other options need heat-reducing components such as fans. By comparison, Nvidia’s Jetson AI platform for robots and drones may consume 30 watts and cost $700 to $800, with a low-cost version at $100. But Mythic is shooting for a lower cost, lower power consumption, and 10-20 times the performance, Henry said. The company is targeting around 25 trillion to 35 trillion instructions per second for its first chip. When the company first launched, it was easy to foresee that AI data models would increase exponentially, Henry said. “We never envisioned the world that we have now, with neural networks with a trillion parameters,” he said. “But we did see the chance to do something ambitious.” Device makers and original equipment manufacturers (OEMs) can choose from the single-chip M1108 Mythic AMP or a variety of PCIe card configurations, including the M.2 M Key and M.2 A+E Key designs, with added design flexibility and easier integration so customers can bring products to market faster. Mythic Analog Compute Engine (Mythic ACETM) aims to deliver revolutionary power, cost, and performance for AI innovation at the edge. The Mythic Analog Matrix Processor (Mythic AMPTM) makes it much easier and more affordable to deploy powerful AI solutions from the datacenter to the edge device. Henry said that the company has production silicon available, and he expects it will go into mass production by the end of the year."
https://venturebeat.com/2021/05/11/redwood-software-raises-379m-for-enterprise-process-automation/,Redwood Software raises $379M for enterprise process automation,"Redwood Software, a provider of cloud-based business and IT process automation solutions, today announced a €315 million (~$379.97 million) strategic investment from growth equity firm Turn River Capital. Redwood says the funds — which represent its first external financing — will be put toward accelerating product development as it looks to grow its customer base. Intelligent process automation and robotic process automation (RPA) — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that intelligent process automation and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than four hours of savings per employee each week. Founded in 1993, Netherlands-based Redwood aims to orchestrate and automate business processes across hybrid IT environments. The company claims it serves over 3,000 enterprise customers in over 150 countries — including Coca-Cola, GM, Guess, Wells Fargo, UBS, Mercedes-Benz, Airbus, Siemens, and Heineken. “Redwood initially started out in the application space building Oracle applications,” founder and CEO Tijl Vuyk told VentureBeat via email. “Over time, as we saw an increased need from our customers to automate many of the manual processes, we pivoted to automate everything we could. If there is a process where someone has to complete an activity twice, we find a way to automate as much of the activity as possible, all without leaving customers with a huge spend on services and maintenance.”  Redwood offers a catalog of tasks that piece together different dependent jobs in a workflow. To mimic existing workflows, the platform analyzes table structures in enterprise resource planning systems, other internal data, and metadata. With Redwood, customers can link and assemble processes using an orchestration engine that stitches the processes together. The platform can combine elements automatically or with human approvals and review cycles when required. Redwood can write results to and from enterprise systems, as well as cloning and editing predefined workflows. It can also kickstart workflows without requiring human intervention and allow them continue until complete, while dashboards track tasks, activities, and bottlenecks in logs that retain auditability at a process level. “Redwood’s business process automation platform is an innovation leader in the workload automation market, where we are still the only company to provide a true cloud-based automation solution purpose-built for enterprise customers. We know that shifting costly IT infrastructure to the cloud is a CEO-level directive over the next five years, and we are proud to support our current and future customers during this crucial transition period,” Vuyk said. “Due to the breadth of our technology portfolio, which spans workload automation, finance automation, and enterprise reporting, we are seeing more and more customers coming to Redwood in search of robotic process automation alternatives, which can provide the scale and flexibility that they demand.” Beyond process automation, Redwood offers services that handle cloud computing tasks like application transfer and file encryption. The platform can move, copy, and manage files between clouds, even where sources and destination protocols don’t align. Redwood can also automatically segment and rejoin large files for fast downloads over multiple connections. And the platform can resume or restart transfers that fail due to timeouts or network issues. Redwood is also capable of starting common OS, database, virtualization, container, and application maintenance tasks when the platform detects degraded performance. Developers get proactive notifications about anomalies and delays in processes they choose to track. On the development side, customers can publish automated processes in Redwood as interactive service endpoints for consumption by third-party services. They’re also afforded access to real-time data feeds, reporting, and analytics tools Redwood’s reporting product, a separate leg of the business, facilitates the evaluation of documents by allowing customers to add feedback, deliver reports, and combine data from multiple disparate sources. For example, health care providers can use it to gather, archive, and distribute data from information systems used by hospitals, physician networks, and health management organizations. “2020 was a year of significant changes and highlighted how the right tools can insulate businesses from hazards. As workers shifted from the office to their homes, existing processes that were inefficient or siloed and dependent on human intervention for continuity were exposed,” Vuyk said. “Going remote revealed how much can be done and gained through distributed and cloud platforms. As company budgets are unlikely to increase in 2021 and cost-reduction remains a significant driver of IT initiatives, a modern automation platform can resolve business inefficiencies, provide wide integration support, help evaluate metrics to optimize the customer experience, and more in a safe and secure fashion. Our value-based pricing model proved successful in 2020.”  Redwood, which has roughly 180 employees across the U.S., Netherlands, U.K., Germany, Switzerland, and Asia Pacific, has a number of competitors in a global intelligent process automation market that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere last secured a $290 million investment from SoftBank at a $6.8 billion valuation. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. And in recent months, legacy providers like WorkFusion have managed to raise hundreds of millions of dollars for their automation products. But the number of industries process automation touches continues to grow, with a Deloitte report predicting the technology will achieve “near universal adoption” within five years. According to the same report, 78% of organizations that have already implemented RPA — which see an average payback period of around 9 to 12 months — expect to “significantly” increase their investment in the technology over the next three years. And Gartner estimates that organizations can lower operational costs 30% by combining automation technologies like RPA with redesigned operational processes by 2024. One Redwood customer, Siemens, had roughly 1,000 annual, quarterly, and monthly financial statement reporting tasks to complete, including 300 undocumented tasks. Now, 97% of Siemens’ critical processes run automatically, and the number of tasks has been reduced to 30, according to Vuyk. “We are excited for the possibilities this investment brings and the new ways we will be able to serve our customers. Automation is a business imperative, and that is a fact,” Vuyk added. “We’re unique in that we’re marrying automation with low-code tools so enterprises can automate without extreme overhead or overhauling their IT department. This investment will give us the resources to meet the needs of IT organizations and business leaders alike.”"
https://venturebeat.com/2021/05/11/contract-automation-platform-contractbook-raises-30m-to-challenge-docusign/,Contract automation platform Contractbook raises $30M to challenge DocuSign,"Contractbook, a contract automation platform that claims users from companies such as Domino’s, KPMG, TED, and E.On, today announced it has raised $30 million in a series B round of funding. Although Contractbook works with larger enterprise customers, the problem it’s trying to solve is perhaps more likely to impact smaller businesses that don’t already have a structured workflow for their contracts. “That leaves them vulnerable, prone to legal errors, inefficient, and with a lot of manual routine work that they don’t like to do and find very difficult,” Contractbook cofounder and CEO Niels Martin Brochner told VentureBeat. Contractbook is all about automating as much of the manual administrative work as possible through integrations, with tools for drafting, editing, collaborating, signing, storing, organizing, and executing contracts. It’s an all-in-one platform that replaces any number of existing services, from Word to Adobe Acrobat, Box, Excel, and DocuSign. “We don’t believe that a contract lifecycle ends with a signature,” Martin Brochner added. “It takes a few days to create a contract and minutes to sign it, but it stays in effect for years. Ninety-nine percent of a contract’s lifecycle lies post-closing, and 99% of all businesses have no workflow for it. That is why we are focused on building the world’s best post-closing contract workflow.” Founded out of Copenhagen, Denmark in 2016, Contractbook had previously raised around $13 million. With its latest cash injection, which was spearheaded by Tiger Global, the company said it’s targeting expansion into the U.S. market. The company’s latest investment also comes just a few months after it announced its $9.3 million series A round. DocuSign is arguably one of the better-known incumbents in the digital contract management space, having spent the better part of two decades introducing tools to make paper-based contracts obsolete, spanning e-signatures, AI-powered automation, and more. DocuSign has thrived since its 2018 IPO, particularly due to the COVID-19 crisis, which has pushed the world toward cloud-based tools that enable remote work. The company’s market cap currently sits at $37 billion, well over double its pre-pandemic valuation. Other notable activity in the remote contract management space includes Ironclad, which recently secured $100 million, and Box, which acquired e-signature startup SignRequest for $55 million. Pactum, meanwhile, secured $11 million last month to expand its automated contract negotiation toolset. According to Martin Brochner, Contractbook is best used for companies with a “high frequency of low complexity contracts,” such as employment contracts, NDAs, and sales agreements. “We allow companies to automate all the mundane manual tasks related to those contracts — whether you want to auto-generate contracts based on data input, push data between systems, or set up automated tasks to make self-executing contracts,” he said. “We make the entire lifecycle of a contract easy, efficient, and error-free.”"
https://venturebeat.com/2021/05/11/fraud-prevention-firm-arkose-labs-raises-70m/,Fraud prevention firm Arkose Labs raises $70M,"Arkose Labs, a startup developing a platform to detect and mitigate online fraud, today announced that it raised $70 million in a funding round led by SoftBank Vision Fund 2. The company says that the fresh capital will go toward platform development, new hires, and global expansion. Javelin Strategy reported that 6.64% of consumers — or about 16.7 million people — fell victim to identity fraud in 2017, up 1 million from 2016. In 2018, over 2.6 billion records were stolen or exposed in more than 1,100 data breaches around the world. And the pandemic has only made things worse — security firm Socure’s first official fraud report this year shows a 134% increase in attempts from March 2020 to August 2020. Arkose’s platform aims to protect customers from account takeover, fake account abuse, scraping, spam, gift card abuse, and other scammy digital activities. According to founder and CEO Kevin Gosschalk, the company’s products analyze data from user sessions to determine the context, behavior, and past reputation of requests and either pass traffic on to the enterprise or gatekeep it. Depending on the risk score, users are presented with challenges that attempt to differentiate between true users and fraudsters. “My cofounder [Matthew Ford] and I started Arkose Labs with the idea of taking our experience in interactive software design (video games) and applying it to a common security challenge in the digital world — deterring bad actors from abusing the internet,” Gosschalk told VentureBeat via email. “Coming from outside the security industry, the approach we take is unique in how we go about stopping fraudsters. Ultimately, Arkose Labs today is designed around this idea of applying friction dynamically — the riskier you look, the more friction we provide.” Arkose Detect, Arkose’s risk engine, unearths behavioral patterns across devices and networks for behavioral analytics and anomaly detection. It profiles traffic and triages it based on underlying intent, which informs dynamic authentication challenges. And it looks for hidden signs of fraud in data from sessions and patterns over time. Arkose Enforce is a challenge-response system that works in conjunction with Arkose Detect to authenticate requests in over 30 languages. Responses are generated from proprietary visual data that ostensibly can’t be recognized or classified by AI algorithms, which prevents attackers from anticipating how Enforce will behave in the future and helps improve the system with assessments. Over the last 12 months, Arkose rolled out enhanced detection and monitoring systems and a protocol to deal with attacks from net new fingerprints. It also introduced machine learning capabilities to proactively identify threats and leverage customer truth data. Beyond this, Arkose expanded its device support to Android TV, printers, and gaming consoles such as the Xbox. It also debuted improved data insights and reporting dashboards via its customer portal and launched sign-on support for major identity providers including Okta, Microsoft, and Ping Identity. Arkose claims to have analyzed more than 15 billion online sessions in 2020, stopped 4.6 billion attacks, and wasted 40 million hours of fraudsters’ time. Annual recurring revenue doubled in the last year, the company says, as brands like Honey, CES, and Minecraft joined a customer base with existing clients Roblox, EA, Dropbox, Microsoft, PayPal, and Sony Interactive Entertainment. (Sony recently became a strategic investor in Arkose.) “We’re busier than ever. The business of fraud touches nearly every corner of everyday lives: online shopping, elearning, financial services, travel/tourism and gaming,” Gosschalk said. “The pandemic kept more people online and more businesses and consumers vulnerable to fraud and abuse. As a result, we’ve had to stay ahead of fraudsters at every turn and have done so by doubling headcount, opening new offices, and, like everyone else, adjusting to what was a new normal than anyone predicted.” Arkose, which doubled its headcount last year to over 150 employees, is based in San Francisco with offices in Brisbane, Australia and London, U.K. Wells Fargo and previous investors M12 and PayPal Ventures also participated in the company’s newest financing round, bringing Arkose’s total raised to $114 million. Fraud detection and prevention is a profitable market some estimate will be worth $43 billion by 2023. Barcelona, Spain-based Red Points, which provides online infringement detection and removal tools for brands in beauty, apparel, luxury, sports, toys, and homeware, recently raised $38 million. In March 2018, fraud detection startup Sift Science snagged $53 million, taking its total funding to $107 million. Not to be outdone, Mountain View-based DataVisor, which develops fraud detection software based on machine learning algorithms, nabbed $40 million that same year. Other online fraud detection companies that have raised big bucks include CashShield, Forter, Shift Technology, Featurespace, Pindrop, Simility, and RedMarlin."
https://venturebeat.com/2021/05/11/digital-customer-experience-platform-airkit-raises-40m/,Digital customer experience platform Airkit raises $40M,"Low-code digital customer experience platform Airkit today announced it has raised $40 million in a series B round led by EQT Ventures. The company says the funds, which bring Airkit’s total raised to over $68 million, will be used to support go-to-market and product development efforts. The user experience monitoring market is large and growing, and its value is expected to climb from $1.5 billion in 2019 to $3.7 billion by 2023, Markets and Markets anticipates. That may be because every dollar invested in user experience is estimated to net a $100 return, a 9,900% return on investment. Forrester Research reports that frictionless user experience design can raise customer conversion rates by up to 400%. After founding RelateIQ and selling the company to Salesforce in 2014 for $390 million, Airkit’s team took their learnings to launch what grew into Airkit. The platform sits on top of existing systems of record to deliver experiences that drive customer action. Airkit can digitize any sales and service touchpoint through the customer lifecycle, including automated onboarding, self-service account updates, digital cross-selling, and churn prevention. With Airkit, customers can abstract away frontend development with prebuilt components and templates for web, mobile, voice, and chat. They can also create brand components and asset libraries while flowing customer data across core systems and Airkit-powered experiences via APIs and integrations. Airkit can measure and optimize experiences with built-in app analytics and reporting on user engagement, and the platform provides debugging and tooling to help ship apps.  Airkit lets customers ideate interactions from templates with drag-and-drop elements like data, forms, controls, triggers, user interfaces, user actions, data manipulations, conversational channels, and more. The platform enables developers to test operations and data flows in a staging environment with app tracing, which allows them to preview an app in different resolutions and device frames. “Airkit can plug into many leading AI frameworks, such as Amazon AI or Salesforce Einstein, but we don’t inherently introduce any AI-related issues. We actually make it easier to get value from your AI investment by providing another way to deploy AI where it makes sense,” Airkit cofounder and CEO Stephen Ehikian told VentureBeat via email. “The pandemic has changed the way people think about digital self-service. It has introduced more people to the concept, and for many it has changed how they define great service. Both of these factors are a plus for our business.” Airkit recently released the Kittyhawk update, which offers an upgraded frontend experience powered by prebuilt data models and templates. Alongside new API integrations, the release addresses a range of use cases spanning customer onboarding, form digitization, payment capture, service dispatch, and call deflection. Research shows that buyers favor brands with personalized, digital self-service. For example, eMarketer found U.S. consumers are 370% more likely to purchase from a company with “very good” customer experiences than “very poor.” And quality customer experiences are likely to become more important as the pandemic accelerates digital transformations. In fact, Forrester predicts digital customer service interactions will increase by 40% this year. Ehikian says brands using the startup’s platform include OpenTable and Royal Automotive Club. “In the short time since Airkit launched, a growing number of the world’s top brands are trusting us to power their most important customer interactions,” he said. “The fact is that no one else is doing what Airkit is doing. Now we’re seeing all of the companies who were struggling with a lack of engineering resources and fragmented data looking at us and saying ‘Yes, this is the solution we’ve been looking for.'” Accel, Emergence Capital, Salesforce Ventures, and other existing shareholders also participated in Palo Alto, California-based Airkit’s latest funding round. The company has nearly 100 employees, and it expects to double that number over the next 12 months."
https://venturebeat.com/2021/05/10/ibm-taps-ai-for-new-workflow-automation-and-data-migration-tools/,IBM taps AI for new workflow automation and data migration tools,"During its Think conference this week, IBM unveiled AI-driven products across its portfolio of enterprise platforms. Mono2Micro, a new capability in WebSphere, taps AI to streamline cloud app migration. Watson Orchestrate helps automate work in business tools from Salesforce, SAP, and Workday. And an updated IBM Cloud Pak for Data ostensibly reduces the cost and complexity of curating data for AI and machine learning workloads. The AI industry is booming, with research commissioned by IBM finding that almost a third of businesses are using some form of AI and machine learning. By 2027, the global AI market is expected to be worth $733.7 billion, according to Grand View Research. But while recent advances in the technology are making AI more accessible, a lack of skills and increasing data complexity remain top challenges. The new Mono2Micro service in WebSphere Hybrid Edition, IBM’s app and integration middleware, optimizes apps and workloads to run in hybrid cloud environments on Red Hat OpenShift. Mono2Micro refactors apps to move them to the cloud, restructuring existing code without changing its behavior or semantics. IDG reports that the average cloud budget is up from $1.62 million in 2016 to a whopping $2.2 million today. But cloud adoption continues to present challenges for enterprises of any size. A separate Statista survey identified security, managing cloud spend, governance, and lack of resources and expertise as significant barriers to adoption. “As IT complexity grows with increased adoption of hybrid cloud, enterprises are looking to bring in the power of AI to transform how they develop, deploy, and operate their IT,” IBM wrote in a blog post. “A significant challenge CIOs face is that many of their core applications were written for an on-premises world and they can have hundreds to thousands of legacy applications that need to be modernized and moved to the cloud.” Mono2Micro uses machine learning and deep learning to analyze large enterprise Java applications. The analysis produces two alternative refactoring options for an application that can be explored in graphs and reports for transparency and explainability. The newest member of IBM’s Watson family, Watson Orchestrate, is designed to give workers across sales, human resources, operations, and more the ability to perform tasks faster. By interacting with existing enterprise systems, Watson Orchestrate can complete to-do list items, like scheduling meetings and procuring approvals. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Forrester predicts 57% of business-to-business sales leaders will invest more heavily in tools with automation. That may be why Salesforce anticipates the addressable market for customer intelligence will grow to $13.4 billion by 2025, up from several billion today. Users can interact with Watson Orchestrate using natural language — the software automatically selects and sequences prepackaged skills needed to perform a task, connecting with apps, tools, data, and history on the fly. For example, a sales director could ask Watson Orchestrate to monitor business opportunities, send an email alert when a deal progresses, and set up a meeting with the sales lead to discuss next steps. Waston Orchestrate also understands and maintains context based on organizational knowledge and prior interactions. Concretely, this means it can act on information informed by a user’s preferences, like a preferred business application or email contact. Watson Orchestrate follows acquisitions to expand IBM’s automation capabilities, including WDG Automation, Instana, MyInvenio, and Turbonomic, signaling the company’s ambitions in the enterprise automation space. According to IBM’s data, 80% of companies are already using automation software and tools — or plan to in the next 12 months. Watson Orchestrate is currently available in preview as part of IBM’s Cloud Pak for Automation. IBM introduced Cloud Pak for Data three years ago as a way to help enterprises apply AI to data across hybrid cloud environments. Beginning this week, the service is gaining new AI-powered functionality, including AutoSQL, which automates the access, integration, and management of data without having to move it. Managing data is highly complex and can be a real challenge for organizations. A recent MIT and Databricks survey of C-suite data, IT, and senior tech executives found that just 13% of organizations are delivering on their data strategy. The report concluded that machine learning’s business impact is largely limited by challenges in managing its end-to-end lifecycle. AutoSQL uses the same query engine across sources, including data warehouses, data lakes, and streaming data. It’s part of an intelligent data fabric that leverages AI to orchestrate data management tasks — discovering, understanding, accessing, and protecting data across environments. The new fabric unifies disparate data into a unified view and aims to ensure data can be accessed without jeopardizing privacy, security, or compliance. “Data quality and integration become major issues when pulling from multiple cloud environments,” IBM wrote in a blog post. “With the new data fabric and AI capabilities, [we’re] delivering what we expect to be a significant differentiator for customers by completely automating the data and AI lifecycle — the potential to free up time, money, and resources — and connect the right data to the right people at the right time, while conserving resources.” Another part of the intelligent data fabric, AutoCatalog, taps AI to automate how data is discovered and maintain a real-time catalog of assets from across enterprises. Meanwhile, AutoPrivacy — another data fabric component — automates the identification, monitoring, and enforcement of policies on sensitive data across organizations. The upgraded Cloud Pak for Data is available to customers starting today."
https://venturebeat.com/2021/05/10/ibms-codenet-dataset-aims-to-train-ai-to-tackle-programming-challenges/,IBM’s CodeNet dataset aims to train AI to tackle programming challenges,"At its Think conference this week, IBM introduced Project CodeNet, which the company claims is the largest open source dataset for benchmarking around AI for code. Consisting of 14 million code examples, 500 million lines of code, and 55 programming languages including C++, Java, Python, Go, COBOL, Pascal, and FORTRAN, CodeNet is approximately 10 times larger than the next most similar dataset, which has 52,000 samples. According to a study from the University of Cambridge’s Judge Business School, programmers spend 50.1% of their work time not programming; half of the rest of their time is spent debugging. And the total estimated cost of debugging is $312 billion per year. AI-powered code suggestion and review tools, then, promise to cut development costs substantially while enabling coders to focus on more creative, less repetitive tasks. CodeNet focuses specifically on the problems of code translation, code similarity, and code constraints. The goal is to advance the development of AI systems that can automatically translate code into another programming language, identify overlaps and similarities between different sets of code, and customize constraints based on a developer’s specific needs and parameters. Programming language translation could be especially useful, given that migrating an existing codebase to a modern or more efficient language like Java or C++ requires expertise in both the source and target languages. For example, the Commonwealth Bank of Australia spent around $750 million over the course of five years to convert its platform from COBOL to Java. Transcompilers could help in theory — they eliminate the need to rewrite code from scratch — but they’re difficult to build in practice because different languages can have a different syntax and rely on distinctive platform APIs, standard-library functions, and variable types. CodeNet contains samples designed to train AI to complete a range of programming tasks, including code search and clone detection. Beyond this, the dataset has metadata and annotations with a rich set of information spanning code size, memory footprint, CPU run time, and status, which helps to distinguish correct code from problematic code. Over 90% of the sample problems in CodeNet come with descriptions that contain a problem statement and specifications of the input and output format. For over half of the problems and seven million examples, IBM also curated sample inputs and outputs from the problem description. Using CodeNet, data scientists can execute code samples to extract additional metadata and verify outputs from generative AI models for correctness. IBM says that this will enable researchers to program “intent equivalence” when translating one programming language into another. “Given its wealth of programs written in a multitude of languages, we believe Project CodeNet can serve as a benchmark dataset for source-to-source translation and do for AI and code what the ImageNet dataset did years ago for computer vision,” Ruchir Puri, IBM fellow and chief scientist at IBM Research, wrote in a blog post. IBM isn’t the only company pursuing AI-driven code completion and auditing. Codota is developing a platform that suggests and autocompletes scripts in Python, C, HTML, Java, Scala, Kotlin, and JavaScript. Ponicode taps AI to check the accuracy of code, and DeepCode is developing an AI-powered system for whole-app code reviews (as are Amazon and Intel). Perhaps one of the most impressive projects to date is TransCoder, an AI transcompiler Facebook researchers developed to convert code from one programming language into another. Another contender is a model from OpenAI that was trained on GitHub repositories to generate entire functions from English-language comments."
https://venturebeat.com/2021/05/10/one-third-of-organizations-are-using-ai-ibm-survey-finds/,"One-third of organizations are using AI, IBM survey finds","Almost a third of organizations are using some form of AI, with 43% reporting that their rollout accelerated as a result of the pandemic. That’s according to a new Morning Consult study commissioned by IBM, which revealed that while AI adoption was flat over the last year, momentum shifted with changing business needs. IBM’s Global AI Adoption Index found that enterprise deployment of AI was flat compared with 2020 but that businesses plan “significant investments” in AI throughout the coming year. Adoption is being driven by both pressures and opportunities, from the pandemic to technological advances that make AI more accessible. Indeed, a third of companies told IBM they plan to invest in AI skills and solutions over the next 12 months. Of the categories of AI organizations are adopting, natural language processing (NLP) is at the forefront. Almost half of businesses say they’re using apps powered by NLP and 1 in 4 organizations plans to use the technology over the course of 2021. Customer service is the top use case, with 52% of companies deploying or considering deploying NLP for it, according to IBM. Even before the pandemic, autonomous agents were on the way to becoming the rule rather than the exception — partly because consumers prefer it that way. According to research published last year by Vonage subsidiary NewVoiceMedia, 25% of people prefer to have their queries handled by a chatbot or other self-service alternative. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Respondents to the survey also said the pandemic changed how they use automation. Eighty percent of companies report they’re using automation software or plan to use it in the next 12 months — and over 30% of organizations said the pandemic influenced their decision. Others told IBM they’ve explored new applications of the technology to make themselves more resilient, like helping automate the resolution of IT incidents. According to IBM, even companies adopting or planning to adopt AI face challenges, like limited AI expertise and a lack of tools for developing AI models. For example, while over 90% of businesses told IBM their ability to explain how AI arrived at a decision is important, more than half cited problems getting there, including biased data. IBM’s findings agree with a recent Boston Consulting Group survey of 1,000 enterprises, which found fewer than half of those that achieved AI at scale had fully mature, “responsible” AI implementations. The lagging adoption of responsible AI belies the value these practices can bring to bear. A study by Capgemini found customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and in turn, punish those that don’t. Laments over the AI talent shortage have also become a familiar enterprise refrain. O’Reilly’s 2021 AI Adoption in the Enterprise paper found that a lack of skilled people and difficulty hiring topped the list of challenges in AI, with 19% of respondents citing it as a “significant” barrier. And in 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers working globally on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” The ability to access data anywhere is increasingly key to responsible, successful AI deployments, IBM asserts. Over 60% of global IT professionals draw from more than 20 different data sources to inform their AI, according to the survey. And almost 90% of IT pros say being able to run their AI projects wherever the data resides is critical. “As organizations move to a post-pandemic world, data from IBM’s Global AI Adoption Index 2021 underscores a major uptick in AI investment,” IBM SVP Rob Thomas said in a press release. “A large majority of those investments continue to be focused on the three key capabilities that define AI for business — automating IT and processes, building trust in AI outcomes, and understanding the language of business. We believe these investments will continue to accelerate rapidly as customers look for new, innovative ways to drive their digital transformations by taking advantage of hybrid cloud and AI.” Still, IDC predicts worldwide spending on cognitive and AI systems will reach $77.6 billion in 2022, up from $24 billion in revenue last year. Gartner agrees: In a recent survey of executives from thousands of businesses worldwide, the firm found AI implementation grew a whopping 270% in the past four years and 37% in the past year alone."
https://venturebeat.com/2021/05/10/attackers-wanted-cash-from-the-colonial-pipeline-cyberattack-not-chaos/,"Attackers wanted cash from the Colonial Pipeline cyberattack, not chaos","(Reuters) — The ransomware gang accused of crippling the leading U.S. fuel pipeline operator said on Monday that it never meant to create havoc, an unusual statement that experts saw as a sign the cybercriminals’ scheme had gone awry. The FBI accused the group that calls itself DarkSide of a digital extortion attempt that prompted Colonial Pipeline to shut down its network, threatening extraordinary disruption as Colonial works to get America’s biggest gasoline pipeline back online by the end of the week. A terse news release posted to DarkSide’s website did not directly mention Colonial Pipeline but, under the heading “About the latest news,” it noted that “our goal is to make money, and not creating problems for society.” The statement did not say how much money the hackers were seeking. Colonial Pipeline did not offer any comment on the hackers’ statement and U.S. officials have said they have not been involved in ransom negotiations. The hackers did not respond to Reuters requests for comment. The FBI, Department of Energy and White House have all been involved in a rapid response to the hack, and a server used by the gang was shut down over the weekend. A person familiar with the matter said on Monday that the server held Colonial data and also files stolen in other DarkSide ransomware operations in progress, and that some of the group’s other victims were in the process of being notified. The FBI office in San Francisco, which had already been investigating DarkSide, was now involved in the law enforcement probe into the Colonial attack along with the FBI in Atlanta, near where the pipeline company is based. The FBI declined comment. DarkSide’s statement went on to say that its hackers would launch checks on fellow cybercriminals “to avoid consequences in the future.” It added the group was “apolitical” and that observers “do not need to tie us” with any particular government. The statement, which had several spelling and grammatical errors, appeared geared toward lowering the political temperature around one of the most disruptive digital extortion schemes ever reported. Gasoline prices at the pump have already risen 6 cents in the latest week — potentially putting them on course for the highest level since 2014. On Sunday the largest U.S. refinery — Motiva Enterprises’s 607,000 barrel-per-day (bpd) Port Arthur, Texas, refinery — shut two crude distillation units because of the outage at Colonial, according to people familiar with the matter. Some security experts said the DarkSide hackers were now trying to put some distance between themselves and the chaos they had unleashed. “This isn’t the first time a threat group has gotten in over their heads,” said Lior Div, the co-founder and chief executive of Boston-based security company Cybereason. He said that ransomware groups like DarkSide depended on being able to squeeze their victims discreetly, without attracting too much law enforcement scrutiny. “The global backlash is hurting their business,” said Div. “It is the only reason they are offering a mea culpa.” There is evidence that the DarkSide group operates out of Russia, U.S. President Joe Biden told reporters on Monday. He said that while there was “so far” no evidence that the Russian government was involved, “they have some responsibility to deal with this.” A U.S. official said investigators were still working out the nuances of whether and to what degree the alleged Russian indifference to the cybercriminals was deliberate. The Russian Embassy in Washington did not immediately return a message seeking comment. The Kremlin routinely denies having anything to do with cyberattacks on the United States. Tackling the steady drumbeat of ransomware incidents taking American businesses hostage has ranked high on the Biden administration’s list of priorities. A senior official with the U.S. Department of Homeland Security’s cyber arm, CISA, said that the dramatic pipeline company hack should serve as a wakeup call well beyond the energy industry. “All organizations should really sit up and take notice and make urgent investments to make sure that they’re protecting their networks against these threats,” said Eric Goldstein, CISA’s executive assistant director for cybersecurity. “This time it was a large pipeline company, tomorrow it could be a different company and a different sector. These actors don’t discriminate.”"
https://venturebeat.com/2021/05/10/replicated-demand-for-on-premises-software-equally-as-strong-as-saas/,"Replicated: Demand for on-premises software high, as strong SaaS","While there is a strong demand for cloud applications and software-as-a-service, security, regulatory, and compliance requirements continue to drive demand for on-premises software. In a new Dimensional Research report, 92% of companies said on-premises software was growing. The report, sponsored by Replicated, a software delivery and management company, found that current customer demand for on-premises software was equal to that of public cloud. While it may be popular to believe that “cloud is king” and SaaS is the best and most in-demand modern enterprise software, data shows that demand for on-premises software is equally as strong. It’s the smart choice for customers operating under security, regulatory, and compliance requirements; many organizations cannot allow their customer data to be shared in multi-tenant environments. Additionally, software companies that do not currently provide an on-premises solution to customers leave money on the table and miss a significant business and competitive opportunity. This new report from Dimensional Research, sponsored by Replicated, highlights the missed business opportunities for software vendors who are not offering an on-premises version. The report provides detailed insights around the current use, need, and challenges for on-premises software and its installation, configuration and management. This report also takes a closer look at the parallel rise in the adoption of container-based applications and the use of Kubernetes. Perhaps the most important findings are that 92% of surveyed participants reported their on-premises software sales as growing, and that on-premises solutions are equally as popular as their public cloud alternatives. This directly counters the popular narrative that SaaS has overtaken on-premises software delivery, as security and data protection stay top of mind for enterprise software customers. The survey from Dimensional Research includes feedback from 405 business and technology professionals at executive and manager seniority levels, representing software companies of all sizes around the world across a wide variety of different industries. Read the full report from Replicated"
https://venturebeat.com/2021/05/10/dack-drives-the-post-booking-economy-for-the-short-term-rental-industry/,DACK Drives the Post-Booking Economy for the Short-Term Rental Industry," Offers Short-Term Rental Operators a Revenue-Generating Technology Suite and Better Guest Experiences  LOS ANGELES–(BUSINESS WIRE)–May 10, 2021– DACK, a mobile-first SaaS platform for short-term rental operators and their guests, signs over 1,000 doors only four months after soft-launching. DACK’s technology suite services rental operators, guests, and vendors with a single-point mobile interface, providing the most-integrated lodging technology on the market. DACK enables short-term rental operators to maximize revenue for every booking with upsell features including amenity purchases, while digital key entry and contactless check-ins translate into better guest experiences. To learn more about DACK, visit www.dackinc.com. DACK founder and investor Damon Mintzer, an early executive at GSI Commerce (acquired by eBay), has coined the term ‘post-booking economy’ to describe the wide-ranging transactional opportunities available to operators, vendors, and guests after a rental booking has been made. Mintzer explained, “The short-term rental industry has lagged behind other travel-related industries when it comes to unlocking ancillary revenue streams by traditionally focusing on limited supplementary services; meanwhile the airline industry has mastered the art of unlocking revenue by charging for add-on benefits such as in-flight F&B, preferred seating, priority boarding, trip modifications, and even baggage. DACK introduces an ancillary revenue strategy for operators that also improves the guest experience.” DACK closed a seed round of $2.2 M in early 2021; investors include Wonder Ventures, Troy Capital Partners, David Blitzer, Michael Rubin, Navid Mahmoodzadegan, and Jarl Mohn. With virtually no marketing output, DACK has attracted established rental operators including 710 Beach Rentals, Righteous Rentals, HomeSlice Stays, Diamond Edge, 4 Degrees, Patriot Rentals and over 1,000 doors in Q1. Ellie Paget, founder of short-term rental management company HomeSlice, shared, “Hands down, DACK is the number one choice for guest technology, room access, IoT control, and more.” Gregoy Rollins, founder of Righteous Rentals, added, “It seems like I’ve been asking company after company when they are going to do exactly what DACK is doing.” Rental operators using DACK report, on average, 88% of guests downloaded DACK, and a staggering 63% of completed an upsell transaction, a promising data point in a 1.4 trillion-dollar market. DACK benefits and application integrations are extensive; guests can expect a safe, contactless check-in, and reliable digital key access; and the guest app also includes intuitive property guides, dynamic local recommendations and stay-customization features. DACK’s IoT integrations mean lodging energy settings can be automated whether occupied or vacant. Room cleaning, car service, and food delivery request tools are among the many enhancements that guests can expect when using the mobile app. DACK’s SaaS Platform is built with cutting-edge technology for the hospitality space; DACK’s AI-powered IoT, Mobile, and PMS Integration solutions ensure a seamless guest experience before they step into a property. Technology agnostic, DACK provides the unique ability to function with any major property management system or IoT device so that the benefit set for operators and guests continues to grow. DACK will open its Series A round in June 2021, with an accelerated expansion strategy already underway. About DACK: DACK is only app you will need when you travel. DACK’s revolutionary AI-powered SaaS Platform delivers guest experience solutions, the guest application and operator toolkit help operators everywhere provide exceptional stays for their guests. DACK is headquartered in Los Angeles, CA. To learn more, visit www.dackinc.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210510005675/en/ Brian TrentVine Media AdvisorsBrian@vinemediaadvisors.com"
https://venturebeat.com/2021/05/10/upswift-iot-device-management-platform-announces-full-support-for-nvidias-jetson-product-line/,Upswift IoT Device Management Platform Announces Full Support for NVIDIA’s Jetson Product Line,"TEL AVIV, Israel–(BUSINESS WIRE)–May 10, 2021– Upswift, a leading IoT device management software company, announced today full support for the famous, AI driven, Nvidia Jetson product line – Jetson Nano, Jetson Xavier NX, Jetson AGX Xavier, Jetson TX1/2 NX and Jetson TX1/2. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210510005646/en/ According to Upswift release note, many of today’s smart connected products providing AI, robotics, and image processing functionalities using Nvidia’s Jetson product line, are working together with Upswift IoT device management platform to maintain and manage their product fleet remotely. The full support Upswift’s announced, enables startups and enterprises building smart IoT products to gain complete visibility that eliminates product recalls, software bugs and security concerns with an all-in-one, full-featured device management platform. As part of the announcement, it was mentioned that it takes less than 60 seconds to connect any type of Nvidia’s Jetson based device to Upswift and begin deploying OTA updates (file system or Docker container based), control, access (by remote SSH or VNC), monitor and secure the product fleet through the platform itself. The exciting statement joins several publications Upswift has lately announced, pointing out the different customers serving a wide range of industries using Upswift’s IoT device management platform to deliver a better solution for their end customers. The company behind Upswift cloud solution is proudly sharing the enormous growth they are experiencing thanks to new agreements with well-known US fortune 500 companies from the automotive industry sector as well to software enterprises producing IoT products in a variety of markets. For more information on Upswift IoT device management platform, visit https://www.upswift.io/. About Upswift:Upswift is a scalable cloud-based solution that delivers visibility, control, OTA update and monitoring tools for IoT Linux-based devices. Our unique technology provides the first end-to-end solution in a plug & play design to eliminate the complexity of deploying and managing edge connected products at scale.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210510005646/en/ Thomas JohnsonUpswift Media Teamcontact@upswift.io"
https://venturebeat.com/2021/05/10/expert-ai-adds-emotion-and-style-detection-tools-to-natural-language-api/,"Expert.ai adds emotion, style detection tools to natural language API","Enterprises and investors are increasing their use of natural language APIs to assist processing in tasks like data mining for sales intelligence, tracking how marketing campaigns change over time, and better defending against phishing and ransomware attacks. Still, AI products that use natural language engines to analyze text have a long way to go to capture more than a fraction of the nuance humans use to communicate with each other. Expert.ai hopes the addition of new emotion- and behavior-measuring extensions and a new style-detecting toolkit for its natural language API will provide AI developers with more humanlike language analysis capabilities. The company this week announced new advanced features for its cloud-based natural language API designed to help AI developers “[extract] emotions in large-scale texts and [identify] stylometric data driving a complete fingerprint of content,” Expert.ai said in a statement. Based in Modena, Italy and with U.S. headquarters in Rockville, Maryland, Expert.ai changed its name from Expert System in 2020. The company’s customers include media outlets like the Associated Press, which uses NL software for content classification and enrichment; business intelligence consultants like L’Argus de la Presse, which conducts brand reputation analysis with NL processing; and financial services firms like Zurich Insurance, which uses Expert.ai’s platform to develop cognitive computing solutions. Expert.ai’s software platform enables natural language solutions that take unstructured language data from sources like social media sites and emails, transforming it into more digestible, usable intelligence before human analysts look at it. An example of a basic NL capability would be to distinguish between different ways a word like “jaguar” is used contextually — to signify the animal, the vehicle, or the name of a sports team. This allows for process automation steps to be introduced to text gathering, categorization, and analysis workloads, which frees up human analysts to perform higher-order tasks with the data. Several NL software developers, including Expert.ai, used algorithms last year to attempt to predict the outcome of the U.S. presidential election, with mixed results. While trying to weed out bot accounts, Expert.ai scraped Twitter and other social media sites to determine which candidate was ahead on “positive” sentiment and thus likely to win the popular vote. The company’s final polling gave Joe Biden a 50.2% to 47.3% edge over Donald Trump — not too far off Biden’s final tally of 51.3% to Trump’s 46.9% of the national popular vote. With the new extensions, the Expert.ai natural language API now captures a range of 117 different traits in analyzed language, the company said. The natural language engine categorizes eight different “emotional traits” found in analyzed text (anger, fear, disgust, sadness, happiness, joy, nostalgia, and shame) and seven different “behavioral traits” (sociality, action, openness, consciousness, ethics, indulgence, and capability). Traits are further rated on a three-point scale as “low,” “fair,” or “high.” Additionally, Expert.ai’s new “writeprint” extension improves the NL engine’s ability to process and understand the mechanics and styles of written language. The writeprint extension “performs a deep linguistic style analysis (or stylometric analysis) ranging from document readability and vocabulary richness to verb types and tenses, registers, sentence structure and grammar,” according to the Expert.ai website. The ability to identify individual authors of texts via the writeprint extension could be put to several uses, such as identifying forgeries or impersonations, as well as categorizing content based on writing style and readability, the company said. “From apps that analyze customer interactions, product reviews, emails or chatbot conversations, to content enrichment that increases text analytics accuracy, adding emotional and behavioral traits provides critical information that has significant impact,” Expert.ai head of product management Luisa Herrmann-Nowosielski said in a statement. “By incorporating this exclusive layer of human-like language understanding and a powerful writeprint extension for authorship analysis into our NL API, we are conquering a new frontier in the artificial intelligence API ecosystem, providing developers and data scientists with unique out-of-the-box information to supercharge their innovative apps,” she added."
https://venturebeat.com/2021/05/10/why-investors-need-to-get-in-on-these-tech-trends-before-the-ipo/,Why investors need to get in on these tech trends before the IPO,"Presented by Alumni Ventures The days of waiting for an IPO are over. Getting in early on an investment is now essential for the opportunity to get the most significant returns — especially in those areas that are gaining tremendous traction because of a world in upheaval. COVID-19 has impacted trends in ways that couldn’t have been predicted, causing new markets to emerge, disruptions across industries, and heightened interest in investment from all quarters. “The pandemic dramatically accelerated existing market trends, causing an increased interest in technology and innovation,” says Mike Collins, CEO of Alumni Ventures. “More people want to participate in that economy as entrepreneurs and investors.” At the same time as markets are shifting, so is the landscape for venture investing. There’s been more democratization of the venture capital asset class, according to Collins. Historically, VC was designed for large institutions, endowments, pension funds, and similar organizations. That’s changing as more individual accredited investors are looking to diversify into venture and finding some disruptive providers, like Alumni Ventures, ready to serve them. Also changing is the ready availability of capital for private companies. They no longer need to go public to raise that same amount of cash. And because there’s significant cost and hassle involved with being a public company, companies are more than willing to wait a long time before taking on the costs and regulatory burdens — if they even decide to take that step at all. So companies going public now are often far more mature, with much higher valuations. Twenty years ago, companies would go public at a $500 million valuation; now they might IPO at a valuation between $25 to $50 billion. In other words, more of the value being created is from the time a company launches to the time it goes public. Collins observes, “If you wait for the IPO, you’re ignoring opportunities for value creation. We want to offer that opportunity to investors. We think about venture like any other asset class, with its own risks and rewards and as just one part of a diversified portfolio. We want to offer investors a chance to allocate a portion of their portfolio into venture — just as they do cash, real estate, equities, maybe some fixed income.” So which trends are Alumni Ventures watching? Here’s a look at some investment areas that are gaining traction. “Health care and education are huge parts of the economy, and they need fixing,” Collins says. “Technology is going to be critical to that fix, changing the health care and education experience over the next five to ten years.” Collins predicts that early interventions and many specialties like dermatology will be handled with a telemedicine approach, while records will be shared and secured with better technology. In education, big tech will disrupt professional training and specialty institutions, with possibly even some of the tech giants building innovative platforms that will change how learning is delivered. Collins sees similar tech innovations in the lifestyle segment of the economy. The seismic move toward screens and digitization will continue. Changes in the way people work will accelerate, with a lot more flexibility in the 9-to-5 in-office routine. And the balance of digital vs. physical lives will tip. As Collins notes, “Digital natives are looking for their life experience to be a combination of the physical and digital worlds.” Some examples of innovations in these areas: Blockchain technology, which shouldn’t be confused with Bitcoin, is still in its infancy, Collins says. However, in his opinion, the technology stack has incredible applications. “There are exciting developments in finance, retail, logistics, banking, law, government — even art. Don’t underestimate the power and uses of the tech. I think it’s similar to the early days of the internet, which was often overhyped in the short term, but also underestimated in the long term.” Just one recent example of a blockchain-related success: Coinbase, an online broker of cryptocurrencies, was listed in April and started its first full week as a public company at a valuation of ~$65 billion. A few other blockchain companies to keep an eye on: “There’s also a lot of hype around big data and AI,” notes Collins, “But we’re continuing to see software and computers take over things that they can do better than humans. It’s a wave that’s inevitable.” Indeed, AI and machine learning are often viewed as the most transformative technologies of the 21st century. Many investors are looking to get in early, and the roster of startups in the space grows at a steady clip. Watch out for: “Energy is another high-impact, high-value sector,” Collins says. “Looking back over the last couple hundred years, you’ll see that many changes in society have been driven by developments in energy. There’s more of that to come — even in the span of the next decade.” Life sciences is another deep tech area where exciting work around health and longevity is being done, according to Collins. The evolution of deep technology, which enabled the recent rapid vaccine development, is just one signal about how the speed and impact of deep tech is increasing. “It’s a golden age, where there are lot of problems, but a lot of smart people with profound technologies working to solve those problems,” Collins says. Up-and-coming energy companies: For Collins, this period of venture capital is the story of diffusion and democratization. VC investing is becoming more global. It has already moved out of traditional hubs like Silicon Valley and Boston to other regions. Entrepreneurial ecosystems are developing and growing around the world, boosted by access to cheaper tech, remote talent, and international capital infusion. And that kind of transformation is impacting investors too. Collins says. “More investors have greater access to venture — and not just at the local level. They can be participants and contributors in the global venture economy.” Collins concludes, “Putting it all together, this is a time of more venture deals from more sectors, accelerated by significant societal trends, with a compelling valuation argument to invest pre-IPO. At Alumni Ventures, we think that adds up to powerful venture opportunity.” Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/05/10/edcast-announces-employee-experience-ex-inside-cisco-webex/,EdCast Announces Employee Experience (EX) Inside Cisco Webex," Webex users can now gain a full digital workspace experience all completely within Webex  MOUNTAIN VIEW, Calif.–(BUSINESS WIRE)–May 10, 2021– EdCast announced the launch of its employee experience (EX) platform inside the Cisco Webex App Hub today. The seamless integration between the two platforms enables users to get unified discovery and access to personalized content and experts directly within Webex. With this integration, companies and employees can now enhance employee experience with EdCast for Webex, bringing AI-powered content from both internal and external sources alongside Webex collaboration workflows. “Today, organizations realize that Customer Experience is not possible to deliver without great Employee Experience (EX),” said Karl Mehta, CEO and Founder of EdCast. “EdCast is pleased to join with Cisco in providing a full digital workspace experience and access to relevant content completely inside Webex.” By leveraging the unique strengths and capabilities of both Webex in enterprise collaboration and EdCast in employee experience and upskilling, HR and IT leaders can now unleash the full potential of their workforce by bringing content and collaboration together in one, unified remote workspace. The EdCast EX platform for Webex provides users the ability to: To access the EdCast EX platform inside Cisco Webex if you’re already an EdCast customer, just go to Cisco AppHub and click install from here: https://apphub.webex.com/messaging/applications/edcast-edcast. Or contact EdCast at marketing@edcast.com. ABOUT EDCAST EdCast offers a unified platform designed to operate end-to-end employee experience journeys spanning learning, skilling and career mobility. Its award-winning platform is used internationally by organizations ranging from large Global 2000 companies to small businesses and governments. With EdCast’s platforms, our customers are able to attract, develop and retain a high-performance and future-ready workforce. EdCast’s offerings include its Talent Experience Platform, Spark for SMBs, EdCast Marketplace and MyGuide Digital Adoption Platform. EdCast is a World Economic Forum Technology Pioneer award recipient.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210510005178/en/ Ash Naik (PR@edcast.com)"
https://venturebeat.com/2021/05/10/servicenow-acquires-devops-observability-platform-lightstep/,ServiceNow acquires DevOps observability platform Lightstep,"ServiceNow has announced plans to acquire Lightstep, a VC-backed software monitoring and observability platform that launched back in 2017 and claims big-name client such as GitHub, Twilio, Big Commerce, and Spotify. The announcement comes amid a flurry of activity in the observability space, with IBM recently snapping up Instana, Datadog buying Sqreen and Timber, and New Relic and Dynatrace battling for dominance. “Observability” is chiefly concerned with deriving insights from raw data gleaned from sources such as logs, metrics, and traces to help understand what may be impacting an application’s performance. ServiceNow, a workflow automation platform used by businesses like American Express and Deloitte, said Lightstep will help it deliver “deep operational insights” for enterprises, extending beyond developer operations (DevOps) to “every team involved in these modern, digital businesses,” Lightstep cofounder and CEO Ben Sigelman said in a press release. Terms of the deal were not disclosed, but a ServiceNow spokesperson said this is one of its “largest acquisitions to date.” The company expects the acquisition to close in Q2, 2021."
https://venturebeat.com/2021/05/10/bigfoot-biomedical-receives-fda-clearance-for-bigfoot-unity-diabetes-management-system-featuring-first-of-its-kind-smart-pen-caps-for-insulin-pens-used-to-treat-type-1-and-type-2-diabet/,"Bigfoot Biomedical® Receives FDA Clearance for Bigfoot Unity™ Diabetes Management System, Featuring First-of-its-Kind Smart Pen Caps for Insulin Pens used to Treat Type 1 and Type 2 Diabetes","  MILPITAS, Calif.–(BUSINESS WIRE)–May 10, 2021– Bigfoot Biomedical, a company dedicated to better health outcomes for people with insulin-requiring diabetes, announced today that the FDA has granted 510(k) clearance for first-of-its-kind Bigfoot Unity™ Diabetes Management System, which features connected smart pen caps that recommend insulin doses for people using multiple daily injection (MDI) therapy. The Bigfoot Unity smart pen caps provide on-demand, insulin dose decision support to minimize guesswork and enable patients to follow their doctor’s instructions in a convenient, simple way. The Bigfoot Unity System is the centerpiece of the larger Bigfoot Unity program that takes a holistic approach to simplify and connect key aspects of insulin management for patients and health care providers. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210510005229/en/ The Bigfoot Unity System is the first and only solution for people with Type 1 or Type 2 diabetes on MDI therapy that directly uses integrated continuous glucose monitoring system (iCGM) data from Abbott’s FreeStyle Libre 2 system to provide an insulin dose recommendation, which is based on a physician’s instructions. The dose is displayed directly on a proprietary, connected smart cap for the person’s disposable insulin pens without the need to manually input glucose data on a separate device. Cleared for use by individuals aged 12 and up, the system is designed to clearly, and in real-time, help answer the question, “How much insulin would my doctor recommend I take right now?” “Diabetes management is incredibly hard because insulin has no fixed dose or timing, leaving individuals to constantly determine their doses and configure devices as they make multiple critical decisions every day about how much insulin to take,” said Jeffrey Brewer, CEO of Bigfoot Biomedical. “At Bigfoot, we want to ease the burden of diabetes for people taking insulin by minimizing the anxious guesswork involved with insulin dosing in a convenient, simple way.” The Bigfoot Unity System is the only FDA-cleared connected solution to: “People with diabetes can be overwhelmed with the amount of data they get from multiple devices, so it’s important to develop connected technologies that simplify the experience,” said Jared Watkin, senior vice president, Diabetes Care, Abbott. “Through Abbott’s collaboration with Bigfoot Biomedical, we’re integrating our revolutionary, easy-to-use FreeStyle Libre 2 technology with the Bigfoot Unity System to provide automated information for personalized diabetes management.” How the Bigfoot Unity System Works The Bigfoot Unity System has three primary components – proprietary smart pen caps for both rapid- and long-acting insulin, mobile app and integrated FreeStyle Libre 2 iCGM sensor – that seamlessly fit into the person’s dose decision process when they need it throughout the day. The Bigfoot Unity smart pen cap for rapid-acting insulin allows the user to scan the FreeStyle Libre 2 sensor, displaying the user’s current glucose value, trend arrow and any recommended correction dose. The smart pen cap also directly displays the health care provider’s suggested meal insulin doses with the correction dose. In just a few steps the system gives the person with diabetes support to make a real-time treatment decision. While the Bigfoot Unity System’s dose recommendations are on demand as needed by the user, the system also provides continuous support with important reminders and alerts. The Bigfoot Unity System notifies the person if they may have missed their usual long-acting insulin dose. The Bigfoot Unity app also provides a very low glucose alert when the person’s glucose value falls below 55 mg/dL as well as an optional low glucose alert when the glucose value falls below 70 mg/dL. Data from the iCGM and time-of-dose data from the Bigfoot Unity pen caps are automatically captured and uploaded to the cloud whenever WiFi or cellular signal is present, replacing the onerous manual logs typically kept by people on MDI therapy. The person’s health care provider also can view this patient data through a secure web portal, the Bigfoot Clinic Hub. Health Care Providers Gain Real-Time Partner to Help Support Patients The FDA-cleared Bigfoot Unity System is included as part of a more comprehensive program offering, focused on helping health care providers deliver individualized and proactive care to their patients with insulin-requiring diabetes. The Bigfoot Unity program supports treatment of a broad spectrum of patients on MDI therapy from those with Type 1 diabetes not on insulin pumps to those with Type 2 diabetes needing to step up their insulin therapy. The program’s holistic approach focuses upon the key aspects of insulin management for patients struggling with glycemic control, insulin dosing or those experiencing hypoglycemic unawareness. “For health care providers the Bigfoot Unity™ diabetes management program could be a gamechanger by connecting them to data that can provide better insight into what their patients are actually doing,” explained diabetologist Jim Malone MD, chief medical officer for Bigfoot Biomedical. “By replacing guesswork and complexity with connectivity and simplicity, the Bigfoot Unity program is truly serving as that real-time partner for both patients and providers when it comes to dose decision support and overall diabetes management.” Key benefits of the Bigfoot Unity program to health care providers: “There are more than 7 million people in the U.S. on insulin therapy and many use insulin injections multiple times a day, yet there have been few advancements for this hugely underserved market,” said Brewer. “Bigfoot Unity is specifically designed to be simple and accessible – no matter the person’s level of technical expertise – removing a key barrier to health equity. We know people with diabetes want to be successful with their insulin therapy, and it’s often complex and expensive technology that gets in the way.” The Bigfoot Unity program will be available through select diabetes clinics beginning in spring 2021. Providers who would like additional information should go to www.bigfootbiomedical.com. About Bigfoot Biomedical, Inc. Bigfoot Biomedical was founded by a team of people with personal connections to Type 1 and Type 2 diabetes. We seek to change the paradigm of care for diabetes. Bigfoot is an unconventional company taking an unconventional approach. Unlike others, we’re looking at insulin therapy holistically and utilizing services, support, and novel business models. We’re partnering with health care providers to deliver simple, connected, and comprehensive solutions for the large number of people who have been overlooked by diabetes innovation. Learn more at www.bigfootbiomedical.com. Follow us on Twitter @BigfootBiomed, Instagram and Facebook. About Bigfoot Biomedical CEO Jeffrey Brewer Jeffrey Brewer is founder and CEO of Bigfoot Biomedical. Since his son’s 2002 diagnosis of Type 1 diabetes, Jeffrey has been a thought leader and catalyst in the diabetes community, first as a philanthropist who funded and launched the Artificial Pancreas Project along with JDRF International, then as JDRF’s CEO where he forged partnerships to drive medical device innovation. Prior, Jeffrey was an entrepreneur and founded and led highly successful dot-com start-ups including CitySearch and Overture/GoTo.com. Indications and Important Safety Information The Bigfoot Unity Diabetes Management System is indicated for the management of diabetes in people ages 12 years and older. The Bigfoot Unity System provides glucose monitoring data via Abbott’s FreeStyle Libre 2 integrated continuous glucose monitoring system. The system incorporates real-time alarm capabilities and is designed to replace blood-glucose testing for diabetes treatment decisions, unless otherwise indicated. The device is intended to provide insulin dose information using the available glucose data to assist people with diabetes mellitus who use disposable pen-injectors for the self-injection of insulin in implementing health care provider recommended insulin dose regimens. The device is intended for single patient use only and requires a prescription. The Bigfoot Unity System is also intended to communicate autonomously with digitally connected medical devices where the patient manually controls therapy sessions. A health care professional must provide appropriate settings for the device based on user specific criteria. It is not intended to be used by individuals who dose insulin in ½ unit increments, take multiple daily doses of long-acting insulin or take high doses of vitamin C (more than 500 mg per day). [1] Based on products available in the U.S. as of May 2021 [2] Table 6.4 – Classification of hypoglycemia. American Diabetes Association. 6. Glycemic targets: Standards of Medical Care in Diabetes–d2021. Diabetes Care 2021;44 (Suppl. 1):S73–S84. https://doi.org/10.2337/dc21-S006 [3] In order to receive alerts, Notifications and Critical Alerts must be turned on and the sensor must be within 20 feet of the phone with the Bigfoot Unity Mobile App active. Alerts only available on the phone. For compatible phones, please visit www.bigfootbiomedical.com/compatible [4] Data on file, Abbott Diabetes Care. Data based on the number of users worldwide for the FreeStyle Libre portfolio compared to the number of users for other leading personal use, sensor-based glucose monitoring systems.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210510005229/en/ Media Contact: Red Maxwellpress@bigfootbiomedical.com"
https://venturebeat.com/2021/05/10/trend-micro-brings-open-source-vulnerability-data-to-security-teams/,Trend Micro brings open source vulnerability data to security teams,"Cybersecurity giant Trend Micro has announced a new software-as-a-service (SaaS) product that offers security teams “continuous insight” into open source vulnerabilities and compliance risks. Designed in partnership with Snyk, Trend Micro Cloud One – Open Source Security by Snyk is the first service on the Cloud One platform to be powered by a third-party company. Trend Micro launched Cloud One in 2019 as a security services platform for cloud-focused development teams. It’s designed to simplify hybrid and multi-cloud security, with solutions spanning workload, container, file object storage, serverless and application, network, and posture management. The vast majority of modern software relies to some degree on open source components, as it saves companies the considerable time and resources needed to develop and maintain everything internally. However, recent data from Synopsys, the silicon design company behind open source security management platform Black Duck, found that 84% of the commercial codebases it scanned in 2020 contained at least one open source vulnerability — up from 75% in the previous year’s report. Indeed, the business of securing open source software is big and only getting bigger. Last month, WhiteSource raised $75 million to bolster its open source security management and compliance platform, which is used by companies like Microsoft and IBM. Meanwhile, Snyk itself recently raised $300 million at a $4.7 billion valuation. Founded out of London in 2015, Snyk helps developers (as opposed to cybersecurity personnel) find vulnerabilities in their open source code, as well as their containers and Kubernetes applications. The platform is used by developers spanning myriad high-profile companies, including Google, Salesforce, Atlassian, and Twilio. The new Trend Micro and Snyk service, which was first announced last year, is designed to improve visibility and tracking automation “by eight hours per vulnerability,” according to Trend Micro. In a nutshell, the integration serves up a series of dashboards for any developer who runs it against their source code and generates visualizations that track issues over time, potential open source license issues, security severity scores, and more. The main question is whether companies can’t just run Snyk by themselves? What, exactly, does the Trend Micro tie-up bring to the table? The key here is that Trend Micro and Snyk have distinct user bases that adhere to different workflows. “Trend Micro has a strong security operations focus, while Snyk has a strong developer focus,” Trend Micro COO Kevin Simzer told VentureBeat. “Combined, this partnership delivers visibility to security operations teams in a manner that allows them to manage the risk found in open source vulnerabilities, gain visibility directly from source code management and build pipeline, and help solve security issues before they become a threat.” Founded in 1988, Trend Micro is essentially a legacy cybersecurity platform born in the era of on-premises software. But as the world transitions to the cloud, Trend Micro has had to follow suit, which is why it launched Cloud One two years ago. But that also opened the door to new security considerations, including the vast array of open source vulnerabilities that exist in a company’s tech stack. With Snyk, a cloud-native platform, developers connect the platform to their code repository (e.g. in GitHub, GitLab, or Bitbucket) and Snyk taps a giant vulnerability database it maintains internally to flag potential weaknesses or even license violations. Rather than trying to create all this from scratch, Trend Micro has recognized the need to partner with specialists in a particular domain. “A part of being a SaaS pioneer in cybersecurity is knowing that customers want products that work together to better protect them,” Simzer said. “No vendor can go it alone in today’s threat landscape, so when there are people with knowledge that complements our projects, we plan to integrate their solutions into our platform and co-build new ones as needed.”"
https://venturebeat.com/2021/05/09/cogniflare-unveils-data-management-platform-for-google-cloud/,Cogniflare unveils data management and catalog platform for Google Cloud,"Cogniflare last week launched an integrated data management and catalog platform that promises to simplify the management of massive amounts of data residing in various Google Cloud services. The Kyrah for Google Cloud data management platform provides IT teams with a workflow engine that enables them to manage data residing in, for example, the Google BigQuery data warehouse, Cogniflare CTO and cofounder Ahamed Khan told VentureBeat. When employing Google cloud services, IT organizations wind up with hundreds of projects using a mix of Google services to store data. Kyrah for Google Cloud employs the application programming interfaces (APIs) exposed by Google to help centrally manage and update all the change requests made to those services whenever a Kyrah for Google Cloud platform user makes a change, Khan said. In addition, all those changes can be tracked via an Activity Log. As the amount of data being stored in the cloud continues to exponentially increase, organizations are approaching the point at which it becomes ungovernable, Khan added. “There’s been a data explosion,” he said. “It feels like we’re starting to lose control.” Kahn said Kyrah for Google Cloud provides the same types of governance capabilities Microsoft added to Azure with the launch of Microsoft Purview. Cogniflare plans to add connectors that will extend Kyra’s data management reach into the various cloud services provided by Microsoft and Amazon Web Services (AWS), Khan said. The company also plans to expand Kyra’s reach into on-premises IT environments running, for example, the Hadoop Distributed File System (HDFS) or the Apache Kafka event streaming platform for sharing data in near real time. In addition, Cogniflare is working to give Kyrah AI capabilities to infuse the platform with machine learning algorithms that make it easier for compliance officers to identify anomalous behavior. This will be a critical capability as compliance officers struggle to comply with privacy regulations such as the General Data Protection Regulation (GDPR), Khan noted. Cogniflare developed Kyrah to address its clients’ data management requirements. The company also makes available Spoko, a lightweight identity and access management (IDAM) platform for Google Cloud. Data management in the age of the cloud has become a more pressing issue because compute capacity is now much more readily available. In an on-premises IT environment, compute and storage capacity acted as a governor on the amount of data an organization could collect and retain. Cloud services theoretically make it possible for an organization to now collect, analyze, and store massive amounts of data because the additional infrastructure resources are made available on demand within the limits of a subscription. IT organizations, however, lack the tools required to manage and secure data at that scale. It’s not clear to what degree a full-blown data management crisis has arisen. However, the rate at which organizations are now collecting data is rapidly exceeding their ability to effectively manage that data. The challenges — and opportunity — are in developing and deploying frameworks that will bring some order to this chaos. Google hasn’t signaled its intention to solve the data management challenge. But what any single cloud provider does to address an inherently multi-platform issue may not be all that relevant at the end of the day."
https://venturebeat.com/2021/05/09/fauna-ceo-says-cloud-apis-help-developers-build-apps-without-a-database/,Fauna CEO says cloud APIs help developers build apps without a database,"Advances in distributed serverless computing have changed how databases are used in modern application development. Fauna is one startup setting out to make database development more developer-friendly with cloud APIs. Fauna is not a database in the traditional sense. The company’s website calls Fauna a “data API for modern applications.” VentureBeat sat down with Fauna CEO Eric Berg to understand what motivates the startup and what it’s doing differently. This interview has been edited for brevity and clarity. VentureBeat: So why did you end up building Fauna? Eric Berg: I think what I’ll start with is the genesis of the company, which kind of explains what’s been built. The founding technical team came out of Twitter. They had started when Twitter was small and then went through all the pains of scaling the infrastructure. Evan Weaver, who’s our CTO, ran the data infrastructure team. He tried to use every tool out there at the time, which was when a plethora of NoSQL options arose. So the team tried Memcache, Redis, Cassandra, and so on. They got frustrated with the fact that there wasn’t a good single solution and said “Why shouldn’t there be a very easy way for a team to start small on an application — then, with an incredible developer experience, be able to seamlessly scale that service without having to become an expert on caching, denormalization, replication, and other architectural issues of the backend?” All the things, the sort of baggage that goes with traditional databases as you scale, right? VentureBeat: So you want to build something simple to use? Berg: Yes, we’ve taken [the traditional database] up even a higher level of abstraction. With Fauna, you don’t know and don’t have to know about the physicality of your database at all. It literally is delivered as an API, much like Twilio’s API or Stripe’s payments API. You don’t have to pick a machine and memory, etc. You just create a database. We take care of all the replication and the scaling and everything that needs to happen on the back end. VentureBeat: So what kind of use cases are they tackling? Berg: We’ve got a set of users who come to us from a more traditional relational database background. They come from an understanding of something like PostgreSQL or MySQL. They really love the fact that we combine that relational capability, that consistency and transaction support with basically no database operations. That’s very compelling. Then we have a set of users who are really rethinking their application architecture. They tend to be more front-end developers, and they don’t have a database background. And so for them GraphQL is a very natural interface to Fauna. So that’s a very easy way for them to get started. As you go forward and you really want to tap into a lot of the underlying power of Fauna, you tend to see people then progress into using some combination of GraphQL and our FQL [query] language. VentureBeat: So developers are using it for any use case that needs to store data, right? Berg: Yes. They’re attracted to the operational flexibility. And the fact that they don’t have to know anything about caching, normalization, replication, architectural rewrites, or anything associated with them. Or anything associated with having to scale and manage a database. That’s huge. Number two is the fact that it is an API, so it’s directly accessible over HTTP. So you don’t have to have any connection pooling or any kind of overhead associated with connecting to and interfacing with the database. You can directly address the database from a web browser or from a mobile application. VentureBeat: And it’s not just the hardware support, right? You’ve also opened up the data structures to be simpler while giving Fauna the power to reason them? Berg: Fauna has a very flexible set of data models. We support both relational and document models for querying data. Then we have this temporal component that allows us to really sort of play [the data] back and forth. We also have very strong consistency guarantees, which you would traditionally associate with a relational database. Oftentimes I’ll tell people that we’re trying to combine the promise that developers loved about traditional Oracle or PostgreSQL or MySQL with the developer flexibility of something like a NoSQL document database. Then we’ve removed that operational complexity so that you can scale very easily. VentureBeat: And the developer approaches it as a basic cloud API? Berg: Yes! You can get started in a more standard interface so we support GraphQL, which is very popular these days.Then we also support a concept that’s very similar to stored procedures that you’re probably familiar with in the SQL world. We have what we call UDFs or “user-defined functions.” We have a language called FQL for that. That’s a very powerful language that allows you to basically capture your business logic [and embed it] in a Fauna database. VentureBeat: This isn’t complex to use, at least if you’re dealing with all of the HTTP AJAX calls that are so common now. Berg: Absolutely. It’s directly accessible over HTTP. You don’t have to have any connection pooling. [There’s not] any kind of overhead associated with connecting to and interfacing with the database. [That’s big], especially for those front-end developers, right? You can directly address the database from a web browser or from a mobile application. That’s very compelling. VentureBeat: This turns the database from a backend tool into something that doesn’t need the other tiers. It talks directly to the user’s machine. Berg: SQL is king in the OLAP/data warehouse world. Developers, not many of them really love it, and it takes work [with all] of the ORMs (object-relational mappings). So we’ve tried to come up with a much more developer-friendly and native experience that you can tap into from the language of your choice. VentureBeat: But then what does the developer do about security? Berg: We have a pretty powerful security model in our authorization framework. It’s an attribute-based access control layer that is native to the database. In addition, for customers that want to use a third-party authentication solution, we have standards-based integration with identity vendors like Auth0. From an identity perspective, we give people that flexibility. If you want to build your own layer, you can. Or if you want to tap into an off-the-shelf layer, you can. VentureBeat: Are there trade-offs? Berg: People might say “Well, this is not much different from a serverless database like DynamoDB.” But DynamoDB has a much less flexible developer and data modeling experience, and you’re not getting the transactionality in those services. DynamoDB evolved from Amazon’s need to solve the sort of shopping cart problem at scale. And so it’s a very simple but extremely fast key-value store that does just that. It started out to do that quickly and does that very, very well. But it doesn’t offer the same consistency or developer and data model flexibility. People might compare us to something like DynamoDB because we are both serverless and say, “Well, Fauna, you’re not sub-5 or 10 milliseconds for your latency. Why is that?” Well, if you want global consistency, then that’s going to take on the order of 100 milliseconds. And if you try to run Dynamo in their mode, which attempts to deliver transactional consistency, you will see similar latency tradeoffs. VentureBeat: And this is a big part of what the developers need. It’s another worry you take off their shoulders. Berg: Weaver has written some great and very detailed blog posts to really help people understand how Fauna’s architecture compares to other serverless databases like DynamoDB or AWS Aurora Serverless. Those posts do a great job of comparing and contrasting the developer experience and the underlying architectural approach, which is important for people to understand when they are considering the impact of those architectural trade-offs. If you’re looking for the absolute lowest latency of, say, sub-5 milliseconds, our architecture is not that. That’s not what we’ve optimized for. We offer a combination of very good performance, a seamless, no-operations-required developer experience, and a commitment to transactions. When people are building business-critical applications, where the consistency of that data is really important, that tends to be a really good fit for Fauna. And many customers are really attracted to the fact that Fauna means they have zero operational overhead for their database. VentureBeat: I noticed that the company’s name doesn’t have the letters “DB” in it. It’s just “Fauna.” Berg: Yes. There are really two things driving that. One is that we are, as we discussed, delivered as an API. Removing the DB helped better communicate the fact that our delivery model is fundamentally different. Also when you look at Fauna, it is a unique combination of a powerful storage engine and a programmable compute layer that is globally distributed. Over time, the architecture will allow us to expand Fauna’s offerings. Keeping DB just felt like it was a little bit more limiting. And so we went to a broader term “Fauna.”"
https://venturebeat.com/2021/05/09/data-estates-creating-an-architecture-thats-built-to-last/,Data estates: Creating an architecture that’s built to last,"Most of us are familiar with the concept of urban sprawl. If a city has a high degree of sprawl, significant time and money is spent on commuting, highways are always congested, and commuters show up to work frazzled — which could lead to errors in their work. At the same time, the amenities available to homes and businesses may be minimal due to lower densities. The same concept applies to data estates. Data lakes and enterprise data warehouses have been known as “slow IT platforms” that rarely serve the agility needs of the business due to an inability to manage mixed workloads. Corporations have previously been forced to sprawl out their data estates to serve the different workload and priority needs of various parts of the business because there are technical constraints from legacy data architectures. But data sprawl, like urban sprawl, can be solved. Even though each business unit, from marketing to operations, functions independently, they each depend on high quality data that is shared among everyone within the business to serve, protect, and enrich the collective. With the challenges that legacy data architecture presents and the emergence of new technologies, businesses must adopt a planning mindset for their data estates. Here are three ways businesses can simplify them: Adopting a data cloud approach where there is a centralized common data store, with multiple independent urban centers of insight, dramatically simplifies the data estate. Reducing sprawl requires increasing application or insight density per data platform, and organizations with high application densities can increase the amount of high value “amenities” such as automation, compliance, audit, and privacy available to each application leveraging the common stack. Organizations with higher application densities on their data platforms can unlock higher degrees of value by minimizing transmission loss, fidelity loss, and loss of trust. The introduction of a compute separate data cloud allows organizations to deploy business unit-specific compute clusters on top of an “enterprise data set”. Each line of business can have its own reporting and or deep learning cluster that runs independently of any other needs. Companies will be able to run or tweak multiple queries and workloads, which was cumbersome previously in a shared compute environment. Low application densities lead to tremendous amounts of waste. When analyzing data estates at client companies, I sometimes see large portions of their data across platforms originating from the same sources — meaning that centralizing data storage while providing computing independence could save high volumes of data from getting duplicated many times. Reducing data duplication reduces storage needs and lowering copies and variants of the same data increases data trust tremendously. Data duplication leads to wasted infrastructure and labor that could be redirected to create positive business outcomes instead of wrangling data. Just like with urban sprawl, replanning a data estate to reduce sprawl requires a clear roadmap and well-executed approach. Adopting data clouds, where the data and computing are separate, while reducing data estate sprawl and increasing application density per platform can improve business outcomes across all levels of the organization. A lower data estate footprint is not only good for businesses and corporations, but also supports an organization’s sustainability goals. Goutham Belliappa is VP of AI Engineering at Capgemini North America."
https://venturebeat.com/2021/05/09/after-ransomware-u-s-fuel-pipeline-colonial-pipeline-shuts-down/,"After ransomware, U.S. fuel pipeline Colonial Pipeline shuts down","(Reuters) — Top U.S. fuel pipeline operator Colonial Pipeline shut its entire network, the source of nearly half of the U.S. East Coast’s fuel supply, after a cyber attack on Friday that involved ransomware. The incident is one of the most disruptive digital ransom operations ever reported and has drawn attention to how vulnerable U.S. energy infrastructure is to hackers. A prolonged shutdown of the line would cause prices to spike at gasoline pumps ahead of peak summer driving season, a potential blow to U.S. consumers and the economy. “This is as close as you can get to the jugular of infrastructure in the United States,” said Amy Myers Jaffe, research professor and managing director of the Climate Policy Lab. “It’s not a major pipeline. It’s the pipeline.” Colonial transports 2.5 million barrels per day of gasoline, and other fuels through 5,500 miles (8,850 km) of pipelines linking refiners on the Gulf Coast to the eastern and southern United States. It also serves some of the country’s largest airports, including Atlanta’s Hartsfield Jackson Airport, the world’s busiest by passenger traffic. The company said it shut down its operations after learning of a cyberattack on Friday using ransomware. “Colonial Pipeline is taking steps to understand and resolve this issue. At this time, our primary focus is the safe and efficient restoration of our service and our efforts to return to normal operation,” it said. While the U.S. government investigation is in early stages, one former official and two industry sources said the hackers are likely a professional cybercriminal group. The former official said investigators are looking at a group dubbed “DarkSide,” known for deploying ransomware and extorting victims while avoiding targets in post-Soviet states. Ransomware is a type of malware designed to lock down systems by encrypting data and demanding payment to regain access. Colonial said it had engaged a cybersecurity firm to help the investigation and contacted law enforcement and federal agencies. The cybersecurity industry sources said cybersecurity firm FireEye was brought in to respond to the attack. FireEye declined to comment. U.S. government bodies, including the FBI, said they were aware of the situation but did not yet have details of who was behind the attack. President Joe Biden was briefed on the incident on Saturday morning, a White House spokesperson said, adding that the government is working to try to help the company restore operations and prevent supply disruptions. The Department of Energy said it was monitoring potential impacts to the nation’s energy supply, while both the U.S. Cybersecurity and Infrastructure Security Agency and the Transportation Security Administration told Reuters they were working on the situation. “We are engaged with the company and our interagency partners regarding the situation. This underscores the threat that ransomware poses to organizations regardless of size or sector,” said Eric Goldstein, executive assistant director of the cybersecurity division at CISA. Colonial did not give further details or say how long its pipelines would be shut. The privately held, Georgia-based company is owned by CDPQ Colonial Partners L.P., IFM (US) Colonial Pipeline 2 LLC, KKR-Keats Pipeline Investors L.P., Koch Capital Investments Company LLC and Shell Midstream Operating LLC. “Cybersecurity vulnerabilities have become a systemic issue,” said Algirde Pipikaite, cyber strategy lead at the World Economic Forum’s Centre for Cybersecurity. “Unless cybersecurity measures are embedded in a technology’s development phase, we are likely to see more frequent attacks on industrial systems like oil and gas pipelines or water treatment plants,” Pipikaite added. The American Automobile Association said a prolonged outage of the line could trigger increases in gas prices at the pumps, a worry for consumers ahead of summer driving season. A shutdown lasting four or five days, for example, could lead to sporadic outages at fuel terminals along the U.S. East Coast that depend on the pipeline for deliveries, said Andrew Lipow, president of consultancy Lipow Oil Associates. After the shutdown was first reported on Friday, gasoline futures on the New York Mercantile Exchange gained 0.6% while diesel futures rose 1.1%, both outpacing gains in crude oil. Gulf Coast cash prices for gasoline and diesel edged lower on prospects that supplies could accumulate in the region. “As every day goes by, it becomes a greater and greater impact on Gulf Coast oil refining,” said Lipow. “Refiners would have to react by reducing crude processing because they’ve lost part of the distribution system.” Oil refining companies contacted by Reuters on Saturday said their operations had not yet been impacted. Kinder Morgan Inc, meanwhile, said its Products (SE) Pipe Line Corporation (PPL) serving many of the same regions remains in full service. PPL is currently working with customers to accommodate additional barrels during Colonial’s downtime, it said. PPL can deliver about 720,000 bpd of fuel through its pipeline network from Louisiana to the Washington, D.C., area. The American Petroleum Institute, a top oil industry trade group, said it was monitoring the situation. Ben Sasse, a Republican senator from Nebraska and a member of the Senate Select Committee on Intelligence, said the cyberattack was a wakeup call for U.S. lawmakers. “This is a play that will be run again, and we’re not adequately prepared,” he said, adding Congress should pass an infrastructure plan that hardens sectors against these attacks. Colonial previously shut down its gasoline and distillate lines during Hurricane Harvey, which hit the Gulf Coast in 2017. That contributed to tight supplies and gasoline price rises in the United States after the hurricane forced many Gulf refineries to shut down."
https://venturebeat.com/2021/05/08/u-s-senate-committee-revised-a-draft-bill-to-fund-ai-quantum-biotech/,"U.S. Senate committee revised a draft bill to fund AI, quantum, biotech","(Reuters) -U.S. Senate committee leaders have drafted a compromise $110 billion measure for basic and advanced technology research and science over five years and the creation of a White House chief manufacturing officer in the face of rising competitive pressure from China, according to a copy of the 131-page draft legislation seen on Friday by Reuters. The revised draft bill by Senate Commerce Committee Chair Maria Cantwell and the committee’s top Republican Roger Wicker is set to be debated by the committee on Wednesday. The bipartisan “Endless Frontier” bill would authorize most of the money, $95 billion, over five years to invest in basic and advanced research, commercialization, and education and training programs in key technology areas, including artificial intelligence, semiconductors, quantum computing, advanced communications, biotechnology and advanced energy. The measure, sponsored by Senate Majority Leader Chuck Schumer, a Democrat, Republican Senator Todd Young and others, would also authorize another $10 billion to designate at least 10 regional technology hubs and create a supply chain crisis-response program to address issues like the shortfall in semiconductor chips harming auto production. The revised version also would create a new Senate-confirmed chief manufacturing officer who would serve in the executive office of the president and would head a new Office of Manufacturing and Industrial Innovation Policy. It would also direct the Commerce Department to establish “a supply chain resiliency and crisis response program,” including “the ability of supply chains to resist and recover in the face of shocks, including pandemic and biological threats, cyberattacks, extreme weather events, terrorist and geopolitical attacks, great power conflict, and other threats.” The bill also seeks to boost basic research to accelerate innovation to advance critical minerals mining strategies and technologies to eliminate “national reliance on minerals and mineral materials that are subject to supply disruptions.” The draft bill would also block Chinese companies from participating in the Manufacturing USA program without a waiver. The program is a government and company-led effort to build up industrial competitiveness, cut energy use, and strengthen U.S. national security."
https://venturebeat.com/2021/05/08/effective-data-leaders-focus-on-decision-making-sharing-gartner-says/,"Effective data leaders focus on decision-making, sharing, Gartner says","The boundaries between chief data officers and chief digital officers are blurring as data officers are increasingly asked to take on more strategic objectives and lead digital transformation initiatives, according to market research firm Gartner. Almost three-quarters (72%) of data and analytics leaders with digital initiatives are either leading — or heavily involved in — their organization’s digital transformation initiatives, Gartner said in its sixth Chief Data Officer survey. Chief data officers with business-facing key performance indicators and partners across the business were 1.7 times more likely to demonstrate return on investment and business value, according to the survey of 469 high-level data and analytics leaders. Less successful data leaders focus on technology instead of people. “Focus on the outcomes: That’s the way to win,” Gartner VP Andrew White said during this week’s Gartner Data and Analytics Summit, which was held virtually this year. “It’s not about technology, it’s not about the dashboard, it’s about the decision.” As more business leaders rely on data-driven insights to make decisions, securing data and developing an analytics strategy become more important. The chief data officer performs a mission-critical function — on par with IT business operations, HR, and finance — in three-quarters of large organizations, Gartner said. Data and analytics leaders typically focus on enhancing data quality, increasing the value of their D&A investments, and improving data sharing. Data sharing accelerates digital transformation because it optimizes higher-quality data to solve business challenges and goals. Gartner predicts that by 2023, organizations that promote data sharing will outperform their peers on most business value metrics. Data leaders also need to focus on meeting business goals and requirements. Data leaders who don’t establish a clear link between data and analytics with business outcomes are less successful, the survey found. “If the CDO does not exert influence across the organization by promoting data sharing, engaging stakeholders, and training the workforce to become data literate, they will likely not perform well in their role,” Gartner VP Debra Logan said in a statement. While every enterprise may not appoint a dedicated chief data officer, many are exploring ways to be more strategic about how the business works with data. By 2023, half of the enterprises that don’t have a chief data officer will require an executive to become the de facto CDO if they want to succeed, Gartner said. “The results indicate that more organizations now understand the synergy between building a data-driven business and leading digital transformation,” Logan said. “D&A strategy is a business strategy infused with D&A thinking; it has a primary role in digital business strategy, affecting everything the organization does.”"
https://venturebeat.com/2021/05/08/the-four-most-common-fallacies-about-ai/,The four most common fallacies about AI,"The history of artificial intelligence has been marked by repeated cycles of extreme optimism and promise followed by disillusionment and disappointment. Today’s AI systems can perform complicated tasks in a wide range of areas, such as mathematics, games, and photorealistic image generation. But some of the early goals of AI like housekeeper robots and self-driving cars continue to recede as we approach them. Part of the continued cycle of missing these goals is due to incorrect assumptions about AI and natural intelligence, according to Melanie Mitchell, Davis Professor of Complexity at the Santa Fe Institute and author of Artificial Intelligence: A Guide For Thinking Humans. In a new paper titled “Why AI is Harder Than We Think,” Mitchell lays out four common fallacies about AI that cause misunderstandings not only among the public and the media, but also among experts. These fallacies give a false sense of confidence about how close we are to achieving artificial general intelligence, AI systems that can match the cognitive and general problem-solving skills of humans. The kind of AI that we have today can be very good at solving narrowly defined problems. They can outmatch humans at Go and chess, find cancerous patterns in x-ray images with remarkable accuracy, and convert audio data to text. But designing systems that can solve single problems does not necessarily get us closer to solving more complicated problems. Mitchell describes the first fallacy as “Narrow intelligence is on a continuum with general intelligence.” “If people see a machine do something amazing, albeit in a narrow area, they often assume the field is that much further along toward general AI,” Mitchell writes in her paper. For instance, today’s natural language processing systems have come a long way toward solving many different problems, such as translation, text generation, and question-answering on specific problems. At the same time, we have deep learning systems that can convert voice data to text in real-time. Behind each of these achievements are thousands of hours of research and development (and millions of dollars spent on computing and data). But the AI community still hasn’t solved the problem of creating agents that can engage in open-ended conversations without losing coherence over long stretches. Such a system requires more than just solving smaller problems; it requires common sense, one of the key unsolved challenges of AI. When it comes to humans, we would expect an intelligent person to do hard things that take years of study and practice. Examples might include tasks such as solving calculus and physics problems, playing chess at grandmaster level, or memorizing a lot of poems. But decades of AI research have proven that the hard tasks, those that require conscious attention, are easier to automate. It is the easy tasks, the things that we take for granted, that are hard to automate. Mitchell describes the second fallacy as “Easy things are easy and hard things are hard.” “The things that we humans do without much thought—looking out in the world and making sense of what we see, carrying on a conversation, walking down a crowded sidewalk without bumping into anyone—turn out to be the hardest challenges for machines,” Mitchell writes. “Conversely, it’s often easier to get machines to do things that are very hard for humans; for example, solving complex mathematical problems, mastering games like chess and Go, and translating sentences between hundreds of languages have all turned out to be relatively easier for machines.” Consider vision, for example. Over billions of years, organisms have developed complex apparatuses for processing light signals. Animals use their eyes to take stock of the objects surrounding them, navigate their surroundings, find food, detect threats, and accomplish many other tasks that are vital to their survival. We humans have inherited all those capabilities from our ancestors and use them without conscious thought. But the underlying mechanism is indeed more complicated than large mathematical formulas that frustrate us through high school and college. Case in point: We still don’t have computer vision systems that are nearly as versatile as human vision. We have managed to create artificial neural networks that roughly mimic parts of the animal and human vision system, such as detecting objects and segmenting images. But they are brittle, sensitive to many different kinds of perturbations, and they can’t mimic the full scope of tasks that biological vision can accomplish. That’s why, for instance, the computer vision systems used in self-driving cars need to be complemented with advanced technology such as lidars and mapping data. Another area that has proven to be very difficult is sensorimotor skills that humans master without explicit training. Think of the how you handle objects, walk, run, and jump. These are tasks that you can do without conscious thought. In fact, while walking, you can do other things, such as listen to a podcast or talk on the phone. But these kinds of skills remain a large and expensive challenge for current AI systems. “AI is harder than we think, because we are largely unconscious of the complexity of our own thought processes,” Mitchell writes. The field of AI is replete with vocabulary that puts software on the same level as human intelligence. We use terms such as “learn,” “understand,” “read,” and “think” to describe how AI algorithms work. While such anthropomorphic terms often serve as shorthand to help convey complex software mechanisms, they can mislead us to think that current AI systems work like the human mind. Mitchell calls this fallacy “the lure of wishful mnemonics” and writes, “Such shorthand can be misleading to the public trying to understand these results (and to the media reporting on them), and can also unconsciously shape the way even AI experts think about their systems and how closely these systems resemble human intelligence.” The wishful mnemonics fallacy has also led the AI community to name algorithm-evaluation benchmarks in ways that are misleading. Consider, for example, the General Language Understanding Evaluation (GLUE) benchmark, developed by some of the most esteemed organizations and academic institutions in AI. GLUE provides a set of tasks that help evaluate how a language model can generalize its capabilities beyond the task it has been trained for. But contrary to what the media portray, if an AI agent gets a higher GLUE score than a human, it doesn’t mean that it is better at language understanding than humans. “While machines can outperform humans on these particular benchmarks, AI systems are still far from matching the more general human abilities we associate with the benchmarks’ names,” Mitchell writes. A stark example of wishful mnemonics is a 2017 project at Facebook Artificial Intelligence Research, in which scientists trained two AI agents to negotiate on tasks based on human conversations. In their blog post, the researchers noted that “updating the parameters of both agents led to divergence from human language as the agents developed their own language for negotiating [emphasis mine].” This led to a stream of clickbait articles that warned about AI systems that were becoming smarter than humans and were communicating in secret dialects. Four years later, the most advanced language models still struggle with understanding basic concepts that most humans learn at a very young age without being instructed. Can intelligence exist in isolation from a rich physical experience of the world? This is a question that scientists and philosophers have puzzled over for centuries. One school of thought believes that intelligence is all in the brain and can be separated from the body, also known as the “brain in a vat” theory. Mitchell calls it the “Intelligence is all in the brain” fallacy. With the right algorithms and data, the thinking goes, we can create AI that lives in servers and matches human intelligence. For the proponents of this way of thinking, especially those who support pure deep learning–based approaches, reaching general AI hinges on gathering the right amount of data and creating larger and larger neural networks. Meanwhile, there’s growing evidence that this approach is doomed to fail. “A growing cadre of researchers is questioning the basis of the ‘all in the brain’ information processing model for understanding intelligence and for creating AI,” she writes. Human and animal brains have evolved along with all other body organs with the ultimate goal of improving chances of survival. Our intelligence is tightly linked to the limits and capabilities of our bodies. And there is an expanding field of embodied AI that aims to create agents that develop intelligent skills by interacting with their environment through different sensory stimuli. Mitchell notes that neuroscience research suggests that “neural structures controlling cognition are richly linked to those controlling sensory and motor systems, and that abstract thinking exploits body-based neural ‘maps.’” And in fact, there’s growing evidence and research that proves feedback from different sensory areas of the brain affects both our conscious and unconscious thoughts. Mitchell supports the idea that emotions, feelings, subconscious biases, and physical experience are inseparable from intelligence. “Nothing in our knowledge of psychology or neuroscience supports the possibility that ‘pure rationality’ is separable from the emotions and cultural biases that shape our cognition and our objectives,” she writes. “Instead, what we’ve learned from research in embodied cognition is that human intelligence seems to be a strongly integrated system with closely interconnected attributes, including emotions, desires, a strong sense of selfhood and autonomy, and a commonsense understanding of the world. It’s not at all clear that these attributes can be separated.” Developing general AI needs an adjustment to our understanding of intelligence itself. We are still struggling to define what intelligence is and how to measure it in artificial and natural beings. “It’s clear that to make and assess progress in AI more effectively, we will need to develop a better vocabulary for talking about what machines can do,” Mitchell writes. “And more generally, we will need a better scientific understanding of intelligence as it manifests in different systems in nature.” Another challenge that Mitchell discusses in her paper is that of common sense, which she describes as “a kind of umbrella for what’s missing from today’s state-of-the-art AI systems.” Common sense includes the knowledge that we acquire about the world and apply it every day without much effort. We learn a lot without being explicitly instructed, by exploring the world when we are children. These include concepts such as space, time, gravity, and the physical properties of objects. For example, a child learns at a very young age that when an object becomes occluded behind another, it has not disappeared and continues to exist, or when a ball rolls across a table and reaches the ledge, it should fall off. We use this knowledge to build mental models of the world, make causal inferences, and predict future states with decent accuracy. This kind of knowledge is missing in today’s AI systems, which makes them unpredictable and data-hungry. In fact, housekeeping and driving, the two AI applications mentioned at the beginning of this article, are things that most humans learn through common sense and a little bit of practice. Common sense also includes basic facts about human nature and life, things that we omit in our conversations and writing because we know our readers and listeners know them. For example, we know that if two people are “talking on the phone,” it means that they aren’t in the same room. We also know that if “John reached for the sugar,” it means that there was a container with sugar inside it somewhere near John. This kind of knowledge is crucial to areas such as natural language processing. “No one yet knows how to capture such knowledge or abilities in machines. This is the current frontier of AI research, and one encouraging way forward is to tap into what’s known about the development of these abilities in young children,” Mitchell writes. While we still don’t know the answers to many of these questions, a first step toward finding solutions is being aware of our own erroneous thoughts. “Understanding these fallacies and their subtle influences can point to directions for creating more robust, trustworthy, and perhaps actually intelligent AI systems,” Mitchell writes. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/05/08/ai-is-turning-us-into-de-facto-cyborgs/,AI is turning us into de facto cyborgs,"Progress in technology and increased levels of private investment in startup AI companies is accelerating, according to the 2021 AI Index, an annual study of AI impact and progress developed by an interdisciplinary team at the Stanford Institute for Human-Centered Artificial Intelligence. Indeed, AI is showing up just about everywhere. In recent weeks, there have been stories of how AI is used to monitor the emotional state of cows and pigs, dodge space junk in orbit, teach American Sign Language, speed up assembly lines, win elite crossword puzzle tournaments, assist fry cooks with hamburgers, and enable “hyperautomation.” Soon there will be little left for humans to do beyond writing long-form journalism — until that, too, is replaced by AI. The text generation engine GPT-3 from OpenAI is potentially revolutionary in this regard, leading a New Yorker essay to claim: “Whatever field you are in, if it uses language, it is about to be transformed.” AI is marching forward, and its wonders are increasingly evident and applied. But the outcome of an AI-forward world is up for debate. While this debate is underway, at present it focuses primarily on data privacy and how bias can negatively impact different social groups. Another potentially greater concern is that we are becoming dangerously dependent on our smart devices and applications. This reliance could lead us to become less inquisitive and more trusting of the information we are provided as accurate and authoritative. Or that, like in the animated film WALL-E, we will be glued to screens, distracted by mindless entertainment, literally and figuratively fed empty calories without lifting a finger while an automated economy carries on without us. In this increasingly plausible near-future scenario, people will move through life on autopilot, just like our cars. Or perhaps we have already arrived at such a place. Caption: Humans on the Axiom spaceship in Pixar’s WALL-E; Source If smartphone use is any indication, there is cause for worry. Nicolas Carr wrote in The Wall Street Journal about research suggesting our intellect weakens as our brain grows dependent on phone technology. Likely the same could be said for any information technology where content flows our way without us having to work to learn or discover on our own. If that’s true, then AI applications, which increasingly present content tailored to our specific interests, could create a self-reinforcing syndrome that not only locks us into our information bubbles through algorithmic editing, but also weakens our ability to engage in critical thought by spoon-feeding us what we already believe. Tristan Green argues that humans are already cyborgs, human-machine hybrids that are heavily dependent on information flowing from the Internet. During a weekend without access to this constant connection: “I found myself having difficulty thinking. By Sunday evening I realized that I use the almost-instant connection I have with the internet to augment my mental abilities almost constantly. … I wasn’t aware of how much I rely on the AI-powered internet as a performance aid.” Which is perhaps why Elon Musk believes we will need to augment our brains with instantaneous access to the Internet for humans to effectively compete with AI, this being the initial rationale behind his Neuralink brain-machine interface company. I’ve read many analyses from AI pundits arguing that AI will be no different from other technology innovations, such as the transition from the horse economy to the automobile. Usually these arguments are made in the context of AI’s impact on jobs and concludes there will be social displacement in the short-term for some but long-term growth for the collective whole. The thinking is that new industries will birth new jobs and malleable people will adapt. But there is a fundamental difference with the AI revolution. Previous instances involved replacing brute force with labor-saving automation. AI is different in that it outsources more than physical labor, it also outsources cognition, which is thinking and decision making. Shaun Nichols, professor in the Sage School of Philosophy at Cornell University, said in a recent panel discussion on AI: “We already outsource ethically important decisions to algorithms. We use them for kidney transplants, air traffic control, and to determine who gets treated first in emergency rooms.” As stated by the World Economic Forum, we are progressively subject to decisions with the assistance of — or even taken by — AI systems. Algorithms now shape our thoughts and increasingly make decisions on our behalf. Wittingly or not, AI is doing so much for us that some are dubbing it an “intelligence revolution,” which forces the question, have we already become de facto cyborgs, and if so, do we still have agency? Agency is the power of humans to think for ourselves and act in ways that shape our experiences and life trajectories. Yet, the algorithms driving search and social media platforms, book and movie recommendations, regularly shape what billions of people read and see. If this was thoughtfully curated for our betterment, it might be okay. But as film director Martin Scorsese states, their purpose is only to increase consumption. It seems we have already outsourced agency to algorithms designed to increase corporate well-being. This may not be overtly malicious, but it is hardly benign. Our thoughts are being molded, either by our existing beliefs that are reinforced by algorithms inferring our interests, or through intentional or unintentional biases from the various information platforms. Which is to say that our ability to perform critical thinking is both constrained and shaped by the very systems meant to aid and hopefully stimulate our thinking. We are entering a recursive loop where thinking coalesces into ever tighter groupings — the often-discussed polarization — that reduce variability and hence diversity of opinion. It is as if we are the subjects in a grand social science experiment, with the resulting human opinion clusters determined by the AI-powered inputs and the outputs discerned by machine learning. This is qualitatively different from an augmentation of intelligence and instead augers a merger of humans and machines that is creating the ultimate group think. It has never been easy to confront large societal problems, but they will become more challenging if humanity continues the path of outsourcing its thinking to algorithms that are not in our collective best interests. All of which begs the question, do we control AI technology or are we already being controlled by the technology? Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/05/07/jpmorgan-names-two-new-cios-for-tech-units/,JPMorgan names two new CIOs for tech units,"(Reuters) – JPMorgan Chase has appointed James Reid as chief information officer for a new unit focused on developing and modernising technology used by the bank’s 250,000 employees, an internal memo sent on Friday and seen by Reuters showed. Reid, who was most recently head of corporate technology’s engineering and architecture team, becomes the bank’s first Black CIO and the first Black member of its technology leadership team. JPMorgan, the largest U.S. bank by assets, has also appointed company-wide chief data officer Melissa Goldman as chief information officer for the renamed Finance, Risk, Data and Controls technology unit, the memo said. In her role, Goldman will continue to lead a team developing technology for risk, compliance, finance, liquidity, controls and data functions. Many banks are seeking to modernise their systems, as more customers seek to interact with them digitally, and employees work and collaborate remotely. The appointments also follow a renewed commitment by JPMorgan late last year to improve the diversity of its workforce and increase racial and gender diversity in its most senior ranks. As CIO for Employee Experience and Corporate Technology, Reid will report to the banking group’s CIO Lori Beer and lead a team that builds and maintains systems used across the bank’s corporate functions, such as HR, legal and audit, tax and robotics, the memo said. Reid is a Distinguished Engineer, a title given to a select group of JPMorgan’s technologists who drive the firm’s technology strategy, as well as a member of the Cloud Design Council, and a leader of diversity and inclusion initiatives at the company. Prior to joining JPMorgan in 2019, Reid worked at Equifax for 17 years, where he was senior vice president of core software engineering."
https://venturebeat.com/2021/05/07/geographic-databases-hold-worlds-of-information/,Geographic databases hold worlds of information,"Geographic databases and associated geospatial information systems were created to hold complex information about the world and answer the questions about location that are often the first questions analysts ask of data. Standard databases can hold basic values like a single number or a snippet of text in tables, but the standard routines for searching or selecting information in a query don’t work with real-world data that occupies two or three dimensions. Geographic databases and geospatial information systems efficiently specialize in this advanced style of data processing. Geographic databases can store more complex elements needed to describe the world and the roads or buildings built upon it. The basic data element is the point that is the combination of the longitude, latitude, and sometimes the altitude. The points can be joined together into polygons that might represent political boundaries or regions of a map. These polygons can be joined together or subtracted with set operations like union to build complex representations. Capable geographic databases can compute complex functions like determining the distance between two points or whether one point lies inside a polygon. Some can adjust the answers to account for the curvature of the earth. Some common uses include: Geographic databases have become increasingly essential in enterprises. Many geographic databases are built as a set of extra features that can be installed in a more traditional database, extending the standard data formats to handle geometry. For example, SQL becomes GEOSQL; XML becomes GML; JSON becomes GeoJSON, and so on. These collections of features take on the form of a GIS (for geographical information system). Many popular tools like ArcGIS rely upon the support of a geographically aware database. All of the major databases have added the ability to store spatial data, either in the main database or through an extension that integrates with it. One of the most popular options is PostGIS, a version of PostgreSQL with extra geographic functions. The U.S. Census Bureau, for example, distributes large files with the boundaries for all of the census tracts in a format that’s simple to load into PostGIS. The data is often used with the census results to answer questions about the population. Restaurant chains, for example, might analyze potential locations by counting how many people live nearby. Oracle’s Spatial Database includes extra features for tracking locations in two or three dimensions. It might be used for geographic data, but it can also work with arbitrary three-dimensional collections of points, the kind that might be generated when analyzing a scene for planning the movement of a robot or an autonomous vehicle. It is regularly used to track networks like collections of roads or fiber optic cables that make up the infrastructure of a region. IBM’s Db2 can be extended to offer the standard GIS features. The company also offers higher level tools like the Maximo Spatial Asset Manager that will track the locations and configurations of infrastructure elements. The database will store the location, and an additional layer will create maps or other visualizations. Microsoft’s SQL server can store two types of spatial data, the so-called geometry for two-dimensional environments and the geography for three-dimensional parts of the world. The elements can be built out of simpler points or lines or more complex curved sections. The company has also added a set of geographic data formats and indexing to its cloud-based Azure Cosmos DB NoSQL database. It is intended to simplify geographic analysis of your data set for tasks such as computing store performance by location. Noted for a strong lineage in geographic data processing, ESRI, the creator of ArcGIS, is also expanding to offer cloud services that will first store geographic information and then display it in any of the various formats the company pioneered. ESRI, traditionally a big supplier to government agencies, has developed sophisticated tools for rendering geographic data in a way that’s useful to fire departments, city planners, health departments, and others who want to visualize how a variety of data looks on a map. There is a rich collection of open source databases devoted to curating geographic information. Some focus on roads and buildings, the information normally found on maps, while others track information that might be shown in colored layers on top of the maps like the population count or the percentage of residents who smoke cigarettes. GeoServer, for instance, collects many standard datasets in an open source tool written in Java. The software is tightly integrated with a collection of mapping modules and extensions that ease the production of maps from the data. MapServer is another open source tool that’s optimized for displaying geographic data through web interfaces. It offers cross-platform support and focuses more information on the easy display of geographic information. The OpenStreetMaps project tracks the location of roads, buildings and street-level infrastructure with an interface that allows anyone to edit these like a wiki. The data set, which is shared widely, is the foundation for many mapping tools and policy analysis packages. The OSM data is easily imported into GIS-compatible databases for further analysis. Many of the GIS databases are integrated into larger products that can be used to create maps or geographically driven policy documents. SimplyAnalytics, PolicyMap, ZeeMaps, and SocialExplorer, for instance, integrate datasets and make it simpler to pick and choose the right information to display so users can visualize the connection between data values and locations. Geographic extensions to databases make it simpler to search for and apply basic measurement algorithms to points, lines, and polygons that may be on a flat surface or a curved one like the Earth’s. They can find closest points, intersect polygons, and measure distances or areas. These extra features extend the querying capabilities of the databases for these special data types because regular search queries don’t work with two- or three-dimensional data. More complicated functions are generally left to tools that are built on top of the databases. Routing algorithms used by navigation systems, for instance, tend to be separate. Tools like MapDust that help find inconsistencies or errors in the geographic data are also appearing as separate tools."
https://venturebeat.com/2021/05/07/ai-weekly-qualcomms-ai-research-and-development-efforts/,AI Weekly: Qualcomm’s AI research and development efforts,"This week marked the start of the International Conference on Learning Representations (ICLR) 2021, an event dedicated to research in deep learning, a subfield of AI inspired by the structure of the brain. One of the world’s largest machine learning conferences, it accepted 860 research papers from thousands of participants this year, up from 687 papers in 2020. One of the participating researchers is Jilei Hou, VP of engineering at Qualcomm. He heads up Qualcomm’s AI Research division, which focuses on advancing AI to bring its core capabilities — including perception, reasoning, and action — to Qualcomm’s portfolio of hardware products. Together with his colleagues at the company, Hou presented new papers at ICLR in the areas of power and energy efficiency, computer vision, natural language processing, and machine learning fundamentals. Qualcomm’s research, while in some cases preliminary, is impactful by nature of the company’s market footprint. In the second quarter of 2020, Qualcomm accounted for 32% of global smartphone application processor revenue, according to Statista. And as of January 2017, the company had shipped more than a billion chips for the internet of things alone. An important research direction for Qualcomm is learning representation, which would allow AI systems to learn with high data efficiency, as well as generalizability. At ICLR, Hou detailed the company’s work in unsupervised learning, where an algorithm is subjected to “unknown” data for which previously defined categories or labels don’t exist. The machine learning system must teach itself to classify the data, processing the unlabeled data to learn from its inherent structure. Hou says his team achieved state-of-the-art performance with end-to-end learning for video compression, a key use case for Qualcomm’s mobile device customers. Beyond this, he and coauthors have explored “neural augmentation,” or the concept that classical algorithms and neural network architectures can be combined to incorporate scientific knowledge. Neural augmentation is essentially the marriage of neural networks and symbolic AI, which involves embedding facts and behavior rules into models. As opposed to neural networks, which map data inputs and outputs, symbolic AI can encode knowledge or programs. The neural networks help identify subtle patterns that may be too complex to model explicitly. Hou believes that neural augmentation could result in compact neural network model sizes and superior efficiency while training. His team has already seen success within the areas of wireless, multimedia, and systems design. More recently, Hou and colleagues investigated using machine learning as a design methodology for combinatorial optimization problems like vehicle traffic routing and chip design placement. They claim to have trained specialized models with unlabeled data and reinforcement learning, which deals with learning via interaction and continuous feedback. “We believe that the intersection of machine learning and combinatorial optimization will produce profound interest in the machine learning research community, as well as toward industrial impact,” Hou told VentureBeat. In the computer vision domain, several of Hou’s projects target segmentation. Object segmentation is used in tasks ranging from swapping out the background of a video chat to teaching robots that navigate through a factory. But it’s considered among the hardest challenges in computer vision because it requires an AI to understand what’s in an image. A Qualcomm-authored paper details improvements in the accuracy of segmentation, and another describes the fastest video segmentation to date on Qualcomm’s Snapdragon chipsets. Hou and colleagues also created a model that improves the consistency of segmentation while allowing fine-tuning on a mobile device. One of the ways Hou aims to attain performance gains is through neural architecture search (NAS) techniques. NAS teases out top model architectures for tasks by testing candidate models’ overall performance, dispensing with manual fine-tuning. In a complementary effort, Hou says Qualcomm is investing in personalization and federated learning technologies that allow neural network models to continually learn on-device while keeping data with users, in the interests of privacy. “The mission of Qualcomm AI Research is to make AI ubiquitous,” Hou said. “Qualcomm AI Research is taking a holistic approach to model efficiency research via research efforts in quantization, compression, NAS, and compilation … By creating these projects and making it easy for developers to use them, we are empowering the ecosystem to run complex AI workloads efficiently. [They’re] already helping the wider AI ecosystem and having real-world impact on a variety of industry verticals.” For AI coverage, send news tips to Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark our AI channel, The Machine. Thanks for reading, Kyle Wiggers AI Staff Writer"
https://venturebeat.com/2021/05/07/gartner-says-composable-data-and-analytics-key-to-digital-transformation/,Gartner says composable data and analytics key to digital transformation,"Gartner wrapped up the Data & Analytics Summit Americas 2021 virtual event this week with a lively overview of top trends for enterprises to watch. Overall, Gartner analysts saw pressing trends around accelerating change, operationalizing business value, and — increasingly — “distributed everything.” Accelerating change these days means feeding and scaling AI, and composable data and analytics are key, Gartner analyst Donald Feinberg said. This is about making it easy to assemble AI from across many different tools for BI, data management, and predictive analytics. This trend will allow companies to use microservices and containerization to bring together the pieces necessary to create a service, Feinberg said. “This is a great way to pursue experiments because you can pick and choose how it works together,” Feinberg added. Composable data and analytics initiatives might uncover new ways of packaging data as part of a service or product. These could be built using low-code and no-code tools that are sold via the cloud or new kinds of data service intermediaries. Providing the foundation for composable data and analytics is the data fabric that allows easy access and sharing across distributed data environments. “You should not have to worry about where it is and how to access it,” Feinberg said of composable data. It is not a single tool but rather the set of tools put together into a solution. Metadata powered by a graph database is the glue that holds this together. It is not easy to do, but the technology is getting better. There is a growing need to weave a wider variety of data into applications to improve situational awareness and decision-making. COVID-19 caused a lot of historical data to become obsolete. There are also many small data use cases where there is just less data to work with. This trend requires investigating technologies like federated learning, few-shot learning, and content analytics that can organize new types of data such as voice, text, and video. There is more on the accelerated road to digital transformation and responsible, scalable AI. Teams now need to pay attention to new privacy and AI models, Gartner analyst Rita Sallam said. Trust is growing in importance, owing to regulations like GDPR in Europe and CCPA in California, along with new AI regulations being proposed in Europe. “We see that many organizations are struggling with scaling AI prototypes and pilots into production, and the effort to integrate AI into production is underestimated,” Sallam said. Gartner said business-facing data initiatives were key drivers of digital transformation in the enterprise. Research showed that 72% of data and analytics leaders are leading, or are heavily involved, in their organizations’ digital transformation efforts. These data leaders now confront emerging trends on various fronts. XOps: The evolution of DataOps to support AI and machine learning workflows is now XOps. The X could also stand for MLOps, ModelOps, and even FinOps. This promises to bring flexibility and agility in coordinating the infrastructure, data sources, and business needs in new ways. Engineering decision intelligence: Decision support is not new, but now decision-making is more complex. Engineering decision intelligence frames a wide range of techniques, from conventional analytics to AI to align and tune decision models and make them more repeatable, understandable, and traceable. Data and analytics as the core business function: With the chaos of the pandemic and other disruptors, data and analytics are becoming more central to an organization’s success. Companies will have to prioritize data and analytics as core functions rather than as secondary activity handled by IT. This will also drive data literacy efforts and new organizational models that distribute analytics functions across more teams. Everything is distributed, and graph relates everything: Graph databases have been around for a while but struggled due to limited tools, data sources, and workflows. But the technology is seeing major growth due to graph data improvements in popular BI and analytics tools. There are a wide variety of graph techniques for representing knowledge, relationships, properties, social networks, business rules, and metadata. Gartner predicts that graph technologies will underpin 80% of data analytics innovations by 2025. Data and analytics at the edge: The internet of things (IoT) allows enterprises to work with data at the edge. What’s new is the different ways enterprises are also embedding analytics, AI, and decision intelligence into edge applications. Use cases include providing better predictive maintenance for factories, delivering new insights to oil rigs, and enabling better mobile apps. The edge improves speed and resiliency because there’s no need for constant cloud connectivity. However, handling analytics at the edge complicates governance, so enterprises need to find tools that help with governance and analytics at the edge, Feinberg said. Rise of the augmented consumer: Gartner is focusing on business consumers and the importance of making analytics exploration easier and richer, such as with the shift from predesigned dashboards to new, more automated and dynamic presentation and delivery of analytics. This will shift the analytics superpower to the augmented consumer, Sallam said. Expect to see significant growth in vendors that deliver more conversational and interactive analytics experiences across new channels, such as voice, mobile, and web applications, she said. Gartner’s presenters advised enterprises to keep in mind that these are all technologies and practices companies can pick up and apply today using commercial software. It’s useful to consider the entire collection of trends in an integrated manner and then prioritize the one worth researching for your own business domain, with an eye toward how it may work for others, the analysts said."
https://venturebeat.com/2021/05/07/the-problem-with-apis/,The problem with APIs,"APIs — the application programming interfaces that efficiently connect disparate systems — have been likened to electrical sockets, bartenders, passport clerks, and magic. Though they have been around for more than two decades, in the past 10 years APIs have enjoyed exponential growth in lockstep with the explosion in mobile and web apps. While not much to look at, these machine-readable snippets — and the data they help shuttle about — let programmers dream up and deliver amazing new combinations of existing resources at a pace previously unheard-of in software development. Browse a real estate app and get a pin on a map to show the location of your dream home? An API did that. Check the weather on your phone before going for a run? That’s an API at work. Have your credit score accidently revealed to folks with no business looking at it? Whoops, also an API. Security researchers revealed last week that major credit bureau Experian was serving up private credit data via a leaky API. Experian is just one of several companies to land on a growing list of API-related security blunders: In a world that increasingly relies on the kinds of apps and functionality APIs enable, such data disclosures are a big problem. In Google’s State of API Economy 2021 research, 58% of global enterprise IT decision-makers said APIs are speeding new app development, and 53% said APIs are vital to building better digital experiences and products. That makes securing APIs an imperative. “We are absolutely seeing more API security incidents of late,” Michael Isbitski, a technical evangelist at API security specialist Salt Security, told VentureBeat. “Applications today are built on APIs, and you can draw a straight line from digital transformation to APIs.” A Salt research report on the state of API security published earlier this year found that 91% of organizations had suffered some sort of API security incident. Despite that, more than a quarter of businesses running production APIs have no API security strategy in place. Two-thirds of respondents told Salt researchers they were holding off on new app deployments because of API security concerns, highlighting how security concerns can drain innovation. Part of this API insecurity is rooted in how modern application development handles code. In decentralized, multi-faceted application development environments, “no single group is really building the full picture for a given app, top to bottom, anymore,” Isbitski said. “Little aspects get missed.” Combine that with the limitations of most dev-focused testing and it becomes impossible for developers to see everything an app will do when it is released to the public. “Attack methods aren’t really changing. They rarely do. But it’s very true that hackers are targeting APIs in a much more concentrated way,” says Isbitski, whose company makes products that inventory APIs and monitor their use and behavior to thwart attacks. “APIs are the map to a company’s crown jewels, and the hackers know that. It’s very lucrative to attack APIs.” Security consulting, testing, and training firm Secure Ideas in Jacksonville, Florida specializes in probing web and mobile applications and the APIs and microservices that fuel them. Its CEO, Kevin Johnson, is a longtime project lead at the Open Web Application Security Project (OWASP) and author of the well-known SamuraiWTF app security training environment. As an API security veteran, Johnson says the problem is growing because the environment itself is ballooning. Digital transformation initiatives and the ability to reach customers, employees, and partners in any place at any time depends on the API. “We are seeing more APIs deployed,” he told VentureBeat. “There’s definitely a bigger target space as more organizations are rolling out APIs either for themselves or to share with partners or customers.” Johnson calls the Peloton incident a prime example of a decent company that had a good idea for using APIs to rapidly develop a new, valuable service “but with security as an afterthought.” He added, “In fairness, a lot of companies like this don’t even know what ‘make it secure’ really means.” Isbitski says when auditing a client’s software environment, three things are universally true: Organizations have many more APIs than they realize, they are unwittingly exposing sensitive data via APIs, and their current API defenses are easily circumvented. Basic technology solutions are rarely the prescription for such ills. “Every company running any amount of APIs has deployed WAFs and API gateways,” Isbitski says. “Even the simplest of API attacks get through every time. Companies are always surprised, and a bit alarmed, to see the level of exposure they’re facing.” Johnson wholeheartedly agrees, saying when it comes to APIs, well-architected software, rare as it may be, trumps security point products every time. “One of the most common issues [we see] is a lack of rate-limiting, particularly around the authentication routes,” Johnson said. “App developers have mostly been forced to put in anti-harvesting and anti-brute-force controls, but APIs rarely do this properly.” One thing nearly all experts agree on is that the solution to the API security problem starts with developers. There’s a desperate need for standardized guidance to help developers target and eliminate the most common API flaws. While the Open Web Application Security Project (OWASP) published the API Security Top 10 to help developers think about ways to protect the API, it isn’t enough. Even an OWASP evangelist like Johnson concedes the Top 10 effort has its shortcomings. “I find the Top 10s to be helpful in that they spread awareness, but harmful as often people will assume that is all they need to worry about,” Johnson said. Gartner recommends organizations up their API governance game in the following ways: For Johnson, the solution includes giving developers the tools to succeed rather than simply blaming them when things go wrong. “We need affordable, accessible training for security professionals and regular, high-quality security training for every developer,” Johnson said. “There’s always going to be issues. There will always be some successful attacks. But mitigating the risk starts with education, which is also a tool to maturing the security program of an organization.” “An organization with a mature security program will still have security incidents, but they’ll have measures in place to detect, respond to, and reduce the scope of the incident,” he said. Isbitski agrees with this approach and said such initiatives need to start soon. “I would love to see companies educate their security teams, mandate better discovery around APIs, and demand runtime protection that’s capable of finding and stopping API attacks,” Isbitski said. “They present too great a risk to wait for the next six months of headlines.”"
https://venturebeat.com/2021/05/07/harnessing-the-power-of-machine-learning-with-mlops/,Harnessing the power of machine learning with MLOps,"MLOps, a compound of “machine learning” and “information technology operations,” is a newer discipline involving collaboration between data scientists and IT professionals with the aim of productizing machine learning algorithms. The market for such solutions could grow from a nascent $350 million to $4 billion by 2025, according to Cognilytica. But certain nuances can make implementing MLOps a challenge. A survey by NewVantage Partners found that only 15% of leading enterprises have deployed AI capabilities into production at any scale. Still, the business value of MLOps can’t be ignored. A robust data strategy enables enterprises to respond to changing circumstances, in part by frequently building and testing machine learning technologies and releasing them into production. MLOps essentially aims to capture and expand on previous operational practices while extending these practices to manage the unique challenges of machine learning. MLOps, which was born at the intersection of DevOps, data engineering, and machine learning, is similar to DevOps but differs in execution. MLOps combines different skill sets: those of data scientists specializing in algorithms, mathematics, simulations, and developer tools and those of operations administrators who focus on tasks like upgrades, production deployments, resource and data management, and security. One goal of MLOps is to roll out new models and algorithms seamlessly, without incurring downtime. Because production data can change due to unexpected events and machine learning models respond well to previously seen scenarios, frequent retraining — or even continuous online training — can make the difference between an optimal and suboptimal prediction. A typical MLOps software stack might span data sources and the datasets created from them, as well as a repository of AI models tagged with their histories and attributes. Organizations with MLOps operations might also have automated pipelines that manage datasets, models, experiments, and software containers — typically based on Kubernetes — to make running these jobs simpler. At Nvidia, developers running jobs on internal infrastructure must perform checks to guarantee they’re adhering to MLOps best practices. First, everything must run in a container to consolidate the libraries and runtimes necessary for AI apps. Jobs must also launch containers with an approved mechanism and run across multiple servers, as well as showing performance data to expose potential bottlenecks. Another company embracing MLOps, software startup GreenStream, incorporates code dependency management and machine learning model testing into its development workflows. GreenStream automates model training and evaluation and leverages a consistent method of deploying and serving each model while keeping humans in the loop. Given all the elements involved with MLOps, it isn’t surprising that companies adopting it often run into roadblocks. Data scientists have to tweak various features — like hyperparameters, parameters, and models — while managing the codebase for reproducible results. They also need to engage in model validation, in addition to conventional code tests, including unit testing and integration testing. And they have to use a multistep pipeline to retrain and deploy a model — particularly if there’s a risk of reduced performance. When formulating an MLOps strategy, it helps to begin by framing machine learning objectives from business growth objectives. These objectives, which typically come in the form of KPIs, can have certain performance measures, budgets, technical requirements, and so on. From there, organizations can work toward identifying input data and the kinds of models to use for that data. This is followed by data preparation and processing, which includes tasks like cleaning data and selecting relevant features (i.e., the variables used by the model to make predictions). The importance of data selection and prep can’t be overstated. In a recent Atlation survey, a clear majority of employees pegged data quality issues as the reason their organizations failed to successfully implement AI and machine learning. Eighty-seven percent of professionals said inherent biases in the data being used in their AI systems produce discriminatory results that create compliance risks for their organizations. At this stage, MLOps extends to model training and experimentation. Capabilities like version control can help keep track of data and model qualities as they change throughout testing, as well as helping scale models across distributed architectures. Once machine learning pipelines are built and automated, deployment into production can proceed, followed by the monitoring, optimization, and maintenance of models. A critical part of monitoring models is governance, which here means adding control measures to ensure the models deliver on their responsibilities. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and will punish those that don’t. The study suggests companies that don’t approach the issue thoughtfully can incur both reputational risk and a direct hit to their bottom line. In sum, MLOps applies to the entire machine learning lifecycle, including data gathering, model creation, orchestration, deployment, health, diagnostics, governance, and business metrics. If successfully executed, MLOps can bring business interest to the fore of AI projects while allowing data scientists to work with clear direction and measurable benchmarks. Enterprises that ignore MLOps do so at their own peril. There’s a shortage of data scientists skilled at developing apps, and it’s hard to keep up with evolving business objectives — a challenge exacerbated by communication gaps. According to a 2019 IDC survey, skills shortages and unrealistic expectations from the C-suite are the top reasons for failure in machine learning projects. In 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers working globally on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” There’s also the fact that models frequently drift away from what they were intended to accomplish. Assessing the risk of these failures as a part of MLOps is a key step not only for regulatory purposes, but to protect against business impacts. For example, the cost of an inaccurate video recommendation on YouTube would be much lower compared with flagging an innocent person for fraud and blocking their account or declining their loan applications. The advantage of MLOps is that it puts operations teams at the forefront of best practices within an organization. The bottleneck that results from machine learning algorithms eases with a smarter division of expertise and collaboration from operations and data teams, and MLOps tightens that loop."
https://venturebeat.com/2021/05/07/privitars-data-management-platform-will-tackle-regulatory-compliance/,Privitar’s data management platform will tackle regulatory compliance,"Privitar this week revealed it plans to make available a platform that enables IT teams to provision data end users can access via a self-service portal in a way that complies with privacy regulations. The Privitar Data Provisioning Platform, available in beta in the second quarter, extends the platform the company currently provides to create, approve, and apply privacy policies, Privitar chief product officer Steve Totman told VentureBeat. The platform is expected to be generally available before the end of the year, he added. Organizations of all sizes are now trying to navigate the large number of privacy regulations being enacted around the globe, Totman noted. But the processes they have in place to ensure employees only access data that complies with those regulations are immature, he said. The Privitar Data Provisioning Platform aims to ensure that the data being accessed by end users complies with regulations such as the General Data Protection Regulation (GDPR) enacted by the European Union and the California Consumer Privacy Act (CCPA). The data management issues around privacy regulation are especially complex in the U.S. There is as yet no national policy for data privacy, so individual states are advancing regulations on their own with slight differences. The Privitar Data Provisioning Platform will enable organizations to implement a set of best data operations (DataOps) practices that reduce their risk of running afoul of these regulations, Totman said. As part of that effort, Totman said the Privitar Data Provisioning Platform will be integrated with data governance platforms and data catalogs such as Collibra, as well as data intelligence platforms like BigID. Data privacy issues, in conjunction with digital business transformations, have heightened interest in a more comprehensive approach to data management. Data has historically been managed within the application employed to create it, but multiple applications wind up collecting data from the same customer while rendering their name differently. Across hundreds of applications, that issue creates thousands of opportunities for data to be created that conflicts with data in another application. Meanwhile, data privacy requirements have cast a spotlight on the problem of end users accessing data in a cavalier manner. This can result in end users gaining access to a wide variety of personally identifiable information (PII) that violates one or more compliance regulations or privacy laws. Those privacy laws in some cases now require organizations to generate reports addressing customer demands to detail how the organization actually employs their data. Other regulations might even go so far as to require organizations to track what data was accessed by not just which parties but also when. These requirements have led some organizations to appoint chief data officers tasked with turning data into an asset while limiting any potential liability. “Data is an asset unless it isn’t treated well,” Totman said. “Then it’s a liability.” It may take organizations years to clean up a data management issue that in some cases has been allowed to fester for decades. On the plus side, organizations have never been more motivated to address these issues — out of both fear and potential gain. For one thing, organizations that manage data well are less likely to be fined. And those that can turn data into an asset that enables them to have better relationships with customers are much more likely to thrive in the age of digital business transformation. Regardless of how data is managed, there is more of it with each passing day, which increases the probability data will at some point be mishandled. The degree to which that event results in a rebuke versus a fine will almost always come down to whether an organization can demonstrate it isn’t a reckless custodian of the data it collects."
https://venturebeat.com/2021/05/07/industrial-robots-surge-to-help-companies-meet-demand/,Industrial robots surge to help companies meet demand,"(Reuters) — North American companies boosted spending on industrial robots in the first quarter as they scrambled to keep up with surging demand in the wake of the COVID-19 pandemic. Companies ordered 9,098 robots in the first quarter, a 19.6% increase over a year ago, according to the Association for Advancing Automation, an industry group based in Ann Arbor, Michigan. The orders were valued at over $466 million in total. Robots were once concentrated in the auto industry but are now moving into more corners of the economy, from ecommerce warehouses to food processing plants. For the first time last year, most of the robots ordered by companies in North America weren’t destined for auto factories or their parts suppliers. The strongest growth in the latest quarter was to metal producers, where orders surged 86%. Orders to life science, pharmaceutical and biomedical companies rose 72%, while orders to consumer goods companies increased 32%. “The strong economy obviously helps,” said Jeff Burnstein, president of the Association for Advancing Automation. “It gives companies the confidence to invest in more things — including in more automation.” Burnstein said the pandemic froze many businesses, as operations shut down to protect human health. “But ultimately it accelerated the adoption of automation, because companies recognized if they were going to do it, now would be the time.” Tyson Foods, the U.S. meat company, is among those looking to use more robots on its production lines. In 2019, the company opened a 26,000 square foot automation research center near its headquarters in Springdale, Arkansas. “For the most part, it’s still too soon for some of the really innovative and proprietary systems we’re developing,” said Marty Linn, the center’s director of engineering. Automating jobs such as deboning chickens is extremely difficult, he noted, because the size and shape of each chicken can vary greatly. Robots work best when they can handle uniform items. With that in mind, Linn said, Tyson has already started installing at its plants robots that sit at the end of production lines and automatically stack and wrap boxes on pallets — a process that involves standard shapes and the repetition of precise movements."
https://venturebeat.com/2021/05/06/syniti-dmr-merger-designed-to-pack-punch-in-a-data-driven-world/,Syniti-DMR merger designed to pack punch in a data-driven world,"The pandemic has pushed enterprises to accelerate their digital transformation plans. That means good data management, analytics, and governance are more important than ever, according to Kevin Campbell, CEO of enterprise data management firm Syniti. Boston-based Syniti this week announced the acquisition of Data Migration Resources (DMR), a similarly enterprise data-focused system integrator headquartered in Mission, Kansas. Private equity firm Bridge Growth Partners is the majority shareholder in the newly merged company, which will retain the Syniti name. “Executives in the C-suite are starting to get the message that their most pressing challenges are likely data challenges. This is especially the case with M&A and divestitures,” Campbell told VentureBeat. “People need access to their data and the ability to analyze it to do these transactions, whether it’s Merck spinning off its women’s health business or Pfizer or Gap making an acquisition.” “In just the last year, we believe the reaction to the pandemic, the rush to enable work-from-home and the cloud, has essentially taken us ahead five years in terms of pushing digital transformation in the enterprise.” The Syniti-DMR merger brings together two software-led service companies that once competed in the high-stakes world of data management services for global enterprises. Syniti brings in about 70% of its revenue via services but has also invested some $75 million in its software platform featuring data solutions such as advanced data migration, quality assurance, replication, advanced management, archiving, analytics, and more. Syniti will incorporate DMR’s Concento Rapid Data Governance software into the Syniti Knowledge Platform over time, Campbell said. The two companies support customers in the Americas, Europe, and Asia in industries ranging from pharma/life sciences and manufacturing to financial services and retail. Syniti and DMR both boast SAP expertise. Combined, the company will now claim more than 2,000 expert consultants on the payroll globally. Syniti has the larger, more structured system integrator and reseller channel, but DMR’s channel partners will also be encouraged to work with the new company. Campbell remains CEO of Syniti, while DMR CEO Ryan Rodenburg joins Syniti as executive VP and COO for North America. After competing for the same deals for years, Syniti and DMR executives decided to try teaming up instead — even before the merger was agreed upon. “We’re both pure-play system integrators, and a little over a year ago we started talking about teaming up,” Campbell said. “We closed on the merger last Friday, but long before that, we were working together. In fact, we’ve done more than 20,000 hours together as partners now, and that teamwork convinced us the merger was an even better idea than we originally thought.” Campbell cited research conducted by HFS Research on Syniti’s behalf to emphasize the growing need for better data solutions at the highest levels of the enterprise services market. Polling 100 executives at Global 2000 enterprises, HFS found that 80% believed data would be a top 3 priority investment area at their company within two years. Nine out of 10 surveyed executives said data was a “critical success factor” for their business, but 75% said they “do not have a very high level of trust in their data” currently, according to the report. Just 5% of respondents said they are able to access all the data they need today. “This study points to the challenge these organizations face,” Campbell said. “Lots of people are selling data analytics, but they basically want you to dump everything you have into their ‘data ocean,’ and that quickly turns into a ‘data swamp.’ So these companies still don’t have good access to the data they need or the ability to intelligently use it. What they need is a dedicated expert partner to help them accomplish their digital transformation goals through better data management.”"
https://venturebeat.com/2021/05/06/proper-data-hygiene-critical-as-enterprises-focus-on-ai-governance/,Proper data hygiene critical as enterprises focus on AI governance,"Today’s artificial intelligence/machine learning algorithms run on hundreds of thousands, if not millions, of data sets. The high demand for data has spawned services that collect, prepare, and sell them. But data’s rise as a valuable currency also subjects it to more extensive scrutiny. In the enterprise, greater AI governance must accompany machine learning’s growing use. In a rush to get their hands on the data, companies might not always do due diligence in the gathering process — and that can lead to unsavory repercussions. Navigating the ethical and legal ramifications of improper data gathering and use is proving to be challenging, especially in the face of constantly evolving legal regulations and growing consumer awareness about privacy and consent. Supervised machine learning, a subset of artificial intelligence, feeds on extensive banks of datasets to do its job well. It “learns” a variety of images or audio files or other kinds of data. For example, a machine learning algorithm used in airport baggage screening learns what a gun looks like by seeing millions of pictures of guns — and millions not containing guns. This means companies need to prepare such a training set of labeled images. Similar situations play out with audio data, says Dr. Chris Mitchell, CEO of sound recognition technology company Audio Analytic. If a home security system is going to lean on AI, it needs to recognize a whole host of sounds including window glass breaking and smoke alarms, according to Mitchell. Equally important, it needs to pinpoint this information correctly despite potential background noise. It needs to feed on target data, which is the exact sound of the fire alarm. It will also need non-target audio, which are sounds that are similar to — but different from — the fire alarm. As ML algorithms take on text, images, audio, and other various data types, the need for data hygiene and provenance grows more acute. As they gain traction and find new for-profit use cases in the real world, however, the provenance of related data sets is increasingly coming under the microscope. Questions companies increasingly need to be prepared to answer are: These questions place AI data governance needs at the root of ethical concerns and laws related to privacy and consent. If a facial recognition system scans people’s faces, after all, shouldn’t every person whose face is being used in the algorithm need to have consented to such use? Laws related to privacy and consent concerns are gaining traction. The European Union’s General Data Protection Regulation (GDPR) gives individuals the right to grant and withdraw consent to use their personal data, at any time. Meanwhile, a 2021 proposal from the European Union would set up a legal framework for AI governance that would disallow use of some kinds of data and require permission before collecting data. Even buying datasets does not grant a company immunity from responsibility for their use. This was seen when the Federal Trade Commission slapped Facebook with a $5 billion fine over consumer privacy. One of the many prescriptions was a mandate for tighter control over third-party apps. The take-home message is clear, Mitchell says: The buck starts and stops with the company using the data, no matter the data’s origins. “It’s now down to the machine learning companies to be able to answer the question: ‘Where did my data come from?’ It’s their responsibility,” Mitchell said. Beyond fines and legal concerns, the strength of AI models depends on robust data. If companies have not done due diligence in monitoring the provenance of data, and if a consumer retracts permission tomorrow, extracting that set of data can prove to be a nightmare as AI channels of data use are notoriously difficult to track down. Asking for consent is a good prescription, but one that’s difficult to execute. For one thing, dataset use might be so far removed from the source that companies might not even know from whom to obtain consent. Nor would consumers always know what they’re consenting to, says Dr. James Giordano, director of the Program in Biosecurity and Ethics at the Cyber-SMART Center of Georgetown University and co-director of the Program in Emerging Technology and Global Law and Policy. “The ethical-legal construct of consent, at its bare minimum, can be seen as exercising the rights of acceptance or refusal,” Giordano said. “When I consent, I’m saying, ‘Yes, you can do this.’ But that would assume that I know what ‘this’ is.” This is not always practical. After all, the data might have originally been collected for some unrelated purpose, and consumers and even companies might not know where the trail of data breadcrumbs actually leads. “As a basic principle, ‘When in doubt, ask for consent’ is a sensible strategy to follow,” Mitchell said. So, company managers need to ensure robust, well-governed data is the foundation of ML models. “It’s rather simple,” Mitchell said. “You’ve got to put the hard work in. You don’t want to take shortcuts.”"
https://venturebeat.com/2021/05/06/its-time-to-train-professional-ai-risk-managers/,It’s time to train professional AI risk managers,"Last year I wrote about how AI regulations will lead to the emergence of professional AI risk managers. This has already happened in the financial sector where regulations patterned after Basel rules have created a financial risk management profession to assess financial risks. Last week, the EU published a 108-page proposal to regulate AI systems. This will lead to the emergence of professional AI risk managers. The proposal doesn’t cover all AI systems, just those deemed high-risk, and the regulation would vary depending on how risky the specific AI systems are: Since systems with unacceptable risks would be banned outright, most of the regulation is about high-risk AI systems. So what are high-risk systems? (From the European Commission): The business impact is significant not only from potentially being fined 6% of revenue but also from accessing the $130 billion EU software market. Anyone who wants market access has to comply — although small vendors are exempted (those with fewer than 50 employees and $10 million in revenue or balance sheet). Europe’s privacy regulations, GDPR, set the tone for global privacy laws. So will its AI proposal now set the tone for broad AI regulations globally? We already know this topic is top of mind for US regulators. The Federal Trade Commission recently published AI guidelines ending with the point “Hold yourself accountable — or be ready for the FTC to do it for you.” Everyone will take this seriously. So what do vendors of high-risk systems need to do? A lot. But I’ll focus here on the need for what the proposal calls conformity assessments, or simply put, audits. Audits are done to certify that the AI system complies with the regulation. Some systems can be audited internally by the vendors’ employees, while other systems, like credit scoring or biometric identification, need to audited by a third party. For startups, this will be a whole-company effort with plenty of founder involvement. Large corporations will start setting up teams. And consulting firms will start knocking on their doors. The audit is comprehensive and requires a team that has “in-depth understanding of artificial intelligence, technologies, data and data computing, fundamental rights, health and safety risks, and knowledge of existing and legal requirements.” The audit covers the following (from the European Commission): Even current financial risk managers in banks are not equipped to address the breadth of the audit. Just understanding how to measure the quality of a dataset is a university-level course by itself. Reading between the lines of the proposal, there is concern about the talent shortage needed to enforce the regulation. The proposed regulation will exacerbate the AI talent shortage. Consulting firms will be the stopgap. While it will take years before the regulation is enforced, 2024 being the earliest, it is time to address the talent gap. A coalition of professional associations, industry practitioners, academics, and technology providers should collectively design a program to train the forthcoming domain-flexible AI risk managers in the form of professional certifications, like GARP’s FRM certification, or university degrees, like NYU’s MSc in risk management. (Full disclosure: I used to be a certified FRM, but am not active anymore, and I’m not affiliated with NYU.) Kenn So is an associate at Shasta Ventures investing in AI and software startups. He previously worked as a financial risk consultant at Ernst & Young, building and auditing bank models and was one of the financial risk managers that emerged out of the Basel standards."
https://venturebeat.com/2021/05/06/deepfake-detectors-and-datasets-exhibit-racial-and-gender-bias-usc-study-shows/,"Deepfake detectors and datasets exhibit racial and gender bias, USC study shows","Some experts have expressed concern that machine learning tools could be used to create deepfakes, or videos that take a person in an existing video and replace them with someone else’s likeness. The fear is that these fakes might be used to do things like sway opinion during an election or implicate a person in a crime. Already, deepfakes have been abused to generate pornographic material of actors and defraud a major energy producer. Fortunately, efforts are underway to develop automated methods to detect deepfakes. Facebook — along with Amazon and Microsoft, among others — spearheaded the Deepfake Detection Challenge, which ended last June. The challenge’s launch came after the release of a large corpus of visual deepfakes produced in collaboration with Jigsaw, Google’s internal technology incubator, which was incorporated into a benchmark made freely available to researchers for synthetic video detection system development. More recently, Microsoft launched its own deepfake-combating solution in Video Authenticator, a system that can analyze a still photo or video to provide a score for its level of confidence that the media hasn’t been artificially manipulated. But according to researchers at the University of Southern California, some of the datasets used to train deepfake detection systems might underrepresent people of a certain gender or with specific skin colors. This bias can be amplified in deepfake detectors, the coauthors say, with some detectors showing up to a 10.7% difference in error rate depending on the racial group. The results, while perhaps surprising to some, are in line with previous research showing that computer vision models are susceptible to harmful, pervasive prejudice. A paper last fall by University of Colorado, Boulder researchers demonstrated that AI from Amazon, Clarifai, Microsoft, and others maintained accuracy rates above 95% for cisgender men and women but misidentified trans men as women 38% of the time. Independent benchmarks of major vendors’ systems by the Gender Shades project and the National Institute of Standards and Technology (NIST) have demonstrated that facial recognition technology exhibits racial and gender bias and have suggested that current facial recognition programs can be wildly inaccurate, misclassifying people upwards of 96% of the time. The University of Southern California group looked at three deepfake detection models with “proven success in detecting deepfake videos.” All were trained on the FaceForensics++ dataset, which is commonly used for deepfake detectors, as well as corpora including Google’s DeepfakeDetection, CelebDF, and DeeperForensics-1.0. In a benchmark test, the researchers found that all of the detectors performed worst on videos with darker Black faces, especially male Black faces. Videos with female Asian faces had the highest accuracy, but depending on the dataset, the detectors also performed well on Caucasian (particularly male) and Indian faces. According to the researchers, the deepfake detection datasets were “strongly” imbalanced in terms of gender and racial groups, with FaceForensics++ sample videos showing over 58% (mostly white) women compared with 41.7% men. Less than 5% of the real videos showed Black or Indian people, and the datasets contained “irregular swaps,” where a person’s face was swapped onto another person of a different race or gender. These irregular swaps, while intended to mitigate bias, are in fact to blame for at least a portion of the bias in the detectors, the coauthors hypothesize. Trained on the datasets, the detectors learned correlations between fakeness and, for example, Asian facial features. One corpus used Asian faces as foreground faces swapped onto female Caucasian faces and female Hispanic faces. “In a real-world scenario, facial profiles of female Asian or female African are 1.5 to 3 times more likely to be mistakenly labeled as fake than profiles of the male Caucasian … The proportion of real subjects mistakenly identified as fake can be much larger for female subjects than male subjects,” the researchers wrote. The findings are a stark reminder that even the “best” AI systems aren’t necessarily flawless. As the coauthors note, at least one deepfake detector in the study achieved 90.1% accuracy on a test dataset, a metric that conceals the biases within. “[U]sing a single performance metrics such as … detection accuracy over the entire dataset is not enough to justify massive commercial rollouts of deepfake detectors,” the researchers wrote. “As deepfakes become more pervasive, there is a growing reliance on automated systems to combat deepfakes. We argue that practitioners should investigate all societal aspects and consequences of these high impact systems.” The research is especially timely in light of growth in the commercial deepfake video detection market. Amsterdam-based Sensity (formerly Deeptrace Labs) offers a suite of monitoring products that purport to classify deepfakes uploaded on social media, video hosting platforms, and disinformation networks. Dessa has proposed techniques for improving deepfake detectors trained on data sets of manipulated videos. And Truepic raised an $8 million funding round in July 2018 for its video and photo deepfake detection services. In December 2018, the company acquired another deepfake “detection-as-a-service” startup — Fourandsix — whose fake image detector was licensed by DARPA."
https://venturebeat.com/2021/05/06/searchunify-wins-a-gold-and-a-silver-stevies-at-2021-asia-pacific-stevie-awards/,SearchUnify Wins a Gold and a Silver Stevies® at 2021 Asia-Pacific Stevie® Awards,"MOUNTAIN VIEW, Calif.–(BUSINESS WIRE)–May 6, 2021– SearchUnify, a leading cognitive platform that powers enterprise search and a suite of next-gen support applications, wins a Gold and a Silver Stevie® Awards in the ‘Innovative Use of Technology in Customer Service – Computer Industries’ and ‘Innovation in B2B Products & Services’ categories respectively at 2021 Asia‑Pacific Stevie® Awards. The Asia-Pacific Stevie Awards are the only business awards program to recognize innovation in the workplace in all 29 nations of the Asia-Pacific region. The Stevie Awards are widely considered to be the world’s premier business awards, conferring recognition for achievement in programs such as The International Business Awards® for 18 years. Nicknamed the Stevies for the Greek word for “crowned,” the winners will be celebrated during a virtual (online) awards ceremony on Wednesday, 14 July. More than 900 nominations from organizations across the Asia-Pacific region were considered this year in categories such as Award for Excellence in Innovation in Products & Services, Award for Innovative Management, and Award for Innovation in Corporate Websites, among many others. SearchUnify was awarded with a Gold Stevie for ‘Innovative Use of Technology in Customer Service – Computer Industries’. “SearchUnify is elevating self-service and support experiences with a unified cognitive platform and a suite of next-gen support apps. Its core AI with NLP, machine learning and insights engine for analyzing user behavior and taking data-driven business decisions is great,” the jury noted. It is “one of the most holistic solutions for customer service, especially the integration to multiple existing support solutions. Great impact and outcomes,” it added. It also bagged a Silver Award in the ‘Innovation in Business-to-Business Products & Services’ category. “SearchUnify is a great unified cognitive platform solving a fundamental problem for multiple large-scale enterprises using next-gen technologies…,” the jury remarked. “SearchUnify has evolved from an enterprise search engine to a unified cognitive search platform that powers next-gen support applications for a future-proof support ecosystem. We aim to empower support organizations to move from reactive to proactive support leveraging AI and elevate the customer and agent experiences. A Gold and a Silver Stevie are the ultimate validation that we’re moving in the right direction,” said SearchUnify CTO Vishal Sharma. Gold, Silver and Bronze Stevie Award winners were determined by the average scores of more than 100 executives around the world acting as judges in March and April. “The eighth edition of the Asia-Pacific Stevie Awards attracted many remarkable nominations,” said Stevie Awards president Maggie Gallagher. “The organizations that won this year have demonstrated that they have continued to innovate and succeed despite the COVID-19 pandemic, and we applaud them for their perseverance and creativity. We look forward to celebrating many of this year’s winners during our virtual awards ceremony on 14 July.” Details about the Asia-Pacific Stevie Awards and the 14 July awards ceremony, and the list of Stevie Award winners, are available at http://Asia.Stevieawards.com. About SearchUnify SearchUnify is a cognitive search platform for enterprises that fuels multiple applications for various industries and functions. SearchUnify was named the “youngest product” in The Forrester Wave: Cognitive Search, Q2, 2019 and was honored with two Silver Stevies® at the Stevie® Awards for Sales and Customer Service 2021 and a Silver and a Bronze at the Stevie® Awards for Sales and Customer Service 2020. It’s been named a finalist for the “Best Technology Innovation” at The Global Contact CenterWorld Awards 2020 and the “Best New Technology Solution” at the ICMI Global Contact Center Awards 2020.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210506005841/en/ Ajay Paul Singh,Head of Marketing, SearchUnifypr@grazitti.com"
https://venturebeat.com/2021/05/06/outsight-introduces-the-first-lidar-pre-processing-software-engine/,Outsight Introduces the First LiDAR Pre-processing Software Engine," The easiest way to use LiDAR  PARIS–(BUSINESS WIRE)–May 6, 2021– Outsight, the pioneer of 3D Spatial Intelligence solutions, announced today the launch of a game-changing product: the Augmented LiDAR Box (ALB). It’s the first real-time LiDAR Software Engine that allows application developers and integrators to seamlessly use LiDAR data from any hardware supplier. Created as a turnkey solution, the ALB enables leveraging 3D Spatial Intelligence’s unique value while avoiding the complexity of processing 3D data in real-time. Being a LiDAR-agnostic solution, it saves the customer the hassle of assessing and choosing the most appropriate LiDAR for each application. LiDAR can now be easily used by everyone! This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210506005797/en/ This new product offering follows thorough early customers’ validation processes across several different markets (Robotics, Automotive, ITS, Security & Surveillance), geographies (USA, Europe, Asia) and user profiles (Market-leading corporates, Start-ups and Universities) as well as strategic partnership agreements and collaborations with the most prominent LiDAR suppliers in the USA and Asia, including Velodyne (NASDAQ: VLDR), Ouster (NYSE: OUST), Hesai, and Robosense. Whether it is for monitoring the flow of people or goods, builders of Mobile Robots & Vehicles as well as Integrators of Solutions are increasingly interested in leveraging the unique value of real-time 3D Spatial Intelligence that LiDAR technology creates, but don’t want to deal with the complexity of processing RAW LiDAR data. Moreover, for the best professionals in most applications, going through the hassle of assessing, selecting and using the right LiDAR sensor out of dozens of hardware suppliers and more than a hundred available products, without any standard, is also a time-consuming, non-value added and inefficient use of engineering resources. Turning any LiDAR into a Spatial Intelligence device The Augmented LiDAR Box is the first LiDAR pre-processor: a real-time software engine that turns any LiDAR into a Spatial Intelligence device. It overcomes the complexity of using RAW 3D data, so any application developer or integrator can efficiently use LiDAR in its own solutions without needing to become a 3D LiDAR expert. In order to provide a seamless integration experience, the Augmented LiDAR software engine is delivered embedded in a convenient LiDAR-Agnostic Plug & Play Edge computing Device: the Augmented LiDAR Box (ALB). The ALB provides a comprehensive set of fundamental features that are commonly required in almost every application (e.g., Localization& Mapping, 3D SLAM, Object ID & Tracking, Segmentation & Classification, among others). Because it only requires an ARM CPU and its AI doesn’t rely on Machine Learning, the solution is power-efficient and doesn’t need any Training or Annotation efforts. According to Raul Bravo, President and co-founder of Outsight: “The hardware aspect of LiDAR is becoming a commodity with prices decreasing very quickly together with impressive performance improvements. However, this new animal in the Computer Vision landscape remains a complex technology for most customers to use. As it happened every time in modern-day History of technology adoption, we’re convinced that the key condition required for LiDAR to become mainstream is the emergence of enabling software.” LiDAR-agnostic There is no LiDAR hardware that can fit all applications and contexts: the ALB is an enabling computing layer regardless of the end-user application and LiDAR supplier, so integrators and solution providers are not constrained by the limitations of specific sensors. ALB, a new standard adopted by the leaders of the market The company has seduced not only leading customers in fields such as Smart City, Robotics and the Automotive industry, but has also established strategic partnerships with the world’s most renowned LiDARs manufacturers, such as American leaders Velodyne (NASDAQ: VLDR) and Ouster (NYSE: OUST), or the Chinese Hesai and Robosense. The launch of ALB also follows the successful deployment of Outsight’s technology at Paris Charles de Gaulle airport of the ADP group, to provide accurate real-time monitoring of people flow while preserving private data. Outsight has grown rapidly by integrating new features into its LiDAR-based processing solutions that enable systems to perceive, understand and interact with their surroundings in real time. With a new generation of hardware and software pre-processing engine, connected to any LiDAR of the market, Outsight offers a unique level of simplicity, efficiency and versatility. Award-Winning Technology In less than a year, Outsight has successfully designed and industrialized this new generation of LiDAR processing solutions, which has been the subject of 63 patent applications. Outsight’s innovation won many awards, including the prestigious Best of CES Innovation Award in Las Vegas, and it’s the youngest company ever to have won the Prism Award by the world leaders in photonics. Outsight has already attracted the largest organizations and equipment manufacturers in the automotive, aeronautics and security-surveillance markets, including Faurecia and Safran. To access visuals, please click here About OutsightOutsight develops real-time 3D LiDAR perception solutions.Our mission is to make LiDAR-based Spatial Intelligence become Plug & Play, so it can be used by application developers and integrators in any market. Using any LiDAR with our pre-processing capabilities allows Smart Machines and Smart Cities to achieve an unprecedented level of situational awareness.We believe that accelerating the adoption of LiDAR technology with easy-to-use and scalable pre-processing will highly contribute to creating transformative solutions and products that will make a Smarter and Safer World. For more information, please access www.outsight.ai 1Based on several Market Research studies and public data from IPOs from LiDAR manufacturers.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210506005797/en/ Outsight Press Contact Jean-François Kitten, jfk@outsight.tech Sophie Renard, s.renard@licencek.com Ilinca Spita, i.spita@licencek.com +33 01 45 03 21 77"
https://venturebeat.com/2021/05/06/microsoft-pandemic-turbo-charged-digital-transformation/,Microsoft says pandemic ‘turbocharged’ digital transformation,"The impact of the pandemic on tech investment has been a major theme over the past year. But a new report commissioned by Microsoft underscores just how profoundly COVID-19 has accelerated digital transformation plans. In a study from Microsoft and the Economist Intelligence Unit titled “The transformation imperative: Digital drivers in the COVID-19 pandemic,” 72% of enterprises reported that their pace of transformation had sped up significantly, thanks to the pandemic. The study notes that “In the race to keep up with their digital-first peers, the majority also say the pandemic has turbocharged their own digital journeys.” According to Microsoft CVP Deb Cupp, that high percentage suggests digital transformation has moved from being something leading enterprises have embraced to a must-have across all sectors. “This concept of digital transformation used to be thought of as a competitive edge,” Cupp said. “Now you need it for business resilience. That became so evident as we continue to work our way through the pandemic.” To get there, 50% of respondents said cloud computing has been the most critical tool in their transformation journeys. And 57% said increased efficiency and productivity were “dominant factors” in their decision to invest more in digital tools. “It’s really created an opportunity for customers to think about how not only do they move to a more agile environment in the cloud, but how do they get data so that they create better insights into decisions that they should be making,” Cupp said. “I think that’s something we learned a lot about, especially with our retail customers and our manufacturing customers.” Enterprises have also become more socially aware during the pandemic. In the survey, 76% said the pandemic “has placed fresh responsibility on companies to play a constructive role in wider society.” And 75% said any digital transformation plans should go beyond business motivations to include the impact on society. “This has helped us all learn how interconnected the world is,” Cupp said. “The survey tells us that they recognize that they have this responsibility. So it’s not only about the work that they’re doing for their own organization, but this wider societal impact, which I think was pretty powerful.” Employees have also become a bigger priority. Prior to the pandemic, 24.3% of enterprises considered employee engagement a priority. That jumped to 35.6% last year. As part of that, training and education have also become more urgent. “I was surprised at how quickly and how much it popped,” Cupp said. “When you have 72% of the respondents saying that COVID has really been accelerating their pace of transformation, then elevating that employee engagement is not just a nice-to-have. It really is an essential priority.”"
https://venturebeat.com/2021/05/06/3-powerful-ways-to-improve-digital-shopping-experiences/,3 powerful ways to improve digital shopping experiences,"Presented by Unity 3D 2020 was a tale of two stories: While worldwide physical retail sales slowed, ecommerce boomed. Out of necessity, consumers embraced digital shopping in droves, accelerating the shift away from brick-and-mortar stores by roughly five years. Pandemic challenges have cut across B2B businesses and B2C brands, turning digital storefronts into their most important piece of real estate seemingly overnight. While savvy companies have already invested in compelling ways to showcase their goods online, many still struggle. Here are three ways to level up how you showcase your products online to better convert shoppers into buyers. Even as marketing becomes more experiential and three-dimensional, static digital media still rules the day. Product images are the most influential factor in a typical purchasing decision for online shoppers. But acquiring the necessary volume of images is becoming more difficult, especially as the number of product variants have multiplied in this era of personalization and customization. Teams have traditionally relied on costly, logistics-intensive photoshoots or offline rendering tools to generate imagery but are increasingly turning to more efficient means. At a recent event, Volkswagen’s visualization team revealed that its global websites present 25 million images per day to consumers as they configure their dream VW. To operate at that scale, the automaker has created a highly efficient production pipeline with real-time 3D rendering at the core. All images of Volkswagen’s ID.4 EV on its global websites are generated with real-time 3D rendering and delivered using cloud technology. (Made with Unity Forma) VW reported its digital media production process is 75% faster — from three months to three weeks — when it uses real-time tools vs traditional offline rendering. Workflow improvements like these help teams meet consumer expectations for product imagery faster and more cost efficiently. Today, buyers expect the ability to build their dream configuration and visualize their choices, whether it’s a personal jet or a pair of shoes. They want to be able to see exactly what they are buying before making the final purchasing decisions. Yet the majority of enterprises (61%) identify product configuration as their top visualization challenge, according to a Forrester Consulting study commissioned by Unity. When all product details are not available in a visual format, it becomes more difficult to empower buyers to make the best choice and commit to buying quickly. On the other hand, when buyers have the agency to explore and interact with your products, they get more invested in their “creations” and often spend more. Church’s Footwear saw this first-hand when it gave consumers the opportunity to customize its most iconic shoe. Church’s Footwear featured configurable 3D shoes on its website and increased the average ATV significantly. (Made with Unity. Image credit: SmartPixels) Created in partnership with SmartPixels, a 3D product configurator on Church’s website contributed to nearly half of made-to-order sales and generated 35% higher average transaction value (ATV) than retail ATV.  Making the jump to 3D no longer requires an army of developers thanks to new codeless solutions. With RestAR, acquiring realistic 3D models is no longer a complex exercise — every consumer product can now be recreated in 3D using only a mobile device. As for interactive authoring and content creation, Unity Forma enables marketing teams to produce and publish interactive 3D product configurators and digital media from 3D product data without programming skills. From dream home designs to car configurators, brands across industries are finding real-time 3D a highly effective way to communicate the complexities and intricacies of customizable goods. Being able to offer high-fidelity representations of products results in better understanding by buyers and upsell opportunities for sellers. Of course, even when showcasing products in 3D, buyers interact with them on a 2D display. Augmented reality (AR) changes that by bringing products beyond screens and into your immediate environment for contextual understanding. AR is not just a glossy add-on — it drives real results. 3D models in AR increase conversion rates by up to 250% on websites, according to Shopify. Notable brands like IKEA, Porsche, and Stratasys have embraced the technology. In Stratasys’s case, the 3D printing company was forced to pivot quickly at the onset of the pandemic when it had to shelve its physical event plans to debut a new printer. Working with the immersive studio Visionaries 777, Stratasys developed an AR application for iOS and Android devices in a matter of weeks that attracted strong interest for its new product on a global scale.  The pandemic accelerated consumer behavior trends toward ecommerce that were already in motion. Companies that had a head start on this shift reaped the benefits, and those that were caught flat-footed now see the value in the agility and flexibility of real-time 3D workflows and tools. The teams that have embraced this transition see no going back to their old ways. Creating dynamic marketing content that resonates with customers and communicates in entirely new ways is a game changer. Durable goods manufacturers are now all too aware of how urgently they need these capabilities and how much they stand to gain — or lose — without them. Julien Faure is the vice president and general manager, verticals at Unity. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/05/06/sendbird-api-enables-group-voice-and-video-calls-for-any-app/,Sendbird API enables group voice and video calls for any app,"Sendbird, a platform that companies such as ServiceNow, Reddit, and Korea Telecom use to embed voice, video, and messaging features into their apps, has updated its application programming interface (API) to support group calls. The launch comes just weeks after the San Mateo-based company announced a fresh $100 million in funding at a $1.05 billion valuation. Founded in 2013, Sendbird had mostly focused on offering a chat and messaging API to developers, powering core in-app communication services for major consumer and enterprise brands around the world. But last March, Sendbird added a voice and video API to the mix, at a time when communication apps such as Zoom and Houseparty were enjoying a massive spike in popularity due to the global pandemic. Fast-forward 14 months, and the latest darling in the digital communications space is Clubhouse, an iOS app that accommodates group voice conversations for up to 5,000 people. And that, essentially, is what Sendbird is bringing to all developers with its latest API, though with the added benefit of video thrown in for good measure. In truth, most of the major messaging apps have embraced group video and voice calls for years, including Facebook, WhatsApp, Skype, and even Slack. But with countless industries undergoing major digital transformation, the need to build similar tools into their own apps is greater now than it has ever been — this is perhaps most evident in the medical realm, which has seen an explosion of telehealth technologies over the past year. Existing Sendbird customer Teladoc, a leader in the telemedicine sphere, has seen its valuation double through the pandemic, and now with group video and voice calls, the company will be able to support participants spread across multiple locations on the same call. This could be particularly important for when different health care professionals are involved with a patient. At launch, Sendbird’s group voice calls support up to 25 people, while video group calls are limited to six participants."
https://venturebeat.com/2021/05/06/cibc-innovation-banking-provides-vanedge-capital-with-a-capital-call-line-of-credit/,CIBC Innovation Banking provides Vanedge Capital with a Capital Call Line of Credit,"VANCOUVER, British Columbia–(BUSINESS WIRE)–May 6, 2021– CIBC Innovation Banking is pleased to announce it has provided a Capital Call Line of Credit to Vanedge Capital III L.P. (“Vanedge”), an early-stage venture capital fund. The financing solution provides Vanedge the ability to make investments in portfolio companies prior to calling capital from the fund’s limited partners and continue its investment strategy in analytics, hardtech and computational biology. Based in Vancouver, Vanedge has raised over $390 million in committed capital across three funds since 2010 and completed over 40 investments with 10 successful exits. The firm will continue to focus on emerging Canadian companies that are disrupting their respective sectors, with a thematic investment strategy that focuses on software, disruptive hardtech platforms and computational biology. “As a well-established Canadian fund, Vanedge is known for its ability to help scale early-stage companies within their sectors,” said Rob Rosen, Managing Director in CIBC Innovation Banking’s Toronto office. “We look forward to deepening our relationship with the team and supporting the needs of their portfolio companies.” “We are thrilled to be working with CIBC Innovation Banking in a number of areas, from foreign exchange to business banking, and lines of credit. We are impressed by the team’s breadth of services and added focus on the innovation ecosystem in Canada,” said Paul Lee, Managing Partner, Vanedge. About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Vanedge Vanedge’s team of professionals are investing in early stage companies that have potential to be leaders in next generation analytics, computational biology and disruptive hardtech platforms across North America. Vanedge is looking for companies that can leverage the team’s focus, track record and experience address risk and position companies to reach their full potential and attract follow-on investments from large established venture firms.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210506005080/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609 "
https://venturebeat.com/2021/05/06/ibm-details-planned-2-nanometer-chip-process/,IBM details planned 2-nanometer chip process,"IBM today announced that it has developed a chip with 2-nanometer technology, ostensibly the world’s first. The company expects the hardware to improve performance by 45% using the same amount of power or use 75% less energy while maintaining the same performance as today’s 7-nanometer-based computer chips. Semiconductors play a role in everything from appliances to smartphones, cars, and even electrical grid infrastructure. Demand for increased chip performance and energy efficiency continues to rise, particularly in the era of hybrid cloud, AI, and the internet of things. For example, at least one study shows that the amount of compute used to train the largest models for natural language processing and other applications has increased 300,000 times in six years. IBM says its 2-nanometer-based chip, which was developed by the company’s research division in Albany, New York lab using nanosheet technology, can fit 50 billion transistors onto a 300-millimeter wafer roughly the size of a fingernail. A leap from IBM’s 7-nanometer Power10 processor, which is scheduled to debut later this year, it will lay the foundation for future systems — proprietary and from third parties. IBM says it will potentially accelerate advancements in AI, 5G, edge computing, autonomous systems, and quantum computing. As the company explains, increasing the number of transistors per chip makes them smaller, faster, and more reliable. Replacing current chips with 2-nanometer-based processors could lead to reductions in energy costs and the carbon footprint of datacenter operators. Datacenters currently account for 1% of global energy use, or 580 million terajoules. Upgrading servers to 2-nanometer-based processors could reduce that number. “The innovation reflected in this new chip is essential to the creation of new, more powerful technology platforms that can help our society address major challenges, from climate and sustainability to food scarcity,” IBM Research SVP Dario Gil said in a press release. “It is the product of a collaborative approach to innovation that shows how quickly breakthroughs can result from deep collaboration and shared investment. As governments explore ways to further push the envelope on chip technology, the ecosystem that delivered the world’s first 2-nanometer chip provides a powerful example to follow.” While companies like IBM tout the benefits of 2-nanometer semiconductor process technology, some experts aren’t convinced the path to market will be worth it. As EE Times’ Rick Merritt pointed out in a recent piece, the increasing complexity and cost of making ever-smaller chips is leading to diminishing returns. TSMC, the world’s largest foundry, recently said its 4-nanometer chip process is on track for risk production at the end of 2021 and mass production in 2022. While its 3-nanometer processors are expected in the second half of 2022, TSMC’s 2-nanometer version — which the company started researching in 2019 — is in development. As for Intel, the chip giant plans to finalize 3-nanometer and 2-nanometer nodes for 2025 and 2027, respectively. If the company sticks to its current roadmap, 1.4-nanometer production could begin in 2029. One wrinkle in the production of 2-nanometer-based chips is likely to be the ongoing global chip shortage. A confluence of factors — including the pandemic, the tech war between the U.S. and China, and secular tailwinds — have squeezed the semiconductor market and will continue to do so until at least 2022, according to TSMC. Intel CEO Pat Gelsinger is less optimistic, saying recently that he believes the chip shortage will take “a couple of years” to abate. Spotty semiconductor availability has forced a number of automakers to halt production and cancel shifts in the U.S. And last week, Apple, Samsung, and Caterpillar warned of current or potential impacts, including to tablets, smartphones, and construction equipment."
https://venturebeat.com/2021/05/06/endpoint-security-platform-huntress-raises-40m/,Endpoint security platform Huntress raises $40M,"Endpoint security startup Huntress today announced it has closed a $40 million series B funding round led by JMI, with additional investment from ForgePoint Capital and Gula Tech Adventures. The company says the funds, which bring Huntress’ total raised to nearly $60 million, will be used to grow its platform, as well as its software development team. According to a recent study published by the University of Maryland, hackers attack every 39 seconds, and they’re often successful. The average time to identify a breach in 2019 was 206 days, at which point the cost could be in excess of $3.92 million. Some 20% of organizations get hit with cyberattacks six or more times a year, and 80% say they’ve experienced at least one incident in the last year so severe it required a board-level meeting, IronNet reports. Against this backdrop, the global endpoint security market is anticipated to reach $15 billion by 2026, Statista reports. A 2021 Forrester study found that 76% of enterprises have increased their use of endpoint devices since the beginning of the pandemic to support their remote, work-from-home, and hybrid workforces. According to the same research, 66% of enterprises believe securing their infrastructure requires a more focused, proactive approach to endpoint resilience that doesn’t leave endpoint security to chance. Former U.S. Air National Guard cyber warfare operator Kyle Hanslovan, National Security Administration cyber veteran Chris Bisnett, and security engineer John Ferrell cofounded Huntress in 2015. Hanslovan previously started the defense consulting firm StrategicIO and participates in Black Hat and Def Con. Bisnett cofounded LegalConfirm, a startup that transferred confirmation letters between attorneys, paralegals, and auditors. As for Ferrell, he spent over 15 years in a consultant role with the U.S. Department of Defense and the U.S. Department of Justice. “We help IT resellers protect their customers from persistent footholds, ransomware, and other attacks — and with a laser focus on a specific set of attack surfaces, vulnerabilities, and exploits,” a spokesperson told VentureBeat. “Most recently, our team was notified about undisclosed Microsoft Exchange vulnerabilities successfully exploiting on-prem servers. The Huntress team was able to confirm this activity, with one of the first detections around March 1. From our research, we’ve checked over 3,000 Exchange servers and saw roughly 800 remain unpatched, identifying over 300 of our partners’ servers that have received webshell payloads. Events like this aren’t anomalies and they aren’t behind us.” Endpoint detection and response (EDR) technology is used to protect endpoints — i.e., computers, mobile devices, and other hardware — from attacks. It goes beyond traditional antivirus in the sense that EDR technology gathers and analyzes data from each device and then applies mitigations to handle threats and issues. This kind of technology typically combines real-time continuous monitoring and data collection with rules-based response and analysis. Huntress’ software-as-a-service platform collects and analyzes metadata about apps scheduled to execute when a computer boots up or a user logs in. An agent inventories the apps and sends data back to an analysis engine, which taps algorithms to discover outliers in the dataset, taking into account file reputation, frequency analysis, and other factors. When an anomaly is detected, Huntress delivers remediation recommendations to ticketing systems to alert affected members of the organization. Huntress recently introduced a service aimed at helping manage Microsoft Defender, Microsoft’s built-in Windows 10 antivirus software. The company also introduced new ransomware detection and external reconnaissance services and acquired intellectual property from Level Effect, maker of an endpoint detection and response solution designed to spot threats through deep network traffic analysis. According to Hanslovan, Huntress, which he claims can be deployed to as many as hundreds of endpoints in less than 10 minutes using existing remote monitoring management software, has discovered tens of thousands of data breaches to date. “The security industry is full of broken promises, and that’s often devastating to businesses and livelihoods,” Hanslovan told VentureBeat via email. “We’re thrilled to have partners who not only believe in Huntress’ mission but are working alongside one another to provide effective security to the 99% — the small and medium-sized businesses who make up the backbone of the U.S. economy.” Competition is fierce in the growing cybersecurity segment. There’s IntSights, TrapX Security, CybelAngel, and Deep Instinct, all of which take an algorithmic approach to threat detection. That’s not to mention San Francisco-based ZecOps, which recently nabbed $10.2 million for its tech that automates analysis and response to cyberattacks, and Trinity Cyber, whose threat-combating suite combines detection with “adversary inference.” There’s also Lacework, which protects cloud environments from data breaches. But buoyed by the pandemic, Huntress has managed to retain a foothold. The company is headquartered in Ellicott City, Maryland and has 89 employees. It now counts 1,500 managed service providers and over 25,000 businesses among its customer base. And annual recurring revenue surpassed $10 million earlier this year. “Security has only become more complicated — and more important — against the backdrop of COVID-19, as a result of the sharp pivot so many businesses have made to work remotely and across distributed teams,” the spokesperson said. “Looking ahead, we’ll continue to focus our efforts on the attack surfaces that hackers target most frequently — and successfully — in SMB environments. We’re planning to enable our partners to more effectively manage preventive solutions like antivirus and firewalls while moving beyond endpoint detection to increase visibility into network traffic and cloud-based email threats.”"
https://venturebeat.com/2021/05/06/how-ai-is-helping-nvidia-improve-u-s-postal-service-delivery/,How AI is helping Nvidia improve U.S. Postal Service delivery,"Nvidia this week detailed a partnership with the U.S. Postal Service to transform the latter’s mail operations with AI. According to Nvidia, its machine learning resources and tools enabled the Postal Service to process over 20 terabytes of images a day from more than 1,000 mail processing machines using 195 edge servers. In 2019, the Postal Service had a requirement to identify and track items in its over 100 million pieces of daily mail. Data scientists at the organization thought they could expand an image analysis system developed internally into something broader, with edge AI servers strategically located at the Post Office’s processing centers. The hope was that the system would enable the Postal Service to analyze billions of images of mail and share the insights quickly over the network. Recruiting half a dozen architects at Nvidia and other companies, the Postal Service arrived at the deep learning models it needed after a three-week sprint. The work was the genesis of the Edge Compute Infrastructure Program, a distributed edge AI system that’s running on the NVIDIA EGX platform at the Postal Service today. Open source software from Nvidia, the Triton Inference Server, acts as a sort of digital mailperson between the edge servers, delivering the necessary AI models on demand. According to the Postal Service’s analysis, a computer vision task that would have required two weeks on a network of servers with 800 processors can now be accomplished in 20 minutes on the four NVIDIA V100 Tensor Core GPUs in one of the edge servers, HPE Apollo 6500s. Triton automates the delivery of AI models to different Postal Service systems that may have unique configurations of GPUs and CPUs supporting deep learning frameworks. An app that checks for mail items alone requires coordinating the work of more than half a dozen deep learning models, each checking for specific features. Departments across the Postal Service, from enterprise analytics to finance and marketing, have spawned ideas for as many as 30 apps for ECIP, Nvidia says. One would determine if a package carries the right postage for its size, weight, and destination. Another would decipher a barcode, even in the presence of damage. The plan is to get several new AI-powered apps up and running this year. Nvidia and the Postal Service say the barcode model could be on ECIP as soon as this summer. Seeking further improvements to its mail processing pipeline, the Postal Service put out a request for what could be the next app for ECIP — one that uses optical character recognition (OCR). In the past, the agency would have bought expensive new hardware and software or used a public cloud service, which takes a lot of bandwidth and has significant costs. This time, leaning on Nvidia expertise, the company deployed an AI-based OCR system in a container on ECIP managed by Kubernetes and served by Triton. In the early weeks of the pandemic, operators rolled out containers to get the first systems running as others were being delivered, updating them as the full network was installed. Nvidia was awarded the contract in September 2019, started deploying systems last February, and finished most of the hardware by August. The new solutions could help the Postal Service improve delivery standards, which have fallen over the past year. In mid-December, during the last holiday season, the agency delivered as little as 62% of first-class mail on time — the lowest level in years. The rate rebounded to 84% by the week of March 6 but remained below the agency’s target of about 96%. The Postal Service has blamed the pandemic and record peak periods for much of the poor service performance. “The models we have deployed so far help manage the mail and the Postal Service — it helps us maintain our mission,” Todd Schimmel — the manager who oversees Postal Service systems, including ECIP — said in a press release. “It used to take eight or 10 people several days to track down items, now it takes one or two people a couple of hours. This has a benefit for us and our customers, letting us know where a specific parcel is at — it’s not a silver bullet, but it will fill a gap and boost our performance … We’re at the very beginning of our journey with edge AI.”"
https://venturebeat.com/2021/05/06/ecommerce-subscriptions-platform-recharge-raises-277m/,Ecommerce subscriptions platform ReCharge raises $277M,"ReCharge, a subscription payments management platform for ecommerce companies, has raised $277 million in what is the seven-year-old company’s first round of funding. The raise comes as consumer brands across the spectrum have had to adapt to a new commercial landscape. This is partly due to the pandemic, which has impacted how consumers buy things, but also to a broader push toward direct-to-consumer (D2C) relationships that seek to take more control by bypassing conventional retail outlets and targeting customers directly. Founded out of Santa Monica, California in 2014, ReCharge offers a range of billing and payment management tools, with support for payment providers such as Stripe, Braintree, Google Pay, Apple Pay, PayPal, and more. It also integrates with many of the prominent commerce tools to support discounts, free trials, taxes, and shipping as part of its subscription billing platform. And it caters to those who would rather make a one-time purchase than commit to a weekly or monthly delivery. Through ReCharge’s APIs, businesses can customize their subscription offering and create their own workflows, with support for a “headless” architecture, in which the front-end content and backend technology are separated. This enables more freedom to combine technologies for each touchpoint. “ReCharge is focused on building a cloud-based subscription management platform for businesses of all sizes,” cofounder and CEO Oisin O’Connor told VentureBeat. “We take an API-first approach to our product development, which allows a high level of customization by our most sophisticated merchants, while most merchants still use our product out of [the] box to start offering subscriptions within minutes.” Tying into this are performance measurement and analytics, which give businesses insights into their revenue and customers. This lets them track their key performance indicators (KPIs) and carry out cohort analyses to improve retention and figure out why customers cancel their subscription. The company, which said it’s cash-flow positive, claims to power subscriptions for 15,000 merchants covering 20 million users, including men’s grooming brand Harry’s and U.K. craft beer giant BrewDog, which entered the subscription sector last year. While there are other players in this space, including Bold Commerce, which raised $27 million a few months back, ReCharge is notable for its largely under-the-radar status — it was entirely bootstrapped until last year’s $277 million growth capital injection, which was just announced today. Now it has secured a number of big-name backers, including Iconiq Growth, Summit Partners, and Bain Capital Ventures."
https://venturebeat.com/2021/05/06/valence-launches-bonds-to-mentor-black-professionals/,Valence launches BONDS to mentor Black professionals,"Los Angeles-based Valence has launched BONDS, a mentorship program aimed at empowering Black professionals on the executive path. BONDS stands for Building Our New Definition of Success and is a membership-based community focused on accelerating the success of emerging Black executives. Inaugural partners for the program span venture capital firms, tech companies, and gaming enterprises and include Accel, Electrolux, GGV Capital, Norwest Venture Partners, Providence Strategic Growth, Roblox, Silicon Valley Bank, and Upfront Ventures. These companies will sponsor emerging Black leaders to help them engage with exclusive resources and mentorship that position them for the C-suite. Corporate leaders, venture capital firms, and private equity organizations are supporting Valence’s approach by sponsoring their rising Black employees to participate in the community. BONDS is continuing to accept applications for its initial cohort at this link through May 31. To commemorate the launch, BONDS is hosting an inaugural virtual gathering called Black Success Built Together on May 6 at 3:00 p.m. Pacific time. The lack of Black leadership in corporate America is an issue that requires attention and progress, Valence said. While Black professionals account for 12% of entry-level corporate jobs, just shy of their 13.4% representation in the U.S. population, their numbers fall to 7% of management roles, according to McKinsey & Co. The drop-off is in part a result of Black professionals leaving early-career jobs after they encounter a lack of professional and cultural support and conclude there is little opportunity for them to advance. Valence CEO Guy Primus said in a statement that it’s no secret Black professionals face unique challenges. Companies are slowly understanding that merely hiring Black talent is not enough; they must help cultivate their success, he said. James Lowry, the first Black consultant hired by McKinsey in 1968, is now a senior advisor at the Boston Consulting Group and advisor at Valence. He said BONDS is an important step for corporations interested in empowering and retaining great Black talent. The annual BONDS membership is open to rising senior managers, directors, and other leaders with eight or more years of experience at large/enterprise companies; executives at startups and small and midsized companies; and founders, investors, technical, and creative individuals pursuing executive or directional positions within their field. Members will have opportunities to develop their talents through executive coaching and a series of programs featuring live masterclasses with renowned content partners. Membership also includes six months of on-demand courses, talent assessments, BONDS event series, and roundtable workshops to address current workplace challenges. BONDS has received broad cross-sector support from across the corporate and financial community. Valence has 13 employees and has raised $7.7 million. GGV led the most recent round, and the company was incubated by Upfront Ventures. Other investors include Silicon Valley Bank, Hi Alpha, Softbank Opportunity Fund, Maveron, Defy Partners, Sinai Ventures, B Capital, and Firstmark."
https://venturebeat.com/2021/05/05/u-s-senate-committee-to-consider-technology-research-spending-bill/,U.S. Senate committee to consider technology research spending bill,"(Reuters) — A U.S. legislative proposal to allocate about $110 billion for basic and advanced technology research and science in the face of rising competitive pressure from China will be debated by the Senate Commerce Committee on May 12, sources said on Wednesday. The bipartisan “Endless Frontier” bill would authorize most of the money, $100 billion, over five years to invest in basic and advanced research, commercialization, and education and training programs in key technology areas, including artificial intelligence, semiconductors, quantum computing, advanced communications, biotechnology and advanced energy. The bill had been expected to be considered on April 28, but was delayed after more than 230 amendments were filed for consideration. Senate Democrats and Republicans are moving closer to reaching agreement. A congressional aide said “there has been very encouraging progress toward a deal.” The measure, sponsored by Senate Democratic Leader Chuck Schumer, Republican Senator Todd Young and others, would also authorize another $10 billion to designate at least 10 regional technology hubs and create a supply chain crisis-response program to address issues like the shortfall in semiconductor chips harming auto production. Many lawmakers want to use the legislation to advance other priorities and attach additional proposals, and some sought to use the bill to speed the deployment of thousands of self-driving cars. Republican Representative Mike Gallagher, another sponsor, warned earlier that U.S. superiority in science and technology is at risk. “The Chinese Communist Party has used decades of intellectual property theft and industrial espionage to close this technological gap in a way that threatens not only our economic security, but also our way of life,” he said. Senator John Cornyn, a Republican, said lawmakers are also likely this month to vote on approving at least $37 billion to fund programs to boost U.S. semiconductor production that was authorized under a law enacted in January."
https://venturebeat.com/2021/05/05/iot-is-critical-to-enterprise-digital-transformation-omdia-says/,"IoT is critical to enterprise digital transformation, Omdia says","The enterprise IoT platform market is expected to enjoy robust growth over the next several years, both in terms of revenue and data generated. Enterprises in the process of deploying IoT platforms are overwhelmingly committed to those projects, with more than 90% saying IoT is a “core” or “broad” part of their digital transformation strategy, according to industry research group Omdia. “IoT deployments across enterprises are continuing to rise year on year as enterprises become more familiar with the technologies available and the huge benefits IoT deployments can provide,” Omdia research, IoT, and AI director Josh Builta told VentureBeat. “While the COVID-19 pandemic has accelerated almost half of all IoT projects, serious concerns around security, data privacy, and governance still remain within many organizations. Over the next year, as more enterprises see firsthand the direct correlation between IoT deployment and business efficiencies, we expect IoT deployments to continue to accelerate,” Builta said. “However, it is up to the vendors to communicate how they are managing and alleviating concerns around privacy and data protection. Once the vendors manage to demonstrate this fully, we expect IoT deployments within enterprises will grow exponentially over the next few years.” Its third report, Omdia’s 2021 IoT Enterprise Survey polled 365 enterprises located in seven countries. Surveyed organizations were currently or in the process of deploying IoT solutions as part of a digital transformation strategy. Some 92% of respondents described their IoT deployments as either “core” and organizationally wide-ranging or “broad,” meaning IoT may not touch every part of their business but would be utilized across multiple departments and product lines. The remaining 8% of respondents said they were using IoT in a “targeted” way for a single department, process, or geographic location. Omdia also said surveyed enterprises were much further along in their IoT deployments and increasing their number of IoT-based projects. Nearly three-quarters of respondents said their IoT projects were “either in full deployment or trial/PoC stage,” up from just 4% the previous year. Some 40% of enterprises were now working on more than five IoT projects, up 25% from the 2020 survey. Enterprises were also increasingly betting on strong returns from IoT investments in the form of productivity gains, energy efficiency, and worker and facility safety, according to Omdia. The percentage of respondents saying their organization’s IoT investment would exceed $1 million in 2021 nearly doubled, going from 21% in 2020 to 38% this year. As in previous surveys, data and network security and compliance with data protection regulations remained top concerns for enterprises deploying IoT solutions. Security concerns (cited by 53% of respondents) and data governance concerns (50%) were the top two challenges to IoT adoption in organizations. The main security concerns for respondents were lateral breaches (23%), device authentication/identity breaches (22%), ransomware (20%), and DDoS attacks (19%). Where are enterprises going for IoT platforms, products, and services? The biggest cloud service providers — AWS, Microsoft Azure, and Google Cloud — accounted for 42% of the IoT products and services used by respondents, according to Omdia. However, nearly half of enterprises are using more than one IoT supplier. Respondents further demanded vendors have key capabilities, including open APIs, proactive/predictive IoT data analysis, and integrated IoT security."
https://venturebeat.com/2021/05/05/netskope-lack-of-collaboration-can-block-digital-transformation/,Netskope: Lack of collaboration can block digital transformation,"Digital transformation helps organizations use data more effectively to reduce costs, increase revenue, and improve customer experience. However, 50% of CIOs believe that a lack of collaboration between specialist teams stops their organization from realizing the benefits of digital transformation, according to new research by Netskope, a a cloud security startup specializing in secure access services edge (SASE) architecture. More than half of CIOs believe that a lack of collaboration between specialist teams stops their organization from realizing the benefits of digital transformation (Dx), according to new research by Netskope, the SASE leader. Netskope surveyed more than 2,500 IT professionals globally, including CIOs, security leaders, and networking leaders, to identify the disconnect between network and security teams and highlight best practices for CIOs hoping to drive better outcomes. The tension is real. According to the report, nearly half (45%) of network and security professionals describe the relationship between the two teams in strongly negative terms such as ‘combative’, ‘dysfunctional’, ‘frosty,’ or ‘irrelevant’. With the global spend by enterprises on Dx projects anticipated to be $6.8 trillion through 2023, CIOs wanting to capitalize on the potential for Dx need to move fast to connect these silos and increase morale. In the era before Dx, the conflict between networking and security teams was seen as a byproduct of doing business. Now, these teams and the products and services they manage converge into a shared set of priorities tied to business objectives. And while the divide between networking and security teams has been an issue for some time, it has been even more amplified with the rapid acceleration to remote work. Digital transformation cannot be accomplished without effective collaboration, but the current reality is that its absence is costing teams time and money, and limiting the ability to successfully deliver on the Dx projects that will fuel business growth to keep enterprises competitive. Read Netskope’s full report Digital Transformation Needs a More Perfect Union."
https://venturebeat.com/2021/05/05/5-awesome-jobs-you-need-to-see-if-youre-looking-for-a-change/,5 awesome jobs you need to see if you’re looking for a change,"Are you looking to make a change in your working life? Then you have come to the right place. Trying to get a new job can be a minefield sometimes, with so much information and roles available at any given time. That’s where we come in! We’ve been highlighting some brilliant opportunities every week, to help you narrow down your search. So all you have to do is click apply. Check out this week’s best picks. Microsoft Information Protection Engineers, also called Information Security Engineers or Information Security Analysts, help to safeguard the organization’s computer networks and systems. They plan and carry out security measures to monitor and protect sensitive data and systems from infiltration and cyber-attacks. Information Security Engineers usually work as part of a larger IT team and report directly to upper management. The successful candidate will help plan and carry out the organization’s information security strategy. They will develop a set of security standards and best practices for the organization and recommend security enhancements to management as needed. They will also develop strategies to respond to and recover from a security breach. Udemy is looking for a (senior) manager to lead their Recommendations team. The team is focused on using machine learning approaches to serve the right learning content based on a user’s need. The team is also responsible for Udemy’s personalized recommendations system that is composed of batch (e.g., feature and machine learning pipelines), streaming (i.e., feature computation in real-time), and online (i.e., microservices to serve personalized recommendations) components. In this role, you will be responsible for leading a cross-functional agile team of data scientists, machine learning engineers, and software engineers to develop new ML products and engineering systems to provide better personalized recommendations. A Developer Advocate for Dolby will join a creative, experienced, and high-energy organization responsible for the growth and adoption of Dolby.io APIs. To drive grass-roots attention and preference for Dolby.io, you will bring flexibility, speed, and enthusiasm to those in broader developer communities. Ideally, you’re someone who comes from a software and/or audio background, but has a drive to be on stage guiding the broader industry towards a technology you firmly believe in. You will nurture credibility with developers, startups, architects, and CTOs. You love to share projects you are passionate about with others and exhibit good judgement in identifying opportunities to do so. Do you want to be noticed for your work? Make a difference every day? Be impactful? Work with cutting-edge technology? If so, you will fit in perfectly at CSC and especially within the Order Fulfillment Team. The world’s leading provider of business, legal, tax, and digital brand services, CSC uses technology to make businesses run smoother and smarter. As a Senior Software QA Analyst, that is what you will be doing, too. You will be responsible for Quality Assurance activities including defining, implementing, executing, and maintaining test plans, test cases, and test automation tools/scripts to ensure software is delivered with high quality. They are looking for someone with a strong background in automation and Back Office processes and workflows to collaborate with your peers to review requirement specifications to better understand the features, and efficiently design and execute applicable test cases. Outbrain is seeking a highly motivated Data Scientist to join their BI group. If you’re eager to expand a data-driven culture and drive actionable insights to meet always-changing business needs, then they would love to meet you! As with any Outbrain employee, they’re looking for someone who is resourceful, bright, proactive, works well independently and as part of a team, and who will be passionate about what they do. For this position, they are looking for a person with exceptional analytical skills, outside-the-box thinking, and a self-learner who has proven to deliver results. They provide a casual, fast-paced culture that is built on top performance. Candidates should be energetic self-starters who can grasp the always-changing complexity of the business and technology."
https://venturebeat.com/2021/05/05/wrike-hybrid-work-requires-collaboration-and-project-management-tools/,Wrike: Hybrid work requires collaboration and project management tools,"As companies shift to a hybrid workforce, 58% of IT leaders — up from 17% one year ago — are prioritizing powerful collaborative work management solutions in 2021 to help dispersed teams share and drive work in a systematic and cohesive manner, according to a survey from Wrike, a project management application service provider and a Citrix company. Wrike, now part of Citrix, surveyed IT leaders to better understand the pain points they’re continuing to face as a result of remote work and the technologies they plan to invest in this year that will solve those challenges and power the next phase — hybrid work. The research shows that while 55% of the top remote working technologies IT leaders invested in last year were remote meeting and internal communication tools, organizations are still seeing a number of challenges with the remote workforce, including a lack of engagement (56%), burnout (53%), and reduced productivity (52%). While basic communication tools are important, they don’t alleviate the complexities of collaborating in a work-from-home model, which leads to burnout and major inefficiencies. As a result, nearly 60% of IT executives surveyed are prioritizing investments in solutions and strategies that power collaboration across the enterprise. In particular, they’ll focus on providing greater visibility into ownership across the organization (45%), enabling more secure collaboration (51%), and powering seamless external collaboration (46%). Specifically, 58% of IT leaders — up from 17% one year ago — have made powerful collaborative work management solutions their top remote tech priority in 2021 to help dispersed teams share and drive work in a systematic and cohesive manner. What we’ll see across enterprises is a strong CIO and leadership focus on establishing strategies that will enable the entire organization to work as one in a single digital workspace. It’s through this reimagination of ‘the future of work’ that companies will be able to achieve a happier workforce, no matter their location. Leveraging the Pulse executive knowledge community, Wrike surveyed more than 300 IT leaders, including CIOs, IT VPs, directors, and managers from organizations with more than 1,000 employees in North America, EMEA, and APAC. Read the full report from Wrike, Technology that will Power Hybrid Work."
https://venturebeat.com/2021/05/05/financial-giant-sp-taps-snowflake-for-better-cloud-data-distribution/,Financial giant S&P taps Snowflake for better cloud data distribution,"When Warren Breakstone wanted to make it easier for S&P Global Market Intelligence customers to consume the trove of finance data the company holds, he turned to cloud data specialist Snowflake. As managing director and chief product officer for data management solutions at S&P Global Market Intelligence, Breakstone recognized that the choice of cloud data platform was a key concern for his organization, which is a division of finance giant S&P Global. His team is continually on the lookout for new ways to create innovative data-led products for its major clients, which include finance firms and blue-chip enterprises across a range of sectors. The organization was keen to take advantage of the cloud and make it easier to use data held on the S&P Global Marketplace, which brings together the firm’s data and information from third-party sources. After a period of evaluation, the organization started working with Snowflake last year. Here, Breakstone discusses why he selected Snowflake and how its technology forms a platform for further innovation. This interview has been edited for brevity and clarity. VentureBeat: What was the aim of the implementation? Warren Breakstone: What we’re focused on is productizing data — creating new data-driven products, linking all of that together and combining it so that clients can get incremental value. And then also making it available to clients in the way they want to consume it. And that’s what we’ve really done with Snowflake, which is make all our data on the S&P Global Marketplace available through the Snowflake distribution and couple it with Snowflake compute power, so that clients can take advantage of bigger data queries, and all the advantages of compute power, so that they can study and research and analyze and evaluate, not just our data, but our data in combination with their own data. VentureBeat: What was the business challenge that you were looking to solve? Breakstone: The big challenge has always been that different clients have different means of bringing data into their environments. Some want it through our Xpressfeed solution, which is our bulk-delivery technology that automates the ingestion of data directly into their environments. Others want to access the data through APIs. Then there’s a third tranche, who want it through pre-packaged software products, such as our Capital IQ platform. The challenge is being able to support all the different clients and the different ways that they want to consume data. What Snowflake provides us is a modern addition to our array of distribution, and has additional advantages such as the ability to utilize the compute power as data gets bigger and bigger. Clients want to do new and interesting things by bringing different data sets together, so the ability to access compute power is so important. That has opened up all sorts of new opportunities for us and for our clients in the way we deliver new capabilities, new content, new products, and additional value. VentureBeat: How did you deal with the build versus buy question? Breakstone: The challenge was more around who we would partner with. We have many home-built delivery solutions, such as Xpressfeed, which we’ve enhanced with what’s called a loader, which is a piece of software that automates the ingestion of data for our clients. And that’s a great product and clients love it. But clients also are increasingly looking to the cloud. And that’s where we had to make a decision: How best do we approach that opportunity, and who do we partner with to get there? And that’s what led us to Snowflake. VentureBeat: Why did you select the Snowflake cloud data platform? Breakstone: First and foremost, it was about being closely connected to our clients — and our clients were talking about Snowflake and the opportunities that it provided to them. So as we were doing a pretty robust review of the landscape and different partners, and knowing that we wanted to get into cloud-based distribution, the question was how best to do it. Snowflake was one of the alternatives we considered. We then needed a solution that would support our clients based on where they are today. Clients are on different solutions — some are on AWS, some are on Google Cloud Platform, some are on Azure. How do we support all of those different clients, based in the environments that they’ve stood up? That also was another plus in the Snowflake column because it’s a cloud-agnostic solution; we can build it once and serve many. VentureBeat: What were some of the other technological factors that led you to Snowflake? Breakstone: We did various tests to see what the compute was like relative to other alternatives in the market and we were very impressed. Some of that came back to the initial architecture that Snowflake has built itself on, where they’ve separated their compute from their storage, and because you’ve separated those two, you’re able to get a bit more performance out of the compute. Snowflake also has connections to other applications and tools in the space. Various visualization and analytic tools are already connected to Snowflake. Once we put our data into Snowflake, if a client wants to consume that data through a third-party visualization or analytics tool, more often than not, that provider is already connected with Snowflake, which makes the process for us to get the data into that solution and into their environment much less complicated because there’s a pre-existing pipe. VentureBeat: How did you implement the Snowflake cloud data platform? Breakstone: That involved a tight partnership between our technology group and our product management organization, where we first prioritized — based on customer needs — what data we were going to add to Snowflake’s environment and in what order. And then we were able to work with Snowflake to develop a rigorous and repeatable process, where we would be able to load the data into that environment. It was a very partnership-oriented approach. And we got there quite quickly; far smoother than we had expected. The challenges were really one of prioritization. We have hundreds of different datasets, so where do you start? Do you start with the bigger, most complex data sets? Do you start with the simpler ones that are easier to load? We had a group of clients who partnered with us and helped us set those priorities. And that was very useful. VentureBeat: What does the implementation mean for other investments in the data stack? Breakstone: We’ve just introduced our Marketplace Workbench, which is a platform that we’ve built on top of Snowflake and Databricks, who are a partner of Snowflake. This new platform enables our clients to use our data in a collaborative development environment, using a programming language of their choice, whether that’s Python or R or SQL, to get more out of the data. So, what we’re happy about is that this isn’t just a singular, one-off type of opportunity for us. This is something that we continue to build on, and we build on it in a way that’s relevant to our clients. It’s not about us, it’s about how our clients are able to generate value and utility from these various connected solutions that are all built on top of our data."
https://venturebeat.com/2021/05/05/google-launches-agent-assist-for-chat-in-preview/,Google launches Agent Assist for Chat in preview,"Google today launched Agent Assist for Chat in public preview, an extension of its Contact Center AI platform that provides call center agents with support via text, in addition to calls. First announced in September, Agent Assist attempts to identify customers’ intents and provide agents with real-time recommendations such as articles and FAQs as well as responses to messages. Online chat is becoming one of the most popular ways to reach out to businesses for customer support. In 2020, although phone and voice was responsible for the bulk of interactions, email makes up for around 13% of customer interactions, IDC reports. Live chat without automation is responsible for around 8% of interactions. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Agent Assist provides two components to help agents manage conversations: Smart Reply and Knowledge Assist. Smart Reply delivers response suggestions taken from top-performing agents, sometimes modified to ensure that they reflect brand tone and voice. Agent Assist also learns when and what recommendations to make by building a custom model that’s trained on a company’s data. As for Knowledge Assist, it leverages a company’s knowledge base to provide articles and FAQ suggestions to agents as the conversation progresses. When using Knowledge Assist, agents don’t need to make the customer wait while they navigate apps and data to find the resolution to an issue — the answer is delivered to them. According to Google, customers using Agent Assist for Chat have been able to manage up to 28% more conversations concurrently while driving up customer satisfaction by 10%. They also respond up to 15% faster to chats on average, reducing chat abandonment rates. One customer, Optus, expects Agent Assist to help minimize repetitive tasks by providing response and typeahead suggestions. Another, LoveHolidays, is using Agent Assist to support their agents and customers in the travel industry. With customer representatives increasingly required to work from home in Manila, the U.S., and elsewhere, companies are turning to AI to bridge resulting gaps in service. The solutions aren’t perfect — humans are needed even when chatbots are deployed — but COVID-19 has accelerated the need for AI-powered contact center messaging. There’s indeed no shortage of competition in the AI-driven call analytics space. Gong offers an intelligence platform for enterprise sales teams and recently nabbed $200 million in funding at a $2.2 billion valuation. Observe.ai snagged $26 million in December for AI that monitors and coaches call center agents. AI call center startups Cogito and CallMiner have also staked claims alongside more established players like Amazon and Microsoft. For its part, Google says that more than a thousand customers have deployed Contact Center AI to date."
https://venturebeat.com/2021/05/05/speech-recognition-system-trains-on-radio-archive-to-learn-niger-congo-languages/,Speech recognition system trains on radio archive to learn Niger Congo languages,"For many of the 700 million illiterate people around the world, speech recognition technology could provide a bridge to valuable information. Yet in many countries, these people tend to speak only languages for which the datasets necessary to train a speech recognition model are scarce. This data deficit persists for several reasons, chief among them the fact that creating products for languages spoken by smaller populations can be less profitable. Nonprofit efforts are underway to close the gap, including 1000 Words in 1000 Languages, Mozilla’s Common Voice, and the Masakhane project, which seeks to translate African languages using neural machine translation. But this week, researchers at Guinea-based tech accelerator GNCode and Stanford detailed a new initiative that uniquely advocates using radio archives in developing speech systems for “low-resource” languages, particularly Maninka, Pular, and Susu in the Niger Congo family. “People who speak Niger Congo languages have among the lowest literacy rates in the world, and illiteracy rates are especially pronounced for women,” the coauthors note. “Maninka, Pular, and Susu are spoken by a combined 10 million people, primarily in seven African countries, including six where the majority of the adult population is illiterate.” The idea behind the new initiative is to make use of unsupervised speech representation learning, demonstrating that representations learned from radio programs can be leveraged for speech recognition. Where labeled datasets don’t exist, unsupervised learning can help to fill in domain knowledge by determining the correlations between data points and then training based on the newly applied data labels. The researchers created two datasets, West African Speech Recognition Corpus and the West African Radio Corpus, intended for applications targeting West African languages. The West African Speech Recognition Corpus contains over 10,000 hours of recorded speech in French, Maninka, Susu, and Pular from roughly 49 speakers, including Guinean first names and voice commands like “update that,” “delete that,” “yes,” and “no.” As for the West African Radio Corpus, it consists of 17,000 audio clips sampled from archives collected from six Guinean radio stations. The broadcasts in the West African Radio Corpus span news and shows in languages including French, Guerze, Koniaka, Kissi, Kono, Maninka, Mano, Pular, Susu, and Toma. To create a speech recognition system, the researchers tapped Facebook’s wav2vec, an open source framework for unsupervised speech processing. Wav2vec uses an encoder module that takes raw audio and outputs speech representations, which are fed into a Transformer that ensures the representations capture whole-audio-sequence information. Created by Google researchers in 2017, the Transformer network architecture was initially intended as a way to improve machine translation. To this end, it uses attention functions instead of a recurrent neural network to predict what comes next in a sequence. Despite the fact that the radio dataset includes phone calls as well as background and foreground music, static, and interference, the researchers managed to train a wav2vec model with the West African Radio Corpus, which they call WAwav2vec. In one experiment with speech across French, Maninka, Pular, and Susu, the coauthors say that they achieved multilingual speech recognition accuracy (88.01%) on par with Facebook’s baseline wav2vec model (88.79%) — despite the fact that the baseline model was trained on 960 hours of speech versus WAwav2vec’s 142 hours. As a proof of concept, the researchers used WAwav2vec to create a prototype of a speech assistant. The assistant — which is available in open source along with the datasets — can recognize basic contact management commands (e.g., “search,” “add,” “update,” and “delete”) in addition to names and digits. As the coauthors note, smartphone access has exploded in the Global South, with an estimated 24.5 million smartphone owners in South Africa alone, according to Statista, making this sort of assistant likely to be useful. “To the best of our knowledge, the multilingual speech recognition models we trained are the first-ever to recognize speech in Maninka, Pular, and Susu. We also showed how this model can power a voice interface for contact management,” the coauthors wrote. “Future work could expand its vocabulary to application domains such as microfinance, agriculture, or education. We also hope to expand its capabilities to more languages from the Niger-Congo family and beyond, so that literacy or ability to speak a foreign language are not prerequisites for accessing the benefits of technology. The abundance of radio data should make it straightforward to extend the encoder to other languages.”"
https://venturebeat.com/2021/05/05/the-gaming-industrys-duty-of-care-in-keeping-players-safe/,The gaming industry’s ‘duty of care’ in keeping players safe,"On the last day of GamesBeat Summit last week, leaders from Roblox, Fair Play Alliance, and Accenture discussed the issue of digital civility, and what challenges need to be addressed as games lean more firmly into the mainstream. “If there’s ever been a silver lining to COVID, it’s been gaming,” said moderator Seth Schuler, managing director with Accenture Strategy. “We estimate 35% of the world’s population now plays video games.” Accenture’s most recent gaming research found that gamers are spending 16 hours a week playing games, and another 14 hours a week engaged with others via social media across platforms like YouTube, Baidu, Discord, and Twitch. Nearly all gamers report that they game online to hang with their friends, meet new people, and during COVID, to have much needed social experiences. As a category, gaming is increasingly becoming a super platform for engaging people across entertainment activities and growing an ever-more diverse set of players. For Roblox, socialization and player-created content is the core of the platform, arguably more so than in traditional games, so trust has had to be central from the start, says Laura Higgins, the company’s director of community safety and digital civility. “Safety has always been our number-one priority,” Higgins said. “We’re a platform that was built around young people, and so those values, for us, that’s table stakes. My role has been to focus on creating healthy communities, positive experiences, and educating the community to then go and be good citizens elsewhere online.” The intent is to teach life skills around kindness and empathy and teamwork, as well as conflict resolution, she added. That requires stringent rules and a multilayered approach to safety. That means a large team of moderators backed by AI and machine learning tools and chat filtering, particularly related to personal information in order to create a safe experience at the foundation. For parents, Roblox includes a suite of parental control tools, locked down with a PIN, to ensure that parents are confident their children are safe, as well as encouraging parents to actually spend time on the platform with their child. As their community grows up with them, from kids to older teenagers and young adults, Roblox is intent on fostering a nurturing experience. To do that, one of your most important goals should be listening, Higgens said. “This is my advice for any developers out there,” she explained. “If you can spend time with your community, you’ll learn so much. I’ve been an online safety professional all my life. There are certain things that I can take for granted that we as a company need to do to keep our community safe. That’s the basic. We start there. But there are certain more nuanced things going on with the community. It’s important that we listen and adapt to what’s going on.” Accenture found that with folks spending more time online now, reports of bad behavior are going up — but one person’s bullying could be seen as another person’s rough play, Shuler said. “One of the biggest questions for us is not just how we make games great experiences and how we have fun together, but how we look at these spaces and fulfill social needs and understand the breadth of needs and opportunities in these spaces,” said Kimberly Voll, co-founder of the Fair Play Alliance. Game communication is very different from face-to-face interaction, even voice chat, with its lack of non-verbal cues and communication, she said, which can lead to mismatched expectations in your gaming experience, and lead to unexpected friction. The other challenge comes from game audiences increasingly crossing multiple cultures. Gamers don’t necessarily have the same shared background or the infrastructure of trust to rely on for successful interactions. “A lot of the work we’re trying to do is to move us away from one size fits all, which is always the classic one size fits none,” she said. “These are spaces where humans gather, and where humans gather there is a full spectrum of behavior. Not everyone is going to get along with everyone else. What does that mean for how we’re making games?” With some bad actors eager to enter the ecosystem, the challenge is building spaces that reduce the vulnerability in these communities as a whole, that make them healthier and more robust, and able to push back against these bad actors. And equally,  foster resilience within individuals and help reduce their vulnerability as they create experiences. “When we look at the root causes of why these behaviors emerge, when we know there’s a possibility of friction or mismatched expectations, we as game developers can invest in reducing the chance of that happening at the beginning, before a game gets off on the wrong foot, before it descends into frustration and folks start taking shots at each other,” said Voll. “In addition, [we must] take steps to understand how bad actors gain access to the system and operate within these systems, and do our best to reduce the chance.” Individuals in a space can use social tools or opportunities to push back against harmful experiences, she added. Consequences are incredibly important; developers need to get better at detection and assessment of hate and harassment, and drawing strong lines in the sand — but the problem is much more complex. Developers also need to start investing in enriching spaces that foster successful interactions and successful coexistence, that speak to people’s need to connect and feel a sense of belonging in a space, wherever they come from. The base case for technology in this space right now is AI, machine learning, and deep customer analytics for the end-to-end customer experience, said Christian Kelly, strategy managing director of internet, software and platforms at Accenture. “Over time what you want to do is use machine learning and AI to understand the experience at an individual gamer level, so that you can reinforce the positives and you can take remediation steps on the negatives,” he said. “That’s a huge thing for all gaming companies to do from a centralized standpoint, and for the company to own.” But there’s also decentralized technology coming out all the time, he adds, pointing to Temper, a tool developed by the Global Innovation Exchange at the University of Washington, which can be attached to a TV or monitor. It listens for things like hate speech and bad behavior, and will actually terminate gaming sessions. “There are things, from a technology standpoint, that the industry can do, but there are also new innovations that are happening based on hardware, software, and cloud services that are going to enable parents to be more educated and do something about it in a decentralized way,” he said. Higgins, who’s also on the executive steering committee of the Fair Play Alliance, noted that it’s the cooperation in the industry that will make great strides in addressing these issues as well. “One thing that’s really joyful about working in the video game industry is the collaboration and the will to work together to solve some of these huge issues,” she said. “There are some wonderful conversations and sharing of best practices, of tools, big platforms making some of these tools that would have been inaccessible to small studios and startups, because they just couldn’t afford them, and so enabling people to use those for free. We know there’s a lot more of that’s coming in the pipeline as well.” Overall, the industry must recognize, as a whole, it’s a shared community, and everyone has a duty of care to keep that community safe and healthy, no matter which platform players end up on."
https://venturebeat.com/2021/05/05/open-source-time-series-database-operator-timescale-raises-40m/,Open source time-series database operator Timescale raises $40M,"Timescale, a company that commercializes the open source time-series PostgreSQL database TimescaleDB, has raised $40 million in a series B round of funding. A time-series database (TSDB) refers to a system that stores and retrieves data points with associated timestamps, helping relate and align each of the data points with each other. This could involve a weather or stock price application that charts how the respective data changes over time, or even a self-driving car that has to collect data related to changes in the environment, such as weather conditions or other variables. A TSDB essentially lets developers and businesses store large swathes of timestamped data to access and manipulate as required, and it is used by just about every industry and discipline from financial markets to machine learning. Founded out of New York in 2015, Timescale is the main developer behind TimescaleDB, a TSDB it offers under an open source license, though with the notable caveat that other public cloud companies can’t offer TimescaleDB-as-a-service. On top of that, Timescale commercializes TimescaleDB via a fully managed and hosted service through AWS, Azure, or Google Cloud. TimescaleDB claims a number of notable users, including Cisco, Walmart, Comcast, IBM, Samsung, and Maersk. Since its official launch four years ago, it claims to have amassed more than 2 million active databases. Timescale had previously raised around $31 million, including a $15 million round two years ago. With its latest cash injection, led by Redpoint Ventures, the company said it plans to double down on its commercial product development. This will include new features to “easily manage petabyte-scale deployments,” as well as a new observability platform for developers."
https://venturebeat.com/2021/05/05/cisco-integrates-with-box-to-reduce-friction-in-webex-collaborations/,Cisco integrates with Box to reduce friction in Webex collaborations,"Box and Cisco today announced they will integrate their cloud platforms to reduce the friction end users encounter when collaborating over Webex. Rather than forcing individual enterprise IT organizations to incur the expense of integrating their platforms to enable collaboration, the companies have decided to make that bi-directional capability available to all customers, Cisco exec Jeetu Patel told VentureBeat. Users will be able to access any Box folder from within Webex messaging, and any content shared in the space will be securely added to the same Box folder. Starting next week, users will also be able to access Webex as a Recommended App within Box and view Webex App Activity in Box Preview. The goal is to enable end users to remain within the context of the application they are primarily using without having to exit either Webex or Box to access data residing in one platform or the other, Patel said. That level of embedded integration will make it easier for organizations to create workflows that span the two platforms. “Application platforms shouldn’t break the flow of work,” Patel added. The level of integration most organizations achieve across multiple cloud platforms is not nearly as deep, Box CEO Aaron Levie told VentureBeat. Providers of cloud platforms expose application programming interfaces (APIs) to enable their applications to be integrated, but these tend to be brittle and difficult to employ, Levie noted. Providers of applications are increasingly responsible for integrations because otherwise organizations won’t employ them within the context of a digital business process that can more easily be implemented elsewhere, Levie noted. “The future of software is one of seamless connectivity,” he said. It’s not clear to what degree the level of integration provided by Box and Cisco will force this issue. However, enterprise IT organizations do spend an inordinate sum of money integrating diverse applications by relying on either an internal IT team or a systems integrator to write code that then needs to be secured, maintained, and updated. All the time and effort spent on creating those custom integrations is a drain on productivity, Levie said. Going forward, the expectation will be that even cloud platforms with overlapping capabilities will be seamlessly integrated to enable organizations to more affordably create workflows that can be accessed from anywhere, Levie added. The COVID-19 pandemic cast a spotlight on the cost of cloud platform integration as organizations relied more on multiple cloud services to accomplish tasks. The lack of integration across those platforms also resulted in workflows that were in many cases disjointed, at best. Now that more individuals will be returning to their office more frequently, many organizations are trying to determine how to best optimize workflows that the bulk of their employees need to access from anywhere. Box and Cisco are essentially making a case for a best-of-breed approach to constructing those workflows versus standardizing on a single platform from Microsoft or others. While acquiring services from a single vendor might present some cost savings opportunities, the level of integration provided across a wide range of services can be uneven, especially if it involves platforms a vendor gained via an acquisition. Regardless of approach, the cost of application and platform integration should decline in the months and years ahead. The challenge will be determining which vendors are willing to assume that cost on behalf of end customers who prefer to allocate resources to something other than application integration."
https://venturebeat.com/2021/05/05/cibc-innovation-banking-provides-financing-solutions-to-maverix-private-equity/,CIBC Innovation Banking Provides Financing Solutions to Maverix Private Equity,"TORONTO–(BUSINESS WIRE)–May 5, 2021– CIBC Innovation Banking is pleased to announce it has provided financing solutions, including a Capital Call Line of Credit, to Maverix Private Equity (“Maverix”). The capital call facility provides Maverix with the flexibility to make investments in portfolio companies prior to calling capital from the fund’s limited partners. Maverix announced the launch of its US$500 million growth fund in March, with a focus on technology-enabled growth and disruptive businesses across North America. The fund has attracted major Canadian pension funds and key institutional investors, including CIBC, to participate. The investment strategy at Maverix is to take minority positions in established, high growth disruptive businesses in sectors such as healthcare and wellness, financial services, transportation and logistics, education technology and retail, that are seeking capital to fuel their expansion. “CIBC Innovation Banking is excited to be working with Maverix and its management team,” said Rob Rosen, Managing Director, CIBC Innovation Banking. “Their pride in Canada and conviction to grow our domestic ecosystem will have a lasting impact on our country’s innovation ecosystem.” “We are pleased to be partnering with CIBC Innovation Banking and its team,” said John Ruffolo, Founder and Managing Partner, Maverix. “Together we hope we can help shape the future of Canada.” About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Maverix Private Equity Maverix is a private equity firm that invests in disruptive technology-enabled businesses that have the potential for rapid revenue growth. The team backs exceptional entrepreneurs, leveraging their extensive experience and networks to help them scale their businesses. In addition to their team, their Advisory Board includes some of Canada’s most successful entrepreneurs.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210505005161/en/ CIBC: Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609Maverix: Vivian O’Leary, voleary@maverixpe.com, 647-695-5867"
https://venturebeat.com/2021/05/05/linux-foundation-launches-open-source-agriculture-infrastructure-project/,Linux Foundation launches open source agriculture infrastructure project,"The Linux Foundation has lifted the lid on a new open source digital infrastructure project aimed at the agriculture industry. The AgStack Foundation, as the new project will be known, is designed to foster collaboration among all key stakeholders in the global agriculture space, spanning private business, governments, and academia. As with just about every other industry in recent years, there has been a growing digital transformation across the agriculture sector that has ushered in new connected devices for farmers and myriad AI and automated tools to optimize crop growth and circumvent critical obstacles, such as labor shortages. Open source technologies bring the added benefit of data and tools that any party can reuse for free, lowering the barrier to entry and helping keep companies from getting locked into proprietary software operated by a handful of big players. Founded in 2000, the Linux Foundation is a not-for-profit consortium that supports and promotes the commercial growth of Linux and other open source technologies. The organization hosts myriad individual projects spanning just about every sector and application, including automotive, wireless networks, and security. The AgStack Foundation will be focused on supporting the creation and maintenance of free and sector-specific digital infrastructure for both applications and the associated data. It will lean on existing technologies and agricultural standards; public data and models; and other open source projects, such as Kubernetes, Hyperledger, Open Horizon, Postgres, and Django, according to a statement. “Current practices in AgTech are involved in building proprietary infrastructure and point-to-point connectivity in order to derive value from applications,” AgStack executive director Sumer Johal told VentureBeat. “This is an unnecessarily costly use of human capital. Like an operating system, we aspire to reduce the time and effort required by companies to produce their own proprietary applications and for content consumers to consume this interoperably.” There are a number of existing open source technologies aimed at the agricultural industry, including FarmOS, which is a web-based application for farm management and planning that was created by a community of farmers, researchers, developers, and companies. But with the backing of the Linux Foundation and a slew of notable industry stakeholders, the AgStack Foundation is well positioned to accelerate interoperable technologies that are free to use and extend upon. “Just like an operating system, we feel there will be a whole universe of applications that can be built and consumed using AgStack,” Johal added. “From pest prediction and crop nutrition to harvest management and improved supply-chain collaboration, the possibilities are endless.” Members and contributors at launch include parties from across the technology and agriculture spectrum. Among these is Hewlett Packard Enterprise (HPE), which already runs a number of agricultural initiatives, including a partnership with global food security research group CGIAR to help model food systems. Other members include Purdue University/OATS & Agricultural Informatics Lab, the University of California Agriculture and Natural Resources (UC-ANR), and FarmOS."
https://venturebeat.com/2021/05/05/alkymi-patterns-tool-uses-ai-to-extract-data-from-documents/,Alkymi Patterns tool uses AI to extract data from documents,"Alkymi, a provider of business intelligence software for enterprises, today announced the launch of Alkymi Patterns, a tool designed to identify and extract data for automating backend processes that run on email and documents. The release comes as Alkymi’s revenue growth reaches 400% over the past 12 months, chiefly through financial services customer and partner acquisition. The potential value in underused enterprise email and file data is immense, with a Veritas report pegging it at $3.3 trillion by 2020. But the process for unlocking can be challenging. Capturing business data — for example, when onboarding a new customer — requires operational resources and time with the possibility of data loss and mistakes. That’s why between 60% and 73% of all data within corporations is never analyzed for insights or larger trends, a Forrester survey found. Alkymi Patterns is aimed at enabling customers to extract data from tables and text, eliminating repetitive processing. Once a data pattern is created, Patterns can automate extraction jobs — like those required in banking, asset management, and insurance operations — to save time, capital, and resources while improving customer service and turnaround times. Employing machine learning, computer vision, and an understanding of tabular data structures, Patterns — akin to Google’s TAPAS — can determine the context and location of data in rows, columns, charts, and text. Extracted data appears in the Alkymi user interface, email inbox, or app of choice, ready for human-in-the-loop review and export. Users can tap Patterns to define the data they want to extract with search terms. These fields are assigned to a schema that can be used to automate the extraction of data points on an ongoing basis. Patterns’ mapping and formatting commands help organize information according to their own business logic and objectives. Moreover, they preserve data lineage, ensuring that data remains traceable back to the source. Alkymi cofounder and CEO Harald Collet claims that Patterns can extract any type of data in virtually any format, layout, or naming convention. “Alkymi brings computer vision and machine learning-powered automation into daily workflows, eliminating manual data entry by analysts and supercharging processes so that users can make more intelligent decisions, faster, and at lower cost,” Collet said in a press release. “Adopting Alkymi Patterns allows organizations to introduce automation and human augmentation even more broadly across their workforces.” When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Salesforce’s recent Trends in Workflow Automation report found that 95% of IT leaders are prioritizing automation, and 70% of execs are seeing the equivalent of over 4 hours saved each week per employee. Moreover, according to market research firm Fact.MR, the adoption of business workflow automation at scale could create a market opportunity of over $1.6 billion between 2017 and 2026. One Alkymi customer, SimCorp, says that it’s integrated Patterns with its platform to address inefficiencies and the growing allocations of limited partners. “Alkymi Patterns opens up a new universe of workflow automation for institutional investors who struggle to extract insights from unstructured data quickly,” VP of innovation Hugues Chabanis said in a statement. “Patterns can address … a lack of automation in alternatives.”"
https://venturebeat.com/2021/05/05/globalfoundries-and-psiquantum-partner-on-full-scale-quantum-computerm-computer/,GlobalFoundries and PsiQuantum partner on full-scale quantum computer,"PsiQuantum and Globalfoundries have teamed up to manufacture the chips that will become part of the Q1 quantum computer. Palo Alto, California-based PsiQuantum has plans to create a million-qubit quantum computer. Globalfoundries is a major chipmaker that will manufacture the silicon photonic and electronic chips that are part of the Q1. The system they’re working on now is the first milestone in PsiQuantum’s roadmap to deliver a commercially viable quantum computer with 1 million qubits (the basic unit of quantum information) and beyond. PsiQuantum believes silicon photonics, or combining optics with silicon chips, is the only way to scale beyond 1 million qubits and deliver an error-corrected, fault-tolerant, general-purpose quantum computer. PsiQuantum wants to deliver quantum capabilities that drive advances with customers and partners across climate, health care, finance, energy, agriculture, transportation, and communications. PsiQuantum and GF have now demonstrated a world-first ability to manufacture core quantum components, such as single-photon sources and single-photon detectors, with precision and in volume, using the standard manufacturing processes of GF’s world-leading semiconductor fab. The companies have also installed proprietary production and manufacturing equipment in two of Globalfoundries’ 300-millimeter factories to produce thousands of Q1 silicon photonic chips at its facility in upstate New York and state-of-the-art electronic control chips at its Fab 1 facility in Dresden, Germany. PsiQuantum’s Q1 system represents breakthroughs in silicon photonics, which the company believes is the only way to scale to a million or more qubits to deliver an error-corrected, fault-tolerant, general-purpose quantum computer. The Q1 system is the result of five years of development at PsiQuantum by the world’s foremost experts in photonic quantum computing. The team made it their mission to bring the world-changing benefits of quantum computing to reality, based on two fundamental understandings. “Globalfoundries is fast becoming a leader in silicon photonics,” Moor Insights & Strategy analyst Patrick Moorhead said in an email to VentureBeat. “Its announcement with PsiQuantum now adds quantum computing to its SiPho repertoire of datacenter and chip-level connectivity.” First, it focused on a quantum computer capable of performing otherwise impossible calculations requiring a million physical qubits. Second, it leveraged more than 50 years and trillions of dollars invested in the semiconductor industry as the path to creating a commercially viable quantum computer. Globalfoundries’ Amir Faintuch said in a statement that we have experienced a decade of technological change in the past year and that the digital transformation and explosion of data now requires quantum computing to accelerate a compute renaissance. Globalfoundries’ silicon photonics manufacturing platform enables PsiQuantum to develop quantum chips that can be measured and tested for long-term performance reliability. This is critical to the ability to execute quantum algorithms, which require millions or billions of gate operations. PsiQuantum is collaborating with researchers, scientists, and developers at leading companies to explore and test quantum use cases across a range of industries, including energy, health care, finance, agriculture, transportation, and communications. Pete Shadbolt, chief strategy officer at PsiQuantum, said in a statement that this is a major achievement for both the quantum and semiconductor industries, demonstrating that it’s possible to build the critical components of a quantum computer on a silicon chip, using standard manufacturing processes. He said PsiQuantum knew that scaling the system was key. By the middle of the decade, PsiQuantum and Globalfoundries hope to create all the manufacturing lines and processes needed to begin assembling a final machine. PsiQuantum and Globalfoundries want to play a critical role in ensuring the United States becomes a global leader in quantum computing, supported by a secure, domestic supply chain."
https://venturebeat.com/2021/05/05/utmost-a-workday-native-workforce-management-system-raises-21m/,"Utmost, a Workday-native workforce management system, raises $21M","Utmost, a platform that helps Workday customers manage their extended workforce, including freelancers, contractors, and other professionals, has raised $21 million in a series B round of funding. The freelance industry is flourishing, by all accounts. In its recent State of Contingent Workforce Management report, Ardent Partners found that 43% of the U.S. workforce is now built on a contingent worker foundation — up from 20% a decade ago. Founded out of Dublin, Ireland in 2018, Utmost touts itself as a Workday-native alternative to legacy vendor management systems (VMS), which companies use to serve their non-employee staffing needs. With Utmost, enterprises are promised full visibility into their broader workforce with data-driven insights for procurement, HR, finance, and IT departments. Utmost provides transparency into extended workforce spend, something that is typically tracked using multiple systems. Companies can also garner data such as what fraction of their workforce is made of contractors and where they are based. “Utmost centralizes this data so executive leadership can see how they are spending across the total workforce,” Utmost marketing VP Neha Goel told VentureBeat. The main problem Utmost is looking to solve is that existing VMS offerings tend to focus on sourcing workers from traditional staffing companies, which limits the pool of available talent. Plus their systems don’t play nicely with modern human capital management systems, such as Workday. So while a company might use Workday for most of their HR needs, hiring personnel have to use another system to book temporary workers. Utmost bypasses this by plugging directly into Workday. According to Goel, the platform’s core selling point is that it brings “total workforce visibility” to enterprises. In its three years in business, the company has managed to accrue major customers such as Ecolab, NortonLifeLock, and Colonial Life. Among the touted benefits is improved compliance. “You can create consistent processes for onboarding and offboarding so that workers gain or lose access to facilities, IT, or systems access,” said Goel, who added that this can also help reduce worker misclassification. “Hiring managers don’t know if they need an employee or a contractor or an agency — they need to get work done. Utmost creates a single place to source workers and recommends the appropriate worker type.” Workday, for the uninitiated, is one of the major players in the enterprise SaaS software space, with products spanning finance, HR, business planning, and more. It also represents part of a growing array of companies adopting a Salesforce-type ecosystem approach that encourages third parties to develop integrations and build on top of their platform while directly investing in startups via a dedicated venture capital fund. Indeed, Utmost counts Workday as a closely integrated technology partner — and also a financial backer. Workday Ventures invested in Utmost’s $11.2 million series A round two years ago, and it has now added to the recent $21 million series B. This round was led by Mosaic Ventures, with participation from Greylock Partners, Acadian Ventures, and Alumni Ventures Group."
https://venturebeat.com/2021/05/05/analytics-as-a-service-platform-startree-nabs-24m/,Analytics-as-a-service platform StarTree nabs $24M,"StarTree, a startup building a platform-as-service version of the Apache Pinot analytics platform, today announced that it closed a $24 million series A round led by Bain Capital Ventures and GGV Capital. The company says the proceeds will be used to spur adoption of its user-facing analytics products as it ramps up its customer acquisition efforts. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Founded by Kishore Gopalakrishna and Xiang Fu, former engineers at LinkedIn and Uber and the original creators of Pinot (which StarTree maintains), StarTree provides a concurrent, scalable, and real-time analytics platform-as-a-service. The company leverages both batch and streaming data, letting operators, analysts, customers, and users understand and take action in real time. And StarTree claims its products can scale from thousands to millions of users with low latency. “While Pinot itself is open source, not every company has the skills to manage, operate and configure Pinot,” a spokesperson told VentureBeat via email. “StarTree’s fully managed Pinot product offering has continued to grow during the pandemic. We have specifically seen increased interest from companies in expanding the use of analytics. In fact, a number of companies realized that the insights they gathered before the pandemic were outdated, as consumer behaviors and interests drastically changed.” With Pinot, businesses like retailers have the ability to track supply chain and inventory changes at a very granular level, Gopalakrishna said in an interview. Fintech companies can use Pinot to provide real-time insights to their clients and individual users, while restaurants can respond to changes in demand and staff up or down in response to search and order behaviors related to their locations. One company, Stripe, tapped Pinot to execute “sub-second, petabyte-scale” aggregation queries over fresh financial events in its internal ledger. Meanwhile, LinkedIn used Pinot to serve more than 150,000 queries per second across over 70 user-facing apps. “Apache Pinot revolutionized the way LinkedIn thinks about data analytics and delivers value to our members and customers through highly dimensional, near-real-time insights. Member experiences … have been enabled at scale thanks to Pinot’s performance profile,” LinkedIn chief data officer Igor Perisic said. “As the first company to use this technology, we are thrilled to see StarTree moving quickly toward commercialization so the rest of the world can reap similar benefits from the platform.” Existing investors CRV and LinkedIn also participated in 18-employee StarTree’s latest funding round. It brings the company’s total raised to more than $28 million. Data analytics is the science of analyzing raw data to extract meaningful insights. Market Research Future predicts that the global data analytics market will be valued at over $132 billion by 2026. A range of organizations can use data to boost their marketing strategies, increase their bottom line, personalize their content, and better understand their customers. Businesses that use big data increase their profits by an average of 8%, according to a survey conducted by BARC research. “Today companies can extract tremendous value, by expanding insights to their customers, partners, and franchisees. StarTree aims to address that need by helping progressive data-driven companies make better use of their massive and growing data,” Gopalakrishna said. “Building on the success of Pinot, we are in the unique position to democratize data by delivering fresh analytics at scale with low latency for thousands or millions of users.”"
https://venturebeat.com/2021/05/05/vim-closes-investments-from-walgreens-anthem-and-frist-cressey-ventures-to-build-digital-infrastructure-for-higher-performing-health-care/,"Vim Closes Investments From Walgreens, Anthem, and Frist Cressey Ventures to Build Digital Infrastructure for Higher Performing Health Care"," Vim builds second quarter with new strategic investments as customer network footprint and product portfolio expand  SAN FRANCISCO–(BUSINESS WIRE)–May 5, 2021– Vim, a leading technology company building digital infrastructure for US health care, today announced investments from Walgreens Boots Alliance (WBA), Anthem (ANTM), and Frist Cressey Ventures. The addition of these strategic investors reflects Vim’s accelerating product and market progress deploying integrated data and workflow solutions for leading health plans and providers across the country. Vim’s solutions are widely used by leading health plans, providers, and pharmacies engaged in value-based care programs. Vim’s Quality Gaps, Diagnosis Gaps, Referral Guidance, and Digital Scheduling solutions easily and seamlessly integrate with existing clinical systems at the point of care to reduce burden for health plans and providers working to succeed on value, outcomes, and experience. WBA’s investment will support accelerated rollout of Vim’s point of care integration technology and continued commercial growth across health plan and provider groups throughout the country. “We believe data connected in clinical workflows is essential to drive real-time decision support at the point of care, and Vim’s approach allows us to deepen our integration capabilities across the clinical ecosystem,” said Jared Josleyn, Vice President of Innovation, Walgreens Boots Alliance. “Supporting Vim’s work to connect health plan payers to providers is a valuable opportunity and we’re looking forward to seeing their technology continue to move health care forward.” Additionally, Vim has welcomed Li Zhong, Vice President of Global Mergers, Acquisitions and Development at WBA to the Board of Directors. Vim also welcomes Elizabeth Canis, Vice President of Emerging Businesses & Partnerships at Anthem and Tim Kaja, Senior Vice President of OptumCare to the Board of Directors. In her role at Anthem, Canis builds market leading solutions, diversifies sources of future growth, and identifies distinct partnerships within the industry. Anthem’s partnership with Vim will enhance Vim’s point-of-care technology to align data and workflows to support Anthem providers for accelerated success. At OptumCare, Kaja oversees operations related to supportive data and workflow technology for OptumCare’s national network of health care providers. Kaja will work closely with Vim’s executive leadership team and existing Board members to support Vim’s ongoing strategy for continued customer and product success. With a focus on investing in technology and service businesses with solutions that improve quality of care, system integration, patient outcomes, and population health and well-being, Frist Cressey Ventures joins existing backers Great Point Ventures, Sequoia, Premera, and Optum. “Frist Cressey is thrilled to join Vim on their journey to build the digital infrastructure of a better working health care system,” said Senator Bill Frist MD, Co-Founder and Partner at Frist Cressey Ventures. “Vim’s point of care integrations and increasingly sophisticated suite of health care performance and access products have the potential to take value-based care to the next level, which is what our health system needs.” “We are honored to be backed by and working in partnership with some of the largest and most innovative companies in US health care,” said Oron Afek, Vim Co-Founder and Chief Executive Officer. “We share a vision for powering a new era of network performance by seamlessly integrating data at the point of care to improve quality, experience, and total cost of care. These investments and partnerships represent major leaps forward for Vim and our opportunity to make an ever deeper and broader impact on US health care.” About Vim Founded in 2015, Vim connects data to workflow at health care’s “last mile”: within clinical operations at the point of patient care. Health plans, patients, and medical providers of every size – from independent practitioners to integrated delivery systems – use Vim software to connect data and care across the health system. Vim’s mission is to power affordable, high quality health care through seamless connectivity. For more information, please visit getvim.com. About WBA Walgreens Boots Alliance (Nasdaq: WBA) is a global leader in retail and wholesale pharmacy, touching millions of lives every day through dispensing and distributing medicines, its convenient retail locations, digital platforms and health and beauty products. The company has more than 100 years of trusted health care heritage and innovation in community pharmacy and pharmaceutical wholesaling. Including equity method investments, WBA has a presence in more than 25 countries, employs more than 450,000 people and has more than 21,000 stores. WBA’s purpose is to help people across the world lead healthier and happier lives. The company is proud of its contributions to healthy communities, a healthy planet, an inclusive workplace and a sustainable marketplace. WBA is a Participant of the United Nations Global Compact and adheres to its principles-based approach to responsible business. WBA is included in FORTUNE’s 2021 list of the World’s Most Admired Companies. This is the 28th consecutive year that WBA or its predecessor company, Walgreen Co., has been named to the list. About Anthem Anthem is a leading health benefits company dedicated to improving lives and communities, and making healthcare simpler. Through its affiliated companies, Anthem serves more than 110 million people, including approximately 43 million within its family of health plans. We aim to be the most innovative, valuable and inclusive partner. For more information, please visit www.antheminc.com or follow @AnthemInc on Twitter. About Frist Cressey Ventures At Frist Cressey Ventures, our mission is focused on accelerating the growth of high potential healthcare enterprises through value-added partnerships. We invest in technology and service businesses with viable solutions that improve quality of care, system integration, patient outcomes, and population health and well-being. We join like-minded entrepreneurs who share similar core values in their pursuit to improve healthcare. Frist Cressey Ventures focuses our time and attention on three key areas that accelerate growth, reduce risk, and maximize return on investment: recruiting top talent, connecting partnerships with our deep industry network, and executing our partnerships’ plans to improve healthcare.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210505005392/en/ Phoebe ByersEmail: phoebe@getvim.com"
https://venturebeat.com/2021/05/05/predictive-analytics-startup-pecan-ai-raises-35m-to-boost-ai-adoption/,Predictive analytics startup Pecan.ai raises $35M to boost AI adoption,"Predictive analytics startup Pecan.ai today announced it has raised $35 million in a series B round led by GGV Capital. Cofounder and CEO Zohar Bronfman says that the funds will allow Pecan to expand its operations globally and help organizations adopt AI and big data. Data analytics is the science of analyzing raw data to extract meaningful insights. Market Research Future predicts the global data analytics market will be valued at over $132 billion by 2026. A range of organizations can use data to boost their marketing strategies, increase their bottom line, personalize their content, and better understand their customers. Businesses that use big data grow their profits by an average of 8%, according to a survey conducted by BARC research. Pecan, which has offices in New York and Tel Aviv, offers a no-code platform that automates data, encoding, restructuring, cleansing, and engineering to create AI-based predictive algorithms from a number of deep neural networks. After a set of recursive competitions between multiple networks, the platform leaves only one fully trained neural network — evolved and refined for accuracy. “After fine-tuning the product and our flagship use cases, Pecan came out of stealth in February of last year,” a spokesperson told VentureBeat via email. “The pandemic affected us in a positive way. The need to predict erratic patterns of behavior, as well as the increase in digital consumption, boosted the demand for Pecan’s brand of fast and easy-to-use AI.”  It’s an approach known as evolutionary computation, a family of algorithms for global optimization inspired by biological evolution. Instead of following explicit mathematical gradients, these models generate variants, test them, and retain the top performers. Pecan supports data in a range of formats and offers dashboards to provide visibility into factors that might affect outcomes. The platform, which can output its predictions to third-party software, continuously monitors and optimizes models while enriching them with external data. The idea is to let analysts and business stakeholders obtain actionable insights and see outputs in a matter of days after adding their raw data. Pecan supports use cases that include demand forecasting, conversion, lifetime value, next best offer, VIP customers, upsell and cross-sell, churn and retention, and sales analytics. “In order to appeal to data analysts, Pecan is both end to end and use case-focused, which greatly reduces the complexity and statistical knowledge required from its users,” the spokesperson said. “Unlike some of its competitors, Pecan handles everything from data prep to monitoring data in production with a drag-and-drop UI. The data prep and feature selection/engineering components are critical. Getting data into proper form for AI models can take weeks or months of efforts on the part of data scientists and engineers, but with some help from the Pecan team, can be automated with minimal effort.” Pecan says during the pandemic Johnson & Johnson used its platform to help predict changing consumer behavior and buying patterns across different consumer product groups, as well as supply chain forecasting. “We improved forecast accuracy in our seasonal business, and we have a deeper understanding of the variables that may influence a consumer demand signal,” Johnson & Johnson VP Bertrand Klehr said in a press release. “[By] partnering together with Pecan, we are continuing our focus on what consumers want to purchase at the right time and place.” Bronfman added: “Pecan was designed to drive business value from AI. In one intuitive platform, analysts and business stakeholders can obtain actionable insights and see outputs in a matter of days after adding their raw data — helping companies evolve from BI to AI. We have seen tremendous uptake from organizations of all sizes and are looking forward to expanding globally and bringing real business value to our customers.” Vintage and existing investors Dell Technologies Capital, S-Capital, and Mindset also participated in Pecan’s latest funding round. It brings the five-year-old company’s total raised to more than $50 million, following an $11 million series A in January 2020."
https://venturebeat.com/2021/05/04/chipmaker-tsmc-may-be-planning-to-build-more-chip-factories-in-arizona/,Chipmaker TSMC may be planning to build more chip factories in Arizona,"(Reuters) — Taiwan Semiconductor Manufacturing Co (TSMC) is planning to build several more chipmaking factories in the U.S. state of Arizona beyond the one currently planned, three people familiar with the matter told Reuters. TSMC, the world’s largest contract chipmaker, announced in May 2020 it would build a $12 billion factory in Arizona, an apparent win by the Trump administration in its push to wrestle global tech supply chains back from China. TSMC is setting up a 12-inch wafer fabrication plant in Phoenix, and the facility is expected to start volume production in 2024, Taiwan’s investment commission of the ministry of economic affairs, which approved the investment, said in December. TSMC manufactures the bulk of its chips in Taiwan and has older chip facilities in China and the U.S. state of Washington. Three sources familiar with the matter, speaking on condition of anonymity as they were not authorised to speak to the media, told Reuters that up to five additional fabs for Arizona are being planned. The initial fab is relatively modest by industry standards, with a planned output of 20,000 wafers — each of which contains thousands of chips — every month using the company’s most sophisticated 5 nanometre semiconductor manufacturing technology. It is not clear how much additional production capacity and investment the additional fabs might represent, and which chip manufacturing technology they would use. TSMC last month said it planned to invest $100 billion over the next three years to increase production capacity, though it did not give details. One person with direct knowledge of the matter told Reuters the expansion was in response to a request from the U.S., but declined to provide further details. “The United States requested it. Internally TSMC is planning to build up to six fabs,” the person said, adding that it was not possible to give a timeframe. The Biden administration is preparing to spend tens of billions of dollars to support domestic chip manufacturing. Under existing legislation, foreign firms are eligible for those funds, but whether they will ultimately receive it is an open question. A second person familiar with the plans said the company had already made sure there was enough space for expansion when they obtained the land for the first plant. “It’s so they can build six fabs,” the source said. The third person, from a TSMC supplier involved in the Arizona project, said TSMC had told them the plan was to build a total of six fabs over the next three years. Reuters was not able to independently confirm the timeframe. TSMC referred to comments by CEO C.C. Wei on an earnings call last month, saying the company was starting chip production in Arizona in 2024 with a 20,000 wafer per month 5-nanometer technology. “But in fact, we have acquired a large piece of land in Arizona to provide flexibility. So further expansion is possible, but we will ramp up to Phase 1 first, then based on the operation efficiency and cost economics and also the customers’ demand, to decide what the next steps we are going to do.” Asked whether the planned expansion was because of a request from the United States, TSMC said it was “not sure” what was meant by “requests” coming from the U.S. side and that it will decide next steps based on operational efficiency, cost economics and customer demand. “Once there is any official decision, we will disclose it accordingly.”"
https://venturebeat.com/2021/05/04/crowdsec-leverages-crowdsourcing-reinvent-cybersecurity-economics/,Crowdsec leverages crowdsourcing to reinvent cybersecurity economics,"Imagine a Ferarri losing Le Mans to a 40-year-old Pinto with a broken headlight and two flat tires. That pretty much describes the cybersecurity industry. Every day, hackers spending just a few thousand dollars manage to thwart defenses that cost companies hundreds of millions of dollars. That’s because these security systems only need to be beaten once out of thousands of tries to have their vulnerabilities exploited. Crowdsec wants to fix this economic imbalance by using crowdsourcing and open source software. The company is building what it calls a “massively multiplayer firewall,” and it believes this collaborative approach represents a critical strategic shift for protecting digital assets. “If you have to be open, then you have to take the risk that all your assets and resources have to be defended wherever they are,” Crowdsec CEO and cofounder Philippe Humeau told VentureBeat. “The point of our thinking is the only way to secure those is to establish trust between two peers. And the only way to establish trust reliably on a large scale is the crowd.” The company, which was founded in December 2020 and is based in Paris, today announced it has raised a $5 million round of funding. Breega led the round, and partner Maximilien Bacot said the concept was too compelling to pass up, even for a firm that doesn’t invest in a lot of security startups. “The product vision that they are putting on the table, to make sure that we can protect everybody by using the crowd, is very important,” he said. “I love the product and the vision.” Enterprises have to spend a lot on security because they face so many disadvantages. Once a vulnerability is found, it can take time for a patch to be released. And it can take more time to deploy the patch. Meanwhile, the perimeter companies have to protect continues to expand as more data moves to the cloud. Not only is this digital footprint larger, it is also more complex, increasing the chances of human or software vulnerabilities. “The bottom line is the castle strategy is over except if you want to go back to pagers and faxes and stuff that would drag your business 30 years back in time,” Humeau said. “And then you would not be competitive anymore.” Humeau believes Crowdsec can succeed because there are more humans working in cybersecurity than there are hackers. If the defenders banded together, that human-driven power could have a big impact. Of course, some companies attempt to leverage this human factor with bug bounty programs. Crowdsec wants to take that further by creating a platform that allows for real-time, large-scale collaboration. Once a member of the Crowsdec community reports an IP address that was used to launch an attack, Crowdsec’s software blocks it across its network. In this way, Humeau said the company will create the largest real-time hacking map on the internet. This will help block threats like port scans, identity theft, and denial of service attacks. In addition to these defensive capabilities, Crowdsec will offer several types of remediation, including simple access prohibition and rights limitation. The solution will work on just about any environment, including public clouds, containers, and virtualized architectures. The basic platform will be free as an incentive for people to adopt it and contribute back to the community. Crowdsec will eventually offer some paid services on top of that. Crowdsec won the grand prize for innovation at the most recent RSA. In the five months it has been live, the company says it has already attracted more than 6,000 users in 90 countries. The company intends to use the funding to expand its team and accelerate efforts to grow its community."
https://venturebeat.com/2021/05/04/zammo-unfurls-conversational-ai-integration-service/,Zammo unfurls conversational AI integration service,"Zammo.ai today launched a conversational AI platform that makes it simpler to engage customers via multiple voice assistants, interactive voice response (IVR)/telephony, and chatbots without having to write any code. That no-code approach, provided via the integrations the company has embedded within its software-as-a-service (SaaS) platform, enables organizations to create workflows that span multiple conversational AI technologies without the aid of an internal IT team or a systems integrator, said company CEO Alex Farr. “No one from IT is required,” he said. That approach provides the added benefit of eliminating the need to force customers to embrace a specific conversational AI platform, noted Farr. Organizations can add support for conversational AI platforms based on customer preferences, he said. Over time, most customers are likely going to prefer to engage organizations using some sort of conversational interface such as Apple Siri or Amazon Alexa. As these platforms become more widely employed by consumers, it’s only a matter of time before additional customer experiences are going to be routinely employed to, for example, find the nearest location to have a car serviced. At this point, it doesn’t appear any single conversational AI platform will emerge to dominate all others. Many users already switch between platforms multiple times a day depending on whether they are standing in a room or using their smartphone to send a text. Organizations that provide customer service, however, can never be sure what conversational AI platform might be employed at any given time. Conversational AI platforms are, of course, core to many digital business transformation initiatives. The simpler it becomes for a customer to make a purchase or request a service, the more likely it becomes they will engage. Many an instinct to impulsively acquire something has been ignored simply because it takes too long to log into a website to order it. Today some organizations are even sending reminders to end customers to reorder goods and services via a conversational AI platform as an alternative to text messages or emails. Not having this capability potentially creates an impression in customers’ minds about how modern an organization is as they begin to incorporate conversational AI into their everyday lives. However, a request made by a speech interface doesn’t always mean the customer wants their request answered via the same medium. Sometimes they may be asking an organization to send them a form via email. Regardless of how speech interfaces are employed, it’s clear that line of business units are becoming more empowered to automate tasks without any help from internal IT teams. The goal is not so much to eliminate the need for IT teams as much as it is to reduce the backlog of projects that IT teams are being asked to take on and then maintain. In most cases that is being accomplished using no-code tools embedded within a platform. However, power users, also known as citizen developers, are employing low-code tools to build more complex applications. The relationship between IT teams and end users is rapidly evolving. It may be a while before conversational AI platforms become the dominant user interface, but in certain use cases they are already having a profound impact. Most end users for the foreseeable future will continue to employ a mix of speech and graphical interfaces to interact with applications. However, the more conversational AI platforms learn about the habits and interests of an end user, the more proactive the platforms can become, especially if end users have opted into a service that provides that capability. The challenge organizations now face is striking the right balance between being actually helpful versus overreaching in a way that some customers might perceive to be intrusive or, for that matter, simply downright creepy."
https://venturebeat.com/2021/05/04/microsoft-open-sources-counterfit-an-ai-security-risk-assessment-tool/,"Microsoft open-sources Counterfit, an AI security risk assessment tool","Microsoft today open-sourced Counterfit, a tool designed to help developers test the security of AI and machine learning systems. The company says that Counterfit can enable organizations to conduct assessments to ensure that the algorithms used in their businesses are robust, reliable, and trustworthy. AI is being increasingly deployed in regulated industries like health care, finance, and defense. But organizations are lagging behind in their adoption of risk mitigation strategies. A Microsoft survey found that 25 out of 28 businesses indicated they don’t have the right resources in place to secure their AI systems, and that security professionals are looking for specific guidance in this space. Microsoft says that Counterfit was born out the company’s need to assess AI systems for vulnerabilities with the goal of proactively securing AI services. The tool started as a corpus of attack scripts written specifically to target AI models and then morphed into an automation product to benchmark multiple systems at scale. Under the hood, Counterfit is a command-line utility that provides a layer for adversarial frameworks, preloaded with algorithms that can be used to evade and steal models. Counterfit seeks to make published attacks accessible to the security community while offering an interface from which to build, manage, and launch those attacks on models. When conducting penetration testing on an AI system with Counterfit, security teams can opt for the default settings, set random parameters, or customize each for broad vulnerability coverage. Organizations with multiple models can use Counterfit’s built-in automation to scan — optionally multiple times in order to create operational baselines. Counterfit also provides logging to record the attacks against a target model. As Microsoft notes, telemetry might drive engineering teams to improve their understanding of a failure mode in a system. Internally, Microsoft says that it uses Counterfit as a part of its AI red team operations and in the AI development phase to catch vulnerabilities before they hit production. And the company says it’s tested Counterfit with several customers, including aerospace giant Airbus, which is developing an AI platform on Azure AI services. “AI is increasingly used in industry; it is vital to look ahead to securing this technology particularly to understand where feature space attacks can be realized in the problem space,” Matilda Rhode, a senior cybersecurity researcher at Airbus, said in a statement. The value of tools like Counterfit is quickly becoming apparent. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and in turn, punish those that don’t. The study suggests that there’s both reputational risk and a direct impact on the bottom line for companies that don’t approach the issue thoughtfully. Basically, consumers want confidence that AI is secure from manipulation. One of the recommendations from Gartner’s Top 5 Priorities for Managing AI Risk framework, published in January, is that organizations “[a]dopt specific AI security measures against adversarial attacks to ensure resistance and resilience.” The research firm estimates that by 2024, organizations which implement dedicated AI risk management controls will avoid negative AI outcomes twice as often as those that don’t.” According to a Gartner report, through 2022, 30% of all AI cyberattacks will leverage training-data poisoning, model theft, or adversarial samples to attack machine learning-powered systems. Counterfit is a part of Microsoft’s broader push toward explainable, secure, and “fair” AI systems. The company’s attempts at solutions to those and other challenges include AI bias-detecting tools, an open adversarial AI framework, internal efforts to reduce prejudicial errors, AI ethics checklists, and a committee (Aether) that advises on AI pursuits. Recently, Microsoft debuted SmartNoise (formerly WhiteNoise), a toolkit for differential privacy, as well as Fairlearn, which aims to assess AI systems’ fairness and mitigate any observed unfairness issues with algorithms."
https://venturebeat.com/2021/05/04/the-democratic-republic-of-congo-signs-a-public-private-partnership-with-insolation-solaire-inc/,The Democratic Republic of Congo Signs a Public Private Partnership with Insolation Solaire Inc.," The impact of the Electrification of Africa could be bigger than the Industrial Revolution  MONTREAL–(BUSINESS WIRE)–May 4, 2021– Jonathan Kalombo Tshimpaka, the Founder & CEO of Insolation Solaire Inc., a Canadian solar energy startup, signed a Public Private Partnership [PPP] with the Democratic Republic of Congo to deploy its novel solar optical module technology called “Oriens Duo” to test its innovative technology in rural areas of the DRC. ANSER’s, the government’s agency assigned to promote the electrification of Congo’s rural sectors, goal is to supply electricity to approximately 15 million inhabitants living in rural areas by 2024. ANSER is the state entity that signed the Protocol of Agreement with Insolation Solaire Inc. This partnership will commence with a Pilot Project in an area named Kabeya Kamuanga, in the province of Kasaï in May 2021. Oriens Duo was invented by the Chief Technical Officer (CTO) of Insolation Solaire Inc., Gilles Leduc. It uses 40% efficient Quantum Dot Triple Junction solar cells coupled with a Winston Cone to concentrate solar radiations towards the substrate. Furthermore, Oriens Duo includes a mechanism of heat transfer to produce thermal energy as a byproduct for the end user, which makes it a hybrid system. The technology enables air conditioner as well, which will be immensely appreciated in Congo where temperatures are very hot and humid. Insolation Solaire Inc. has captured the essence of evolution and is setting a new standard in the solar industry. As the African market is growing at a staggering rate, the development of novel technologies using a methodology resembling that of Oriens Duo is practically non-existent. Consequently, it is an unprecedented opportunity for Insolation Solaire Inc. to capitalize and forge an enduring empire fueled by the sun. The political climate of the Democratic Republic of Congo has been stabilizing since the peaceful and mindful strategist, President Felix Antoine Tshilombo Tshisekedi, has been in power and recently been elevated to the status of President of the African Union. “I believe that President Tshisekedi can unite Africa. Congo being the heart and the trigger of Africa, our work could send shockwaves of a modernization across the continent. Hence, I am wholeheartedly welcoming North American investors to help lay the foundation of this magnificent project,” said the 32 years old CEO, Jonathan Kalombo Tshimpaka. www.insolationinc.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210401005981/en/ 438-378-7284Jonathan@oriensduo.com Jonathan K. Tshimpaka"
https://venturebeat.com/2021/05/04/microsoft-launches-power-bi-goals-to-help-manage-productivity/,Microsoft launches Power BI Goals to help manage productivity,"At its virtual Business Applications Summit, Microsoft today announced the launch of Power BI Goals, a set of tools in preview that lets customers tap the AI capabilities in Power BI. Microsoft says Goals is designed to empower users to make data-driven decisions and stay on top of team or individual targets.​ The way goals and targets are tracked has changed due to modified work environments and teams scaling at a rapid pace. In the modern workplace, effective data use is essential to ensuring actions produce measurable outcomes. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion in 2020. Power BI Goals enables users to build specialized scorecards to delegate tasks and measure progress toward specific goals.​ Managers can use these scorecards to identify areas of improvement and collaborate via platforms like Microsoft teams, as well as proactively taking action with check-in notes.  Goals offers a hub from which users can stay on top of tasks and navigate. The top section shows goals curated automatically by Power BI, including assigned and edited goals. The final section contains sample scorecards for new users. Scorecards in Power BI bring together all the different goals and subgoals teams are tracking.  Users can assign owners to a goal, and goals have a status to let stakeholders know if something’s on track or falling behind. Scorecards also support data and notes, which can be used to proactively keep teams up to date. And they feature a details pane that provides goal status, check-ins, and note histories.  When users start from a blank scorecard, they can add a goal and start defining the data. Scorecard owners can choose whether their targets and values are primarily data-driven. If a user wants to define those directly in the scorecard, they can type them in, but they can also connect to a Power BI app or report. Scorecards can even have goals that span reports across multiple enterprise workspaces.  After navigating to a report, users can select any visuals they want, including specific filters and drill-downs. Power BI can bring the full history of the data into the scorecard and perform lightweight formatting, as well as defining status and due date. Alongside the scorecard artifact, Power BI creates a dataset with a predefined schema that’s regularly updated. It contains information about the scorecard, goals, their values, and notes. During the pandemic, as shelter-in-place orders and office closures forced employees to work from home, companies increasingly experimented with or adopted productivity management tools. According to a June study by Gartner, 26% of HR leaders report having used some form of software or technology to track remote work since the start of the health crisis. It’s an uptick driven in part by concerns over performance dips that could arise from work-from-home setups. A survey by global recruitment firm Robert Walters found that 64% of businesses are concerned about remote employees’ productivity. Reflecting the trend, Microsoft CVP Alysa Taylor says the Power Platform and Dynamics 365 communities now total more than 775 user groups and 2.85 million active community members. “Even as challenges [from the pandemic] continue to reverberate across industries — from supply chain disruptions to recurring restrictions — most businesses are taking steps toward recovery. They’re taking stock of vulnerabilities and opportunities across their organizations, markets, and industries — and the steps needed to move forward into the future.” The market that Power BI Goals addresses is a profitable one. According to Research and Markets, task management software is expected to reach a collective $4.72 billion in value by 2026, from the present estimate of $2.35 billion at a compound annual growth rate of 12.32%. In the future, Microsoft plans to bring Power BI Goals to mobile and introduce automated status rules, in addition to “rollups” that determine how subgoals roll up to their parent goals. Beyond this, Power BI Goals will gain formatting capabilities and scorecard visuals in Power BI Desktop, plus Power Automate integration to trigger business workflows when goals change and hierarchies based on Power BI data models."
https://venturebeat.com/2021/05/04/vanta-raises-50m-to-automate-cybersecurity-compliance/,Cybersecurity compliance startup Vanta raises $50M,"Vanta, a San Francisco, California-based automated security and compliance platform, today announced it has closed a $50 million funding round led by Sequoia Capital. The startup says it will use the capital to meet customer demand, as well as launching new products and opening a second office in New York. In a 2017 Deloitte survey, only 42% of respondents considered their institutions to be “extremely” or “very” effective at managing cybersecurity risk. The pandemic has certainly done nothing to alleviate these concerns. Despite increased IT security investments companies made in 2020 to deal with distributed IT and work-from-home challenges, nearly 80% of senior IT workers and IT security leaders believe their organizations lack sufficient defenses against cyberattacks, according to IDG. Vanta, which was founded in 2017, allows companies to prepare for SOC 2 audits, with an automated monitoring platform that connects to services, including Google Cloud Platform, Amazon Web Services, GitHub, Okta, and Slack. Developed by the American Institute of Certified Public Accountants, SOC 2 is designed to assess the security of service providers storing customer data in the cloud.  While conventional compliance prep methods can be laborious and error–prone, Vanta claims to help startups become secure by integrating with their infrastructure, generating a list of items to fix, and facilitating audits with certified public accountants. “We’re at a crucial turning point as an industry: Software is a huge part of our lives, but we trust software companies less than ever before,” Vanta CEO Christina Cacioppo said in a press release. “At Vanta, we’re pioneering a continuous, automated approach to security and compliance that protects customer data and helps grow these businesses.” Vanta also offers prep tools for the Health Insurance Portability and Accountability Act (HIPAA) and ISO 27001, the international standard for information security management. HIPPA, which was signed into U.S. law in 1996, implements rules to protect sensitive patient health information from being disclosed without the patient’s consent. Vanta has a number of competitors in a cybersecurity market estimated to be worth over $162.5 million. SecurityScorecard, a cybersecurity rating and risk-monitoring platform, recently announced it has raised $180 million. There’s also Securiti.ai, Safeguard Cyber, and DefenseStorm, which consolidates security data from multiple sources and uncovers anomalies with AI. But Vanta says it has seen substantial growth since coming to market three years ago, surpassing $10 million in annual recurring revenue and continuing to double its customer base every six months. Today, Vanta has more than 1,000 customers, including Clubhouse, Lattice, Calm, Loom, Notion, and UserTesting. Existing investor Y Combinator also participated in the series A round announced today."
https://venturebeat.com/2021/05/04/releviate-therapeutics-appoints-jennifer-maynard-to-its-scientific-advisory-board/,Releviate Therapeutics Appoints Jennifer Maynard to Its Scientific Advisory Board," As One of the Pharmaceutical Industry’s Leading Antibody Specialists, Maynard Brings Releviate Therapeutics Over 20 Years of Experience in Antibody Therapeutics  SAN DIEGO–(BUSINESS WIRE)–May 4, 2021– Releviate™ Therapeutics, a bio-pharmaceutical company addressing the needs of patients suffering from neuropathic pain, today welcomes Jennifer Maynard to its scientific advisory board. Maynard, who holds a doctorate in chemical engineering, bears deep expertise in engineering and development of antibody therapeutics. As a member of the board, she will provide strategic guidance for Releviate Therapeutics’ scientific research and technology development. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210504005017/en/ “Jennifer is recognized as one of the country’s leading antibody specialists for pharmaceutical companies,” said Sergey Sikora, CEO of Releviate Therapeutics. “Her experience with drug development is second to none. She has a proven track record of overseeing antibodies through their entire lifecycle, from discovery and engineering to development, including one that ultimately received Food and Drug Administration approval. Jennifer’s expertise will play an integral role in the success of Releviate Therapeutics’ technology.” Since 2018, Maynard has held the Henry Beckman Professorship in the McKetta Department of Chemical Engineering at the University of Texas at Austin. Her current research focuses on development of protein therapeutics to address unmet medical needs in infectious diseases, with specific interests in Bordetella pertussis and cytomegalovirus as target pathogens. Maynard is credited with engineering an antibody to neutralize anthrax toxin, which was subsequently developed by liscensee Elusys and received FDA approval in 2016 as Anthim. In addition, she engineered two pertussis-toxin neutralizing antibodies for reduced immunogenicity and extended serum half-life in her own lab that mitigated all clinical symptoms of disease when administered prophylactically to infant baboons. Since 2017, Maynard has served on the editorial boards of Scientific Reports, Frontiers in Immunology and BMC Biotechnology. She has coauthored over 60 publications and is the recipient of numerous awards, including the 2017 Fellow of the American Institute of Medical and Biological Engineers, a Packard Fellowship, Bill and Melinda Gates Grand Challenge Awards and a National Institutes of Health National Research Service Award. Maynard served her postdoctoral fellowship at Stanford University where she focused on microbiology and immunology. She holds a doctorate in chemical engineering from the University of Texas at Austin and a bachelor’s degree in human biology from Stanford University. About Releviate Therapeutics Founded in 2020, Releviate™ Therapeutics is a bio-pharmaceutical company addressing the needs of patients suffering from neuropathic pain. Our treatment strategy deploys specific human monoclonal antibodies to inactivate certain matrix metalloproteinases – MMP-9 and MMP-14 – and potentially improve patient outcomes. Unlike traditional neuropathic pain medication strategies, Releviate Therapeutics directly targets pain with antibodies whose development is rooted in actual pain pathology and in directly inactivating pain pathways. Initial indications address small fiber neuropathy and diabetic neuropathy. The company has headquarters in San Diego, California, and can be visited online at www.releviatetherapeutics.com. Releviate™ and its logo are trademarks of Releviate Therapeutics in the U.S. and other countries. All other trademarks and registered trademarks are the property of their respective owners.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210504005017/en/ Olivia MetcalfeReleviate Therapeuticsolivia@releviatetherapeutics.com"
https://venturebeat.com/2021/05/04/third-party-ransomware-risk-is-real-but-black-kites-latest-tool-can-help/,"Third-party ransomware risk is real, but Black Kite’s latest tool can help","A new assessment service from cybersecurity ratings provider Black Kite aims to let enterprise defenders know which of their third-party partners and vendors could be vulnerable to a ransomware attack. Ransomware was the scourge of information security in 2020, as the malware brought all kinds of organizations — financial services, health care facilities, educational systems, municipalities, and enterprises — to a screeching halt. Ransoms are getting larger, and tactics have evolved as attackers shift away from just encrypting data to actually stealing the data. The Ransomware Susceptibility Index analyzes technical data from open source intelligence sources to calculate the probability that a company will suffer a ransomware attack within 12 months, Bob Maley, Black Kite’s chief security officer, told VentureBeat. The Index developed a machine learning model that considers 26 controls to calculate a score between 0 and 1. The higher value means the company has a greater likelihood of being hit by a successful ransomware attack. The goal is to give enterprises reliable data about their ransomware risk so they can make informed decisions about how they work with third-party partners, Maley said. Many ransomware attacks now target third-party suppliers and partners instead of going straight for a single company. This is in part because the partners may have weaker security defenses. They could be behind on security updates, or their employees might be more likely to fall for phishing schemes. Another reason is that attacking a supplier would net the gang more victims because a supply chain attack would affect all of the supplier’s customers. In August 2019, 22 towns in Texas were hit by a ransomware attack when the gang targeted the managed service provider the towns used. When cloud services provider Blackbaud was hit by ransomware, dozens of its customers had to disclose the breach. Enterprises have to look beyond their own environment when assessing their ransomware risk, Maley said. If the third-party providers are hit, the malware may be able to cascade into their networks. Or the gang will steal data from the provider that actually belongs to the client organizations. Enterprise defenders can use the Index to gauge the risks of a ransomware attack for each of their partners. The Index isn’t just a score. It includes a detailed report showing which of the 26 controls are missing. If a partner has a high score, the security team can contact that partner and demand the issues be fixed, Maley said. Black Kite’s team of researchers needed a way to check the Index’s accuracy, so they turned to the dark web. Many ransomware gangs now sell the stolen data on criminal marketplaces if the victim doesn’t pay the ransom. The team looked for data dumps that were the result of ransomware attacks and checked the Index to see the victim organization’s score. Just two weeks ago, notorious ransomware gang REvil said it had stolen schematics of unreleased Apple products from an Apple supplier. The group demanded Apple pay $50 million and threatened to sell the data to the highest bidder. The RSI score for that Apple supplier was 0.729, Maley said. A prominent health care provider whose data was put up for sale after a ransomware attack (which has not been publicly discussed at this time) had an RSI score of 0.928, Maley said. Black Kite was able to validate the Index’s accuracy by checking multiple victims across different industries, Maley said. Many defenders are beginning to feel there is no way to avoid an attack so they should instead focus on making sure recovery is possible, Maley said. But while recovery planning is important, defenders shouldn’t give up trying to block the attack. Attackers research their targets before launching attacks. This research includes identifying potential phishing victims, searching for user credentials, scanning for unpatched vulnerabilities and outdated software, uncovering fraudulent domains, and looking for exposed ports. With this information in hand, the attackers craft a campaign to get a foothold in the network in order to deploy the ransomware. RSI relies on the same data sources to calculate ransomware risk. “You can either be fatalistic or you can look at what the attackers look at,” Maley said."
https://venturebeat.com/2021/05/04/honeybook-boosts-contractor-payment-booking-invoicing-platform-with-155m/,"HoneyBook boosts contractor payment, booking, invoicing with $155M","Business management platform HoneyBook today announced that it raised $155 million at a $1.1 billion valuation post-money. The company says that it’ll use the funding to expand its platform and acquire new customers nationally. The gig economy is alive and well. One estimate pegs the number of U.S. workers who have an “alternative work arrangement” as their primary job at 57 million, contributing to a global freelance and independent contractor market worth $3.7 trillion, according to Staffing Industry Analysts. Statista projects that the gross volume of the freelancer economy is expected to reach $455.2 billion by 2023. HoneyBook, an alum of startup accelerator UpWest Labs, offers a financial and business management service for freelancers and “solopreneurs.” CEO Oz Alon, who founded the company in 2013 with his wife Naama Alon, Dror Shimoni, and Shadiah Sigala, initially launched HoneyBook as a wedding album service targeting gig economy photographers. The goal was to build a crowdsourced database of wedding vendors, but the company soon decided to broaden the platform’s focus. Today, HoneyBook offers tools to help facilitate freelancer booking, proposals, invoicing, and payments. HoneyBook users create profile pages using a set of preconfigured templates. After a client responds and agrees to terms, the parties draft a contract together using a module that automatically pulls in the relevant details. HoneyBook then highlights important fields and generates notifications, alerting all parties when the paperwork has been reviewed and signed. HoneyBook’s apps make project files and documents shareable while collating text, email, and chat messages in a single view. The platform’s billing service handles recurring, scheduled, and one-off payments and walks customers through the invoicing, contract, and closure processes. Clients can sign digitally, freelancers can brand the workflow with banners and logos, and HoneyBook’s automation toolset can be programmed to send reminders via email. There’s also a community component. HoneyBook hosts a curated classified ads board where hirers can post and solicit replies about opportunities and ping the company’s network of over 75,000 workers. A search tool lets clients drill down by location and expertise or view profile pages highlighting past projects and collaborations. HoneyBook shares the crowded gig networking space with Fiverr, which recently acquired Phoenix-based ClearVoice, and dozens of others, including Upwork (which filed for an IPO in October), Freelancer.com, and Guru.com (which raised $25 million in December). But Oz Alon believes HoneyBook’s breadth of features — particularly its automation and contract management tools — put it a cut above the rest. In 2020, HoneyBook saw $3 billion in business booked on its platform, $1 billion of which occurred in 2020 alone. Durable Capital Partners led HoneyBook’s latest funding round with participation from Tiger Global Management, Battery Ventures, Zeev Ventures, and O1 Advisors, bringing the company’s total raised to $241 million. Existing investors including Citi Ventures, Norwest Venture Partners, Aleph, Vintage Investment Partners, Hillsven Capital, and UpWest Labs also contributed."
https://venturebeat.com/2021/05/04/cibc-innovation-banking-to-fuel-growth-for-technology-clients-across-borders/,CIBC Innovation Banking to Fuel Growth for Technology Clients Across Borders," Deepened North American focus will enable cross border growth ambitions for clients  ATLANTA & DENVER–(BUSINESS WIRE)–May 4, 2021– CIBC Innovation Banking today announced the appointment of Charlie Kelly and Alan Spurgin as Co-Heads of its U.S. Technology Banking team. The appointments of Mr. Kelly and Mr. Spurgin within the bank’s broader North American Innovation Banking group position CIBC to meet the needs of fast-growing technology firms, as the innovation ecosystem is poised for accelerated growth within the economic recovery. “Technology companies are well-positioned to be a growth engine for the North American economy going forward, particularly as we emerge from the pandemic,” said Mark Usher, Managing Director, North American Market Leader, CIBC Innovation Banking. “Charlie and Alan bring a wealth of expertise to the table and a very strong focus on our clients and the tech sector. Together, they will ensure we bring an integrated and seamless approach to our clients in this important market as we build on our strong momentum and deliver value to our clients as they start up and scale up.” Both Mr. Kelly and Mr. Spurgin joined CIBC three years ago and have extensive experience in the sector. “Technology and innovation are not constrained by borders, and increasingly the conversations we are having with clients in these sectors are focused on growth across North America,” added Mr. Usher. “We’re excited to have Charlie and Alan take on these leadership roles within our broader North American team, and we’re committed to helping innovative companies grow their business and achieve their ambitions as one team.” Charlie can be reached at charlie.kelly@cibc.com.Alan can be reached at alan.spurgin@cibc.com About CIBC Innovation BankingCIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210504005339/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609"
https://venturebeat.com/2021/05/04/datanomix-raises-6m-to-monitor-factory-operations/,Datanomix raises $6M to monitor factory operations,"Datanomix, a production intelligence software vendor, today announced it has raised $6 million in series A funding to expand the reach of its software. The round was co-led by Gutbrain Ventures and PBJ Capital, and Datanomix says it will put the funds toward growing its sales, marketing, customer success, and engineering departments. Manufacturing is undergoing a resurgence as business owners look to modernize their factories and speed up operations. According to ABI Research, more than 4 million commercial robots will be installed in over 50,000 warehouses around the world by 2025, up from under 4,000 warehouses as of 2018. Oxford Economics anticipates 12.5 million manufacturing jobs will be automated in China, while McKinsey projects machines will take upwards of 30% of these jobs in the U.S. Founded in 2016 and based in Nashua, New Hampshire, Datanomix offers an operations monitoring solution that requires no operator input and automatically benchmarks production using only manufacturing equipment data. Datanomix’s dashboards deliver statuses on factory KPIs and allow users to drill down into specific metrics, jobs, and clients at any time. The platform can connect to existing enterprise resource management systems for maintenance, tuning, and calibration workloads and supports alerts to notify customers when conditions — e.g., temperatures, pressures, and vibrations — require their attention. “Fundamentally, there were three significant gaps in the real-time factory data market that we sought to address: (1) Is this data meaningful enough right now that it can change my day in progress? (2) Does the system deliver information in such a way that it naturally aligns with the chaotic workflow of manufacturing people? and (3) Is the data contextual enough that it can immediately improve my estimating/costing/profitability metrics?” a spokesperson told VentureBeat via email. “Customers see what benchmarks our software creates, with literally no input required from them at all. They are blown away that we basically know their jobs as well or better than they do.” According to a 2020 PricewaterhouseCoopers survey, companies in manufacturing expect efficiency gains over the next five years attributable to digital transformations. McKinsey’s research with the World Economic Forum puts the value creation potential of manufacturers implementing “Industry 4.0” — the automation of traditional industrial practices — in their operations at $3.7 trillion in 2025. With a customer’s Wi-Fi information, Datanomix draws on devices plugged into a factory’s CNC machines. The platform creates benchmarks for every job run around cycle time, parts per hour, and utilization. Performance is predicted in terms of how many parts a factory should be making when a job is running — Datanomix automatically takes into account scenarios like multiple jobs in a shift, rotating operations, jobs that end partway through a shift, and more. Datanomix has 15 employees and says it has attracted “dozens” of new customers this year and is “consistently” doubling its business every quarter. “Datanomix is well positioned, given the current landscape of manufacturers who have a clear mandate to digitize the information they use to manage production and growth of their companies, products, and profits,” CEO John Joseph told VentureBeat via email. “We purpose-built our software around the need for real-time intelligence frameworks that drive people to action, higher levels of productivity, and bigger outcomes.” Beyond Gutbrain and PBJ, CEAA Investments and previous backers participated in Datanomix’s latest funding round — including Argon Ventures, York IE, Wasabi Ventures, Alumni Venture Group, and Millworks Fund. This brings the company’s total raised to date to $9 million."
https://venturebeat.com/2021/05/04/workboard-raises-75m-to-help-companies-track-okrs/,WorkBoard raises $75M to help companies track OKRs,"OKR-tracking platform WorkBoard today announced that it raised $75 million in a series D round led by Softbank. The company says it’ll use the funds, which bring its total raised to over $141 million and its valuation to $800 million post-money, to grow the WorkBoard platform and invest in product development. Millions of employees transitioned to remote work, either permanently or temporarily, during the pandemic. Against this backdrop, organizations have increased investments in project management software to support collaboration in the absence of physical workspaces. According to Accenture, pre-pandemic, 40% of executives felt “highly agile” to flexibly make changes, a percentage that plummeted to 18% when that agility was tested over the past 12 months. Headquartered in Redwood City, California and founded in 2013, WorkBoard offers automated business reviews, dashboards, and customizable meeting agendas to help companies align and iterate on strategic priorities. Deidre Paknad and her husband Daryoush Paknad started the company after Deidre’s previous startup was acquired by IBM. Daryoush, a technologist, was one of the first three members of the Adobe Acrobat team and an engineering leader at Netscape. WorkBoard lets companies chart, calculate, and share progress on key results and success metrics across mobile, web, and other platforms, including Slack and Microsoft Teams. The software can run virtual or in-person meetings with metrics automatically generated. It can also capture decisions and actions, presenting the list of actions from the last meeting to follow up in the next meeting. WorkBoard provides Trello-like Kanban boards for organizing tasks, due dates, and statuses. And the platform integrates with third-party solutions like Jira and Azure DevOps to enable companies to, for example, bring OKRs into line of sight in Jira and update results automatically from projects. According to a spokesperson, WorkBoard uses natural language processing to make recommendations on what outcomes to measure as well as where duplicative efforts are working at cross purposes. “By capturing and managing the strategic priorities and desired business results for its customers over time for several years, WorkBoard has the ability to provide them with analytics and intelligence on what their patterns of achievement (and non-achievement) are, what causes and what correlates to the best business or strategic outcomes, and how and where to repeat what is most potent for growth,” the spokesperson told VentureBeat via email. The development of OKRs is generally attributed to Andrew Grove, who implemented the approach at Intel during his tenure there. Intel salesperson John Doerr introduced OKRs to Google, where they took hold. Today, OKRs are used by companies like Amazon and Spotify to define goals or objectives and then track the outcome. The global OKR software market is estimated to surpass $1.59 billion by 2026, according to Coherent Market Insights. According to a June study by Gartner, 26% of HR leaders report having used some form of software or technology to track worker productivity since the start of the health crisis. It’s an uptick driven in part by concerns over performance dips that could arise from work-from-home setups — a survey by global recruitment firm Robert Walters found that 64% of businesses are concerned about remote employees’ output. Despite competition from Ally.io, Asana, and others, 250-employee WorkBoard says it’s nabbed hundreds of clients including Cisco, IBM, Microsoft, and a number of financial institutions, manufacturers, life science, and health care companies. The company says it grew more than 100% in 2020, more than 200% in 2019, and is on track to double this year. “During the pandemic, WorkBoard saw an uptick in business from a wide range of industries — including financial institutions, manufacturers, life science, and healthcare,” the spokesperson said. “Companies everywhere needed to adjust their strategic priorities iteratively and rally people around them without the benefit of office proximity; visibility and cohesion got exponentially more important, and they are remarkably hard without a platform like WorkBoard. Now, in an expanding economy and pervasive changes in the way customers buy and people work, companies operate with high agility and velocity to thrive.” Existing investors Andreessen Horowitz, GGV Capital, Workday Ventures, Microsoft M12, and new backers Intel Capital, Capital One Ventures, and SVB Capital also participated in WorkBoard’s latest funding round. The company previously raised $30 million in a series C that closed January 2020. “We are in an era of epic disruption and untethered talent. Engaging and mobilizing teams in creating value for customers at velocity are now table stakes in most industries,” Deidre Paknad said. “This funding helps us expand our reach and value to companies who are driving bold visions with high focus and urgency.”"
https://venturebeat.com/2021/05/04/data-backup-company-acronis-secures-250m-to-expand-datacenter-footprint/,Data backup company Acronis secures $250M to expand datacenter footprint,"Data recovery startup Acronis today announced that it raised $250 million at a valuation of over $2.5 billion post-money. The funds bring Singapore- and Switzerland-based Acronis’ total raised to over $408 million, following angel, debt, and later-stage raises from March 2014 to September 2019. Founder and CEO Serguei Beloussov said the new funds will be used to acquire more companies, expand Acronis’ engineering team, and build over 111 datacenters around the globe. There are few catastrophes more disruptive to an enterprise than data loss, and the causes are unfortunately myriad. In a recent survey of IT professionals, about a third pegged the blame on hardware or system failure, while 29% said their companies lost data because of human error or ransomware. It’s estimated that upwards of 93% of organizations that lose servers for 10 days or more during a disaster filed for bankruptcy within the next 12 months, with 43% never reopening. Those statistics are more alarming in light of high-profile outages like that of OVHCloud earlier this year, which took down 3.6 million websites ranging from government agencies to financial institutions to computer gaming companies.  Acronis traces its origins to Parallels’ corporate parent SWsoft, where Beloussov, Ilya Zubarev, Stanislav Protassov, and Max Tsyplyaev founded it as a separate division in 2001. Acronis spun out in 2003, after which it pivoted focus from disk partitioning and bootloader utilities to backup and disaster recovery software based on disk imaging technology. Over the next decade or so, it acquired three firms — BackupAgent (specializing in cloud backup), nScaled (disaster recovery), and GroupLogic (enterprise data transformation and storage) — and launched a global partner program, prior to which Acronis started an R&D wing in Acronis Labs. Acronis’ largest revenue drivers are backup, disaster recovery, secure file access, sync and share, and partitioning, with clients ranging from single-license home users to large enterprises. True Image, the backup software for which Acronis is perhaps best known, uses virtualization and machine learning techniques to mirror images, clone disks, provide blockchain data notarization, and scan for malware in copied files. As for the Acronis Cloud platform, it encompasses backups, disaster recovery, and file synchronization with a selected set of cloud apps curated by service providers. Acronis Backup and Acronis Backup Advanced, two additional products in Acronis’ disk-based backup and recovery suite, protect files from modification and encryption while minimizing process disruption to a few seconds. Through a centralized management dashboard, admins can back up to cloud storage providers and convert backups into a set of virtual machine files that can run on hypervisors. Aside from backup software, Acronis also offers Disk Director, a shareware app that partitions machines and allows them to run multiple operating systems. Disk Director joins Acronis Snap Deploy, which creates a standard machine configuration that can be deployed across up to hundreds of live Windows computers simultaneously. Acronis Files Advanced secures access to files that are synced between devices. And Acronis Access Connect enables Mac users to connect to and mount directories on a Windows file server just as native Apple Filing Protocol volumes. Acronis occupies a data backup and recovery market anticipated to be worth $11.59 billion by 2022, according to Markets and Markets. It competes to a degree with San Francisco-based Rubrik, which has raised hundreds of million in venture capital to date for its live data access and recovery offerings. There’s also Cohesity, which has raised over $650 million, and Clumio, which raked in $186 million so far for its cloud-hosted backup and recovery tools, as well as data recovery companies Veeam and HYCU. But Beloussov claims that Acronis has rivals beat when it comes to install base. The company now counts over 5.5 million consumers and 500,000 businesses among its client roster, including 80% of the Fortune 1000 companies. Since its previous funding round, Acronis has made a string of acquisitions including of consultancy CyberLynx, cloud management startup 5nine, distributor Synapsys, and endpoint data loss prevention firm DeviceLock. The company also launched cloud datacenters in Canada and Brazil and launched a new, no-cost version of its Cyber Protect Cloud service provider solution. Acronis has more than 1,300 employees in 18 countries, the bulk of whom are based in Singapore, Bulgaria, and Arizona. Acronis Labs is based in the U.S. and Singapore."
https://venturebeat.com/2021/05/04/market-intelligence-platform-crayon-raises-22m/,Market intelligence platform Crayon raises $22M,"Crayon, a Boston, Massachusetts-based market intelligence company, today announced that it raised $22 million in a series B round led by Baird Capital. CEO Jonah Lopin says that the proceeds will be put toward expanding the platform while growing Crayon’s engineering, sales, and marketing teams. Standing out and differentiating through product design, packaging, and messaging is critical, with 75% of technology buyers claiming they don’t understand how vendors are different, according to Gartner. With nearly one-third of all sales pitches lost to competitors, discovering and sharing compelling insights can mean the difference between winning or losing a competitive deal. Crayon’s own research suggests that competitive intelligence should be a strategic priority for every company with at least one competitor in their segment. Over 60% of businesses report competitive intelligence has positively impacted revenue, Crayon found in a 2021 survey, a 17% increase from 2019. Crayon, which was founded in 2014, helps midsize and large enterprises capture, analyze, and act on competitive intelligence to drive business execution and decision-making. It enables businesses to track, understand, and react to market changes, delivering a holistic view of a business to foster sales, long-term revenue, and relationships.  “My cofounder, John Osborne, and I met at MIT Sloan doing our MBAs back in 2007, although it took us almost a decade after graduation before we founded Crayon in 2015,” Lopin told VentureBeat via email. “The thesis behind Crayon is that there’s never been a good way to do competitive intelligence, because it’s always been a human-driven, research-driven discipline. This means insights are very expensive to generate, and even worse, they lag the market by weeks or months, and therefore often aren’t actionable or impactful.” Crayon’s platform automatically captures moves that companies make, drawing on more than 46,000 data points from over 300 million sources to track pricing adjustments, customer reviews, marketing campaigns, and other intelligence. AI and human review highlight key updates, trends, and more, while analytics tools let customers filter data by category, company, keyword, and date. Users can also save views to monitor changes, share insights, or home in on data. Or they can partner with a market intelligence analyst to curate intel and call out trends weekly. “It’s hard to track website changes well because of rampant false-positives, and that’s where we apply AI and machine learning: to filter out the noise. We’ve actually compared over a billion web pages in the past 4 years for customers, so the dataset feeding our AI is quite large,” Lopin explained. “Crayon [also] uses AI and machine learning to prioritize insights for customers through an ‘importance’ scoring model … We flag approximately 3% of all data points for customers as ‘high importance’ for them to review, so they never miss a critical update. [Beyond this,] we do natural language processing to support visualizations such as this: how did HubSpot’s content marketing change from the pre-COVID to post-COVID period?” The competitive intelligence tools market is anticipated to be worth $82.0 million by 2027, up from $37.6 million in 2019, according to Fortune Business Insights. Partially driving the growth is predictive modeling challenges related to the pandemic. For enterprises modeling future consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the global health crisis. “At Crayon, we drive competitive intelligence to a software discipline. What if competitive intelligence could be programmatic and continuous? When CI is software-driven, not only are the insights inexpensive, they are real-time and therefore much more actionable,” Lopin said. “Finally, competitive intelligence can be timely and hyper-actionable in sales, customer success, marketing, and product.” Lopin says that Crayon has more than 33,000 users across 500 customers including Discover, Gong, Intuit, SurveyMonkey, Zendesk, and ZoomInfo. At Dropbox, “won” and “lost” competitive deals flow through the platform via the Salesforce integrations. Crayon sends competitive information via email and Slack and serves as the system of record for employee-sourced intelligence. “In Crayon, Dropbox employees can select the competitive set for a product line, pull all insights with takeaways, and see the patterns and trends from a quarter with a few clicks. So it’s days down to hours to get insight for execs in the process,” Lopin said. Baseline Ventures, Bedrock Capital, C&B Capital, Oyster Funds, and Gaingels also participated in 100-employee Crayon’s latest funding round. It brings the company’s total raised to date to $38 million following a $6 million series A in February 2019."
https://venturebeat.com/2021/05/04/jupiterone-nabs-30m-to-help-companies-manage-cybersecurity-data/,JupiterOne nabs $30M to help companies manage cybersecurity data,"JupiterOne, a cybersecurity management automation startup, today closed a $30 million series B round led by Sapphire Ventures. Cofounder and CEO Erkang Zheng says the capital will be used to grow JupiterOne’s engineering, product development, and go-to-market capabilities, including building out its remote workforce and extending the reach of partnership and integration teams. Cybersecurity asset management, or the process of creating and continually updating an inventory of IT resources, can be a resource drain. According to a 2019 Deloitte survey, executives spend 13% of their time addressing cyber monitoring and operation challenges. Despite this, relatively few businesses are proactive about asset management, perhaps owing to logistical challenges. Gartner estimates that only 35% of companies are designing, documenting, and regularly testing assets using inventory tools and software. JupiterOne claims to make security teams more efficient by centralizing the data from dozens of cloud services into a single hub for management, analysis, and alerts. Via the platform’s integrations and API, it automatically pulls in read-only data to generate a real-time inventory of resources and assets, including code, repositories, and endpoints. A one-word search across the inventory returns detailed information like account access, devices in use, resources, and even user-made changes to code repositories. JupiterOne was founded as a subsidiary of Indianapolis, Indiana-based health software company LifeOmic. Zheng served as LifeOmic’s chief information security officer and initially built JupiterOne to support LifeOmic’s security and compliance needs. In pursuit of a cybersecurity asset management segment that’s anticipated to reach $8.5 billion in spending by 2024, according to Zheng, LifeOmic productized the solution as JupiterOne and spun it out in March 2018. “JupiterOne was born on the back of the idea that maintaining a deep understanding of what cyber assets are in your environment and how they are interconnected would provide the foundation for any security programs you were trying to build,” a spokesperson told VentureBeat via email. “[The company] provides innovative cyber asset management and governance solutions to over 100 enterprises around the world. Additionally, JupiterOne has a free software-as-a-service offering that any company of any size can use. [The company] believes that security is a basic right for all and provides free service levels to support that level of basic right.” JupiterOne’s algorithms fetch and classify entities in environments automatically and map them to tools like the compliance dashboard. There, users get an overview of the top-level controls and policies that make up their company’s security framework. Users can dig into specific requirements or use JupiterOne’s policy builder to review, update, and visualize asset relationships and craft a set of procedures from templates covering 24 major security policy domains. Using the JupiterOne Insights app, customers can build customized reporting dashboards and visualizations with searches and queries for inadvertent self-reviews, suspicious code commits, pull requests, code repos, and more. Each dashboard can be configured as a shared team board or as a personal board, and the layout of each board is individually saved per user, allowing a user to customize layouts without impacting other users. For alerts, JupiterOne’s rules panel leverages a knowledge graph to factor in things like a user’s permissions and whether multifactor authentication has been enabled before triggering a new alert. JupiterOne boasts a library of preconfigured rules and intelligent rules that can be set to run from every 15 minutes to every 24 hours to ensure security teams remediate when a new, high-severity alert occurs. “Customers’ primary use case is continuous knowledge of the state of their entire infrastructure and security universe. Specifically, JupiterOne customers gain continuous knowledge of their cyber assets, including but not limited to: cloud providers’ code repositories, vulnerability scanning technologies, training systems, identity providers, and much more,” the spokesperson said. “Many JupiterOne customers build their entire security program around JupiterOne as the underpinning continuous visibility data source.” JupiterOne, which has 50 employees and recently added strategic investors — including Atlassian CTO Sri Viswanath and Netflix VP Jason Chan — has clients in Reddit, Databricks, and Auth0. It claims to have tripled its annual revenue in 2020 and anticipates tripling revenue again this year. “JupiterOne grew substantially during 2020. While the pandemic is a negative for all businesses, including ours, JupiterOne was able to navigate the early downturn of business and grow at an exceptional rate in 2020,” the spokesperson said. “We feel very good as we progress into the second half of 2021 and plan to substantially grow JupiterOne as the world exits the pandemic later this year.” Previous investors Bain Capital Ventures also participated in the round announced today. It brings Morrisville, North Carolina-based JupiterOne’s total raised to more than $49 million. “We have grown significantly since our series A funding just eight months ago,” Zheng said. “This funding round allows us to expand faster, grow our already amazing employee base, accelerate our go-to-market plan, and bring new capabilities to our customers sooner. In the next 12 months, we are going to take our technology to the next level and really showcase the art of the possible for our customers and free community.”"
https://venturebeat.com/2021/05/04/aryaka-gains-sase-features-with-secucloud-acquisition/,Aryaka gains SASE features with Secucloud acquisition,"Aryaka, provider of a software-defined wide area network (SD-WAN) service, today revealed it has acquired Secucloud GmbH, a startup offering a secure access service edge (SASE) service. Terms of the deal, which closed in April, were not disclosed. Secucloud will operate as a wholly owned subsidiary of Aryaka to give customers the option to continue employing other security frameworks on top of the Aryaka SD-WAN service, Aryaka exec Shashi Kiran told VentureBeat. Secucloud’s firewall-as-a-service capabilities will be embedded in the Aryaka service starting later this year, Kiran added. Organizations will have the option of employing that embedded capability or continuing to rely on firewalls and gateways from the third-party partners — such as Check Point Software, Palo Alto Networks, and Zscaler — that Aryaka deploys in various points of presence on behalf of customers. In some cases, customers will opt to do both, he noted. Interest in SASE platforms and services has risen in the wake of the COVID-19 pandemic. Gartner predicts the SASE market will grow from $4.5 billion in 2021 to $10.9 billion in 2024, a 42% compound annual growth rate. The return of on-site employees following increased vaccination rates highlights SASE services’ ability to help people flexibly work from anywhere. “First we saw the great migration from the office to the home,” Kiran said, “Now we’re starting to see the reverse migration.” As a result, more IT organizations are debating the degree to which they can continue relying on legacy virtual private networking (VPN) platforms and services that secure connections by creating a tunnel through a firewall. A SASE service, by contrast, secures connections over an SD-WAN using firewalls. That approach provides the added benefit of routing network traffic more efficiently by enabling direct access to a cloud service versus backhauling network traffic through a datacenter. In effect, the SD-WAN replaces a traditional router with a platform that ensures a better application experience within the context of a larger zero trust IT architecture. In comparison, it’s relatively easy to compromise the credentials of a VPN user. Many organizations are already moving toward integrated SASE/SD-WAN platforms and services. Aryaka recently surveyed 1,300 global enterprises and found 29% are deploying networks based on what they describe as a SASE architecture. Another 56% are planning to deploy a SASE architecture in the next 12-24 months. Only 12% plan to rely on their security vendor to provide all the WAN components, which is one of the reasons the company is acquiring Secucloud. It’s not clear to what degree IT organizations are going to rely on a network service managed via the cloud versus deploying SD-WANs themselves. But as organizations devote more resources to building and deploying custom applications, many of them are opting to reallocate resources by, for example, relying more on networking services instead of hiring additional network engineers. The transition to the next generation of secure networking services will take time, given the current widespread reliance on VPN software and services. In the early days of the pandemic, many organizations simply increased the number of VPN licenses they could simultaneously employ to enable employees to work from home. The inertia within organizations that employ VPNs is fairly high. VPNs are relatively simple to employ when less than 10% of the workforce needs to work remotely, but when the bulk of employees are working remotely in ways that are hard to predict from one day to the next, VPNs don’t tend to scale well. An integrated SASE/SD-WAN approach will more flexibly scale up and down to address the needs of an unknown number of remote users requiring access to an undetermined amount of network bandwidth. It’s too early to say how integrated SD-WAN/SASE service providers will fair as legacy providers of telecommunications services and various providers of cloud services and content delivery networks (CDNs) continue to evolve. The probability of a wave of mergers and acquisitions is high. Regardless of how that network bandwidth is delivered, however, securing connections will always be a paramount concern."
https://venturebeat.com/2021/05/04/onetrust-acquires-shared-assessments-to-standardize-third-party-risk-management/,OneTrust acquires Shared Assessments to standardize third-party risk management,"OneTrust, the heavily VC-backed data privacy, governance, and compliance platform, has announced plans to acquire Shared Assessments, a membership-based organization that develops best practices for third-party risk management. Atlanta, Georgia-based OneTrust offers a range of tools designed to help major enterprise clients such as Oracle and Marketo manage their users’ privacy and determine how well the companies are adhering to legal frameworks such as GDPR in Europe and CCPA in California. Its products include privacy management software; data discovery and classification; ethics and compliance; risk management; and consent and preference management software. Founded in 2005, Shared Assessments has developed the standardized information gathering (SIG) questionnaire to help companies assess third-party risk in their supply chain, with notable members including Iron Mountain, Ellie Mae, SAP, and OneTrust itself. Under OneTrust’s auspices, Shared Assessments said that it intends to “remain a vendor-neutral industry organization,” but now with the backing of a billion-dollar company to help scale and become the “de facto international third-party risk standard,” according to a press release. This latest deal represents OneTrust’s third acquisition in the past two months, after it snapped up DocuVision, an AI-powered platform that companies use to find and redact sensitive data in large volumes of documents; and Convercent, an ethics and compliance software platform. The company also just last month raised $210 million at a $5.3 billion valuation. OneTrust’s acquisition spree also mirrors activity elsewhere in the data privacy space, with the likes of LiveRamp acquiring Datafleets and HelpSystems buying Vera. Ultimately, businesses are on the hunt for unified privacy solutions rather than having to mix and match different components from multiple providers, which is why OneTrust has been hell-bent on extending its product suite."
https://venturebeat.com/2021/05/04/aws-launches-finspace-a-data-analytics-service-for-finance-industry/,"AWS launches FinSpace, a data analytics service for financial industry","Amazon’s AWS cloud business has launched a new data management and analytics service for the financial sector. As is now the case in most industries, data serves as the driving force behind financial service businesses, spanning banks, insurance companies, hedge funds, and more. However, this data — structured or unstructured — often lives in a silo separate from a company’s other data. Being able to aggregate and catalog this data can help companies unlock insights. This may include identifying transaction patterns, profiling customers, and even predicting future buying behaviors using historical data. Dubbed FinSpace, Amazon’s new service essentially helps reduce the amount of time it takes financial firms to achieve all of this “from months to minutes,” replacing a traditionally manual process fraught with governance and compliance policies with a more automated approach to finding and preparing the disparate data for analysis. This includes an analytics engine built on Apache Spark (an open source analytics engine for big data processing) that supports many of the data transformations used in capital markets, and it also allows businesses to define data access controls that adhere to strict governance policies. As with most AWS services, FinSpace is priced on a per-usage basis, including the amount of data stored, the number of users, and the computing resources consumed to process the data. Amazon said Legal & General and Deloitte are among the first companies to use FinSpace, which is available to all companies from today. With cloud spending going through the roof over the past year — a trend that shows no sign of slowing — it’s clear the big public cloud companies have to specialize. A one-size-fits-all approach won’t work if they’re trying to lure billion-dollar businesses from on-premises infrastructure to the cloud. Indeed, FinSpace fits into a much broader trend that has seen the cloud giants target industries with differentiated toolsets tailored to a specific target market. Amazon, for example, already offers a range of sector-specific cloud services, including Smart Factory for manufacturing companies, and it recently introduced Amazon HealthLake to help health care and life sciences organizations aggregate and analyze their data. Microsoft also launched an industry cloud for health care last year, and it recently announced clouds for financial services, manufacturing, and nonprofits."
https://venturebeat.com/2021/05/03/businesses-to-support-remote-workforce-even-after-offices-reopen/,Businesses to support remote workforce even after offices reopen,"(Reuters) – U.S. businesses have been spending more on technology than on bricks and mortar for more than a decade now, but the trend has accelerated during the pandemic, one more sign that working from home is here to stay. As spending on home-building has risen, spending on nonresidential construction has dropped, with that on commercial, manufacturing and office space slumping to under 15% of total construction outlays in March, Commerce Department data showed Monday. Business spending on structures fell in the first quarter, data from the Bureau of Economic Analysis showed last week. It was the sixth straight quarterly decline, showcasing one of the few weak spots in the economy as it regains steam amid a receding pandemic. Meanwhile, spending on technology rose, with investments in software and information processing equipment contributing more than 1 percentage point to the economy’s overall 6.4% annualized rise in economic output in the quarter, the BEA data showed. Technology spending has added to growth in all but two of the past 32 quarters, back to 2013. Spending on structures has pulled GDP downward in 14 of those quarters. The implications of the shift are broad: the economy emerging from the depths of the pandemic will be more technology-driven and less reliant on in-person transactions, leaving jobs permanently changed and potentially fewer in number. Accelerated by the pandemic, the divergence between the two types of business spending is here to stay, says Stanford economics professor Nicholas Bloom. “This is the surge in (work-from-home) which is leading firms to spend heavily on connectivity,” Bloom said. He and colleagues have been surveying 5,000 U.S. residents monthly, and found that from May to December about half of paid work hours were done from home. Workers’ own spending to equip their home offices with computer connectivity, desks and other necessities comes to the equivalent of 0.7% of GDP, their surveys found, suggesting the business investment data likely underestimates what’s actually being spent on technology. Those sunk costs are one reason that on average Americans will work one day a week from home even after the pandemic, up from about one day a month before, Bloom says. American firms’ reliance on hybrid working should continue to lift business spending on technology for the forseeable future, said ING chief international economist James Knightley. Spending on office buildings particularly will likely remain weak at least until the end of the summer, he predicted, when the return of most kids to school should allow more parents to return to work. Even then, he said, businesses will need to continue to spend more than ever on connectivity and computers to support the remote, or partially remote, workforce. “I think there’s still a lot more to do there,” he said."
https://venturebeat.com/2021/05/03/zapier-automation-helped-small-businesses-survive-the-pandemic/,Zapier: Automation helped small businesses survive the pandemic,"The past year was marked with a lot of uncertainty for businesses. A new study from Zapier, which allows users to automate tasks for web applications, found that 63% of small businesses said automation helped them survive COVID-19. With a specific focus on the small and medium-sized business community — more than 3,000 partners apps on its platform for small businesses — key findings include:
Despite the common narrative of automation taking over jobs, it isn’t a competitor against humans. The future of automation is about how technology can support humans, especially in the small and medium-sized businesses. Software automation continues to grow and it enables workers to be more efficient, which gives humans time back to do things only people can do. Automation is essential software for small and medium-sized businesses. Sixty-three percent of SMBs say automation allowed their company to quickly pivot as a result of the pandemic — whether it was bringing their goods and services online or changing their business model completely. That benefit isn’t likely to shrink, either: even as things get back to “normal,” we’ll live much more of our lives online. Small businesses are already using technology to prepare for that reality: 66% say automation is now essential for running their business. Why is automation essential? SMBs rely on software to perform specific functions, like capturing leads from Facebook Lead Ads. Rather than spend time manually sending data from one place to another, SMBs use automation to connect different software and create scalable systems and processes. While automation helps SMBs eliminate repetitive, everyday tasks, there are bigger benefits as well: 88% of SMBs say automation allows them to compete with larger companies by allowing them to move faster, close leads quickly, spend less time on busywork, reduce errors, and offer better customer service. By identifying repetitive tasks that take the most time, SMBs can develop a strategy to automate manual and repetitive processes and free up time for more creative or strategic tasks. In fact, nearly 70% of SMB employees say using automation software has helped them be more productive at work. The future of automation is already here. Many SMBs are using automation to increase worker productivity and happiness, create efficient and scalable systems, and compete with larger businesses. Zapier surveyed 2,000 U.S. knowledge workers from small and medium businesses [fewer than 250 total employees] on whether or not workflow automation tools are being used at their company. This survey was completed online in March 2021 and responses were random, voluntary, and completely anonymous. Read more in Zapier’s full report 2021 Zapier State of Business Automation Report"
https://venturebeat.com/2021/05/03/apple-hires-ex-google-ai-scientist-who-resigned-after-researcher-firings/,Apple hires ex-Google AI scientist who resigned after researcher firings,"(Reuters) — Apple said on Monday it has hired former distinguished Google scientist Samy Bengio, who left the search giant amid turmoil in its artificial intelligence research department. Bengio is expected to lead a new AI research unit at Apple under John Giannandrea, senior vice president of machine learning and AI strategy, two people familiar with the matter said. Giannandrea joined Apple in 2018 after spending about eight years at Google. Apple declined to comment on Bengio’s role. Bengio who left Google last week after about 14 years said last month he was pursuing “other exciting opportunities”. His decision followed Google’s firings of fellow scientists Margaret Mitchell for taking company data and Timnit Gebru after an offer to resign. Mitchell and Gebru had co-led a team researching ethics issues in AI, and had voiced concern about Google’s workplace diversity and approach to reviewing research. Bengio had expressed support for the pair. As one of the early leaders of the Google Brain research team, Bengio advanced the “deep learning” algorithms that underpin today’s AI systems for analyzing images, speech and other data."
https://venturebeat.com/2021/05/03/immunomet-therapeutics-granted-fast-track-designation-by-u-s-fda-for-im156-in-idiopathic-pulmonary-fibrosis/,ImmunoMet Therapeutics Granted Fast Track Designation by U.S. FDA for IM156 in Idiopathic Pulmonary Fibrosis,"HOUSTON–(BUSINESS WIRE)–May 3, 2021– ImmunoMet Therapeutics, Inc., a clinical stage biotechnology company targeting metabolism to develop novel anti-fibrotic and anti-cancer therapies, today announced that the U.S. Food and Drug Administration (FDA) has granted Fast Track designation for its lead compound IM156, an investigational Protein Complex 1 (PC1) inhibitor, being evaluated for idiopathic pulmonary fibrosis (IPF). “Fast Track designation is another important milestone for ImmunoMet with the potential to speed our ability to advance IM156 to patients,” commented Benjamin Cowen, Chief Executive Officer of ImmunoMet Therapeutics. “Additionally, we are pleased with the progress we are making in the ongoing U.S. Phase 1 study in healthy volunteers.” About Fast Track DesignationFast Track designation is intended to facilitate the development and review of drugs used to treat serious conditions and to fill an unmet medical need. Fast Track designation enables the company to have more frequent interactions with the FDA throughout the drug development process, so that an approved product can reach the market expeditiously. About IM156IM156 is a Protein Complex 1 (PC1) inhibitor that targets the oxidative phosphorylation (OXPHOS) pathway, decreasing the supply of energy and anabolic precursors that are required to drive fibrotic disease and tumor growth. IM156, ImmunoMet’s lead drug candidate, is solely owned by ImmunoMet and is currently in development for the treatment of IPF and selected cancers. About ImmunoMet TherapeuticsImmunoMet is a clinical stage biotechnology company targeting metabolism for the treatment of fibrotic diseases and cancer. ImmunoMet’s lead molecule, IM156, is a PC1 inhibitor and is the first potent PC1 inhibitor to complete Phase 1 with good tolerability. In addition to IM156, ImmunoMet owns a large library of biguanides with the potential for development, internally or with partners, for multiple indications. The company was founded in 2015, is headquartered in JLABS @ TMC in Houston and has raised $31M to date. For more information about the company, please visit www.immunomet.com. Forward-Looking StatementsThis press release contains “forward-looking statements” concerning the development of ImmunoMet products, the potential benefits and attributes of those products, and the company’s expectations regarding its prospects. Forward-looking statements are subject to risks, assumptions and uncertainties that could cause actual future events or results to differ materially from such statements. These statements are made as of the date of this press release. Actual results may vary. ImmunoMet undertakes no obligation to update any forward-looking statements for any reason.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210503005641/en/ Benjamin Cowen, PhD, MBAPresident and CEOImmunoMet Therapeuticsbcowen@immunomet.com"
https://venturebeat.com/2021/05/03/calendlys-automated-meeting-scheduler-gets-new-enterprise-features/,Calendly’s automated meeting scheduler gets new enterprise features,"Calendly, an automated meeting scheduling tool for businesses, has launched a dedicated enterprise plan with added support for businesses that need advanced security and control of their Calendly deployment. The launch comes just a few months after the Atlanta, Georgia-based company raised $350 million, its first notable external funding in its eight-year history. Founded in 2013, Calendly is focused primarily on helping businesses schedule meetings with people outside their company, such as sales staff looking to set up a briefing with a potential client. The company claims a host of major customers including LinkedIn and Zendesk. So far, Calendly has offered three plans — basic (free), premium ($8 per user / month), and Pro ($12 per user / month). From today, Calendly has thrown an additional enterprise plan into the mix, which includes support for identity and access management systems and standards such as single sign-on (SSO) for Okta, Ping Identity, Azure, OneLogin, and Auth0, as well as System for Cross-domain Identity Management (SCIM) which is an open standard for automating user provisioning. Moreover, Calendly also now gives admins a centralized user management dashboard, allowing them to group users by team, location, or department and designate admins to manage each group without it impacting others. The crux of the problem that Calendly has been setting out to solve so far is that everyone works to different schedules and priorities, with location and time zone an added hurdle to setting up an appropriate time to meet. This will usually end up with multiple emails and / or phone calls as they try to agree the most suitable time, a matter that’s compounded as the number of people involved in the meeting increases. With Calendly, the meeting organizer simply shares a scheduling link with all parties, who are then asked to choose from a set of time slots based on rules stipulated by the organizer — the chosen slot is then added to everyone’s calendars. It also has a feature called round robin, which can automatically assign meetings (such as with a sales prospect) to any available team member — this increases the chances of accommodating the invitee’s schedule. Calendly supports integrations with myriad business apps including calendars from Google and Microsoft so that it always knows your availability, as well as CRM tools. Salesforce, for example, automatically creates a new lead whenever a meeting is scheduled with a fresh prospect — if the prospect already exists, the meeting will be added to their existing record. Separately, the Calendly API enables custom integrations. Now, with support for SSO and SCIM, and more granular user controls for admins, Calendly is better positioned to lure companies that have requirements that are more typical to larger enterprises. Moreover, in the future, the company is planning to introduce new reporting tools, such as automatically capturing scheduling data such as cancellation and rescheduling rates, and tie this to other enterprise tools such as CRMs, which can help measure company or team goals and help managers figure out whether any changes need to be made."
https://venturebeat.com/2021/05/03/google-cloud-and-vodafone-partner-on-data-analytics/,Google Cloud and Vodafone partner on data analytics,"(Reuters) — Mobile operator Vodafone Group and Alphabet’s Google Cloud entered a strategic partnership to jointly develop data services, Vodafone said on Sunday. About 1,000 workers in Britain, Spain and the United States will be to asked by both companies to create ‘Nucleus’, a new cloud-based storage and analytics portal which will host Vodafone’s data. Nucleus will be capable of processing around 50 terabytes of data a day within the cloud, Vodafone said in a statement. “Both companies will drive the use of reliable and secure data analytics, insights, and learnings to support the introduction of new digital products and services for Vodafone customers simultaneously worldwide,” the statement added. Google did not respond to Reuters’ request for a comment. The news was first reported by the Financial Times on Sunday. As part of the six-year agreement, both companies will also develop a system called ‘Dynamo’, which can extract and transport data across different countries where the British-based telecom company operates. According to the FT report, the two companies also want to sell consultancy services to other multinational businesses looking to move huge amounts of data to the cloud in the future."
https://venturebeat.com/2021/05/03/imperva-acquires-api-cybersecurity-company-cloudvector/,Imperva acquires API security company CloudVector,"Cybersecurity company Imperva today announced it plans to acquire application programming interface (API) security company CloudVector for an undisclosed sum. Imperva says the deal will help accelerate its roadmap and differentiate it in the web app and API protection market. According to Markets and Markets, the security orchestration, automation, and response (SOAR) segment is expected to reach $1.68 billion this year, driven by a rise in security breaches and incidents and the rapid deployment and development of cloud-based solutions. Data breaches exposed 4.1 billion records in the first half of 2019, Risk Based Security found. This may be why 68% of business leaders in a recent Accenture survey said they feel their cybersecurity risks are increasing. Los Altos, California-based CloudVector, which was founded in 2018, enables customers to discover, monitor, and protect API traffic in environments from exploits and breaches. The startup’s platform discovers APIs based on actual traffic, classifying data with machine learning, identifying data exposure, and detecting anomalous user and data activity. CloudVector’s API and endpoint catalog is dynamically computed and continuously monitored, helping organizations secure “shadow” APIs. All security events, including anomalous activities, are presented in dashboards and reports. Deployment of CloudVector’s microsensor architecture doesn’t require any API code change and can be incorporated into DevOps CI/CD pipelines, with value. CloudVector’s solution provides visibility and security for traditional public-facing APIs, as well as more modern microservices. APIs dictate the interactions between software programs. They define the kinds of calls or requests that can be made, how they’re made, the data formats that should be used, and the conventions to follow. As over 80% of web traffic becomes API traffic, they are coming under increasing threat. Gartner predicts that by 2021, 90% of web apps will have more surface area for attacks in the form of exposed APIs than frontends. That may be why Markets and Markets anticipates the global API management market will be worth $5.1 billion by 2023, growing at a 32.9% compound annual growth rate from $1.2 billion in 2018. “As a pioneer in modern API security, Imperva protects our customers from the risks associated with the misuse of APIs via exposures or attacks, and the exfiltration of sensitive data. Over the last year, we’ve seen significant acceleration in the number and volume of production APIs, such that API-related traffic now makes up more than 70% of our cloud web application firewall traffic,” Imperva CEO Pam Murphy said in a press release. “Combined with an expanding surface area and novel exploits, all organizations need stronger API visibility and advanced protection. The addition of CloudVector fits perfectly with our vision, advances our API security solution, and most importantly, broadens the security of our customers’ applications and data.” Imperva’s acquisition of CloudVector, which had raised $3.6 million, is subject to customary conditions and is expected to close in May 2021, the company says. The deal comes after the launch of Imperva Sonar, a feature that uses machine learning to surface key risks and offer single-action resolution capabilities to streamline enterprise IT team efforts. Seed rounds for cybersecurity startups have risen during the pandemic as enterprises search for new defenses. Cybersecurity pre-money valuations at the seed and series A stages have risen over the past 10 years — in line with overall venture funding — growing at an annual rate of 13.2% for an increase from $800,000 to $3 million, according to DataTribe."
https://venturebeat.com/2021/05/03/oracle-revamps-cloud-analytics-service-to-simplify-access/,Oracle revamps cloud analytics service to simplify access,"Oracle today unveiled a revamped cloud analytics service that aims to reach a wider range of users via a Redwood user interface (UI) the company is publicly showing for the first time within a live application environment. The UI will eventually be employed across the entire Oracle applications portfolio, Oracle Analytics VP Joey Fitts told VentureBeat. The Redwood UI is at the core of an Oracle Analytics Cloud strategy that surfaces a common pool of data to end users, business analysts, and data scientists, rather than requiring organizations to acquire, populate, and manage data across multiple platforms to address each use case, Fitts added. The goal is to make it easier for users with varying levels of analytics expertise to collaborate more effectively, he explained. Today Oracle is also launching a mobile application that makes Oracle Analytics Cloud more accessible to members of a geographically distributed team. That application includes a “podcast” capability that leverages a natural language processing (NLP) engine to identify and narrate the relationships between various sets of data surfaced through a dashboard via a speech interface. Oracle Analytics Cloud allows users to query data in natural language using either text or speech in 28 different languages. And Oracle is expanding its machine learning capabilities to offer users simple explanations of the factors that influenced a recommendation. Users can employ those explanations to adjust factors in a way that fine-tunes results. That capability makes artificial intelligence (AI) capabilities accessible to all types of users, Fitts said, adding “AI should be both applied and invisible.” At the same time, Oracle is adding support for built-in text analytics, affinity analytics to discover relationships between datasets more easily, graph analytics, and custom map analytics for embedding images using the Web Map Service (WMS) protocol and XYZ tile layers. Oracle is also adding a data profiling engine that samples and scans data to identify quality issues and proactively flag the misuse of sensitive data, along with recommendations to fix issues, such as zip codes and data in end user-defined product categories. Data preparation tools will also automatically associate geographic content to the right type of visualization. As a provider of relational database platforms that are widely employed in on-premises IT environments, Oracle is moving to ensure it remains relevant in the age of the cloud. In addition to Oracle Analytics Cloud, the company makes available a managed Autonomous Database service through which lower-level database administration tasks are automated. Regardless of use case, Oracle is encouraging customers to employ one of its cloud services rather than rival database and analytics services provided by Amazon Web Services (AWS), Microsoft, and Google. It’s too early to say how that titanic battle might play out, but Oracle — and to a lesser degree Microsoft — has a strategic advantage, in that the bulk of data still resides in an on-premises IT platform it provides. As hybrid cloud computing continues to evolve, it becomes easier for IT organizations to federate the management of data across multiple platforms using an incumbent vendor than it is to replace entire on-premises environments. AWS and Google are both making hybrid cloud computing cases that would require organizations to replace existing infrastructure or migrate all of their data into a cloud platform. There are plenty of examples of organizations deciding to abandon local datacenters entirely. But many companies continue to deploy applications in on-premises IT environments, citing compliance, security, and performance advantages. After more than 10 years of cloud computing, the bulk of enterprise data remains in an on-premises IT environment, which suggests most organizations will continue to selectively migrate applications to the cloud at a time and place of their choosing."
https://venturebeat.com/2021/05/03/okta-completes-6-5-billion-auth0-acquisition/,Okta completes $6.5B Auth0 acquisition,"Okta has announced that its $6.5 billion Auth0 acquisition is now complete. Companies are increasing their cloud IT spend as part of ongoing digital transformation efforts, which generate an even greater need to invest in security. For enterprises, managing the online identities of their workforce is a key part of this shift. The Okta and Auth0 deal, which was first announced back in March, brings together two heavyweights from the identity and access management (IAM) sphere. Okta is a $35 billion publicly traded company specializing in software and APIs that enable businesses to manage how their users access online systems. Auth0, a developer-focused identify platform for “application teams,” is a venture-backed company backed by Salesforce and other big-name investors. Collectively, the companies claim Siemens, T-Mobile, Pfizer, and AMD among their customers. As a combined entity, Okta and Auth0 are essentially leveraging their clout and experience in the enterprise and developer communities. The companies haven’t divulged specific integration plans, but they said both platforms will be “supported, invested in, and integrated over time.” Auth0 will continue to operate as an independent unit inside Okta, led by cofounder and current CEO Eugenio Pace, who will report directly to Okta CEO Todd McKinnon."
https://venturebeat.com/2021/05/03/cibc-innovation-banking-expands-north-american-coverage-with-new-boston-office/,CIBC Innovation Banking Expands North American Coverage With New Boston Office," Coverage footprint grows to 11 North American Offices  BOSTON & TORONTO–(BUSINESS WIRE)–May 3, 2021– CIBC Innovation Banking is pleased to announce that Joe Hammer is joining its growing North American team as Managing Director, Head of East Coast U.S. Life Sciences and Healthcare. He will be based in CIBC Innovation Banking’s new office in Boston, MA. Mr. Hammer has over two decades of experience in the financial services and healthcare industries. Most recently, he was a Managing Director of Silicon Valley Bank’s Life Science and Healthcare Corporate Banking practice, working with middle-market private and public companies across the U.S. In this role, he gained deep experience in originating credit facilities for venture-backed and growth-stage companies across the U.S. Previously, Mr. Hammer was the Chief Development Officer at a life sciences subsidiary specializing in business process outsourcing for orphan drug, biopharma, and medical device manufacturers. “We are thrilled to have Joe join our growing team and help us serve the large number of life sciences and healthcare companies on the East Coast,” said Jeff Chapman, Head of Life Sciences and Healthcare at CIBC Innovation Banking. “Joe has a long track record of success in this space and his experience will be extremely valuable to our team.” Mr. Hammer graduated from GE Healthcare’s Financial Management Program and GE Corporate’s Experienced Commercial Leadership Program with distinction. He recently served as Chair of BioForward, Inc. – Wisconsin’s Biotechnology Industry Organization (BIO) and AdvaMed Affiliate and was a speaker at the New York Stock Exchange’s Life Sciences IPO Summit. This announcement follows the Fall 2020 expansion of CIBC Innovation Banking’s coverage of venture-backed technology companies and venture capital sponsors in the U.S., with additional team members joining CIBC Innovation Banking offices in New York City and Menlo Park. Recent CIBC Innovation Banking financings within the life sciences, healthcare and HCIT sectors include: CipherHealth, Ginger.io, Health Fidelity, PointClickCare, Pomelo, PulmonX (LUNG:NASDAQ) and Vapotherm (VAPO:NYSE). Joe can be reached at joseph.hammer@cibc.com. About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Boston, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210503005051/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609 "
https://venturebeat.com/2021/05/03/safetyculture-boosts-its-workplace-safety-tools-hits-1-6b-valuation/,"SafetyCulture boosts its workplace safety tools, hits $1.6B valuation","SafetyCulture, a startup developing software for workplace safety and quality management, today announced that it closed a new $73 million round of funding, valuing the company at $1.6 billion. Led by Insight Partners and Tiger Global, SafetyCulture CEO Luke Anear says the funds will support growth as the company evolves from a checklist app into an operations platform for working teams. Failure to provide a safe atmosphere at work is a costly mistake for both businesses and their employers. The National Safety Council estimates a worker is injured on the job every seven seconds, which equates to 4.6 million injuries a year. And the Centers for Disease Control and Prevention’s National Institute for Occupational Safety and Health pegs the costs of work-related injuries and illnesses at $170 billion a year. Queensland, Australia-based SafetyCulture, which was founded in 2004, offers a mobile and web app called iAuditor for workplace safety and quality standards; it’s designed to collect data, standardize operations, and identify problems. The platform enables users to create customized forms from paper checklists, Word documents, and Excel spreadsheets, as well as take and annotate photos, add notes, and assign follow-up remedial actions. With SafetyCulture, managers can have teams share observations outside of regular inspections and generate reports for contracts, clients, and more, which can be exported to third-party platforms and tools. They can also connect to sensors — either SafetyCulture’s own or third-party sensors — to monitor work conditions in real time. “The SafetyCulture journey started with a simple question: How do we keep people in the workplace safe? After witnessing the tragedy of workplace incidents in my time as a private investigator, I realized something needed to be done when it came to safety in the workplace,” Anear told VentureBeat via email. “I went on to recruit a team to help develop a mobile solution and so, iAuditor was born. When we started out this journey in my garage in regional Australia, we had no idea where it would lead. We’d created the first iteration of a digital platform that went on to revolutionize safety inspections globally. It’s now the world’s largest checklist app — and that’s just the beginning.” SafetyCulture supports text and email notifications that trigger as soon as sensors detect changes that are out of the normal range. Customers can set alerts for things like local weather and have employees capture test results with barcode scanners, drawing tools, Bluetooth thermometers, and more. Last September, SafetyCulture acquired EdApp, a learning management system akin to LinkedIn Learning, for a reported $29 million. At the time, SafetyCulture said that EdApp, which was delivering around 50,000 lessons per day, would enable it to offer “micro-learning” resources to workers in a range of industries. SafetyCulture claims to host over 1.5 million users across 28,000 companies on its platform in more than 85 countries. It powers 600 million checks per year and millions of corrective actions per day, according to Anear. “Statistics-wise, we’ve seen inspections within iAuditor grow 108% year-over-year and actions grow 161%. This indicates our customers are using our software across a far broader range of use cases, from safety to quality and customer experience. It also suggests our customers are benefiting strongly from the workflows we have built into iAuditor over the last couple of years,” Anear said. “This additional data makes our analytics functionality all the more powerful. As working teams input more and more data, organizations benefit from increased visibility and a broader base of staff contributing to driving safety, quality, and efficiency improvements.” With 54% of U.S. employees worried about exposure to COVID-19 at their job, the pandemic has helped to drive SafetyCulture’s growth — even in the face of rivals like Aclaimant. According to a recent Pew Center survey, about 25% of employees say that they’re “not too” or “not at all” satisfied with the steps that have been taken in their workplace to keep them safe from the coronavirus. To address this, SafetyCulture created a number of checklists tailored for companies affected by the crises, including cleaning and disinfection logs as well as employee temperature log sheets. “We’ve built a reputation globally for helping some of the most dangerous industries digitize and adhere to safety and quality protocols. Unfortunately, everywhere is a high risk in a pandemic, so 2020 saw a major push to ensure retailers, schools, hotels, and many other industries had the right support and tech to manage COVID-19,” Anear said. SafetyCulture’s most recent funding round — which brings the company’s total raised to roughly $241 million — follows a $36 million tranche contributed by TDM Growth Partners, Blackbird Ventures, Index Ventures, former Australian prime minister Malcolm Turnbull and his wife Lucy Turnbull, and Atlassian cofounder Scott Farquhar. The company’s employee base grew 2.5 times in the past three years to over 500 people, and SafetyCulture says it plans to continue the same hiring trajectory “for the foreseeable future.”"
https://venturebeat.com/2021/05/02/intel-will-invest-3-5-billion-in-new-mexico-chip-factory/,Intel will invest $3.5B in New Mexico chip factory (updated),"Intel today announced it will spend $3.5 billion to upgrade its factory in New Mexico as part of a plan to expand domestic manufacturing investments. Intel CEO Pat Gelsinger, who took over as CEO on February 15, appeared in a 60 Minutes segment that included the announcement. More information will be released on Monday. Intel is also planning to spend $10 billion on a new factory in Israel and has already said it will spend $20 billion to build two new factories in Arizona. Gelsinger noted in the 60 Minutes interview that the U.S. had about 37% of the worldwide chip production about 25 years ago, but that percentage has sunk to 12% today. And while there were once 25 major companies making leading-edge chips based on the best manufacturing processes, today only three are doing so: Intel, Taiwan’s TSMC, and Samsung. 60 Minutes also interviewed TSMC CEO Mark Liu, who responded to Intel’s investments with a pledge to spend $100 billion on chip research and manufacturing. This includes a new U.S. factory in Phoenix, Arizona, where Intel’s efforts have been concentrated. Both companies are responding to a worldwide shortage of chips, which are used in everything electronic. The global shortage of semiconductors can be traced to heightened demand for electronics during the pandemic; the voracious appetite of countries such as China; and a lack of investment in chip factories, which can cost at least $10 billion. U.S. President Joseph Biden has pledged to provide $50 billion in support for the country’s chip industry as part of an effort to make the industry more competitive. This has raised some concerns, as much of that money will go toward helping companies like Intel, which had $78 billion in revenues last year and doesn’t necessarily need government help. But Gelsinger pointed to the semiconductor investments Asian countries have made and their overwhelming 74% share of global production. He said the chip industry is critical and that more of it should be on American soil. He also acknowledged that Intel had stumbled in both chip design and manufacturing and that it may take a couple of years for the company to catch up to rivals. And he noted that Intel’s board decided to spend less money on stock buybacks as part of its decision to bring back Gelsinger, who spent decades working at Intel, as the CEO. On Monday morning, Intel provided additional information. The company said the New Mexico factory in Rio Rancho will be upgraded to handle Foveros, Intel’s advanced 3D packaging technology. The multi-year investment is expected to generate 700 tech jobs, 1,000 construction jobs, and an additional 3,500 jobs in the state. Planning has begun and construction is expected to start in late 2021. Keyvan Esfarjani, senior vice president of Intel, said in a statement that other companies are interested in using the Foveros technology as part of the Intel Foundry Services business, which will provide chip production for other companies. The Foveros tech enables Intel to build processors with vertically stacked functions, rather than side-by-side, providing greater performance in a smaller space. Intel has invested $16.3 billion in the New Mexico factory since 1980. Its annual economic impact in the state is $1.2 billion. The company purchases renewable energy for the factory’s electrical system. [Updated 9 a.m. Pacific time on 5/3/21 with additional info on the New Mexico factory.]"
https://venturebeat.com/2021/05/02/in-a-year-of-major-shifts-the-self-driving-car-market-is-consolidating/,"In a year of major shifts, the self-driving car market is consolidating","News broke this week that Woven Planet, a Toyota subsidiary, will acquire Level 5, Lyft’s self-driving unit, for $550 million. The transaction, which is expected to close in Q3 2021, includes $200 million paid upfront and $350 million over a five-year period. Toyota will gain full control of Lyft’s technology and its team of 300. Lyft will remain in the game as a partner to Toyota’s self-driving efforts, providing its ride-hailing service as a platform to commercialize the technology when it comes to fruition. The Toyota-Lyft deal is significant because it comes on the back of a year of major shifts in the self-driving car industry. These changes suggest the autonomous vehicle market will be dominated by a few wealthy companies that can withstand huge costs and very late return on investment in a race that will last more than a few years. Costs remain a huge barrier for all self-driving car projects. The main type of software powering self-driving cars is deep reinforcement learning, which is currently the most challenging and expensive branch of artificial intelligence. Training deep reinforcement learning models requires expensive compute resources. This is the same technology used in AI systems that have mastered complicated games such as Go, StarCraft 2, and Dota 2. Each of those projects cost millions of dollars in hardware resources alone. However, in contrast to game-playing AI projects, which last between a few months to a few years, self-driving car projects take several years—and maybe above a decade—before they reach desirable results. Given the complexities and unpredictability of the real world, designing and testing the right deep learning architecture and reward, state, and action space for self-driving cars is very difficult and costly. And unlike games, the reinforcement learning models used in driverless cars need to gather their training experience and data from the real world, which is fraught with extra logistical, technical, and legal costs. Some companies develop virtual environments to complement the training of their reinforcement learning models. But those environments come with their own development and computing costs and aren’t a full replacement for driving in the real world. Equally costly is the talent needed to develop, test, and tune the reinforcement learning models used in driverless cars. All of these expenses put a huge strain on the budgets of companies running self-driving car projects. According to reports, the sale of Level 5 will cut Lyft’s net annual operating costs by $100 million. This will be enough to make the company profitable. Uber, Lyft’s rival, also sold its driverless car unit, Advanced Technologies Group (ATG), in December because it was losing money. So far, no company has been able to develop a profitable self-driving car program. Waymo, Alphabet’s self-driving subsidiary, has launched a fully driverless ride-hailing service in parts of Arizona. But it is still losing money on the project and is in the process of expanding the service to other cities in the U.S. Not long ago, it was generally believed that self-driving cars were a solved problem and it would only take a couple of years of development and training to get them ready for production. Several companies had hailed launching robo-taxi services by 2018, 2019, and 2020. A few carmakers promised to make full self-driving cars available to consumers. But we’re in 2021, and it’s clear that the technology is still not ready. Our deep learning algorithms are not on par with the human vision system. That’s why many companies need to use complementary technologies such as lidars, radars, and other sensors. Added to that is precision mapping data that provide the car with exact details of what it should expect to see in its surroundings. But even with all these props, we haven’t reached self-driving technology that can run on any road, weather, and traffic condition. The legal infrastructure for self-driving cars is also not ready. We still don’t know how to regulate roads shared by human- and AI-driven cars, how to determine culpability in accidents caused by self-driving cars, and many more legal and ethical challenges that arise from removing humans from behind steering wheels. In many ways, the self-driving car industry is reminiscent of the early decades of AI: The technology always seems to be right around the corner. But the end goal seems to be receding as we continue to approach it. What does this all mean for companies that are running self-driving car projects? Many more years and billions of dollars’ worth of investment in developing a technology that doesn’t seem to get off the ground. This will make it very difficult for companies that don’t have a highly profitable business model to engage in the market. And this includes ride-hailing services, which are under extra pressure due to the coronavirus pandemic. Startups that are living on VC money will also be hard-pressed to deliver on timelines that are shaky at best. Lyft’s sale to Toyota is part of a growing trend of self-driving car projects and startups gravitating toward deep-pocketed automotive or tech giants. Waymo will continue to operate and push forward for self-driving technology because its parent company has a long history of funding moonshot projects, most of which never reach profitability. Amazon acquired Zoox last year. Apple is considering creating its own electric self-driving car. And Microsoft is casting a wide net in the market, investing in several self-driving car projects at the same time. Traditional carmakers are also becoming big players in the market. Argo AI is backed by Ford and Volkswagen, both of whom have a major stake in the future of self-driving cars. General Motors owns Cruise. Hyundai has poured $2 billion into a joint self-driving car venture with green tech startup Aptiv. And Aurora, the company that acquired Uber’s ATG, is developing partnerships with several automakers. As the self-driving car industry shifts from hype to disillusionment, the market is slowly consolidating into a few very big players. Startups will be acquired, and we can probably expect one or more mergers between big tech and big automotive. This is going to be a race between those who can withstand the long haul. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/05/02/divided-we-fall-why-fragmented-global-privacy-regulation-wont-work/,Divided we fall: Why fragmented global privacy regulation won’t work,"The lack of a federal data privacy law is a gaping void at the heart of present-day American competitiveness, and it’s growing larger every day. As consumers prioritize trustworthiness more than ever, they find it harder and harder to trust businesses, with Pew finding nearly 80% concerned about companies’ data practices. Meanwhile, companies themselves are grappling with a surge in distinct state-level requirements. Over 20 new privacy bills have already been introduced this year, and one week in March alone saw the introduction of New York’s A6042, Colorado’s SB21-190, and West Virginia’s HB 3159. It’s a welcome sight to see state legislators addressing public concerns about misuses and exploitation of personal information. But a patchwork of state-by-state privacy regulations is not a viable framework for actually restoring user trust in the internet, or for the advancement of American business interests in the wake of a pandemic. A federal law is the only way out of this data morass. In many important ways, patchwork state laws only sink us in deeper. Here’s the current state privacy law landscape: California’s CCPA is the trailblazer and other states are quickly following the Golden State’s lead. Virginia’s Consumer Data Protection Act was signed into law with bipartisan support last month, and 18 states are actively considering their own bills. Each new bill comes with a descriptor like, “This bill resembles legislation in State X, but with the following key differences…” None of them are exactly the same in terms of either the rights they grant to citizens of the obligations they place on businesses. In other words, America’s current privacy path will not deliver what it should: harmonization for Americans and their personal data. This patchwork of state-level requirements is actively doing the opposite, in fact. And in three key respects, this approach has significant, tangible costs. First, state-level regulations cannot restore American businesses’ leadership in the international data economy. The US is playing catch-up while Europe sets global privacy standards, signified by its “A Europe for the digital age” initiative unveiled in late 2020. Because the EU found US data practices inadequate for handling EU citizens’ data, the US and the EU are now working to replace the invalidated EU-US Privacy Shield: an agreement relied upon by over 5,300 businesses for transatlantic data exchanges. EU leaders have specifically cited the implementation of a US federal privacy law as a stepping-stone to a new agreement. Without an agreement in place, SMEs are paying the price: in legal fees to complete data transfers and in local infrastructure to house data. Federal privacy legislation is a needed ingredient for SMEs to regain a competitive edge in data-driven business. Next, patchwork state laws mean companies must grapple with growing lists of requirements for technical infrastructure. On the front lines of privacy tech, we see the technical effort needed for businesses to get into compliance with just one state law, the CCPA. This year’s batch of state laws are causing Technical & Legal teams untold confusion about how to best plan for 50 slightly different sets of business requirements. Ultimately, this confusion drives companies to view privacy as avoiding fines rather than building trust. Digital customer interactions are increasing drastically because of the pandemic — by roughly 25%, according to McKinsey. This shift enables companies to expand user-bases in faraway states, but it also puts those companies within the scope of more state-level privacy requirements. Further complicating compliance with additional laws is the opposite of what organizations need today: 44% of organizations listed lack of privacy awareness as 2021’s key data privacy challenge, and 67% didn’t believe they could sustain privacy compliance. A confusing patchwork will worsen knowledge gaps precisely when Edelman’s Trust Barometer says we need to be more information-literate than ever. Of course, the ultimate utility of a legislative approach should be primarily assessed by its benefit to regular citizens. And here also, a patchwork approach to privacy law is a disservice to Americans. We know the public already has trust issues with the internet — 68% of consumers worldwide attest to not trusting companies to treat their data responsibly, and 52% of Americans decided against using a product/service because they thought it collected too much data, per Pew Research Center. And while one might say: “So? People still use Facebook,” recent trends show that Americans will increasingly “vote with their feet” when presented with viable privacy-conscious alternatives. A great example: the way users flooded to Telegram and Signal when Whatsapp unveiled sweeping, invasive updates to its data processing practices in January. Telegram signed up 25 million users in a mere 72 hours. Furthermore, market leaders like Apple are using privacy features as a point of product differentiation. It’s clear public appetite for privacy is impacting marketplace offerings. So what’s stopping the American public from attaining privacy literacy? The problem is education. Pew Research Center finds that 63% of Americans report little or no knowledge of privacy regulations, yet 75% express support for greater regulation. If privacy rights aren’t set to a common federal standard throughout the United States, that education mission becomes substantially more difficult. Consolidated resources, decision-making, and messaging can all go a long way in bridging the education gap for American citizens — witness the public penetration of GDPR awareness in Europe. A state-by-state approach to privacy law nullifies any opportunity for economies of educational scale in America. If anything, it’s likely to leave the average citizen more confused than before. As federal privacy bills emerge, lawmakers should seize the opportunity to elevate the US to be a global leader in privacy while harmonizing privacy for businesses. Last month, Representative DelBene introduced the Information Transparency and Personal Data Control Act, and Senator Schatz reintroduced the Data Care Act. Just last week, Representative Jerry McNerney named a bipartisan federal privacy law a priority by the end of 2021. We must promptly fill the federal privacy void; but not just any privacy bill will do. The long-term legislative solution needs to codify comprehensive privacy rights for all Americans, like GDPR does for EU residents. The legislation must set clear applicability and enforcement criteria for businesses nationwide, and it must be strong enough to restore the international trust in US data systems. In the meantime, teams should configure their privacy ops in compliance with GDPR. If you comply with GDPR, you probably meet the foreseeable privacy requirements. Cillian Kieran is CEO and founder of privacy company Ethyca. He has extensive technical experience working with legacy enterprise organizations such as Heineken, Sony, Dell, and Pepsi, building data platforms, visualization tools, and leading strategic advisory in change management and data governance policy definitions, liaising with CIO, CDO and legal counsel."
https://venturebeat.com/2021/05/02/what-is-robotic-process-automation/,A definitive primer on robotic process automation,"Robotic process automation (RPA) — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that RPA and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than four hours of savings per employee each week. Switching repetitive tasks to RPA functions not only eliminates errors, it also garners significant cost savings. That’s because RPA addresses bottlenecks with workflows, data, and documentation while providing audit trails and reducing compliance expenses and risks. RPA can also boost legacy integration and record digitization and enable data-driven decisions and “path-to-cognitive” technologies, according to Technologent’s Kevin Buckley. But as RPA expands to increasingly complex domains, the technology itself grows more complicated. This makes it harder for business decision-makers to understand where and when RPA might be appropriate, factoring in their industry and particular challenges. RPA is the category of software that automates tasks traditionally done by a human, using software robots that follow a set of rules and interact with enterprise systems via user interfaces. These robots can complete repeatable tasks, perform system integrations, and automate transactions from task-level to enterprise-level via scheduled orchestration. There’s nuance within this definition, however. RPA often begins with what’s called backend task discovery, or process mining. An RPA client pulls log data from existing systems — including desktop, IT, and email apps and workflows — to identify root cause issues through recommendations, KPIs, and more. Task capture is the next step in the onboarding chain. It comes as employees move through a work process they’d like to automate by taking screenshots, using drag-and-drop designers, and pulling data like window names and descriptions together into a process definition document. Most RPA platforms leverage AI to map tasks to automation opportunities and identify the most frequent patterns from the data, recording metrics from apps, including steps and execution time. Document understanding capabilities allow these platforms to ingest, analyze, and edit PDFs and images, even those with handwriting, checkboxes, signatures, rotated or skewed elements, and low resolutions. Computer vision algorithms enable RPA software to recognize and interact with on-screen fields and components like Flash and Silverlight. Drawing on AI, optical character recognition, and approximate string matching, RPA robots can “see” virtual desktop interfaces via clients like Citrix, VMWare, Microsoft RDP, and VNC. Not every RPA robot is created equal. Platforms such as UiPath offer three types: attended, unattended, and hybrid robots. Attended robots act like a personal assistant that resides on a user’s computer to take a series of user-triggered actions and complete simple, repetitive tasks. By contrast, unattended robots require very little intervention to perform intensive data processing and data management workloads. Hybrid robots, as their name implies, are a combination of attended and unattended robots and deliver user support and backend processing in a single solution. Choosing which type of RPA robot to deploy depends on the application. Because attended robots are tailored to the requirements of the user, they are a shoo-in for contact centers, field sales, retail, service engineers, and insurance agents. The scalable nature of unattended robots makes them a fit for application, claims, and invoice processing, as well as data and documentation search and retrieval. As for hybrid robots, they tend to work best in end-to-end scenarios like HR management, application processing, service delivery, and customer support and engagement. Regardless of the bot type, RPA platforms typically leverage scalability to their technological advantage. For instance, startup WorkFusion claims its bots aggregate and share learnings across the bot ecosystem to create network effects from which all of its customers benefit. RPA software lets customers manage up to thousands — or tens of thousands — of robots from a single dashboard. Customers can view the robots’ tasks and supporting documents, take remedial actions in the event of a bottleneck, and visualize automation complexity and payback costs. Some software offers toolsets developers can use to borrow prebuilt automation activities, integrate third-party components, and share and reuse components. RPA software also typically lets customers import their own machine learning models or choose from a marketplace of prebuilt options and keep tabs on versioning. In the areas of AI and machine learning, Indico and other RPA providers apply techniques like transfer learning — where a model tailored to one task is used for another, related task — to deploy to unstructured content more effectively. The company’s out-of-the-box models, which were trained on large datasets of documents, ostensibly learn to analyze industry-specific data from just a few hundred training examples. Connectors also add enormous value in the world of RPA. For example, RPA startup Bizagi integrates with Azure Cognitive Services to automatically recognize new kinds of paper forms and extract data from them. Sources include contracts, claims forms, emails, spreadsheets, purchase orders, and field reports. And Blue Prism offers a library that gives partners and customers the ability to create, share, and deploy plugins for the company’s RPA solutions. RPA can handle a vast number of different tasks, from contract audits and customer onboarding to commercial underwriting, financial document analysis, mortgage processing, billing form reviews, and insurance claims analysis. That is one reason the overall RPA market is expected to grow by more than 7% annually over the next few years to reach $379.87 million by 2027, up from $182.8 million in 2019. Early in the pandemic, RPA companies like Automation Anywhere worked with health care centers to implement bots and automate laborious processes. For example, Olive, a Columbus-based health care automation startup, used a combination of computer vision and RPA to support COVID-19 testing operations by simplifying manual data entry. UiPath partnered with a Dublin-based hospital to process COVID-19 testing kits, enabling the hospital’s on-site lab to receive results in minutes and saving the nursing department three hours per day, on average. Beyond health care, Gryps, an RPA startup focused on the construction industry, is applying machine learning to organize construction project files and documents. For the Javits Convention Center in New York, Gryps’ software automatically ingested over 20,000 documents and 100,000 data points, collated them, and handed them over to the Javits team, with estimates putting the savings at hundreds of hours of staff time. The number of industries RPA touches continues to grow, with a Deloitte report predicting the technology will achieve “near universal adoption” within the next five years. According to the same report, 78% of organizations that have already implemented RPA — which see an average payback period of around 9 to 12 months — expect to “significantly” increase their investment in the technology over the next three years. This isn’t to suggest that RPA is without challenges. The credentials enterprises grant to RPA technology are a potential access point for hackers. When dealing with hundreds to thousands of RPA robots with IDs connected to a network, each could become an attack vessel if companies fail to apply identity-centric security practices. Part of the problem is that many RPA platforms don’t focus on solving security flaws. That’s because they’re optimized to increase productivity and because some security solutions are too costly to deploy and integrate with RPA. Of course, the first step to solving the RPA security dilemma is recognizing that there is one. Realizing RPA workers have identities gives IT and security teams a head start when it comes to securing RPA technology prior to its implementation. Organizations can extend their identity and governance administration (IGA) to focus on the “why” behind a task, rather than the “how.” Through a strong IGA process, companies adopting RPA can implement a zero trust model to manage all identities — from human to machine and application. A privileged access management (PAM) setup that can secure and govern RPA systems can also help. PAM systems allow enterprises to secure, control, and audit the credentials and privileges RPA technology uses without compromising the return on investment (ROI). RPA challenges don’t stop at security. Deloitte reports that 17% of organizations face employee resistance when piloting RPA and that 63% of those organizations struggle to meet time-to-implement expectations. But the RPA’s return on investment often outweighs difficulties in deployment. According to the Everest Group, top performers earn nearly 4 times on their RPA investments, while other enterprises earn nearly double. And Gartner estimates that by 2024, organizations can lower operational costs 30% by combining automation technologies like RPA with redesigned operational processes. “The first wave of robotic process automation brought the power of technology to users’ desktops in all industries and companies of all sizes. Today, we see a second wave emerging,” WorkFusion CEO Alex Lyashok recently told VentureBeat via email. “Cloud-based, AI-enabled robots [are] bringing intelligent automation to all enterprises.”"
https://venturebeat.com/2021/05/01/reinforcement-learning-competition-pushes-the-boundaries-of-embodied-ai/,Reinforcement learning competition pushes the boundaries of embodied AI,"Since the early decades of artificial intelligence, humanoid robots have been a staple of sci-fi books, movies, and cartoons. Yet after decades of research and development in AI, we still have nothing that comes close to The Jetsons’ Rosey the Robot. This is because many of our intuitive planning and motor skills — things we take for granted — are a lot more complicated than we think. Navigating unknown areas, finding and picking up objects, choosing routes, and planning tasks are complicated feats we only appreciate when we try to turn them into computer programs. Developing robots that can physically sense the world and interact with their environment falls into the realm of embodied artificial intelligence, one of AI scientists’ long-sought goals. And even though progress in the field is still a far shot from the capabilities of humans and animals, the achievements are remarkable. In a recent development in embodied AI, scientists at IBM, the Massachusetts Institute of Technology, and Stanford University developed a new challenge that will help assess AI agents’ ability to find paths, interact with objects, and plan tasks efficiently. Titled ThreeDWorld Transport Challenge, the test is a virtual environment that will be presented at the Embodied AI Workshop during the Conference on Computer Vision and Pattern Recognition, held online in June. No current AI techniques come close to solving the TDW Transport Challenge. But the results of the competition can help uncover new directions for the future of embodied AI and robotics research. At the heart of most robotics applications is reinforcement learning, a branch of machine learning based on actions, states, and rewards. A reinforcement learning agent is given a set of actions it can apply to its environment to obtain rewards or reach a certain goal. These actions create changes to the state of the agent and the environment. The RL agent receives rewards based on how its actions bring it closer to its goal. RL agents usually start by knowing nothing about their environment and selecting random actions. As they gradually receive feedback from their environment, they learn sequences of actions that can maximize their rewards. This scheme is used not only in robotics, but in many other applications, such as self-driving cars and content recommendations. Reinforcement learning has also helped researchers master complicated games such as Go, StarCraft 2, and DOTA. Creating reinforcement learning models presents several challenges. One of them is designing the right set of states, rewards, and actions, which can be very difficult in applications like robotics, where agents face a continuous environment that is affected by complicated factors such as gravity, wind, and physical interactions with other objects. This is in contrast to environments like chess and Go that have very discrete states and actions. Another challenge is gathering training data. Reinforcement learning agents need to train using data from millions of episodes of interactions with their environments. This constraint can slow robotics applications because they must gather their data from the physical world, as opposed to video and board games, which can be played in rapid succession on several computers. To overcome this barrier, AI researchers have tried to create simulated environments for reinforcement learning applications. Today, self-driving cars and robotics often use simulated environments as a major part of their training regime. “Training models using real robots can be expensive and sometimes involve safety considerations,” Chuang Gan, principal research staff member at the MIT-IBM Watson AI Lab, told TechTalks. “As a result, there has been a trend toward incorporating simulators, like what the TDW-Transport Challenge provides, to train and evaluate AI algorithms.” But replicating the exact dynamics of the physical world is extremely difficult, and most simulated environments are a rough approximation of what a reinforcement learning agent would face in the real world. To address this limitation, the TDW Transport Challenge team has gone to great lengths to make the test environment as realistic as possible. The environment is built on top of the ThreeDWorld platform, which the authors describe as “a general-purpose virtual world simulation platform supporting both near-photo realistic image rendering, physically based sound rendering, and realistic physical interactions between objects and agents.” “We aimed to use a more advanced physical virtual environment simulator to define a new embodied AI task requiring an agent to change the states of multiple objects under realistic physical constraints,” the researchers write in an accompanying paper.  Reinforcement learning tests have different degrees of difficulty. Most current tests involve navigation tasks, where an RL agent must find its way through a virtual environment based on visual and audio input. The TDW Transport Challenge, on the other hand, pits the reinforcement learning agents against “task and motion planning” (TAMP) problems. TAMP requires the agent to not only find optimal movement paths but to also change the state of objects to achieve its goal. The challenge takes place in a multi-roomed house adorned with furniture, objects, and containers. The reinforcement learning agent views the environment from a first-person perspective and must find one or several objects from the rooms and gather them at a specified destination. The agent is a two-armed robot, so it can only carry two objects at a time. Alternatively, it can use a container to carry several objects and reduce the number of trips it has to make. At every step, the RL agent can choose one of several actions, such as turning, moving forward, or picking up an object. The agent receives a reward if it accomplishes the transfer task within a limited number of steps. While this seems like the kind of problem any child could solve without much training, it is indeed a complicated task for current AI systems. The reinforcement learning program must find the right balance between exploring the rooms, finding optimal paths to the destination, choosing between carrying objects alone or in containers, and doing all this within the designated step budget. “Through the TDW-Transport Challenge, we’re proposing a new embodied AI challenge,” Gan said. “Specifically, a robotic agent must take actions to move and change the state of a large number of objects in a photo- and physically realistic virtual environment, which remains a complex goal in robotics.” While TDW is a very complex simulated environment, the designers have still abstracted some of the challenges robots would face in the real world. The virtual robot agent, dubbed Magnebot, has two arms with nine degrees of freedom and joints at the shoulder, elbow, and wrist. However, the robot’s hands are magnets and can pick up any object without needing to handle it with fingers, which itself is a very challenging task. The agent also perceives the environment in three different ways: as an RGB-colored frame, a depth map, and a segmentation map that shows each object separately in hard colors. The depth and segmentation maps make it easier for the AI agent to read the dimensions of the scene and tell the objects apart when viewing them from awkward angles. To avoid confusion, the problems are posed in a simple structure (e.g., “vase:2, bowl:2, jug:1; bed”) rather than as loose language commands (e.g., “Grab two bowls, a couple of vases, and the jug in the bedroom, and put them all on the bed”). And to simplify the state and action space, the researchers have limited the Magnebot’s navigation to 25-centimeter movements and 15-degree rotations. These simplifications enable developers to focus on the navigation and task-planning problems AI agents must overcome in the TDW environment. Gan told TechTalks that despite the levels of abstraction introduced in TDW, the robot still needs to address the following challenges: This highlights the complexity of human vision and agency. The next time you go to a supermarket, consider how easily you can find your way through aisles, tell the difference between different products, reach for and pick up different items, place them in your basket or cart, and choose your path in an efficient way. And you’re doing all this without access to segmentation and depth maps and by reading items from a crumpled handwritten note in your pocket. The TDW-Transport Challenge is in the process of accepting submissions. In the meantime, the authors of the paper have already tested the environment with several known reinforcement learning techniques. Their findings show that pure reinforcement learning is very poor at solving task and motion planning challenges. A pure reinforcement learning approach requires the AI agent to develop its behavior from scratch, starting with random actions and gradually refining its policy to meet the goals in the specified number of steps. According to the researchers’ experiments, pure reinforcement learning approaches barely managed to surpass 10% success in the TDW tests. “We believe this reflects the complexity of physical interaction and the large exploration search space of our benchmark,” the researchers wrote. “Compared to the previous point-goal navigation and semantic navigation tasks, where the agent only needs to navigate to specific coordinates or objects in the scene, the ThreeDWorld Transport challenge requires agents to move and change the objects’ physical state in the environment (i.e., task-and-motion planning), which the end-to-end models might fall short on.” When the researchers tried hybrid AI models, where a reinforcement learning agent was combined with a rule-based high-level planner, they saw a considerable boost in the system’s performance. “This environment can be used to train RL models, which fall short on these types of tasks and require explicit reasoning and planning abilities,” Gan said. “Through the TDW-Transport Challenge, we hope to demonstrate that a neuro-symbolic, hybrid model can improve this issue and demonstrate a stronger performance.” The problem, however, remains largely unsolved, and even the best-performing hybrid systems had around 50% success rates. “Our proposed task is very challenging and could be used as a benchmark to track the progress of embodied AI in physically realistic scenes,” the researchers wrote. Mobile robots are becoming a hot area of research and applications. According to Gan, several manufacturing and smart factories have already expressed interest in using the TDW environment for their real-world applications. It will be interesting to see whether the TDW Transport Challenge will help usher new innovations into the field. “We’re hopeful the TDW-Transport Challenge can help advance research around assistive robotic agents in warehouses and home settings,” Gan said. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/05/01/data-science-in-a-post-covid-world/,Data science in a post-COVID world,"I am often asked about the state of data science and where we sit now from a maturity perspective. The answer is pretty interesting, especially now that it’s been more than a year since COVID-19 rendered most data science models useless — at least for a time. COVID forced companies to make a full model jump to match the dramatic shift in daily life. Models had to be rapidly retrained and redeployed to try to make sense of a world that changed overnight. Many organizations ran into a wall, but others were able to create new data science processes that could be put into production much faster and easier than what they had before. From this perspective, data science processes have become more flexible. Now there is a new challenge: post-pandemic life. People all over the world believe an end to the pandemic is in sight. But it is highly unlikely we will all just magically snap back to our pre-pandemic behaviors and routines. Instead, we’ll have a transition period that will require a long, slow shift to establish a baseline or new set of norms. During this transition, our data models will require near-constant monitoring as opposed to the wholescale jump COVID prompted. Data scientists have never encountered anything like what we should expect in the coming months. If asked what we most miss about life before the pandemic, many of us will say things like traveling, going out to dinner, maybe going shopping. There is tremendous pent-up demand for all that was lost. There’s a large group of people who have not been adversely affected financially by the pandemic. Because they haven’t been able to pursue their usual interests, they probably have quite a bit of cash at their disposal. Yet the current data science models that track spending of disposable income are probably not ready for a surge that will likely surpass pre-pandemic spending levels. Pricing models are designed to optimize how much people are willing to pay for certain types of trips, hotel nights, meals, goods, etc. Airlines provide a great example. Prior to COVID-19, airline price prediction engines assumed all sorts of optimizations. They had seasonality built in as well as specific periods like holiday travel or spring break that drove prices even higher. They built various fare classes and more. They implemented very sophisticated, often manually crafted optimization schemes that were quite accurate until the pandemic blew them up. But for life after COVID, airlines have to look beyond the usual categories to accommodate the intense consumer demand to get out and about. Instead of going back to their old models, they should be asking questions like “Can I get more money for certain types of trips and still sell out the airplane?” If airlines consistently run models to answer these and other questions, we’ll see an increase in prices for certain itineraries. This will go on for a period of time before we see consumers gradually begin to self regulate their spending again. At a certain point, people won’t have any piled up money left over anymore. What we really need are models that identify when such shifts happen and that adapt continuously. On the flip side, there is another segment of the population that experienced (and continues to experience) economic difficulties as a result of the pandemic. They can’t go wild with their spending because they have nothing or little left to spend. Maybe they still need to find jobs. This also skews economics, as millions of people are attempting to climb back up to the standard of where they were pre-COVID. People who previously would have played a sizable role in economic models are effectively removed from the equation for the time being. COVID was one big bang where things changed. That was easy to detect, but this strange period we will now be navigating — toward some kind of new normal — will be much harder to interpret. It’s a case of model drift, where reality shifts slowly. If organizations simply start deploying their pre-COVID models again, or if they stick with what they developed during the pandemic, their models will fail to give them proper answers. For example, many employees are ready to return to the office, but they may still opt to work from home a few days a week. This seemingly small decision affects everything from traffic patterns (fewer cars on the road at peak periods) to water and electric usage (people take showers at different times and use more electricity to power their home offices). Then there are restaurant and grocery sales — with fewer employees in the office, catered lunches and meals out with colleagues drop from pre-pandemic levels, while grocery sales must account for lunch at home. And here we’re only looking at the effects of a single behavior (transitioning to partial work-from-home). Think about the ripple effects of changes to all the other behaviors that emerged during the pandemic. In establishing an environment to contend with this unprecedented challenge, organizations need to unite entire data science teams, not just the machine learning engineers. Data science is not just about training a new AI or machine learning model; it’s also about looking at different types of data as well as new data sources. And it means inviting business leaders and other collaborators into the process. Each participant plays a role because of all of the mechanics involved. These teams should look at patterns that are emerging in geographies that have opened up again post-COVID. Is everything running at full capacity? How are things going? There is quite a bit of data that can be leveraged, but it comes in pieces. If we combine these learnings with what we saw prior to and during COVID to retrain our models, as well as ask new questions, then we’re looking at highly valuable data science with mixed models that accounts for swings in practices and activities. It is imperative that teams persistently monitor models — what thesey do, how they perform — to identify when they become out of whack with reality. This goes way beyond classic A/B testing and also involves challenger models and mixing models from pre-COVID with newer ones. Try out other hypotheses and add new assumptions. Organizations might be surprised to see what suddenly works much better than before — and then to see those model assumptions eventually fail again. Organizations should prepare themselves by putting in place a flexible data science function that can continuously build, update, and deploy models to represent an ever-evolving reality. Michael Berthold is CEO and co-founder at KNIME, an open source data analytics company. He has more than 25 years of experience in data science, working in academia, most recently as a full professor at Konstanz University (Germany) and previously at University of California, Berkeley and Carnegie Mellon, and in industry at Intel’s Neural Network Group, Utopy, and Tripos. Michael has published extensively on data analytics, machine learning, and artificial intelligence. Follow Michael on Twitter, LinkedIn and the KNIME blog."
https://venturebeat.com/2021/05/01/how-intel-is-leveraging-ai-to-drive-sales/,How Intel is leveraging AI to drive sales,"The pandemic greatly accelerated businesses’ digital transformation efforts. This is particularly true in the marketing domain, where enterprises began to embrace automation and enablement technologies. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Forrester predicts that 57% of business-to-business sales leaders will invest more heavily in tools with automation. While Intel might be best known for its chip business, it’s among the companies embracing this automation and digitization. The company expedited plans to apply AI throughout the customer lifecycle over the past 12 months, particularly on the marketing side of the house, where the goal was to tap AI to help identify and solve selling pain points. Intel also sought to adopt predictive tech to give its sellers a competitive edge, ideally months ahead of a potential close with buyers. As Jake Tatel, global director of sales enablement and productivity at Intel, told VentureBeat via email, Intel started its AI marketing tech journey about five years ago. The company’s analytics teams started collecting a wide range of data from prospect websites and social media accounts. Then they combined it with Intel’s own website activity and overlaid it with customer buying patterns to drive actionable insights. “Our internal team has really challenged itself to look at use cases where we could utilize data that we’re able to amass from customer buying patterns, in addition to how potential buyers are engaging across all our properties — whether that be the website, our training properties, or other Intel-owned channels,” Tatel said. “While tapping the data we have internally is a key part of the equation, it was also important for us to weave it together with publicly available information, like prospect websites and social media data. We have a complex ecosystem, so it was critical for us to take a wide view of it all to figure out the best way to stitch it together for continuous, real-time scanning.” Two applications sprung out of the analytics team’s early work: Sales Assist and Autonomous Sales. Sales Assist provides insights — referred to as assists — to account managers. These assists broaden managers’ opportunities to interact with customers so that they can cover more accounts. Meanwhile, Autonomous Sales creates automatic sales motions, which are actions that Intel offers customers and partners through emails, website ads, and newsletters. Autonomous Sales operates daily and automatically, without human intervention, and applies to all partners’ accounts — even if they aren’t covered by an Intel sales team. Sales Assist now has over 1,500 users at Intel and has provided more than 17,000 assists relating to nearly 5,000 accounts, 80% of which account managers have taken action on. For its part, Autonomous Sales delivers a yearly volume of about 30,000 emails to more than 10,000 contacts within Intel’s customers, with an open rate averaging around 36% and a first-time purchase conversion rate of 16%. Together, Tatel says that the applications are generating “significant” business value for Intel, with the incremental contribution of Sales Assist estimated to be greater than $100 million per year. Autonomous Sales is helping to create $30 million in new sales, he says. And in 2020, Sales Assist and Autonomous Sales together delivered more than $168 million. “Salespeople won’t adopt any technology if it doesn’t actually serve their needs or help them do their job more efficiently. So, we made it a top priority to ensure that the ‘assists’ being delivered to the sales organization were actionable,” Tatel said. “We did this by building in a feedback loop that informed the development team on whether the recommendations and insights being delivered were helpful. This fostered collaboration between the sales and development team, and made it so we could actually increase pipeline via the assists.” Intel created Sales Assist in 2017, built on the company’s broader Sales AI platform. Sales AI — which is made up of the modules Sense, Reason, Interact, and Learn — is designed to collect and interpret customer and ecosystem data and translate it into useful recommendations. The Sense module continuously scans, mines, and collects data about Intel’s customers from a variety of sources. The data reflects interactions and engagements between customers including billing information, past opportunities, and first-party data engagements on Intel.com in addition to responses to communications and any affiliations with partnership programs. Sales AI also incorporates external data like a customer’s or partner’s website, news mention, social media information, and so on, regardless of their specific connection with Intel. Tatel says that Sense runs on millions of webpages, tweets, sales transactions, and customer and partner engagements, transforming them into thousands of data points on over 750,000 companies. In 2020, Sense scraped 15 million webpages and monitored over 347,000 million tweets. Sales AI employs a number of web-mining techniques to collect data. Leveraging natural language processing (NLP), the platform identifies cross-reference information about products, brands, advertisements, verticals, and other key variables. It then extracts data about the industries in which the customer is operating, such as automotive, communications, or health care. Thanks to pretrained language models including BERT and GloVe, Sales AI can understand the customer’s role (e.g., manufacturer, integrator, or reseller) and the technologies it’s using, according to Tatel. “The Sales Assists AI engine automatically tags content and web pages using NLP and then notifies the seller whenever there is product intent of interest which is significantly different from a usual behavior pattern,” Tatel explained. “The approach imitates the way a seller thinks and acts, identifying potential opportunities and helps them to proactively engage customers at the right time.” Sales AI also analyzes the text, website structure, links, and images in a customer’s website to learn more about them. When the platform spots mentions of a competitor’s product, it generates an assist, notifying the account manager than there might be an opportunity for conversion. Once the Sense module gathers a customer’s digital representation, the Reason module uses this data to begin mining, correlating, and generating business insights. An insight — a prediction of a likely sales motion — might be a suggestion that a customer needs an existing Intel product, for example, based on a new direction published on their website. Or a customer might announce an acquisition of a smaller company in a new business line, and an Intel salesperson could recommend what they need to grow this business line. According to Tatel, Reason can deduce insights from one or multiple data sources by attempting to identify a customer’s goals, needs, organizational changes, and shifts in focus areas. This enables Sales AI to identify trends, analyzing customers’ purchasing history and product-related activities on Intel’s platform and monitoring information regarding product lifecycles, examining past purchase patterns to identify the customers that might be impacted. “We’re combining the ‘human intelligence; of our sales team with the AI in a unique way,” Tatel said. “For example, Sales Assist can now spot when a customer has a product that’s end-of-life and can recommend an alternative product for a salesperson to suggest to the customer to aid in their transition or make sure they have the latest and greatest solution.” For sales teams at Intel, Sales AI predicts topics of interest to a company by connecting between different webpages using links and users’ journeys to build a topic network. The platform can identify changes in this network over time, alerting salespeople with an insight when a customer shows a shift in interest toward a new industry or technology. And it looks for “unusual behavior,” such as a sudden increase in download activity from Intel’s Research and Development Center. To assist with product recommendation, Sales AI offers a recommender system that considers multiple customer objectives. Combining three components — features creation, a recommender model, and an optimization model — the recommender system provides updated recommendations while focusing on revenue opportunities, according to Tatel. The recommender model considers products that a customers did or didn’t purchase in the past, plus the customer’s expected volume of purchases and their internal priorities. This nets a ranked list of products, which is updated weekly based on the customer’s activities and characteristics. The optimization model refines the list by taking into account sales strategies and feedback, so that it doesn’t always recommend the same products. The Sales AI Interact module takes over at this stage. Working with results from Reason, it aims to push recommendations to customers at the right time, way, and format, from web to email. The Learn module feeds information from customer and partner interactions into the algorithms powering Reason and the rest of the Sales AI modules throughout, allowing them to self-improve over time. In the future, Intel plans to broaden the applicability of Sales Assist by expanding the number of account types that receive assists. The company also plans to add the ability for Sales Assist to recommend specific actions to sales representatives, like sending a customer a link to products they recently viewed on Intel.com. “Now that Sales Assist is rolled out more broadly, we’re continuously looking for new ways to add intelligence into our sales processes, and have started to look at off-the-shelf solutions … to help with content enablement,” Tatel said. “We’re going to be taking all the learnings from building our own internal AI-powered sales tool. These tools only work — and will only be adopted — if the insights they deliver are insightful and actionable. So, we’ll be building in similar feedback loops and beta programs to ensure they’re fine-tuned for success.”"
https://venturebeat.com/2021/04/30/intel-ceo-seeks-10b-subsidy-to-build-a-chip-factory-in-europe/,Intel CEO seeks $10B subsidy to build a chip factory in Europe,"(Reuters) — Intel wants 8 billion euros ($9.7 billion) in public subsidies towards building a semiconductor factory in Europe, its CEO was cited as saying on Friday, as the region seeks to reduce its reliance on imports amid a shortage of supplies. The pitch is the first time Pat Gelsinger has publicly put a figure on how much state aid he would want, as Intel pursues a multibillion-dollar drive to take on Asian rivals in contract manufacturing. “What we’re asking from both the U.S. and the European governments is to make it competitive for us to do it here compared to in Asia,” Gelsinger told Politico Europe in an interview. Politico cited Gelsinger saying that he was seeking roughly 8 billion euros in subsidies. The company later distanced itself from the report, saying he had not given a specific figure, although he had made it clear that EU leaders needed to invest to ensure a vibrant semiconductor industry. Gelsinger, on his first European tour since taking charge, met European Commissioner Thierry Breton in Brussels on Friday. The visit followed the launch of a plan for Intel to invest $20 billion in chip production in the United States. On top of that, Gelsinger is prospecting for a location for a plant in Europe that he says would back Breton’s goal of doubling the region’s share of global chip output to 20% over the next decade. Breton held talks earlier on Friday with Taiwan Semiconductor Manufacturing Co (TSMC), the world’s No.1 chip maker ahead of Korea’s Samsung and Intel. In separate tweets, Breton described his meeting with Intel’s Gelsinger as an “in-depth discussion”, while a video call with Maria Marced, president of TSMC Europe, had been a “good exchange”. “To meet current & future semiconductor industry demand, Europe will drastically increase production capacity – both on its own and through selected partnerships to ensure security of supply,” said Breton. TSMC said the talks with Breton demonstrated its commitment to the region. “Our desire to support our customers as fully as possible means that we’re always willing to establish open communications with governments and regulators wherever they, and we, are based,” the company said. The Commission said Breton would hold further talks on May 4 with the CEOs of two Dutch semiconductor players: ASML, the leading maker of semiconductor lithography tools, and with chipmaker NXP. Recent disruption to semiconductor supply chains has added urgency to efforts to reduce import dependency, yet analysts caution Europe’s shrunken technology base means it doesn’t offer a viable market for a leading-edge plant, or “fab”. Industry and diplomatic sources say that, of the Big Three chipmakers, Intel is the only one so far to express concrete interest in Breton’s goal of producing the most advanced chips in Europe. Breton’s drive to attract a major foreign chipmaker has unnerved home-grown players, and he is also discussing the creation of a European semiconductor alliance that would bundle their interests. Germany’s Infineon said on Friday it welcomed Breton’s initiative to strengthen chip production in Europe. “As financial resources are naturally limited it is important to discuss most urgent needs and the most reasonable ways of investment,” Infineon said. Gelsinger, who met Economy Minister Peter Altmaier and Bavarian governor Markus Soeder on the German leg of his European tour, was quoted as saying Germany would be a suitable location for a potential European foundry. “Geopolitically, if you’re in Europe, you want to be in continental Europe,” he told Politico, in remarks echoed in a second interview with German business daily Handelsblatt. “We think of Germany as a good candidate – not the only, but a good candidate – for where we might build our fabrication capabilities,” he said, also indicating interest in the Benelux countries. In Germany, Gelsinger also met executives from carmaker BMW and telecoms operator Deutsche Telekom, Intel said. Sources said he also visited the headquarters of Volkswagen, although neither side confirmed this. Gelsinger travels on next week to Israel, where Intel is due to announce a $200 million investment in a new chip development campus and the hiring of 1,000 staff."
https://venturebeat.com/2021/04/30/cybersecurity-firm-started-by-ex-spies-surges-on-london-market-debut/,Cybersecurity firm started by ex-spies surges on London market debut,"(Reuters) — Darktrace, a cybersecurity company advised by former MI5 and CIA spymasters, leapt more than 40% on its market debut on Friday, delivering a boost to the London market after the spectacular flop of Deliveroo last month. The company was priced at 250 pence in the initial public offering, giving it a valuation of just over half the original aim of $4 billion. But it rocketed to 352 pence when trading started, well above the 220-280 pence range set by its bankers when its roadshow began on Monday. Darktrace uses AI to understand IT networks and then detect attacks by identifying unusual behaviour from within. Its advisory board includes a former director general of the British security service, Jonathan Evans, an ex-CIA chief information officer, Alan Wade, and former UK interior minister Amber Rudd. “Today is just the beginning,” Chief Executive Poppy Gustafsson said. The loss-making company, which says it’s presently focused on growth rather than profits, was founded in the English university city of Cambridge in 2013. It is backed by entrepreneur Michael Lynch, who was on its board until 2018 and is still an adviser. He and his wife own stakes worth a total of 440 million pounds after the share debut. Lynch is fighting a U.S. extradition request to face fraud charges related to the sale of Autonomy, a software company he founded and led, to American tech firm Hewlett-Packard. He is also waiting for the verdict of a multi-billion dollar civil claim by HP at London’s High Court. Darktrace detailed the risks related to Lynch in its registration documents, including potential liability in relation to allegations of money laundering made by U.S. prosecutors, although the company said the risk of the latter was low. Lynch denies all the allegations in the cases. He declined to comment on Darktrace. Darktrace’s Gustafsson, who like many of the senior management team previously worked at Autonomy, said investor sentiment was not affected by the Lynch association. “Ultimately whilst Mike is a visionary technologists and was an early investor in Darktrace, he’s not involved in the day to day running of the business,” she told Reuters this month. The Lynch connection was, though, enough to deter top-tier U.S. banks from pitching for roles on the IPO, sources have previously told Reuters. UBS was originally appointed as a global coordinator but then stepped down over compliance concerns related to the Autonomy saga, another separate source confirmed to Reuters. UBS did not respond to a request for comment, while Gustafsson said UBS had their own issues, without elaborating. Such big bank reticence opened the field for lesser-known names. Jefferies, Berenberg and KKR Capital Markets, none of which has ever topped equity capital markets (ECM) league tables, were joint-global coordinators. Jefferies placed ninth in Refinitiv’s first-quarter ECM league tables while Berenberg and KKR Capital Markets didn’t make it into the top 15. Yet, they succeeded in winning over investors and restoring confidence in London after Deliveroo’s flop in an IPO that involved bulge bracket banks such as Goldman Sachs. CMC Markets chief market analyst Michael Hewson said Darktrace’s launch was a welcome boost for the London market. “Given the sharp boost in initial trading there will inevitably be some criticism that the listing was priced too low,” he said. “However given what happened with Deliveroo maybe expectations were adjusted lower by a little too much.” Darktrace’s advanced technology is used by more than 4,700 companies and organisations, including intelligence services. The tech company, whose revenue rose from $79.4 million to $199.1 million between 2018 and 2020 but is yet to make a profit, offered 66 million shares in the IPO, valued at 165 million pounds and representing 9.6% of the capital. It raised gross proceeds of about 143.4 million pounds by selling new shares, excluding any over-allotment option, to accelerate product development and strengthen its balance sheet. Other investors in the company included Talis Capital, Hoxton Ventures, Summit Partners, KKR, TenEleven Ventures, Insight Partners, Vitruvian and Balderton Capital."
https://venturebeat.com/2021/04/30/a-blowout-earnings-cycle-hides-strategy-shift-from-cloud-vendors/,A blowout earnings cycle hides strategy shift from cloud vendors,"Earnings season this week again produced some incredible performances from the big three cloud providers. Revenue at Microsoft’s Azure was up 50% annually, Google Cloud rose 46% over the same period, and market leader Amazon Web Services reported a 32% year-on-year leap in revenue, making it a $54 billion annual business just 15 years after its launch. The performances highlight the significant shift to the cloud that’s taking place globally. The hyperscale cloud providers continue to benefit from the structural changes affecting most industries, where cloud computing has been instrumental in helping businesses transform operations, save money, and address shifts in customer demand over the past year. There’s also little sign of this demand abating. According to CCS Insight’s survey data, 65% of enterprises are increasing their IT budgets for the next 12 months, with over half expecting to have migrated at least 50% of their IT workloads to the cloud by the end of the year. This compares to just 20% of those surveyed in 2018. But beneath the growth, the earnings this time around also highlight other important factors driving the hyperscalers’ direction. Cloud computing may dominate the headlines, but arguably the most important story is the huge investments over the past few months in the areas of data, artificial intelligence and, in particular, higher-level services in these domains as they apply to vertical markets. These areas are where hyperscalers are making the biggest attempts at differentiation and focusing their efforts. Take, for example, AWS, which in December at its re:Invent conference implemented a big change in strategy by unveiling a host of AI-driven solutions focused on specific industries such as industrial manufacturing and healthcare. They include defect detection and predictive maintenance in industrial manufacturing, as well as a number of HIPPA-compliant solutions in healthcare such as HealthLake, which applies machine learning to large volumes of health data. Another industry is telecoms, where AWS is becoming an early leader in the industry’s transformation with the advent of 5G, highlighted by a deal with Dish Networks announced last week. The announcements indicate AWS is doubling its efforts to target industry verticals as part of its strategy and reaching new audiences beyond its core developer and IT communities, such as business leaders and operations executives. In focusing on higher-level services targeting industry and business problems, it is also trying to become more purposeful with the new products it releases. Google Cloud is following a similar playbook. Its bottom line, although improving this quarter, has been hit hard recently by the investments it’s making in hiring senior industry executives to build its vertical market expertise. Since CEO Thomas Kurian took the helm in January 2019, Google Cloud’s strategy has been laser-focused on making the most of Google’s credibility in data and AI to push industry-specific solutions in the areas of Contact Center AI, Document AI, and in fraud detection. Google’s foundations in data analytics and large-scale AI, applied to vertical markets, form an important differentiator for its customers. As Google CEO Sundar Pichai implied during this week’s earnings announcement, this approach is contributing to its growth: “Our expertise in real-time data and analytics is winning companies like Twitter and Ingersoll Rand, who are moving their complex data workloads to Google Cloud. Our strength in AI and machine learning is also helping financial services customers like HSBC, Commerce Bank, SEB Group, and BBVA improve efficiency of payments, reduce fraud and risk, and deliver faster payment solutions.” The biggest stake in this game has been placed by Microsoft, which earlier in April announced it would acquire Nuance Communications, a healthcare software and AI supplier, for $19.7 billion, its second-largest acquisition on record. Nuance offers Microsoft a more mature set of AI solutions for the healthcare industry as part of Microsoft’s expanding vertical market strategy, which saw the launch of Microsoft Cloud for Healthcare in October 2020. Areas where Nuance specializes, such as speech recognition, document processing, fraud detection and image recognition, will now become central to how Microsoft differentiates its cloud services to the healthcare industry. Commenting on the acquisition in Microsoft’s earnings call, CEO Satya Nadella summed up the strategy by stating, “They’ve done a fantastic job of taking what is perhaps the most defining technology of our times, which is AI, and applying it to healthcare, which is the most important application space.” While revenue growth will rightly garner most of the headlines in the current earnings cycle, it hides some important changes in direction of the cloud providers during the past year. As customers shift from a reactive approach to technology to a phase now characterized more by growth, reimagining their businesses and taking advantage of their data as a strategic asset, the nature of competition between the cloud providers is also changing. Customers are requiring more from these giants as they push more heavily into AI and look for solutions that align innovation with success in their particular industry. This trend will shape the direction of the cloud providers perhaps more than any other in the future. Nick McQuire is Chief of Research, Enterprise at CCS Insight."
https://venturebeat.com/2021/04/30/ai-weekly-how-the-power-grid-can-benefit-from-intelligent-software/,AI Weekly: How the power grid can benefit from intelligent software,
https://venturebeat.com/2021/04/30/how-the-global-chip-shortage-could-impact-your-business/,How the global chip shortage could impact your business,"In its quarterly report this week, Apple cautioned that although it had a terrific quarter, it may not be able to keep up with growing demand for its products given the worldwide shortage of computer chips. Apple is certainly not alone here – others have called out concerns about the shortage, including Intel, Nvidia, AMD, and Samsung. And it’s not just mobile and consumer devices that could be hit by this shortage – cloud and enterprise data centers could be impacted too, and automakers have already had to reduce production due to chip shortages. The skyrocketing demand of new equipment to enable 5G networks worldwide is also placing pressure on this market. In this “smart” and highly connected world, there is little produced that doesn’t need some embedded computer brains. Some have wondered if this shortage is overblown. My opinion is that the chip shortage is real, but it’s not affecting everyone the same way. The biggest purchasers of chips (e.g., Qualcomm, Nvidia, AMD, Apple, Samsung, etc.) have placed high volume standing orders from the contract fabs primarily in the Far East (the largest being TSMC, but also producers like Global Foundries, Samsung, etc.) and as a result get priority due to the size and long duration production of their purchases (long-duration, high-volume products are where the fabs make their profits). They order products over a long time period and maintain inventory on hand; they don’t just place spot orders when needed as some companies are prone to do (e.g., the automakers who don’t “order to inventory” but instead order for just-in-time manufacturing). Meanwhile, “lower priority” customers who can fill in chip line production gaps in normal times have to wait in an increasingly long queue when they do place new or additional orders, so they don’t get the product they need in what they consider a timely fashion. Even Intel, which makes most of its own chips in house and can therefore more or less control its own manufacturing destiny, has seen an inability to ramp up production fast enough to meet increased market demands, especially in the red-hot PC space. Of course, Intel also buys a substantial number of chips from TSMC and Samsung for several of its product offerings, so it is affected by the outsourced fabs’ capacity constraints as well. The major problem Apple and similar very high volume mobile players (such as Samsung) are facing is that the fabs are already running at maximum capacity. If demand suddenly goes up, as it has in the pandemic for not only consumer products but also for servers in cloud and enterprise installations, it’s very difficult for the contract fabs to increase output. And there’s no quick fix. Building a new fab can take 2-3 years and can cost $10 billion-$20 billion, so even with all the announcements lately of companies committing to new fabs (e.g., Intel, Samsung, TSMC) and even with government incentives from the US and other countries, you can’t simply turn on a new manufacturing line in short order. The issue is further compounded by the fact that much of the increased demand is in new-generation chips that can’t readily be run on older production lines that may have capacity to spare; and those lines would be too costly and take too long to retrofit. There is talk of expanding production sites to other areas of the world (e.g., India, mainland China), but starting up totally new production from scratch, and with companies that are new to the game, is a slow process. So will all of this have a long-term effect on companies like Apple and Samsung, and potentially Google, AWS, Microsoft, and other cloud providers, as well as cutting edge companies like Nvidia? It will likely take at least 18-24 months to stabilize the supply chain, unless there is a sudden case of markets shrinking due to some catastrophe or economic collapse (that’s unlikely but possible). Until this gets resolved, we can expect to see many companies negatively affected (some more, some less) by the chip shortage, even while all the chip makers race to add capacity. But it’s not all gloom for semiconductor-related companies; it’s a great time to be a chip manufacturing equipment supplier! The effect on enterprise customers will be varied. In the short term, you can expect to see shortages of some products like PCs, and even Chromebooks, as well as some high end mobile devices. They’ll be available, but perhaps not in the numbers or at the discounted prices many enterprises are used to. In data center servers, organizations can expect to see increased delivery times, so getting orders in ahead of the curve of when you’ll actually need them would be a wise move. Public clouds (e.g., AWS, Google Cloud Platform, Microsoft Azure) should not be dramatically affected as they have a good deal of capacity and can usually get new supplies of computers in a priority fashion, but some of the custom chip solutions they are deploying (e.g., AWS Graviton) may be impacted. Finally, enterprises should be aware that “business as normal” in procuring computing systems may not return to normal for several quarters at least. Plan accordingly. Jack Gold is the founder and principal analyst at J.Gold Associates, LLC., an information technology analyst firm based in Northborough, MA., covering the many aspects of business and consumer computing and emerging technologies. Follow him on Twitter @jckgld or LinkedIn at https://www.linkedin.com/in/jckgld."
https://venturebeat.com/2021/04/30/steam-education-is-critical-to-opening-up-new-opportunities-across-the-game-industry/,STEAM education is critical to opening up new opportunities across the game industry,"While many now understand that science, technology, engineering, art, and math education, or STEAM, is important to the success of the video game industry, it’s critical to creating meaningful opportunities for marginalized people. On the second day of GamesBeat Summit 2021, Stanley Pierre-Louis, CEO of the game industry trade group the Entertainment Software Association, spoke to Yvette Clark, U.S. Congresswoman from New York’s 9th District, and Laila Shabir, co-founder and CEO of Girls Make Games, to talk about how STEAM education is critical to development in the games industry — as well as to our national and global economy and the creation of new opportunities for advancement into the workforce, especially for women and racialized individuals. “Video games play a key role in spurring interest in STEM,” said Pierre-Louis. “And learning game-making skills creates career opportunities in a broad array of STEM and STEAM fields, because video games and arts and sciences produce skills that are highly desirable in adjacent fields like aerospace, coding, and more.” Clark believes in smart technology helping communities create sustainable, resilient, and livable lives, and she works to ensure that no communities are left behind. “At the heart of everything that we’re trying to accomplish in Washington D.C. is equity,” Clark said. “That means assuring equitable access to a quality education in the STEAM field, and the career and entrepreneurial opportunities that it leads to, which are essential for not only ensuring American competitiveness in the economy, but also to make sure that our nation is prepared to meet the demands of the 21st century.” In this new evolution of the industrial age, all roads lead to tech, innovation, and artistic ability, she said, and the sweet spot of those skills is STEAM education. It’s estimated that 3.5 million STEAM and STEM jobs will be needed to be filled by the year 2025. Combating disparities and removing barriers to equitable opportunities are a top priority for Clark in Congress. “If we’re going to indeed pursue an equity and inclusive agenda, so much of that is within the domain of the private sector,” she said. “We can, through government, create pathways and corridors, incentives for the private sector to partner with us in making sure that every American, regardless of race, sexual orientation, home of origin, has an ability to access opportunities.” Shabir, born in Pakistan, raised in the United Arab Emirates, and a graduate of MIT, warns that it’s important to reflect on where people come from, because that shapes who they become and how they view the world. Her pathway to Girls Make Games speaks to her commitment to give back while looking to build the next generation of innovators. She’s set a personal goal of teaching one million girls how to make games through her work. It’s vital to empower youth in two ways. The first in making sure they are digitally ready, which means everyone should be tech literate as they grow up, with computer science as essential as English and history. “It’s important to speak the language that the future is going to be built on,” Shabir said. “And it’s important to teach kids that anything and everything they can dream of, that they want to do, is possible and is within them.” It’s also essential give them not only technical skills, but the confidence and internal assurance that says, no matter the challenges, no matter the problems, I can come up with a solution myself. That’s what Girls Make Games focuses on, the validation and the technical skills, she adds. Girls Makes Games came from her own struggles, while founding a game studio, to find qualified women to fill the necessary positions. “I was recruiting for my studio and I couldn’t find women,” she said. “This was at a time when I was new to the industry, so I had no idea what the gender gap was like. The deeper I looked, the more I uncovered. Essentially, people came back and said, girls don’t play games, or they’re not interested in games.” As she was designing an educational game with her husband, she decided she wanted to build an equitable educational solution that all genders could enjoy. At the end of the first Girls Build Games summer camp, it was obvious to her that a place where girls could gather and talk about their interest in STEAM and games helped fill that need. Seven years out, that summer camp is still ongoing, with a virtual program and workshops expanding to nearly 100 cities worldwide. Organizations that are interested in recruiting and investing in this next generation of innovators and creators, particularly from underrepresented communities, need to stop talking about it and do something about it, Clark said. “In education, we need an all-around approach to providing diversity and inclusion,” she said. “Let us all wrap our arms around widening the aperture, so that [no matter] your demographics, where you live, where you were born, your economic circumstances, we provide portals, gateways, pathways, and corridors for those scientifically inclined, or those who just need the exposure and that spark of genius to get engaged to have access.” “Women and girls can play an integral role given our lived experiences to help create solutions through gaming that can be applied in practical, everyday experiences,” Clark adds. “My hat is off to Laila and the work that she’s doing with girls and young folks who open the world of possibility for so many. Not only just in the gaming space, but in the overall educational space, the social space, and the ability to analyze the world around you.”"
https://venturebeat.com/2021/04/30/hsr-healths-gis-platform-helps-target-covid-19-resources/,HSR.health’s GIS platform helps target COVID-19 resources,"Resolution of the COVID-19 crisis has come down to how quickly governments can vaccinate individuals before more contagious variants of the virus evolve and spread. One platform playing a critical role in helping health care organizations win that race is a geographic information system (GIS) platform created by Health Solutions Research (HSR.health) and accessed as a cloud service. The GeoHealth Platform HSR.health developed combines social determinants of health with social media data and estimated health care costs to surface potential hot spots. Created prior to the pandemic, the platform relies on a Health Risk Index model created using geospatial mapping software from Esri and open source Geoserver software for sharing geospatial data, HSR.health CEO Ajay Gupta told VentureBeat. “We wanted to track social determinants of health,” he said. Shortly after the pandemic began, HSR.health extended that model using open data sources various agencies had made available to create a wider range of index models that can, for example, predict the location of future outbreaks and identify areas with the highest risk of death or critical illness by county, zip code, and census tracts. As infection rates wax and wane, the index makes it possible to pinpoint specific areas where hospitalization rates are about to increase based on all the risk factors analyzed, Gupta noted. That information can also play a critical role in identifying regions and communities where people are for one reason or another hesitant to get vaccinated. Thus far, the indices created by HSR.health have been employed by the Pan American Health Organization (PAHO) in an effort to respond to COVID-19 throughout Central America. This push was enabled via the efforts of the Open Geospatial Consortium. Separately, the Graph Foundation, which maintains a set of open application programming interfaces (APIs), has made risk maps and other insights available to the regional African office of the World Health Organization (WHO) using the transmission and mortality risk indices created by HSR.health. In addition, state agencies in the U.S. used a Medical Device Index to provide visibility into the number of ventilators needed to provide care for currently hospitalized and anticipated COVID-19 patients, while manufacturers of personal protective equipment used a Medical Supply Index provided via consulting firm Portals Global to determine how to optimize supply chains. Elsewhere, the U.S. Federal Emergency Management Agency (FEMA) and other emergency response agencies have been employing the Health Risk Index to identify the health and medical needs of populations impacted by a specific disaster. The COVID-19 pandemic has conclusively demonstrated that GIS platforms can play a crucial role in mitigating any health care crisis, Gupta said. For example, the city of Baltimore, Maryland is now in the early stages of a community trial for a maternal mortality risk stratification index that identifies social determinants of health for expectant mothers at risk for labor and delivery complications, as well as other child health issues. Interest in community outreach in the health care community was just starting to gain momentum when the pandemic hit. Many health care providers were compensated based on patient outcomes, which necessitates having a better understanding of the root causes of diseases that might impact the community surrounding a health care facility. Now the immediate challenge is identifying communities that might be reluctant to participate in a COVID-19 vaccination program. Armed with that insight, health care advocates can better target their education efforts. The COVID-19 pandemic is far from the only major health care crisis countries currently face. Once the pandemic subsides, there will be more time and energy to focus on widespread diseases ranging from diabetes to cancer. But location is a major indicator for many diseases, with COVID-19 only the latest and, arguably, most pressing."
https://venturebeat.com/2021/04/30/ibm-is-acquiring-turbonomic-to-advance-aiops-agenda/,IBM is acquiring Turbonomic to advance AIOps agenda,"IBM announced this week that it is acquiring Turbonomic, provider of application resource management (ARM) and network performance management (NPM) software infused with machine learning algorithms. Terms of the acquisition, which is expected to close this quarter, were not disclosed. The two companies have a long-standing relationship under which IBM has been reselling Turbonomic’s ARM platform. Cisco also resells tools developed by the company. Turbonomic, which is privately held, claims revenues were up 41% for fiscal 2021 and counts Avon, HauteLook, and Litehouse Foods among its customers. The decision to acquire Turbonomic comes after IBM began revamping its application and systems management portfolio last fall. This push began in earnest with the acquisition of Instana, provider of an application performance management (APM) platform for monitoring and observing applications. IBM now plans to further integrate the ARM software Turbonomic developed with the APM software from Instana and an IBM Cloud Pak for Watson AIOps platform that employs machine learning algorithms to identify anomalies in real time. “Turbonomic provides actionable observability,” IBM Automation GM Dinesh Nirmal told VentureBeat in an interview. IBM is further extending its IT management portfolio via the recent acquisition of WDG Automation, provider of a robotic process automation (RPA) platform, and MyInvenio, which offers process mining tools, he noted. As IT environments become more complex, Nirmal said it won’t be feasible to manage these environments without augmenting IT staff with capabilities enabled by AI platforms. It’s not likely AI platforms will replace the need for human IT administrators, but the job functions themselves will continue to evolve as lower-level manual tasks become automated, Nirmal added. Now that companies are becoming more cognizant of the scope of IT management challenges, IT teams are increasingly embracing AI platforms. Organizations are now deploying a new generation of microservices-based applications that are more difficult to manage than the existing monolithic legacy applications, which are not likely to be retired anytime soon, Nirmal said. Those applications make use of cloud-native technologies such as containers, Kubernetes, and serverless computing frameworks that all need to be managed alongside virtual machines. At the same time, the IT environment has become more distributed than ever, thanks to the rise of both cloud and edge computing platforms. The only way to contain the total cost of managing that extended enterprise is to rely more on automation enabled by AIOps platforms, Nirmal said. IT teams need to come to terms with the fact that it takes time for machine learning algorithms to learn IT environments that are unique and subject to change. Implementing AI requires patience, Nirmal said, adding, “IT teams need to accept that AI comes with an upfront cost.” But the return on investment in AIOps becomes apparent as rote tasks are eliminated and more potential issues are addressed before they impact an application, Nirmal noted. IT teams, for example, will be able to predict the impact new code is likely to have on the overall IT environment before it’s deployed. IBM’s investments in AIOps are a natural extension of the capabilities IBM has developed to automate a wide range of business processes using AI technologies, Nirmal added. IT leaders can’t make a credible case for applying AI to automate business processes if the IT team isn’t using the same technologies to automate IT operations, he noted. At this juncture, AI is about to become a mainstream component of IT operations. The issue now is determining to what degree. In some cases, AI capabilities will be slipstreamed into existing platforms, while in others, IT teams will decide to move to a new platform. Either way, machine learning algorithms will be present in one form or another."
https://venturebeat.com/2021/04/30/facebook-details-self-supervised-ai-that-can-segment-images-and-videos/,Facebook details self-supervised AI that can segment images and videos,"Facebook today announced that it developed an algorithm in collaboration with Inria called DINO that enables the training of transformers, a type of machine learning model, without labeled training data. The company claims it sets a new state-of-the-art among unlabeled data training methods and leads to a model that can discover and segment objects in an image or video without a specific objective. Segmenting objects is used in tasks ranging from swapping out the background of a video chat to teaching robots that navigate through a factory. But it’s considered among the hardest challenges in computer vision because it requires an AI to understand what’s in an image. Segmentation is traditionally performed with supervised learning and requires a volume of annotated examples. In supervised learning, algorithms are trained on input data annotated for a particular output until they can detect the underlying relationships between the inputs and output results. However, with DINO, which leverages unsupervised learning (also called self-supervised learning), the system teaches itself to classify unlabeled data, processing the unlabeled data to learn from its inherent structure. Transformers enable AI models to selectively focus on parts of their input, allowing them to reason more effectively. While initially applied to speech and natural language processing, transformers have been adopted for computer vision problems as well as image classification and detection. At the core of so-called vision transformers are self-attention layers — each spatial location builds a representation by “attending” to other locations. That way, by “looking” at other, potentially distant pieces of an image, the transformer builds a rich, high-level understanding of the overall scene. DINO works by matching the output of a model over different views of the same image. In doing this, it can effectively discover object parts and shared characteristics across images. Moreover, DINO can connect categories based on visual properties, for example clearly separating animal species with a structure that resembles the biological taxonomy. Facebook claims that DINO is also among the best at identifying image copies, even though it wasn’t designed for this. That means that in the future, DINO-based models could be used to identify misinformation or copyright infringement. “By using self-supervised learning with transformers, DINO opens the door to building machines that understand images and video much more deeply,” Facebook wrote in a blog post. “The need for human annotation is usually a bottleneck in the development of computer vision systems. By making our approaches more annotation-efficient, we allow models to be applied to a larger set of tasks and potentially scale the number of concepts they can recognize.” Facebook also today detailed a new machine learning approach called PAWS that ostensibly achieves better classification accuracy than previous state-of-the-art and semi-supervised approaches. Notably, it also requires an order of magnitude — 4 to 12 times — less training, making PAWS a potential fit for for domains where there aren’t many labeled images, like medicine. Residing between supervised and unsupervised learning, semi-supervised learning accepts data that’s partially labeled or where the majority of the data lacks labels. The ability to work with limited data is a key benefit of semi-supervised learning because data scientists spend the bulk of their time cleaning and organizing data.  PAWS achieves its results by leveraging a portion of labeled data in conjunction with unlabeled data. Given an unlabeled training image, PAWS generates two or more views of the image using random data augmentations and transformations. It then trains a model to make the representations of these views similar to one another. Unlike self-supervised methods that directly compare the representations, PAWS uses a random subsample of labeled images to assign a “pseudo-label” to the unlabeled views. The pseudo-labels are obtained by comparing the representations of the unlabeled views with representations of labeled support samples. Because of this, PAWS doesn’t learn “collapsing representations” where all images get mapped to the same representation, a common issue for self-supervised methods. “With DINO and PAWS, the AI research community can build new computer vision systems that are far less dependent on labeled data and vast computing resources for training,” the Facebook statement continued. “We hope that our experiments will show the community the potential of self-supervised systems trained on [visual transformers] and encourage further adoption.” Both DINO and PAWS are available in open source."
https://venturebeat.com/2021/04/30/amri-adds-to-accelerated-rd-and-manufacturing-solutions-for-orphan-products-to-treat-rare-diseases/,AMRI Adds to Accelerated R&D and Manufacturing Solutions for Orphan Products to Treat Rare Diseases,"ALBANY, N.Y.–(BUSINESS WIRE)–April 30, 2021– Albany Molecular Research, Inc. (AMRI), a leading global provider of advanced contract research, development and manufacturing solutions, today announced expanded access to its accelerated solutions platform, spanning research & development to manufacturing for orphan and rare disease products. The company has made targeted investments globally to expand its scale and compound handling capabilities as well as made additions to the expert teams that address the specific needs of these complex products in its facilities in Albany, New York; Grafton, Wisconsin and Glasgow, Scotland. The expansion includes additional suites in Albany that meet Good Manufacturing Practice (GMP) requirements which are designed to manufacture batch sizes of 10-15 kilograms, which is critical for the production of rare-disease therapies. AMRI’s Grafton facility has expanded its hydrogenation, filtering and drying and analytical capabilities. It has added liquid chromatography-mass spectrometry and gas chromatography-mass spectrometry instruments in expanded laboratory space, and recruited additional analytical experts who assure products meet the intended specifications. AMRI’s Glasgow facility is an integrated single site with formulation development and clinical sterile drug product manufacture capabilities, which supports formulation and process development and GMP manufacture of orphan drug products. Compact filling technologies coupled with a bracketed media fill approach and single-use philosophy help minimize process losses at the site to enable efficient and robust delivery for low-volume, high-value products. Rare diseases are conditions that affect a limited population, defined in the United States as fewer than 200,000 people and in the European Union as fewer than one in 2,000 people. The products used to treat rare diseases are complex and innovative, addressing unmet needs using high-value drug substance. The need for an integrated, accelerated and agile pathway is acute to speed potential treatments to patients. “AMRI is committed to making a difference for the innovators that meet the challenges of orphan drugs and, in turn, for the patients living with rare diseases,” said John Ratliff, CEO, AMRI. “AMRI’s scientists and operators dedicate both hearts and minds to these programs, including our regulatory and intellectual property professionals who work closely alongside customers to navigate the complex landscape associated with orphan designations and our scientists who can manage the complexity of these innovative products. Our teams deliver seamless programs for orphan products from R&D through manufacturing with a passionate commitment to ‘right first time’ throughout.” In 2020, 31 of the 53 (58%) novel drug approvals by FDA’s Center for Drug Evaluation and Research were approved to treat rare or orphan diseases1. Evaluate Pharma anticipates a 12% compound annual growth rate in prescription sales of orphan drugs from 2020 through 2026. AMRI’s specialized solutions for rare and orphan drugs leverage deep expertise in sterile injectable formulation and development; flexible scale with grams to kilograms within one facility to minimize timeframe and technical transfers from site to site; expert regulatory services for faster and accurate filings; and intellectual property consulting to enhance product strategy. This expertise and scale enables swift progression from laboratory to cleanroom while mitigating scale-up risk. About AMRI AMRI, a contract research development and manufacturing organization, partners with the pharmaceutical and biotechnology industries to improve patient outcomes and quality of life. AMRI’s team combines scientific expertise and market-leading technology to provide a complete suite of solutions in discovery, development, analytical services, and API and drug product manufacturing. Learn more at www.AMRIGlobal.com. [1] https://www.fda.gov/drugs/new-drugs-fda-cders-new-molecular-entities-and-new-therapeutic-biological-products/new-drug-therapy-approvals-2020#:~:text=In%202020%2C%2031%20of%20CDER’s,available%20to%20treat%20their%20conditions  View source version on businesswire.com: https://www.businesswire.com/news/home/20210430005122/en/ Jane ByramSCORR Marketing512-626-2758jane@scorrmarketing.com"
https://venturebeat.com/2021/04/30/ransomware-task-force-unveils-broad-manifesto-for-fighting-back/,Ransomware task force unveils broad manifesto for fighting back,"The Ransomware Task Force (RTF) yesterday unveiled its comprehensive guidance for battling ransomware, information security’s preeminent scourge. The 81-page report, titled Combatting Ransomware: A Comprehensive Framework for Action, gives enterprise defenders their first structured standardized guidance for ransomware defenses. The project began in January 2019 and was organized by the Institute for Security and Technology (IST), a Bay Area-based nonpartisan nonprofit group that champions networking and collaborative efforts to address information security challenges. “The cost of ransom paid by organizations has nearly doubled in the past year and is creating new risks, many that go far beyond monetary damage,” IST CEO  Philip Reiner said in a statement. “We felt an urgent need to bring together world-class experts across sectors to create a framework that government and industry can pursue to disrupt the ransomware business model and mitigate the impact of attacks.” The RTF, made up of 60 industry experts, spent more than two years engaged in intense collaboration to develop these recommendations. The task force includes an eclectic mix of organizations representing government agencies, technology vendors, financial institutions, and academia. The RTF Framework mirrors the well-known NIST Cybersecurity Framework (CSF) by grouping recommendations into logical target areas. Where NIST describes specific technical actions in its five “functions,” the RTF authors opted to distribute 48 higher-level recommendations across four goals: “deter,” “disrupt,” “prepare,” and “respond.” Defenders looking for specific NIST-like technology controls for ransomware mitigation, response, and recovery will have to wait a little longer. On the whole, the RTF Framework addresses high-level policies and processes, including advocating for the creation of more technical guidance, particularly for underfunded and critical industries. “Guides and technological tools to mitigate ransomware are currently available, however, many are insufficient, overly simplified, or too complicated, and the general level of noise surrounding this problem is confusing and problematic,” the RTF report authors wrote. “The single most impactful measure that could be taken to help organizations prepare for and respond to ransomware attacks would be to create one internationally accepted framework that lays out clear, actionable steps to defend against, and recover from, ransomware.” Jen Ellis is vice president of community and public affairs at security vendor Rapid7 and a task force committee co-chair. She told VentureBeat the framework’s approach developed, in part, from taking a hard look at what organizations were – and were not – doing to protect themselves. “Over recent years, there has been a great deal of investigation into ransomware attacks and trends, and many cybersecurity vendors have provided responses either in the form of technology solutions and services, or guidance and best practices,” Ellis said. “Yet adoption is slow or possibly ineffective, which suggests that organizations either lack an appetite for these offerings, presumably because they don’t understand the ransomware threat or how the solutions can help mitigate it, or because they lack the capability or resources to adopt. “The Task Force included end user organizations of all sizes and we sought their perspective on the reality here,” Ellis added. “What we heard from them was that the amount of noise on this topic is hard to navigate and interpret, and guidance often seems overly-simplified, while technologies on the other hand often seem complicated or too time-consuming to deploy.” Where the RTF Framework shines is in challenging the public and private sectors to take bold action to beat ransomware at every stage of its miserable lifecycle. In addition to developing future technology guidance, the framework’s top recommendations include: Kevin Johnson is CEO of Secure Ideas, a security consultancy, incident response, and training firm in Jacksonville, Florida. He said the RTF Framework’s lack of technical specificity aside, the framework addresses a clearly pressing need to find an organized, structured way to tackle the ransomware problem. “Over the last few years, it has become abundantly clear that organizations must prepare for a ransomware attack,” Johnson told VentureBeat. “This preparation includes understanding what resources are actually within your organization and how you will deal with those resources being encrypted.” “Way too often in our testing, we find that not only are companies not prepared for this type of attack, but they also are surprised when we show them the machines and services they actually run,” Johnson said. The RTF makes clear in its report that the framework is not a choose-your-own-adventure exercise designed for piecemeal implementation. Each recommendation interlocks with other actions, and the strength of the total effort depends on coordinated and complete execution. For example, reducing the profitability of ransomware through financial controls thwarts crimes in progress and also acts as a deterrent, discouraging future actors from engaging in similar malefactions. “Our hope with the recommendation of a single, unified framework, is to produce consistent guidance that breaks deployment down, making it more relatable and manageable, and thus more actionable, said Rapid7’s Ellis. “We hope to create a single source of truth that provides some sense of what a path to maturity might look like, while also giving less-resourced organizations a reasonable and impactful starting point.”"
https://venturebeat.com/2021/04/30/sweep-helps-track-carbon-emissions-across-enterprise-supply-chains/,Sweep helps track carbon emissions across enterprise supply chains,"Despite banks’ net-zero carbon pledges, a recent survey carried out by the Carbon Disclosure Project (CDP) found that nearly half of global banks had not conducted any analysis of the climate impact within their investment portfolios. But with growing pressure from all corners of society, it’s clear the financial powerhouses of the world can’t ignore the need to curtail climate change. Last year, BlackRock even announced sustainability was its “new standard for investing.” But good intentions don’t necessarily translate into meaningful action. Managing and accounting for emissions is a complex process, particularly within global enterprises with vast supply chains and partner networks — these “scope 3 emissions” are tricky to track. Enter Sweep, a French startup that launched in public beta this week with $5 million in funding to help reduce the inherent complexities of capturing carbon emissions data across an enterprise’s entire value chain, spanning internal operations and external partners. Founded out of Montpellier in 2020, Sweep is the brainchild of Rachel Delacour and Nicolas Raspal, who previously launched a business intelligence startup called Bime Analytics that they sold to customer service software giant Zendesk in 2015, and Raphael Güller and  Yannick Chaze. Any enterprise looking to launch an emissions program would typically follow four basic steps, according to Delacour: measure their emissions, reduce their emissions, contribute to carbon projects, and communicate their actions. When it comes to measurement, businesses currently use any combination of consulting services, spreadsheets, and business intelligence tools. The bigger the company is, the more difficult it is to keep an accurate record of all its emissions across the board. “Determining a comprehensive and precise footprint in the most simple organization is already hard,” Delacour told VentureBeat. “In a relatively complex, multi-site, multi-product or business unit enterprise, it is a serious undertaking. With existing tools, these companies need to juggle Excel sheets aggregation, no approval workflows, bare-bones emission factors, and no real auditability.” Sweep enables businesses to measure emissions continuously, giving everyone in the company and supply chain access to the data to understand what’s working and which areas need to change. It also allows companies to set targets, including individual or teamwide goals that gamify the process through friendly competition. Externally, Sweep also offers granular controls in terms of who can access what information across a company’s subsidiaries or partners, which is crucial for protecting other sensitive data. Common data types Sweep processes include things like land use, electricity generation, travel, and material transportation. Sweep offers APIs that companies can connect via any system, service, or database with just a couple of lines of code, affording them the flexibility to funnel in data from just about anywhere. Sweep is also working on prebuilt integrations for specific services, such as travel booking software and other SaaS tools, though these are not available yet. Other companies are working to modernize the carbon accounting and tracking process, with the likes of fledgling Arizona startup Persefoni recently raising $9.7 million in funding. While Sweep may not be alone in its endeavors, Delacour is hopeful that its user-friendliness will help it gain traction among corporations across all sectors. “Making real change in a big company requires teamwork, which is why Sweep is all about collaboration, sharing data, approvals, and so on,” Delacour said. “We realized that a good carbon tool needs to be usable by as many people as possible, collaborating on analysis and climate action. And so we built it.” It’s important not to underestimate the significance of scope 3 emissions in any carbon accounting process. Businesses might tightly control their internal emissions, but their suppliers may contribute significant carbon output. Coca-Cola European Partners (CCEP), a bottling partner for Coca-Cola, has estimated that 93% of its greenhouse gas (GHG) emissions in 2019 were scope 3. This is what Sweep wants to address. Sweep can connect climate data across companies, allowing all parties in a supply chain to collaborate. “This is the only way companies can seriously tackle their indirect scope 3 emissions,” Delacour added. “Companies cannot truly claim ‘net zero’ emissions status unless they have tackled their scope 3 emissions, given that their scope 3 emissions come from their suppliers, customers, and entire value chain.” Sweep also offers a curated marketplace for carbon offsetting projects, which helps it map a company’s emissions. “If companies want to buy negative carbon on their Scope 1 and 2 emissions with Sweep, it can be done with literally one click of a button.” It’s still early days for Sweep, but the problem it’s seeking to tackle impacts businesses of all sizes across every industry. As public and governmental pressure mounts, companies will have to not only measure their carbon footprint but demonstrate that they are reducing it, something Sweep’s reporting features can enable. Sweep also includes “investor-grade” reports that can be set to automatically publish “in line with the latest carbon accounting standards.” According to Delacour, Sweep currently has “very significant projects underway” with two large global manufactures and a telecom company, though the only business it was at liberty to divulge was sustainable clothing company Picture Organic Clothing. “We’ve seen that a lot of companies want to do good and just need the proper tools to make it happen — which is where Sweep comes in,” Delacour said. In terms of pricing, Sweep operates a SaaS subscription business model, ranging from $254 per month for smaller teams to bespoke pricing plans for enterprises."
https://venturebeat.com/2021/04/30/amazon-posts-record-profits-as-aws-hits-54b-annual-run-rate/,Amazon posts record profits as AWS hits $54B annual run rate,"(Reuters) — Amazon.com, one of the biggest winners of the pandemic, posted record profits on Thursday and signaled that consumers would keep spending in a growing U.S. economy and converts to online shopping are not likely to leave. Since the start of the coronavirus outbreak, shoppers have relied increasingly on Amazon for delivery of home staples, and the company sees this trend continuing post-pandemic, particularly for groceries. While brick-and-mortar stores closed, Amazon has now posted four consecutive record quarterly profits, attracted more than 200 million Prime loyalty subscribers, and recruited over 500,000 employees to keep up with surging demand. Amazon said it expects operating income for the current quarter to be between $4.5 billion and $8 billion, which includes about $1.5 billion in costs related to COVID-19. Shares rose 4% in after-hours trade. Throughout the pandemic, the world’s largest online retailer has been at the center of workplace tumult, with a failed attempt by organized labor to unionize an Amazon warehouse in Alabama and litigation in New York over whether it put profit ahead of employee safety. Amazon’s business has largely been unfazed by the developments. Michael Pachter, an analyst at Wedbush Securities, said a jump in Prime subscriptions, consumers’ embrace of grocery delivery amid COVID-19 and an improving economy worked to Amazon’s advantage. “Habit. Good quality grocery. Stimulus checks,” Pachter said. “They’re going to thrive.” Slower sales growth in the current period relative to the last quarter reflected a tougher comparison to last year, when lockdowns were in full swing, Pachter said. CEO Jeff Bezos touted the results of the company’s cloud computing unit Amazon Web Services (AWS) in a press release, saying, “In just 15 years, AWS has become a $54 billion annual sales run rate business competing against the world’s largest technology companies, and its growth is accelerating.” The plaudits were a nod to Andy Jassy, AWS’s long-time cloud chief who will succeed Bezos as Amazon’s CEO this summer. Amazon announced a deal for Dish Network to build its 5G network on AWS last week, and the division increased revenue 32% to $13.5 billion, ahead of analysts’ average estimate of $13.2 billion, according to IBES data from Refinitiv. Brian Olsavsky, Amazon’s chief financial officer, said businesses increasingly wanted to outsource their technology infrastructure to AWS. “We expect this trend to continue as we move into the post-pandemic recovery,” he said. Adding to Amazon’s second-quarter revenue will be Prime Day, the company’s annual marketing blitz. Amazon disclosed the event will take place in June rather than July, as is more typical, to reach customers before they head on vacation. Grocery sales anchored by Amazon’s subsidiary Whole Foods Market remain a bright spot, too. Olsavsky called grocery “a great revelation during the post-pandemic period.” The company’s first-quarter profit more than tripled to $8.1 billion from a year ago, on sales of $108.5 billion, ahead of analysts’ estimates. Amazon saw its stock price nearly double in the first part of 2020 as it benefited from the pandemic. This year, however, it has underperformed the S&P 500 .SPX market index. Its shares were up about 8.5% year to date versus the index’s 13% gain. Spending on COVID-19 and logistics has chipped away at Amazon’s bottom line. The company has poured money into buying cargo planes and securing new warehouses, aiming to place items closer to customers to speed up delivery. It said Wednesday it planned to hike pay for over half a million employees, costing more than $1 billion — and it is still hiring for tens of thousands more positions. Olsavsky said Amazon was still working to restore one-day package delivery rates to pre-pandemic levels. He told reporters the company intends to increase spending on video content this year as well. Consumers have been watching content for more hours on Amazon, Olsavsky said. While far behind ad sales leaders Facebook Inc and Alphabet Inc’s Google, Amazon is growing its ad business because brands’ placements often result directly in sales, reaching customers who are on Amazon with an intention to shop. Jesse Cohen, senior analyst at Investing.com, said, “Outside of its core retail and cloud units, advertising revenue is increasingly becoming another substantial growth driver for Amazon.” Amazon said ad and other sales rose 77% to $6.9 billion, ahead of analysts’ estimate of $6.2 billion."
https://venturebeat.com/2021/04/30/cloud-infrastructure-spending-grew-35-to-41-8b-in-q1-2021/,Cloud infrastructure spending grew 35% to $41.8B in Q1 2021,"Global cloud services infrastructure spending grew to $41.8 billion in Q1 2021, a 35% year-on-year (YoY) rise and 5% quarter-on-quarter (QoQ) increase. The figures, published by research and analytics firm Canalys, follow hot on the heels of the “big three” public cloud companies releasing their quarterly earnings data. Amazon revealed that its AWS cloud unit grew revenue by 32%, a notably faster pace than many analysts had predicted, while both Microsoft’s “commercial cloud” and Google Cloud Platform (GCP) also reported accelerated growth. However, it’s difficult to compare these figures, given that Microsoft and Google bundle their respective cloud-based software (i.e. Office and Google Workspace) alongside their server infrastructure for quarterly earnings reports. Canalys defines “cloud infrastructure services” as companies that provide both infrastructure-as-a-service and platform-as-a-service. While it excludes direct software-as-a-service (SaaS) expenditure, it does include revenue generated from the infrastructure services used to operate them. At 32%, AWS still leads the pack in terms of public cloud infrastructure services spend, followed by Microsoft’s Azure at 19% and Google Cloud at 7%. It’s worth noting that Q1 2021 was the first time cloud infrastructure services spending passed the $40 billion mark in a given quarter, with real-term spending growing by nearly $11 billion compared to the same period last year. The main driving force in Q1, much like every quarter over the past year, has been the pandemic-driven rapid digital transformation enveloping industries from construction to local offline eateries. Moreover, remote work has played a major role in accelerating demand for services across the cloud communication and collaboration space. Last week, Gartner predicted public cloud spending was on course to reach $332 billion in 2021, up 23.1% on last year’s $270 billion."
https://venturebeat.com/2021/04/29/indias-it-companies-scramble-to-handle-covid-19-surge/,India’s IT companies scramble to handle COVID-19 surge,"(Reuters) — India’s giant IT firms in Bengaluru and other cities have set up COVID-19 “war-rooms” as they scramble to source oxygen, medicine and hospital beds for infected workers and maintain backroom operations for the world’s biggest financial firms. Banks including Goldman Sachs and Standard Chartered, who run much of their global back office operations from large office parks in Bengaluru, Chennai or Hyderabad, have put in place infrastructure to vaccinate thousands of employees and their families when age restrictions are lifted on May 1. Workers at huge technology service providers Accenture, Infosys and Wipro say teams are working 13-14 hours daily, under growing pressure and struggling to deliver on projects as staff call in sick and take time off to care for friends and relatives. They play down any threat of a collapse in operations — but at stake if the surge continues is the infrastructure put in place by the world’s biggest financial companies in cost-cutting drives that have left them deeply reliant on the big Indian offices. “Employees have contracted COVID-19 since the second wave began, causing severe pressure for projects that are nearing deadlines,” said one employee at Accenture, asking not to be identified because he was not authorised to speak to the media. Five other sources at Accenture confirmed the growing issues with pressure of work. Accenture said it was providing some medical care and covering the cost of vaccinations for its employees but did not comment on the impact on productivity. Wipro said it has not seen any disruption to operations and has transferred some client projects to offices outside India. Only about 3% of its nearly 200,000 employees are now working from the office on critical projects, and it expects more of those employees to work from home, it said. For those who have to work from the office, Wipro said it had made living arrangements at guest houses and hotels nearby. Infosys, India’s second largest software services firm, said it was operating remotely across all offices and had not seen any impact on client projects, despite the deteriorating health situation in the country in recent weeks. Tata Consultancy Services, India’s top information technology (IT) services firm, similarly said its operations had not been affected. India’s second wave of infections has seen at least 300,000 people test positive each day for the past week, overwhelming healthcare facilities and crematoriums and driving an increasingly urgent international response. Asia’s IT capital Bengaluru, desperate to calm a daily infection rate five times higher than in last year’s first wave, on Monday ordered a full lockdown that allows ordinary residents to leave their homes only briefly between 6 a.m. and 10 a.m. Local IT managers say they struggled to get global chiefs outside India to recognise the seriousness of the outbreak. India’s gigantic IT and call centre service industry employs more than 4.5 million people directly and relies on huge numbers of graduates under the age of 30. They are paid a fraction of Western salaries and had largely ridden out the COVID-19 pandemic working from home until the relaxing of restrictions in recent months spurred companies to call more employees back to the office. Managers at Goldman Sachs’ massive complex in Bengaluru, for example, told staff in early March to prepare to return to full-scale office working. Chief Executive Officer David Solomon said then that the bank owed it to its incoming class of analysts and interns to have them come to work in offices for at least part of the summer. The company quickly U-turned, sending all but essential employees home on March 27 as cases began to rise. Another large bank, Wells Fargo, said its employees in India would continue to work remotely till at least early September. New strains of the virus have since sent India’s case numbers soaring to global records and brought more infections among younger Indians. All 15 of the large companies Reuters spoke to this week said that they now had vaccination schemes in place. Several outlined COVID-19 “war-rooms” they had launched to support staff and secure oxygen and other supplies. Initially, managers outside India had not wanted their companies’ Indian operations to be seen to be jumping the queue for vaccines, says a senior manager who runs a workforce of more than 600 staff at a global bank in Bengaluru, asking not to be identified. “The India CEO and others here said: we don’t care what it looks like, people are dying.”"
https://venturebeat.com/2021/04/29/acorio-digital-transformation-was-the-top-business-initiative-in-2021/,Acorio: Digital transformation was the top business initiative in 2021,"Digital transformation grew 20% year-over-year, becoming the top business initiative in 2021 with 79% of organizations working on it, over service management, which was at only 64% this year, Acorio, an NTT Data company and ServiceNow consultancy, said in its third annual ServiceNow Insight and Vision survey.  The survey found that 60% of organizations have been working on their digital transformation initiatives for over a year, but only 14% are “almost complete” with their transformation. While many organizations were slowly evolving towards transformation, the past 14 months have put the need for business digitization at a different level of strategic conversations. Digitization cuts across all industries and company sizes as every enterprise has been impacted in a variety of ways by the global health crisis and ensuing response. Before COVID-19, many enterprises were incrementally improving parts of their businesses. We now see these same traditional enterprises were forced to rethink their technology into a more widespread strategy and execution in order to serve the new needs of their operations, employees, and customers. The survey found that 26% of organizations have started implementing AI and machine learning since the start of the COVID-19 pandemic. As a key focus of this survey, Acorio examined how businesses were using ServiceNow’s technology platform to underpin their initiatives in 2021. This year’s survey results reveal major platform growth amongst ServiceNow clients with 86% of companies using ServiceNow reporting implementations of two or more ServiceNow products, 63% companies reporting implementations three or more ServiceNow products, and 41% companies reporting implementations of four or more ServiceNow product in 2021. These numbers are significantly higher than the numbers reported in 2019, which were 54%, 27%, and 12%, respectively. This first look Executive Summary comes from the largest global survey focused exclusively on ServiceNow and technology transformation. With nearly 500 responses, this year’s third annual report is Acorio’s largest study conducted to date. The responses were gathered anonymously via an online platform and come from a wide range of industries including healthcare, technology, and financial services. Of the respondents, 68% were from companies with more than 5,000 employees and 44% named themselves as directors or had higher job titles. Read more in Acorio’s full ServiceNow Insight and Vision survey."
https://venturebeat.com/2021/04/29/its-now-or-never-society-must-respond-to-the-ransomware-crisis/,It’s now or never: Society must respond to the ransomware crisis,"Companies in the hyper-competitive technology industry rarely work together, making their collaboration on a framework to combat ransomware noteworthy. Representatives from companies such as Microsoft and Amazon Web Services teamed up with security vendors, insurance providers, non-profits, think tanks, and government agencies to join the Institute for Security and Technology’s (IST) Ransomware Task Force, which today published a sobering report with detailed recommendations on ways to fight back against these attacks, “Combating Ransomware: A Comprehensive Framework for Action.” Only a few years ago, ransomware was mostly an economic nuisance that did not attract this level of scrutiny. It primarily affected individual machines, with ransoms of a few hundred dollars. Now, ransomware is a national security and public health and safety threat. It affects entire corporate networks and disrupts critical services. Meanwhile, payments in the hundreds of thousands or even millions of dollars are fueling an entire criminal ecosystem. As a society, we must combat this threat more effectively or we will suffer catastrophic consequences. Identifying the threat is easy. The hard part is figuring out what precisely to do to fight ransomware. To meet this challenge, IST convened a large group of experts from the technology industry and other sectors and fields to review the threat and develop responses. This group met and deliberated intensely for three months. The result: today’s report that lays out an integrated set of 48 recommended actions that would, if fully resourced and implemented, achieve the goal of significantly reducing the ransomware threat. As the report indicates, many different types of organizations will have to work together to implement these recommendations. Government agencies, for-profit security companies, platform providers, the insurance industry, the financial sector, telecommunications companies, critical infrastructure companies, academia and think-tanks, and cybersecurity non-profit organizations all have critical roles to play. Combatting this threat requires each sector to bring its capabilities to the table. Within the cyber ecosystem, non-profits perform several unique functions. Non-profits take on functions and look after interests that the market has little incentive to perform or consider. Non-profits serve as neutral conveners among for-profit actors; since non-profits do not have to exclusively support a given product or service, they can work with different vendors on equal footing and develop industry-wide viewpoints.  hey serve as interlocutors between different sectors that might find it difficult to interact directly for a variety of reasons. They serve as reliable information collators, collecting input from a wide range of sources and sifting through it to identify the most useful ones. In the RTF’s recommendations, non-profits are called upon to perform all these functions. The report identifies specific tasks for non-profits in multiple areas, including working with the National Institute of Standards and Technology to develop a ransomware framework, supporting a global hub to combat ransomware, developing complementary materials to support framework adoption, participating in an information-sharing Ransomware Incident Response Network, and creating a standard format for ransomware incident reporting. Without active participation from cybersecurity non-profits, these recommendations will likely prove difficult to implement, thereby limiting the ability to fully counter this burgeoning threat. Of course, performing these functions requires resources. For non-profits, that means garnering support from the other sectors in the ecosystem, whether it is from for-profit companies paying membership dues, governments giving grants, or philanthropists and foundations donating to specific projects. Put simply, for non-profits to play our unique part in combating ransomware, we will need support from the other sectors in this effort. Non-profits such as the Cyber Threat Alliance and the Global Cyber Alliance are dedicated to improving the security of the digital ecosystem and reducing the threat of ransomware. Along with the Center for Internet Security, the Cybercrime Support Network, the Cyber Readiness Institute, the Cybersecurity Coalition, the Global Resilience Federation, the Institute for Security and Technology, Aspen Digital, the CyberPeace Institute, the CyberPeace Foundation, and Third Way, we will work with each other and representatives from other sectors to implement the RTF recommendations. We urge other cybersecurity non-profits to join us in this important work, and we call on the other sectors in the digital ecosystem to support the non-profit sector in performing their unique functions. We must redouble our efforts against this dangerous threat. Michael Daniel is president and chief executive officer of the Cyber Threat Alliance and a co-chair of the Ransomware Task Force Working Group. Megan Stifel is executive director for the Americas at the Global Cyber Alliance and a co-chair of the Ransomware Task Force."
https://venturebeat.com/2021/04/29/the-early-decision-that-helped-adjust-go-from-start-up-to-global-success/,The early decision that helped Adjust go from startup to global success,"The second day of GamesBeat Summit 2021 offered attendees a look at companies that skyrocket from startup to big success, and how they faced the challenges and opportunities of rapid expansion, on the panel “Scaling the right way: When a startup is no longer a startup.” Katie Jansen, CMO at AppLovin and Katie Madding, chief product officer at Adjust, spoke about their career trajectories at startups, going from early employees to company leaders, as well as the journey of the companies themselves. Jansen was AppLovin’s first hire on the marketing team about eight and a half years ago, and now leads several teams. Madding, now chief product officer at Adjust, started at the company seven years ago, and was the third employee in their San Francisco office. “Our internal mantra has always been, your growth is our growth,” Madding said. “We never wanted to get to a point where we would have to start nickel and diming our clients.” The company made the decision early on, in terms of infrastructure, to go bare metals rather than the cloud, which she admitted isn’t the traditional way companies scale. One of the primary reasons they chose to go with their own internal infrastructure from the get-go was that cloud computing was going to be prohibitively expensive with the goal of tracking as much of their customers’ data as possible. “A big piece of advice I’d give to other companies is to make sure that when you’re making some of those huge decisions that will carry on with you throughout your growth stages, make sure they’re there to support you, and they’re in line with your business values from the start,” she said. “It’s been a key to our success and our ability to support our clients’ growth. Not just our own.” Along with that, data and quantitative measurement matters in every way, shape, and form, she adds, and should never be the last thing you think about as you scale, but the first. “I can’t stress enough that giving democratized data to every person at your company, so they can be smart and intelligent about what’s going on, feeds every single team, every department,” she said. “It’s huge, and very critical.” Hiring for those teams, and making sure those practices and goals are strong from the start, is another huge piece of scaling a company. For a startup that’s gone global, it’s important to assemble a group of employees that still embody entrepreneurial spirit that drives the company toward growth. “It’s identifying those types of people that aren’t just seeing the problems, but are coming through with solutions,” Madding said. “That’s an entrepreneurial spirit, that mentality of, if we spot something, if there’s an opportunity there, let’s go and get it done. Finding those people that feel similarly and have those similar values was always something I tried to look for at the beginning, and I still look for today.” There are risks in growing the brand and evolving the brand message and product, and losing yourself along the way. Madding’s best advice, especially for scaling your product, is knowing and understanding your personas and your segments — not your vertical and your country, but identifying what clients you’re going after, and what their business models and their interests are. If you have a unique opportunity in that area, you should absolutely grow your product to fit that business need, but when you’re growing globally, that can result in a ton of noise versus signal, and demands from every side. “As a strong product leader that understands why you started this company in the first place and what your customers’ needs are, you have to say no to certain things,” Madding said. “In your gut you typically know when something is perhaps exciting, but also still not in line, long term, with what your clients are actually going to drive growth and value from. Without that, then you have a diluted product that tries to fill the need for everyone and misses the mark on all counts.” The branding perspective is very similar, Jansen said. Your brand voice needs to start clear and stay clear; you can’t try to make the brand speak to all customers at all times. As you evolve your brand, you want to apply your overarching message to your growing customer base, and then segment it out for the individuals."
https://venturebeat.com/2021/04/29/google-led-paper-pushes-back-against-claims-of-ai-inefficiency/,Google-led paper pushes back against claims of AI inefficiency,"Google this week pushed back against claims by earlier research that large AI models can contribute significantly to carbon emissions. In a paper coauthored by Google AI chief scientist Jeff Dean, researchers at the company say that the choice of model, datacenter, and processor can reduce carbon footprint by up to 100 times and that “misunderstandings” about the model lifecycle contributed to “miscalculations” in impact estimates. Carbon dioxide, methane, and nitrous oxide levels are at the highest they’ve been in the last 800,000 years. Together with other drivers, greenhouse gases likely catalyzed the global warming that’s been observed since the mid-20th century. It’s widely believed that machine learning models, too, have contributed to the adverse environmental trend. That’s because they require a substantial amount of computational resources and energy — models are routinely trained for thousands of hours on specialized hardware accelerators in datacenters estimated to use 200 terawatt-hours per year. The average U.S. home consumes about 10,000 kilowatt-hours per year, a fraction of that total. In June 2020, researchers at the University of Massachusetts at Amherst released a report estimating that the amount of power required for training and searching a certain model involves the emissions of roughly 626,000 pounds of carbon dioxide, equivalent to nearly 5 times the lifetime emissions of the average U.S. car. Separately, leading AI researcher Timnit Gebru coauthored a paper that spotlights the impact of large language models’ carbon footprint on marginalized communities. Gebru, who was fired from her position on an AI ethics team at Google in what she claims was retaliation, was told her work didn’t meet Google’s criteria for publication because it lacked reference to recent research. In an email, Dean accused Gebru and the study’s other coauthors of disregarding advances that have shown greater efficiencies in training and might mitigate carbon impact. This latest Google-led research, which was conducted with University of California, Berkeley researchers and focuses on natural language model training, defines the footprint of a model as a function of several variables. They include the choice of algorithm, the program that implements it, the number of processors that run the program, the speed and power of those processors, a datacenter’s efficiency in delivering power and cooling the processors, and the energy supply mix — for example, renewable, gas, or coal. The coauthors argue that Google engineers are often improving the quality of existing models rather than starting from scratch, which minimizes the environmental impact of training. For example, the papers suggests that Google’s Evolved Transformer model, an improvement upon the Transformer, uses 1.6 times fewer floating point operations per second (FLOPS) and takes 1.1 to 1.3 times less training time. Another improvement — sparse activation — leads to 55 times less energy usage and reduces net carbon emissions by around 130 times compared with “dense” alternatives, according to the researchers. The paper also makes the claim that Google’s custom AI processors, called tensor processing units (TPUs), enable energy savings in the cloud far greater than previous research has acknowledged. The average cloud datacenter is roughly twice as energy efficient as an enterprise datacenter, the coauthors posit, pointing to a recent paper in Science that found that global datacenter energy consumption increased by only 6% compared with 2010, despite computing capacity increasing by 550% over the same time period. Earlier studies, the paper says, made incorrect assumptions about model training approaches like neural architecture search, which automates the design of systems by finding the best model for a particular task. One energy consumption estimate for Evolve Transformers ended up 18.7 times “too high” and 88 times off in emissions, in the Google-led research team’s estimation. And publicly available calculators like ML Emissions and Green Algorithms estimate gross carbon dioxide emissions as opposed to net emissions, which could be up to 10 times lower, the paper says. “Reviewers of early [research] suggested that … any tasks run in a green datacenter simply shift other work to dirtier datacenters, so there is no net gain,” the coauthors wrote. “It’s not true, but that speculation reveals many seemingly plausible but incorrect fallacies: datacenters are fully utilized, cloud centers can’t grow, renewable energy is fixed and can’t grow, Google … model training competes with other tasks in the datacenter, training must run in all datacenters, [and] there is no business reason to reduce carbon emissions.” The coauthors evaluated the energy usage and carbon emissions of five recent large natural language processing models, using their own formulas for the calculations. They concluded that: “We believe machine learning papers requiring large computational resources should make energy consumption and carbon dioxide emissions explicit when practical,” the coauthors wrote. “We are working to be more transparent about energy use and carbon dioxide emissions in our future research. To help reduce the carbon footprint of machine learning, we believe energy usage and carbon dioxide emissions should be a key metric in evaluating models.” The thoroughness of the paper belies the conflict of Google’s commercial interests with viewpoints expressed in third-party research. Many of the models the company develops power customer-facing products, including Cloud Translation API and Natural Language API. Revenue from Google Cloud, Google’s cloud division that includes its managed AI services, jumped nearly 46% year-over-year in Q1 2021 to $4.04 billion. While the Google-led research disputes this, at least one study shows that the amount of compute used to train the largest models for natural language processing and other applications has increased 300,000 times in 6 years — a higher pace than Moore’s law. The coauthors of a recent MIT study say that this suggests that deep learning is approaching its computational limits. “We do not anticipate [meeting] the computational requirements implied by the targets … The hardware, environmental, and monetary costs would be prohibitive,” the MIT coauthors said. Even if the Google-led paper’s figures are taken at face value, the training of Google’s models produced a total of over 200 metric tons of carbon dioxide emissions. That’s equivalent to average greenhouse gas emissions from roughly 43 cars or 24 homes over the course of the year. Matching the threshold of emissions reached by training OpenAI’s GPT-3 alone would require driving a passenger vehicle just over 1.3 million miles. It’s been established that impoverished groups are more likely to experience significant environmental-related health issues, with one study out of Yale finding low-income communities and those comprised predominantly of minorities experienced higher exposure to air pollution compared to nearby white neighborhoods. A more recent study from the University of Illinois at Urbana-Champaign shows that Black Americans are subjected to more pollution from every source, including industry, agriculture, all manner of vehicles, construction, residential sources, and even emissions from restaurants. Gebru’s work notes that while some of the energy supplying datacenters comes from renewable or carbon credit-offset sources, the majority is not sourced from renewable sources, and many sources in the world aren’t carbon neutral. Moreover, renewable energy sources are still costly to the environment, Gebru and coauthors note, and datacenters with increasing computation requirements take away from other potential uses of green energy. “When we perform a risk/benefit analyses of language technology, we must keep in mind how the risks and benefits are distributed, because they do not accrue to the same people,” Gebru and coauthors wrote. “Is it fair or just to ask, for example, that the residents of the Maldives (likely to be underwater by 2100) or the 800,000 people in Sudan affected by drastic floods pay the environmental price of training and deploying ever-larger English language models, when similar large-scale models aren’t being produced for Dhivehi or Sudanese Arabic?” The Google-led paper and prior works do align on recommendations to reduce the carbon impact of models, at least on the topic of transparency. As have others, the Google coauthors call on researchers to measure energy usage and carbon dioxide emissions and publish the data in their papers. They also argue that efficiency should be an evaluation criterion for publishing machine learning research on computationally intensive models, as well as accuracy and related metrics. Beyond this, the Google-led paper calls for researchers to publish the amount of accelerator hardware they used and how much time they took to train computationally intensive models. “When developing a new model, much of the research process involves training many model variants on a training set and performing inference on a small development set. In such a setting, more efficient training procedures can lead to greater savings,” scientists at the Allen Institute for AI, Carnegie Mellon University, and the University of Washington wrote in a recent paper. “[Increasing] the prevalence of ‘green AI’ [can be accomplished] by highlighting its benefits [and] advocating a standard measure of efficiency.”"
https://venturebeat.com/2021/04/29/how-5g-is-turning-cloud-gaming-into-growth-opportunities/,How 5G is turning cloud gaming into growth opportunities,"“With shorter latency times, less jitter, and less packet loss in mobile gaming, 5G brings the ability for true cloud gaming services to become more of a reality by bringing the world closer together,” said Tim Guhl, vice president of sales at Singtel. Guhl was in conversation with Lisa Hanson, founder and president of Niko Partners, during the 12th annual GamesBeat Summit. This year, the summit’s theme is “Growing the next generation,” and Guhl explored how 5G is permanently cutting the cord, changing the world of gaming, with the potential to become a key integrator in allowing true augmented reality in game systems, cloud gaming, and more. The average network in the U.S. is 4G, and most people with internet average about 30 to 35 megabits. Additionally, cell phone towers support about 30 to 40 simultaneous connections at any given time. In the switch to 5G, the average speed jumps to about 70-80 megabits, more than double the speed, and the towers will be able to support about a million devices per square kilometer, or about one third of a square mile. “Wide area networks and the need for routers in our homes, for routers, will go away, and SIM cards to connect devices to the mobility network will become standard in laptops and TVs, not just cell phones and tablets,” Guhl added. Singtel, which has about 706 million wireless customers around the world, has been able to observe case studies across the globe. The most interesting case study is Singapore-specific, he said. Implementing new technology is far easier in Singapore than in the U.S. because of its small size, and as a result, 5G technology is completely rolled out in there. “Singapore specifically is a fantastic place to look to see where the U.S. can be in roughly five to seven years in the future,” Guhl says. There’s been major growth around the internet of things, he says, with average consumers owning upwards of five to eight mobile connected devices. In the realm of gaming, that means the number of devices that can connect directly to the network has surged, from smart watches to AR glasses, cutting the cord completely and bringing 3D gaming that much closer. 5G will also have a tremendous impact on esports, where milliseconds can make a difference between living or dying, winning or not winning the first-place purse, which continues to grow. How do you control latency and play conditions with teams separated by wide distances, especially when cloud gaming performance depends so much on geography. “Geography impacts the ability to have a seamless, real-time event,” he said. “5G is among the advancements in making sure that latency, no matter the distance, is getting better, especially for mobile game platforms like PUBG and others that are purely for mobile devices.” Content providers are starting to use cache servers in order to implement cloud gaming, setting up multiple servers in a geographic region. The gamer pings the one that’s closest, so the edge comes to them. Being able to set up multiple points of presence in a single region is quite capital-intensive; to make this a more practical strategy will require additional technology advancements. Those advancements, such as quantum communications, where distance no longer matters, are closer than most people realize, Guhl says. “In the next decade we’re going to see some significant advancements that will allow cloud computing and cloud gaming to become more mainstream opportunities for end users and the companies involved,” he said. Singtel is currently working with gaming content companies, such as Riot Games and Ubisoft, to penetrate the highly populated, very lucrative Asia Pacific market to grow their base of end users. “We’re working on what’s considered a general, nebulous term — the internet of things — in order to make this happen,” he says. “It’s a purely connected reality, where there are multiple connections, including in the gaming industry with the actual hardware, and the actual games.” The conversations they’re having, about the current technologies that will work best, and the next-gen technology to prepare for, are advancing the industry throughout the Asia Pacific region, he added. “We’re helping the video game community become a much larger and more inclusive place, throughout the region and throughout the world,” he said."
https://venturebeat.com/2021/04/29/vesper-and-chainlink-to-develop-an-oracle-for-defis-most-cited-metric-total-value-locked/,Vesper and Chainlink to Develop an Oracle for DeFi’s Most Cited Metric – Total Value Locked,"CHICAGO–(BUSINESS WIRE)–April 29, 2021– Vesper Finance, a leading DeFi ecosystem and growth engine for crypto assets, today announced that it is working with Chainlink to build a better means for establishing the U.S. dollar price of the total value locked (TVL) within the Vesper platform. Every DeFi platform’s calculation of TVL (similar to “assets under management” or AUM in conventional finance) is unique to that platform. Often, this takes the form of retrieving price data from a single source or very few. This may result in a less-accurate representation of a digital asset’s price at a particular point in time. Working with Chainlink, Vesper seeks to deliver the most accurate possible view of the total U.S.-dollar-denominated value of assets in its platform. This is accomplished via leveraging Chainlink’s time-tested decentralized oracle networks to integrate reliable data from multiple sources that exist outside of a cryptocurrency network (e.g., a digital asset’s price in fiat currency). Additionally, this data cannot be tampered with by outside parties or even the Vesper team. “On behalf of the Vesper community, we are proud to work with technology innovators and leaders such as Chainlink,” said Jordan Kruger, co-founder of Vesper. “We intend to explore more comprehensive relationships with the Chainlink community at-large, which not only includes this work on a TVL oracle but our new Vesper Grow pool for the LINK token.” Further detail is available on Vesper’s Medium post. “We look forward to supporting the creation of Vesper DeFi products by providing secure and reliable decentralized price oracles that will enable even more advanced DeFi smart contracts,” stated Sergey Nazarov, co-founder of Chainlink. “By having premium data quality and robust oracle infrastructure, Vesper Pools can both properly calculate TVL and go on to create even more advanced capabilities.” About Chainlink Chainlink is the most widely used and secure way to power universally connected smart contracts. With Chainlink, developers can connect any blockchain with high-quality data sources from other blockchains as well as real-world data. Managed by a global, decentralized community of hundreds of thousands of people, Chainlink is introducing a fairer model for contracts. Its network currently secures billions of dollars in value for smart contracts across the decentralized finance (DeFi), insurance and gaming ecosystems, among others. Chainlink is trusted by hundreds of organizations to deliver definitive truth via secure, reliable data feeds. To learn more, visit chain.link, subscribe to the Chainlink newsletter, and follow @chainlink on Twitter. About Vesper Finance Vesper Finance is a DeFi ecosystem and growth engine for crypto assets, providing a suite of yield-generating products focused on accessibility, optimization, and longevity. Vesper is dedicated to creating and supporting the conditions for DeFi’s expansion and success, to the benefit of all participants. Learn more about Vesper at www.vesper.finance and follow on Medium, Twitter, Telegram, and Discord. Vesper was built by Bloq, a leader in blockchain innovation and infrastructure.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210429005935/en/ Phil GomesVesper Financemedia+vesper@bloq.com"
https://venturebeat.com/2021/04/29/customer-experience-disruptor-dixa-expands-operations-in-the-us-to-fix-broken-customer-service-market/, Customer Experience Disruptor Dixa Expands Operations in the US to Fix Broken Customer Service Market," UiPath executive Thomas Hansen joins board and Scott Sinatra as new CRO Stellar sales team recruited across the US Move follows recent $15m acquisition  NEW YORK–(BUSINESS WIRE)–April 29, 2021– Dixa, the Nordic customer service innovator that has transformed the EMEA customer experience (CX) market, is continuing to strengthen its footprint in the US and its disruptive new perspective on customer experience. Dixa is backed by Notion Capital, Project A Ventures and SEED Capital, and is building on its recent $15m acquisition of Australian knowledge management platform Elevio by appointing Thomas Hansen, Chief Revenue Officer at UiPath, to its board of directors. It has also appointed Scott Sinatra as its new CRO, as well as opening a new headquarters in New York and recruiting teams across the North East, South East, Central and Western America. Customer service is one of the largest software categories in the world, but the market continues to operate in transactional silos and compartmentalizes customer interactions into separate channels. 8 out of 10 companies use multiple solutions to deliver an “omnichannel” experience, which limits a brand’s ability to properly connect meaningfully to their customers, damaging both agent experience and consumer confidence. Dixa’s platform tackles this by allowing consumer-facing brands to serve customers across multiple channels from a single screen including social media, chat, voice and messaging apps. The platform also uses smart routing powered by data and algorithms to allow customer queries to reach the right agents, along with relevant data and insights, in real-time. By choosing customers over tickets, the platform boasts a First Contact Resolution (FCR) rate of 75% versus the 54% industry benchmark* and is purpose built for conversational scalability. Dixa has appointed Thomas Hansen to its board of directors. Thomas brings a wealth of global, scale and hyper-growth experiences in Enterprise software having filled senior executive roles in global companies including Carbon Black (formerly NASDAQ listed, acquired by VMware), Dropbox and Microsoft, over and beyond his current role of Chief Revenue Officer at NYSE-listed UiPath. Dixa has also recruited a stellar team to lead its operations in the US. Scott Sinatra joins as Chief Revenue Officer having been key to scaling the employee engagement and retention platform Glint across Europe and Asia before its acquisition by LinkedIn in 2018. Fraser Aitken joins as Vice-President of Sales, while Shannon Franzen and Audrey Sullivan join the team to lead Western and Central US operations. The new team boasts an incredibly impressive track record in scaling innovative technology startups on a global basis and helping them break into and disrupt new markets. Mads Fosselius, CEO of Dixa commented: “Our mission has always been to build a platform for companies that love their customers. The US is the most customer-centric market in the world, but it has been held back by customer interaction technology being developed in silos and not brought together at the point of interaction. However, we are seeing a huge appetite amongst US brands to move away from the siloed, ticketed model and towards a single platform approach that can help them understand their customers in a more efficient, data-driven and conversational way. He continued: “While we have been taking over the EMEA market, we have been building a strong base of customers across the US that have realized how a platform, SaaS approach can deliver unparalleled customer service. Building out our team in the US is the next step in our journey to responding to this growing demand and ultimately winning the US market.” The Dixa platform targets “customer-centric” brands with customer-facing agents, such as scale-ups and companies in e-commerce, transport/delivery, fintech, gaming and more. Its rapidly growing customer base currently spans over 20 countries and includes US brands like Wistea, Thule, and Jessica Alba’s Honest Baby Clothing. Kevin Tucker, Director Customer Services Operations at Thule Group, added: “After switching to Dixa from one of the most successful players in this market, we’ve seen tremendous gains. Dixa offers all channels natively, which means we now have a clear overview of our customer conversations – all from one screen. Their phone support has been particularly advantageous, with the callback function reducing our abandonment rate by 5%. With Dixa’s smart routing, we can ensure that the right agent gets the right query at the right time. This has meant that agent skills are deployed more effectively, improving both agent experience and efficiency.” Scott Sinatra, Dixa’s new Chief Revenue Officer, added: “The opportunity in front of Dixa in the US is enormous. Every sector needs to connect better with their customers and there is a palpable pain point in the US that brands are simply not connecting with customers in the most efficient and agile way possible. Dixa is already helping a number of US brands transform their customer relationships and I’m excited to use this new wave of investment to disrupt the market even further and tap into the clear appetite for a non-ticketed, personalized and platform-centered approach.” ENDS * Call Centre Helper, Industry Standards for Call Centre Metrics, June 2020 About Dixa Since the global launch in 2018, Dixa has grown from just 12 to over 170 employees, with offices in Copenhagen, London, Kyiv, Berlin, Melbourne and New York, and plans to open an office in Amsterdam by the end of this year. Dixa doubled its revenues during 2020 and has raised more than $50 million in funding to date with backing from Notion Capital, Project A Ventures, and SEED Capital.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210429005863/en/ Sarah MulderFire on the Hillsmulder@fireoth.com +44 7720 401466"
https://venturebeat.com/2021/04/29/solidia-technologies-closes-78-million-fundraise-and-names-bryan-kalbfleisch-ceo/,Solidia Technologies Closes $78 Million Fundraise and Names Bryan Kalbfleisch CEO," Low-carbon cement and concrete leader attracts A-list venture investors committed to addressing climate change  PISCATAWAY, N.J.–(BUSINESS WIRE)–April 29, 2021– Solidia Technologies® today announced a $78 million fundraise and named concrete industry leader Bryan Kalbfleisch as CEO. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210429005707/en/ The funding round attracted a range of leading investors committed to advancing low-carbon solutions for industry. Imperative Ventures and Zero Carbon Partners led the round and were joined by new investors Canada Pension Plan Investment Board (CPP Investments), Breakthrough Energy Ventures, Prelude Ventures, and PIVA Capital. Existing investors John Doerr, BP, OGCI Climate Investments, and Bill Joy demonstrated their ongoing commitment by investing additional funds. By reducing CO2 emissions in the production of cement and consuming carbon in the production of concrete, Solidia significantly reduces the carbon footprint of an industry responsible for 8% of global emissions. The new funding will support the continued development and deployment of these leading technologies to accelerate the decarbonization of critical building materials industries. “Joining Solidia provides me the rare privilege of helping move cement and concrete into their next generation with higher-performing materials that are better for industry, people, and the planet,” said Bryan Kalbfleisch. “Having devoted my career to this industry, I am excited to help build a new legacy for it with the support of some of the world’s most committed leaders in advancing sustainable innovation.” Bryan brings over two decades of experience leading manufacturing operations producing concrete, asphalt, and other building materials. He comes to Solidia from Summit Materials, a leading aggregates-based construction materials company, where he served as president of both its Texas Region and Houston-based Alleyton Resource. He also previously served as president of Fayetteville, Ark.-based APAC Central for Oldcastle (CRH), North America’s largest manufacturer of building products and materials. His career was launched in the ready-mix concrete division of Central Pre-Mix Concrete Company, a Spokane, Wash.-based firm that was sold to Oldcastle in 1997. “Solidia creates value while lowering industrial carbon emissions and advancing solutions that use captured CO2, which is unique in the industry and aligns well with our long-term investment focus,” said Bruce Hogg, Managing Director and Head of the Sustainable Energy Group at CPP Investments. “We are pleased to have a leader of Bryan’s caliber taking Solidia to the next stage of development.” Participants in this fundraise join Solidia’s other existing investors including Kleiner Perkins, BASF Venture Capital, LafargeHolcim, Total Carbon Neutrality Ventures, Air Liquide Venture Capital (ALIAD), and other private investors. About Solidia Technologies® Based in Piscataway, N.J. (USA), Solidia Technologies® helps manufacturers produce superior building and construction materials using low-carbon cement and concrete.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210429005707/en/ Ellen Yui, YUI+Company, Inc.M: 301-332-4135ellenyui@yuico.com"
https://venturebeat.com/2021/04/29/microsoft-provides-progress-update-on-open-data-campaign/,Microsoft provides progress update on Open Data Campaign,"In April 2020, Microsoft launched the Open Data Campaign, an effort to furnish countries and companies with data and make it easier to use, share, and collaborate with that data. Today, the company shared the progress it’s made toward those goals, as well as the focus areas for the remaining year ahead. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why organizations have taken an interest in partnerships and technologies that help to ingest, understand, organize, share, and act on digital content from multiple sources. Microsoft says it launched nine data collaborations with an emphasis on addressing challenges in the areas of sustainability, health, equality, and inclusion. That’s roughly halfway toward the company’s aim of 20 new data collaborations by 2022. Where possible, Microsoft says it also opened and shared its own data, including U.S. broadband usage data and Bing Maps aerial and streetside imagery. “Using … lessons learned and building on the progress we’ve made alongside our partners  we plan to spend the next year focused on the practical aspects of data sharing and making the process easier,” Jennifer Yokoyama, VP of the IP group at Microsoft, said in a blog post. “Many organizations want to do more around open data and data sharing, but when it comes to the practical aspects of how to do it, they often don’t know where to start.” To this end, Microsoft this morning published course materials about how organizations can use open source resources to guide their data reuse strategies. And the company says that the Open Data Policy Lab, an initiative with support from Microsoft and New York University’s GovLab, will shift its mission to scaling data stewardship guidance for public and private sectors. A new Data Stewardship Academy will be designed for a broad reach, and the Open Data Policy Lab will launch a project, called Open Cities, to build community and share insights among cities using data to drive change. Beyond this, Microsoft says it’ll continue to identify and assist with data collaborations to address societal issues. Together with the Open Data Institute (ODI), Microsoft says it’s launching three collaborations centered on one of six priority areas: Microsoft also says it’ll be announcing an open call for a new Peer Learning Network for data collaborations to participate and learn from each other. Another focus area of the ODI going forward will be to build momentum through case studies across sectors that highlight the value of opening data. “To fully realize the benefits of data, policymakers must work with industry, academia, and civil society to develop incentives, infrastructure, and mechanisms to responsibly share public and private sector data within — and across — organizational and national boundaries that are in line with the rule of law and safeguard human rights, while allowing for effective data re-use for innovation,” Yokoyama continued. “In addition to properly maintained and funded national open data programs, data governance frameworks create trust in the integrity of the data sharing ecosystem by ensuring that the benefits of data are equitably shared and by providing adequate safeguards to protect cybersecurity, human rights, and privacy.” Microsoft stands to benefit from data collaboration on its Azure cloud platform, of course. As of the most recent fiscal quarter, the company’s biggest growth driver — and the component of its business that pushed its market capitalization close to $2 trillion — was Azure. For Q3 2020, the company saw $15.12 billion in revenue from the segment, a 23% year-over-year increase."
https://venturebeat.com/2021/04/29/cibc-innovation-banking-provides-fund-banking-solutions-to-yaletown-partners-inc/,CIBC Innovation Banking Provides Fund Banking Solutions to Yaletown Partners Inc.,"TORONTO & VANCOUVER, British Columbia–(BUSINESS WIRE)–April 29, 2021– CIBC Innovation Banking is pleased to announce it has provided financing solutions, including a Capital Call Line of Credit to Yaletown Partners Inc. (“Yaletown”) for its latest fund, the Innovation Growth Fund (“IGF”). The capital call facility provides IGF with the flexibility to make investments in portfolio companies prior to calling capital from the fund’s limited partners. With over $250 million in active capital across multiple funds, Yaletown invests in leading emerging growth companies across Canada with a focus on software, data and device technologies. Yaletown invests in technology companies that are driving climate-resilient growth and enabling enterprise clients to make better decisions that lead to the consumption of fewer resources. IGF leverages Yaletown’s pan-Canadian platform and is focused on the Digital Transformation and Intelligent industries. The fund finances and supports the disruption and modernization of traditional industries through digitalization, transformation and sustainability. “Yaletown is a long-standing Venture Capital firm in Canada with a proven track record of helping emerging growth companies scale,” said Rob Rosen, Managing Director in CIBC Innovation Banking’s Toronto office. “We are proud to strengthen our relationship with their team and look forward to continuing to support the needs of their portfolio investments.” “CIBC Innovation Banking is a valued team, supporting our business with deep experience in providing credit and other facilities to many components of the Canadian technology ecosystem,” noted Hans Knapp, Co-Founder and General Partner of Yaletown. “We look forward to leveraging our relationship with CIBC Innovation Banking to bring additional creative solutions to our portfolio companies.” About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Yaletown Partners Yaletown Partners is a leading Canadian investor dedicated to closing the scale-up capital gap and focused on the Intelligent Industry opportunity. We invest in emerging-growth companies that enhance sustainability and productivity for industrial and enterprise customers. Our investments enable the application of data and technologies to digitally transform traditional industries, drive innovation, create operational efficiencies and reduce the impact of climate change. In 2017, Yaletown received the CVCA’s Venture Capital Deal of the Year award for its investment in BitStew, Canada’s largest venture financed exit of 2016. Backed by leading institutional investors, including pension funds, and a network of successful technology entrepreneurs, Yaletown has offices in Vancouver, Calgary, Edmonton, Toronto and Montreal. For more information, please visit www.yaletown.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210429005174/en/ CIBC: Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609Yaletown: Ashton Arsenault, Ashton.Arsenault@crestviewstrategy.com, 613-914-1035"
https://venturebeat.com/2021/04/29/terratrue-which-brings-privacy-by-design-to-product-teams-nabs-15m/,"TerraTrue, which brings privacy-by-design to product teams, nabs $15M","The growing array of data privacy regulations — including GDPR in Europe and CCPA in California, not to mention rising data security expectations from customers across industries — has opened the door to a swathe of startups aimed at making it easier for businesses to manage and automate their privacy programs. This is why San Francisco-headquartered TerraTrue went to market last year with a “privacy-by-design” ethos that helps companies proactively manage their privacy programs before a new product or feature ships. To accomplish this, TerraTrue essentially unifies product development with privacy standards. “TerraTrue can power a fast, scalable privacy program because it’s purposefully designed to bring privacy into the product-development lifecycle,” COO and cofounder Chris Handman told VentureBeat. “That’s a radically different approach to how legacy solutions have built their products. They design tools for compliance teams, not product teams or developers.” In its short tenure (TerraTrue exited beta in the third quarter of 2020), the startup has amassed a reasonable roster of customers that includes freshly IPO’d ecommerce giant Wish and VC-backed photo app maker VSCO. To help its platform reach more businesses around the world, the company today announced it has raised $15 million in a series A round of funding led by 3L, with participation from Chris Sacca and Anthos Capital. Founded in 2019, TerraTrue is the brainchild of Handman and CEO Jad Boutros, who worked on security at Google before joining Snap (and then Snapchat) in 2014 as director of information security and later served as chief security officer. Handman joined Snap in 2014 as general counsel. The duo were brought in shortly after a major hack compromised the data of millions of Snapchat users. This breach was followed by a settlement with the Federal Trade Commission (FTC), which alleged that Snapchat had deceived its users about the amount of personal data it collected and the security measures it had in place. Now, having developed a rigorous privacy program that allowed Snap to scale quickly while adhering to all applicable laws and submit to regular audits, they are offering a similar platform any company can use. “Our time at Snap — and before that at Google — taught us the basic principle that guides TerraTrue’s product design,” Boutros explained. “To do privacy right and to ship features on time, privacy must be a seamless part of product development, not an afterthought done in isolation. That ensures consistency, promotes timely guidance and feedback that won’t jeopardize a sprint cycle, and minimizes complexity as a company grows.” A slew of data privacy management and compliance platforms have emerged in recent years. This month alone we’ve seen OneTrust close a $210 million round of funding and BigID lock down $30 million, while last month DataGrail secured $30 million to help enterprises manage data privacy requests. TerraTrue is tackling the privacy problem from a slightly different perspective — rather than focusing on data that has already been collected, it’s targeting predeployment privacy compliance. “Whether it’s managing cookie consents, responding to data subject access requests, or mapping data that a company has been sharing with third parties, the focus [from other companies in this space] is on data that’s already being collected, stored, and processed,” Handman said. “These are all worthwhile tools, but they’re also reactive tools. They don’t address privacy risks, offer guidance, or ensure new features get reviewed before a company ships them.” By contrast, TerraTrue tracks new features as they’re being developed and surfaces potential privacy risks in real time while issuing recommendations and automating many of the processes required to address pressing issues. TerraTrue also tracks all regulations that are scheduled to come out so that by the time a particular statute is in place, companies are equipped to stay on the right side of the law. “In privacy parlance, this predeployment work is known as ‘privacy-by-design,” Handman added. “Simply stated, it’s the idea that companies should consider privacy risks, edge cases, and safeguards before they ship features and potentially introduce mischief to their consumers.” Of course, many companies try to build privacy into their products from the outset, but that’s incredibly difficult to execute while continuously pushing out new features and products, something most modern “agile” software development principles promote. “Companies lacked proper tooling to pull this off — instead, most companies repurposed spreadsheets, ad-hoc pings through Slack, or email and Google Docs to try to understand what features product teams are building, how they map onto global privacy rules, and how they should address shortfalls,” Handman explained. “But that work is painfully manual, repetitive, and slow.” TerraTrue integrates with many of the tools companies use, including GitHub, Jira, Google Drive, and Slack, and is designed to “keep privacy and product teams in sync” without slowing down the product development process. “Everything we do at TerraTrue works to seamlessly integrate privacy into the product-development life cycle,” Handman said. “And integrations are one of the most powerful ways to deliver that experience to customers.” For instance, a project manager might launch a ticket in Jira, Atlassian’s project management product for software developers, and TerraTrue can instantly flag whether this will have any privacy implications and kickstart a review process, including issuing updates and notifications to all the relevant stakeholders in Jira, Slack, or elsewhere. All comments and responses, regardless of their source, are collated and centralized in TerraTrue. Although TerraTrue offers many prebuilt integrations that are available out of the box, it also allows customers to develop custom workflows, to, for example, develop independent review processes that funnel into their product development. “For example, a company might create a vendor security questionnaire but craft it so that TerraTrue will send it to the relevant team members only when a feature would onboard a new vendor,” Boutros said. “What’s more, TerraTrue lets the company quickly triage work by assigning risk scores to responses inside the workflows.” TerraTrue had previously raised $4.5 million, and with a fresh $15 million, the company is well-financed to support businesses of any size as they face an ever-growing litany of privacy regulations. While TerraTrue might appeal to smaller startups without the resources to keep on top of everything themselves, enterprises also have better things to focus on than aligning every new feature they build with the latest legislation to come out of Switzerland. Boutros was careful not to give too much away, but he pointed to two strategic priorities on the company’s product roadmap. “One is to support the needs of very large enterprises that have developed their own proprietary toolchains for managing development and deployment activities over years,” he explained. In real terms, this means building external APIs that enable these businesses to integrate TerraTrue into their own custom tools while also giving them the flexibility to manage the TerraTrue platform across disparate teams. Boutros also said companies can expect to see more integrations with other popular third-party productivity and development tools so that “TerraTrue works even better with the way organizations currently work.”"
https://venturebeat.com/2021/04/29/ast-private-company-solutions-seals-partnership-integration-with-texture-capital/,AST Private Company Solutions Seals Partnership & Integration with Texture Capital,"MENLO PARK, Calif. & NEW YORK–(BUSINESS WIRE)–April 29, 2021– AST Private Company Solutions, Inc. (AST PCS) and Texture Capital Inc. announce a new strategic partnership to facilitate compliant transactions in private company shares by integrating Texture’s SEC-approved Alternative Trading System with AST PCS’s Astrella® platform. Astrella was developed by AST PCS, the Silicon Valley-based business unit of ownership data management leader AST, using transformative technology to build a new cloud-based software-as-a-service (SaaS) platform. The solution brings together private blockchain technology, artificial intelligence (AI), and predictive analytics to catapult the space forward. Astrella is built with an open API ecosystem seamlessly integrating with leading providers like Texture Capital, whose blockchain technology and smart contracts streamline the current market structure for private placements. “AST is an acknowledged leader in corporate registrar services, and Astrella is transforming the experience for private companies seeking cap table management and ownership tracking solutions,” Texture CEO Richard Johnson states. “AST PCS’s willingness to build strong partnerships with complementary solution providers like Texture is key to private companies’ ability to enjoy integrated best-in-class solutions that work seamlessly together. We are excited to announce this partnership and excited to partner with AST PCS as part of their extended private companies ecosystem.” As the first step in this partnership, AST PCS and Texture have developed a proof-of-concept integration, allowing Astrella users to tokenize their cap table and enable compliant trading of their shares as digital securities on the Texture ATS. AST PCS President Carine Schneider adds, “We are delighted to partner with Texture Capital to provide our clients with access to the institutional marketplace for private capital. Blockchain technology has tremendous potential to improve market structure by enhancing transparency, streamlining workflows, and automating processes. Astrella is the only cap table management provider with blockchain technology. Combined with Texture Capital’s use of blockchain, Astrella allows issuers to manage and track the ownership history of each share from the time they are issued until exit.” About Texture Capital Texture Capital, the institutional marketplace for private capital, is a technology-driven marketplace for institutions and issuers to more efficiently and directly participate in the private markets, seeking to improve liquidity, transparency, and access. We leverage blockchain technology and smart contracts to streamline the current market structure for private placements; supporting the primary issuance and secondary trading of digital securities. Texture Capital Inc. is a FINRA member broker-dealer operating SEC-registered ATS. Please visit https://texture.capital for more information and to stay informed of future updates. About AST Private Company Solutions, Inc. Founded in 2019, AST PCS is an AST affiliate serving private companies worldwide. Astrella, a cloud-based SaaS solution, allows private companies to manage their ownership data, including the cap table and employee equity plans, and connect directly with related service providers to support efficient workflow and access investors, advisors and employees. Visit astrella.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210429005206/en/ Richard JohnsonTexture Capital+1 646-979-8558Richard@texture.capital Heather DopsonAST PCS+1-650-272-4560heather@astrella.com"
https://venturebeat.com/2021/04/29/a-new-pool-of-players-is-changing-ua/,A new pool of players is changing UA,"This article is part of a Gaming Insights series paid for by Facebook. When it comes to games app advertising, the key to continued growth is user acquisition and audience expansion. But with such a vast and diverse audience, it can be tricky to know where to begin. Many games advertisers also face the challenge of finding new players among an already over-saturated market. Similarly, creative approaches might not be varied enough to motivate someone to download your game, while changes and disruptions within the advertising landscape can make personalization more difficult. The answer? It’s time to tap into gamer motivations. Over the past few years, many gaming companies have been targeting their advertising campaigns to the same group of audiences. It ends up slowing down their growth, as everybody is trying to grab this rather small group of gamers, so it gets harder to convert them. Expanding beyond this overfished pond becomes critical to gaming companies. Gaming has become a mainstream culture, with more and more people playing. According to research, 86% of internet users now play mobile games, and 80% of self-described “non-gamers” actually play games on their phones. This signals a much larger pool of potential players that gaming companies can tap into.  Source: GlobalWebIndex Q3 2018 Base: 113,392 Internet Users Aged 16-64 Furthermore, the COVID-19 pandemic drew millions more to gaming. We’ve seen an influx of new mobile gamers around the world as well as reengaged lapsed players, with many of them embracing gaming as a new outlet for much-needed connection and entertainment in isolation.  Source: “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 6,238 mobile gamers ages 18+ across DE, KR, UK, US, Jul/Oct 2020) And this audience is more diverse than ever, with 50% of recent and established mobile gamers in the U.S. identifying as female, 43% of established mobile gamers 45 or older, and 39% of recent adopters under the age of 34. All these new players create new opportunities for game growth, and new ways to evolve your digital advertising strategy and creative approach.  Source for all stats: online survey responses of 1,434 mobile gamers in US, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, KR, UK, US, VN Jul-Oct 2020) This diversity includes the reasons why people play as well as who is playing. Recent surveys have shown the reasons people play vary greatly, which is why it’s so important to understand their motivations, and we’re going to show you how to do just that.  Source: Online survey responses of 728 established mobile gamers in the US, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, KR, UK, US, VN Jul-Oct 2020) In 2020, we collaborated with 16 global gaming advertisers to build an advertising creative framework called The Big Catch. We discovered that by experimenting with different ads based on player motivations, you can expand the pond in which you’re fishing for new players. This was effective in converting different players in 100% of our tests, and attracted and converted higher-value audiences(1). Big Catch digital ad creatives also had 26% higher Ads Quality Ranking than control creatives(2), meaning ads performed better in the auction and saw greater distribution at a reasonable cost.  We have identified eight player motivations, which we’ve explored in great detail in the free-to-download Big Catch Playbook. They are:  That’s not to say these are the only eight motivations, but they provide a solid starting point to begin examining your games, and how to use your ad creatives to attract players with different motivations. Take Relaxation for example. This applies to players who just want to kill time, relax or calm down. 62% of established mobile gamers in the U.S. play to relieve stress(3). We’re talking about gameplays like puzzles, repetition, and zen mode. Some players are motivated by constantly making progress in the game, whether building or improving things, while some just love to connect and bond with friends. This Merge Dragons ad below showcases relaxation by positioning its merging gameplay as a therapy — clean your land to clear your mind.  When it comes to escapism, we know that 30% of gamers in the U.S.(4), 30% in the U.K.(5) and 17%(6) in Korea play to immerse themselves in a character/world. Players who are motivated by escapism often seek distraction and relief from possibly a potentially unpleasant reality. Role-playing, fantasy, and alternate worlds all speak to this motivation. Look for the extraordinary in your game and show wild adventures and non-humanoid characters to appeal to players looking to escape. As this ad creative for Happy Color demonstrates, upbeat music and scenes becoming washed with colour. Even leading with a positive reason to play the game “When days are darker, fill them with color.”  Discovery is also a powerful motivator for curious gamers, who play to learn something new. Creative that shows gameplay with big maps, multiple worlds, and hidden objects can spark a potential player’s interest. Consider showing side quests, enigmas, and escape rooms to instil the thrill of discovery. This creative for Klondike shows how to pique their curiosity. After a brief intro, it shows sweeping landscapes, prompting the viewer to “Explore and discover many secrets and breathtaking adventures.”  This innovative new framework for games advertising will help you significantly expand your reach and impact. Gaming companies of any size can benefit from the Big Catch advertising approach. It’s an ongoing process of learning what works and why, and incorporating these learnings into your everyday advertising methods. To get started, get our free-to-download, in-depth Big Catch Playbook to learn more about the eight player motivations, how to test and learn, and key considerations, as well as info on expert regional partners who can help you bring this to life if needed. Sources 1. Source: Chi-squared test of 16 Gaming A/B tests2. Source: Creative Shop analysis of ads quality score, new vs. BAU creatives, 18 gaming AB tests.3. Source: Online survey responses of 728 established mobile gamers in the US, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, KR, UK, US, VN Jul-Oct 2020)4. Source: Online survey responses of 728 established mobile gamers in US, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, JP, KR, UK, US, VN Jul–Oct 2020).5. Source:  Online survey responses of 1166 established mobile gamers in GB, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, JP, KR, UK, US, VN Jul–Oct 2020).6. Source: Online survey responses of 1318 established mobile gamers in KR, from “Mobile Gaming Behavior Post COVID-19” by Interpret (Facebook IQ-commissioned online survey of 13,246 mobile gamers ages 18+ across BR, CA, DE, FR, JP, KR, UK, US, VN Jul–Oct 2020). Rodrigo Zannin is Creative Industry Lead, Creative Shop at Facebook VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/29/huawei-trained-the-chinese-language-equivalent-of-gpt-3/,Huawei trained the Chinese-language equivalent of GPT-3,"For the better part of a year, OpenAI’s GPT-3 has remained among the largest AI language models ever created, if not the largest of its kind. Via an API, people have used it to automatically write emails and articles, summarize text, compose poetry and recipes, create website layouts, and generate code for deep learning in Python. But GPT-3 has key limitations, chief among them that it’s only available in English. The 45-terabyte dataset the model was trained on drew exclusively from English-language sources. This week, a research team at Chinese company Huawei quietly detailed what might be the Chinese-language equivalent of GPT-3. Called PanGu-Alpha (stylized PanGu-α), the 750-gigabyte model contains up to 200 billion parameters — 25 million more than GPT-3 — and was trained on 1.1 terabytes of Chinese-language ebooks, encyclopedias, news, social media, and web pages. The team claims that the model achieves “superior” performance in Chinese-language tasks spanning text summarization, question answering, and dialogue generation. Huawei says it’s seeking a way to let nonprofit research institutes and companies gain access to pretrained PanGu-α models, either by releasing the code, model, and dataset or via APIs. In machine learning, parameters are the part of the model that’s learned from historical training data. Generally speaking, in the language domain, the correlation between the number of parameters and sophistication has held up remarkably well. Large language models like OpenAI’s GPT-3 learn to write humanlike text by internalizing billions of examples from the public web. Drawing on sources like ebooks, Wikipedia, and social media platforms like Reddit, they make inferences to complete sentences and even whole paragraphs. Akin to GPT-3, PanGu-α is what’s called a generative pretrained transformer (GPT), a language model that is first pretrained on unlabeled text and then fine-tuned for tasks. Using Huawei’s MindSpore framework for development and testing, the researchers trained the model on a cluster of 2,048 Huawei Ascend 910 AI processors, each delivering 256 teraflops of computing power. To build the training dataset for PanGu-α, the Huawei team collected nearly 80 terabytes of raw data from public datasets, including the popular Common Crawl dataset, as well as the open web. They then filtered the data, removing documents containing fewer than 60% Chinese characters, less than 150 characters, or only titles, advertisements, or navigation bars. Chinese text was converted into simplified Chinese, and 724 potentially offensive words, spam, and “low-quality” samples were filtered out. One crucial difference between GPT-3 and PanGu-α is the number of tokens on which the models trained. Tokens, a way of separating pieces of text into smaller units in natural language, can be either words, characters, or parts of words. While GPT-3 trained on 499 billion tokens, PanGu-α trained on only 40 billion, suggesting it’s comparatively undertrained.  In experiments, the researchers say that PanGu-α was particularly adept at writing poetry, fiction, and dialog as well as summarizing text. Absent fine-tuning on examples, PanGu-α could generate poems in the Chinese forms of gushi and duilian. And given a brief conversation as prompt, the model could brainstorm rounds of “plausible” follow-up dialog. This isn’t to suggest that PanGu-α solves all of the problems plaguing language models of its size. A focus group tasked with evaluating the model’s outputs found 10% of them to be “unacceptable” in terms of quality. And the researchers observed that some of PanGu-α’s creations contained irrelevant, repetitive, or illogical sentences. The PanGu-α team also didn’t address some of the longstanding challenges in natural language generation, including the tendency of models to contradict themselves. Like GPT-3, PanGu-α can’t remember earlier conversations, and it lacks the ability to learn concepts through further conversation and to ground entities and actions to experiences in the real world. “The main point of excitement is the extension of these large models to Chinese,” Maria Antoniak, a natural language processing researcher and data scientist at Cornell University, told VentureBeat via email. “In other ways, it’s similar to GPT-3 in both its benefits and risks. Like GPT-3, it’s a huge model and can generate plausible outputs in a variety of scenarios, and so it’s exciting that we can extend this to non-English scenarios … By constructing this huge dataset, [Huawei is] able to train a model in Chinese at a similar scale to English models like GPT-3. So in sum, I’d point to the dataset and the Chinese domain as the most interesting factors, rather than the model architecture, though training a big model like this is always an engineering feat.” Indeed, many experts believe that while PanGu-α and similarly large models are impressive with respect to their performance, they don’t move the ball forward on the research side of the equation. They’re prestige projects that demonstrate the scalability of existing techniques, rather, or that serve as a showcase for a company’s products. “I think the best analogy is with some oil-rich country being able to build a very tall skyscraper,” Guy Van den Broeck, an assistant professor of computer science at UCLA, said in a previous interview with VentureBeat. “Sure, a lot of money and engineering effort goes into building these things. And you do get the ‘state of the art’ in building tall buildings. But there is no scientific advancement per se … I’m sure academics and other companies will be happy to use these large language models in downstream tasks, but I don’t think they fundamentally change progress in AI.” Even OpenAI’s GPT-3 paper hinted at the limitations of merely throwing more compute at problems in natural language. While GPT-3 completes tasks from generating sentences to translating between languages with ease, it fails to perform much better than chance on a test — adversarial natural language inference — that tasks it with discovering relationships between sentences. The PanGu-α team makes no claim that the model overcomes other blockers in natural language, like answering math problems correctly or responding to questions without paraphrasing training data. More problematically, their experiments didn’t probe PanGu-α for the types of bias and toxicity found to exist in models like GPT-3. OpenAI itself notes that GPT-3 places words like “naughty” or “sucked” near female pronouns and “Islam” near terms like “terrorism.” A separate paper by Stanford University Ph.D. candidate and Gradio founder Abubakar Abid details the inequitable tendencies of text generated by GPT-3, like associating the word “Jews” with “money.” Among others, leading AI researcher Timnit Gebru has questioned the wisdom of building large language models, examining who benefits from them and who’s disadvantaged. A paper coauthored by Gebru earlier this year spotlights the impact of large language models’ carbon footprint on minority communities and such models’ tendency to perpetuate abusive language, hate speech, microaggressions, stereotypes, and other dehumanizing language aimed at specific groups of people. In particular, the effects of AI and machine learning model training on the environment have been brought into relief. In June 2020, researchers at the University of Massachusetts at Amherst released a report estimating that the amount of power required for training and searching a certain model involves the emissions of roughly 626,000 pounds of carbon dioxide, equivalent to nearly 5 times the lifetime emissions of the average U.S. car. While the environmental impact of training PanGu-α is unclear, it’s likely that the model’s footprint is substantial — at least compared with language models a fraction of its size. As the coauthors of a recent MIT paper wrote, evidence suggests that deep learning is approaching computational limits. “We do not anticipate that the computational requirements implied by the targets … The hardware, environmental, and monetary costs would be prohibitive,” the researchers said. “Hitting this in an economical way will require more efficient hardware, more efficient algorithms, or other improvements such that the net impact is this large a gain.” Antoniak says that it’s an open question as to whether larger models are the right approach in natural language. While the best performance scores on tasks currently come from large datasets and models, whether the pattern of dumping enormous amounts of data into models will pay off is uncertain. “The current structure of the field is task-focused, where the community gathers together to try to solve specific problems on specific datasets,” she said. “These tasks are usually very structured and can have their own weaknesses, so while they help our field move forward in some ways, they can also constrain us. Large models perform well on these tasks, but whether these tasks can ultimately lead us to any true language understanding is up for debate.” The PanGu-α team’s choices aside, they might not have long to set standards that address the language model’s potential impact on society. A paper published by researchers from OpenAI and Stanford University found that large language model developers like Huawei, OpenAI, and others may only have a six- to nine-month advantage until others can reproduce their work. EleutherAI, a community of machine learning researchers and data scientists, expects to release an open source implementation of GPT-3 in August. The coauthors of the OpenAI and Stanford paper suggest ways to address the negative consequences of large language models, such as enacting laws that require companies to acknowledge when text is generated by AI — perhaps along the lines of California’s bot law. Other recommendations include: The consequences of failing to take any of these steps could be catastrophic over the long term. In recent research, the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism claims that GPT-3 could reliably generate “informational” and “influential” text that might radicalize people into violent far-right extremist ideologies and behaviors. And toxic language models deployed into production might struggle to understand aspects of minority languages and dialects. This could force people using the models to switch to “white-aligned English,” for example, to ensure that the models work better for them, which could discourage minority speakers from engaging with the models to begin with. Given Huawei’s ties with the Chinese government, there’s also a concern that models like PanGu-α could be used to discriminate against marginalized peoples including Uyghurs living in China. A Washington Post report revealed that Huawei tested facial recognition software that could send automated “Uighur alarms” to government authorities when its camera systems identified members of the minority group. We’ve reached out to Huawei for comment and will update this article once we hear back. “Humans are also full of biases and toxicity, so I don’t think learning like a human is a solution to these problems,” Antoniak said. “Scholars think that perhaps we should try to better model how humans learn language — [at least] in relation to language understanding, not toxicity. It would be possible to understand language and still be very toxic, after all.”"
https://venturebeat.com/2021/04/29/ai-powered-cybersecurity-platform-vectra-ai-raises-130m/,AI-powered cybersecurity platform Vectra AI raises $130M,"San Jose, California-based cybersecurity startup Vectra AI today announced it has raised $130 million in a funding round that values the company at $1.2 billion. Vectra says the investment will fuel the company’s growth through expansion into new markets and countries. According to Markets and Markets, the security orchestration, automation, and response (SOAR) segment is expected to reach $1.68 billion in value this year, driven by a rise in security breaches and incidents and the rapid deployment and development of cloud-based solutions. Data breaches exposed 4.1 billion records in the first half of 2019, Risk Based Security found. This may be why 68% of business leaders in a recent Accenture survey said they feel their cybersecurity risks are increasing. Vectra was founded in 2012 by Hitesh Sheth, and the company provides AI-powered network detection and response services. Vectra’s platform sends security-enriched metadata to data lakes and security information and event management (SIEM) systems while storing and investigating threats in this enriched data. The aforementioned metadata is wide-ranging but includes patterns, precursors, account scores, saved searches, host scores, and campaigns. It’s scraped from sensors and processing engines deployed across cloud environments, where the sensors record metrics from traffic and ingest logs and other external signals. AI is a core component of Vectra’s product suite. Algorithms suss out and alert IT teams to anomalous behavior from compromised devices in network traffic metadata and other sources, automating cyberattack mitigation. Specifically, Vectra uses supervised machine learning techniques to train its threat detection models and unsupervised techniques to identify attacks that haven’t been seen previously. Vectra’s data scientists build and tune self-learning AI systems that complement the metadata with key security information. “For us, it all starts with collecting the right data because attack behaviors always vary. We’re continuously creating machine learning models for any type of new or current threat scenario,” Sheth told VentureBeat via email. “AI-based security defenses are the right tool for modern network defenders, not because current threats will become some dominant force, but because they are transformative in their own right.” Signaling its global ambitions, in August 2019 Vectra opened a regional headquarters in Sydney, Australia. Last July, the company launched a range of new advisory and operational cybersecurity services, weeks after revamping its international channel partner program. A growing number of cloud users are suffering malicious account takeovers, according to a survey conducted by Vectra. Nearly 80% of respondents claimed to have “good” or “very good visibility” into attacks that bypass perimeter defenses like firewalls. Yet there was a contrast between opinions of management-level respondents and practitioners, with managers exhibiting greater confidence in their organizations’ defensive abilities. “The pandemic has caused a further shift toward the cloud, even for organizations that were previously cloud adverse. Companies needed to prioritize the health and safety of their employees, which in many cases meant a shift to remote work,” Sheth said. “However, the only way to keep teams connected and productive was to further adopt cloud applications that enable collaboration from anywhere. For security teams, this means finding new solutions to protect assets and users because traditional network security doesn’t translate to securing a dispersed workforce that has adopted cloud technologies … Vectra can automate threat detection and investigation, provide visibility, and audit remote endpoint security posture to make sure users and company assets are secure.” Vectra, which has 375 employees and claims a 100% compound annual growth rate in 2020, counts Texas A&M University and Tribune Media Group among its customer base. In February, the company closed the strongest quarter in its history. “To me, this funding round confirms today’s cybersecurity capital markets are rewarding the most effective and innovative technology — not just the best pitch,” Sheth said. “Contrary to flashy Hollywood headlines about some Skynet-like AI hacker coming to get you, actual human attackers are far more clever than any contemporary offensive AI systems. This is in part because AI systems conform to a series of ‘rules,’ and as every human hacker knows, rules are made to be broken.” Sheth added “The most likely scenario is that some AI techniques merely make it into the toolkit of human adversaries, such as incorporating natural language AI into large-scale phishing attacks. We shouldn’t downplay the impact of a good phishing campaign, but if this is the sum total of what your C-suite is preparing for, you have your work cut out [for you]. Decisions about which AI cybersecurity solution to commit to should be driven based on outcome-based evaluations. This means selecting functional, rather than purely ornamental, solutions.” Blackstone Growth led Vectra’s latest round of funding. Existing investors also participated, bringing the company’s total raised to over $350 million. Vectra previously nabbed $100 million in a growth equity round led by TCV."
https://venturebeat.com/2021/04/29/cloud-storage-startup-wasabi-nabs-112m-plans-new-datacenters/,"Cloud storage startup Wasabi nabs $112M, plans new datacenters","Cloud data storage company Wasabi today announced that it raised $112 million in series C financing, bringing its total raised to $219 million. The company says that the funding will be put toward expanding its network of resellers, partners, and distributors as it opens new datacenters, grows its team, and builds its brand via new marketing initiatives . With cloud data storage growing 60% year over year, low-cost, reliable, and scalable storage services are in demand by a range of organizations. As of 2020, around 50% of all corporate data was stored in the cloud, according to Statista. And a 2018 report from Domo estimated that humans were creating 2.5 quintillion bytes (or 2.5 exabytes) of data per day, a number that has only increased since. Launched by Carbonite cofounders David Friend and Jeff Flowers in 2017, Boston-based Wasabi offers what it calls “hot cloud storage,” a cloud object storage technology that provides an interface to use with storage apps, gateways, and other platforms. The company claims its offering is faster than traditional frequent-access storage products while treating all data equally and making it readily accessible. Wasabi supports three types of data: hot data, active archive “cool data,” and inactive archive cool data. Hot data is data that needs to be readily available to the operating system or app with which it was created. Active archive cool data is considered too valuable to discard, but is only accessed occasionally. And inactive archive cool data is seen as essential to save but retrieved infrequently, typically for regulation and compliance reasons. Wasabi, which doesn’t charge fees for egress or API requests (i.e., when data leaves the network in transit), claims its storage services work out to one-fifth of the cost of Simple Cloud Storage (S3) on Amazon Web Services (AWS). But as developer Robert Hook notes in a blog post, it’s difficult to perform a direct cost comparison because the pricing structure for Wasabi and AWS differs. AWS provides a range of options for different use cases and access patterns, levying fees on transferring objects in and out of storage. Wasabi’s pay-as-you-go pricing model is simpler, but customers who store objects are charged the full amount even if they delete them. Every object has a retention period of 90 days such that when an object is deleted, it’s moved into a deleted object store and remains recoverable for three months. Wasabi customers are charged the same for the deleted store as for their active store. “[For example], if you delete the 100TB of storage in less than 23 days, Wasabi will be more expensive than AWS S3,” Wasabi explains on its website. “However, if you store the 100TB of data for more than 23 days, Wasabi will be less expensive than AWS S3 … In summary, if you have data that you only need to keep for short periods of time before deletion, then it may be more cost effective for you to keep this data in AWS.” Wasabi’s pay-as-you-go pricing model is $5.99 per TB per month. The company also offers a reserved capacity storage pricing model with a 30-day storage retention policy that allows customers to purchase 50TBs or more capacity for one-, three-, or five-year terms. The structure evidently appeals to some companies, as Wasabi says it has over 15,000 businesses storing “hundreds of petabytes” of backup, surveillance, medical imaging, life science, education, genomics, AI and machine learning, television, movie, and government data on its servers. Revenue grew by 5 times in 2019, and Q1 growth that year was in excess 40% compared with Q1 2018. The real trick for Wasabi will be maintaining competitiveness in a cloud storage market that’s projected to grow to $137.3 billion in value by 2025. Beyond AWS, Wasabi counts Google Cloud Platform and Microsoft Azure among its competitors, as well as Cohesity, Datrium, Reduxio, and others. One rival, Rubrik, which offers cloud-based data and storage management, has raised over $292 million since launching in 2014. “With revenues tripling for each of the last 3 years and over $100 million of new investment led by one of the world’s largest and most prestigious financial institutions, our customers will know that their data is in the hands of a reliable, fast-growing company with the substantial resources to meet their growing needs,” Friend said in a press release. “Storing the world’s data in the cloud is one of the biggest opportunities in the IT industry, and we are now well positioned to secure a leadership role in the evolution of the cloud.” Fidelity Management & Research Company led Wasabi’s series C announced today, which had participation from existing investors. It follows on the heels of $27.5 million in debt financing announced in January."
https://venturebeat.com/2021/04/29/business-travel-management-and-data-platform-travelperk-raises-160m/,Business travel management and data platform TravelPerk raises $160M,"TravelPerk, a business travel booking and management platform that gives companies data and insights into their spending, has raised $160 million in a series D round of funding led by Greyhound Capital. The raise comes shortly after rival TripActions raised $155 million at a $5 billion valuation. But TripActions had pivoted from being a purely business travel management platform to offer a broader expense management toolset — a move made that made sense, given the pandemic’s disruption to the global travel industry. TravelPerk, on the other hand, has mostly elected to stick to its core proposition. Founded out of Barcelona, Spain in 2015, TravelPerk serves up tools for booking and managing business trips. This includes finding and reserving flights and hotels, centralized invoicing, and — crucially — expense reporting. The tools allow admins to track travel spend in real time by project, team, date, and location, among other criteria. “We also offer a carbon footprint report,” TravelPerk cofounder and CEO Avi Meir told VentureBeat. “Customers can view their CO2 impact in real time for all their travel, not just flights, and if they choose to use GreenPerk they can automatically offset the carbon footprint for all of their corporate travel.” Through its open marketplace, companies can integrate with third-party services, including Expensify, Divvy, Slack, Zoom, and OneLogin. Although TravelPerk hasn’t gone down any major pivot route over the past year, as others in the space have, it hasn’t remained still either. Instead, the company has enabled a new flexible booking tool that guarantees refunds on cancelled trips and launched a risk-management API that provides up-to-date information on COVID-19-related guidelines and restrictions. Perhaps most interestingly, the company launched an open API that enables its customers and partners to develop customized integrations and applications. “This open approach is fundamental to our strategy and a key part of our long-term mission to set new standards for business travel and deliver the best possible experiences to travelers and businesses,” Meir added. “We believe that TravelPerk can become the go-to platform for anyone that wants to build new and exciting travel apps.” TravelPerk had previously raised around $133 million, and with another $160 million in the bank the company is now well-financed to cater to global business travel as it gradually cranks into gear. “Travel is definitely coming back — we can see that already with the numbers,” Meir said. “In the U.S., for instance, we can see a 70-75% recovery in domestic flights compared to the baseline before COVID. In Europe, it’s a little less certain right now, as vaccine rollout isn’t as fast, but you can look to other parts of the world and with some degree of certainty predict what the European recovery will eventually look like.” Although Meir is confident travel will recover, it might look a little different in the future — which is why he hinted that the company is working on some new things. “Overall, travel will make a full comeback because the meetings that matter happen in person, but the travel industry will be very different, and we’re building the products for that future,” Meir said."
https://venturebeat.com/2021/04/29/government-and-industry-to-combat-ransomware-with-bitcoin-regulation/,Government and industry to combat ransomware with Bitcoin regulation,"(Reuters) — Government and industry officials confronting an epidemic of ransomware, where hackers freeze the computers of a target and demand a payoff, are zeroing in on cryptocurrency regulation as the key to combating the scourge, sources familiar with the work of a public-private task force said. In a report on Thursday, the panel of experts is expected to call for far more aggressive tracking of bitcoin and other cryptocurrencies. While those have won greater acceptance among investors over the past year, they remain the lifeblood of ransomware operators and other criminals who face little risk of prosecution in much of the world. Ransomware gangs collected almost $350 million last year, up threefold from 2019, two members of the task force wrote this week. Companies, government agencies, hospitals and school systems are among the victims of ransomware groups, some of which U.S. officials say have friendly relations with nation-states including North Korea and Russia. “There’s a lot more that can be done to constrain the abuse of these pretty amazing technologies,” said Philip Reiner, chief executive of the Institute for Security and Technology, who led the Ransomware Task Force. He declined to comment on the report before its release. Just a week ago, the U.S. Department of Justice established a government group on ransomware. Central bank regulators and financial crime investigators worldwide are also debating if and how cryptocurrencies should be regulated. The new rules proposed by the public-private panel, some of which would need Congressional action, are mostly aimed at piercing the anonymity of cryptocurrency transactions, the sources said. If implemented, they could temper enthusiasm among those who see the cryptocurrencies as a refuge from national monetary policies and government oversight of individuals’ financial activities, having surged past $1 trillion in total capitalization. The task force included representatives from the FBI and the United States Secret Service as well as major tech and security companies. It will recommend steps such as extending “know-your-customer” regulations to currency exchanges; imposing tougher licensing requirements for those processing cryptocurrency; and extending money-laundering rules to facilities such as kiosks for converting currency. It also calls for the creation of a special team of experts within the Justice Department to facilitate seizures of cryptocurrency, a process currently fraught with logistical and legal challenges. Some of the ideas echo those proposed by the Financial Crimes Enforcement Network, which would expand disclosure rules for transactions worth more than $10,000. Federal investigators said a proposal to register accounts would be especially helpful for identifying drug smugglers, human traffickers and terrorists as well as ransomware groups. “That would be huge,” said a senior Homeland Security Official, who spoke on condition of anonymity to discuss emerging policy proposals. “This is a world that was created exactly to be anonymous, but at some point, you have to give up something to make sure everyone’s safe.” Governments are already using the blockchain ledger that documents all bitcoin transactions to bring some charges. Last week, authorities arrested a man in Los Angeles and accused him of laundering more than $300 million through a service that combines transactions from multiple cryptocurrency wallets to obscure who is paying whom. Records from the U.S. Marshals Service show that more than $150 million in crypto assets were seized last year and offered to the public at auction. Last week, the Marshals Service signed a $4.5 million deal with BitGo, a California-based exchange, to hold and sell more forfeited cryptocurrency. But many of the exchanges, which conduct the critical operation of turning cryptocurrency into dollars or other widely accepted currencies, are in countries outside the reach of U.S. regulators. The Institute for Security and Technology’s Reiner said that international cooperation will be critical, and that pressure could be brought by allies with similar regulations, which could help push exchanges into countries where Americans will hesitate to send their funds. “However much crypto markets think they have created their own networks, they still rely on existing financial markets,” Reiner said."
https://venturebeat.com/2021/04/28/membership-inference-attacks-detect-data-used-to-train-machine-learning-models/,Membership inference attacks detect data used to train machine learning models,"One of the wonders of machine learning is that it turns any kind of data into mathematical equations. Once you train a machine learning model on training examples—whether it’s on images, audio, raw text, or tabular data—what you get is a set of numerical parameters. In most cases, the model no longer needs the training dataset and uses the tuned parameters to map new and unseen examples to categories or value predictions. You can then discard the training data and publish the model on GitHub or run it on your own servers without worrying about storing or distributing sensitive information contained in the training dataset. But a type of attack called “membership inference” makes it possible to detect the data used to train a machine learning model. In many cases, the attackers can stage membership inference attacks without having access to the machine learning model’s parameters and just by observing its output. Membership inference can cause security and privacy concerns in cases where the target model has been trained on sensitive information. Each machine learning model has a set of “learned parameters,” whose number and relations vary depending on the type of algorithm and architecture used. For instance, simple regression algorithms use a series of parameters that directly map input features to the model’s output. Neural networks, on the other hand, use complex layers of parameters that process input and pass them on to each other before reaching the final layer. But regardless of the type of algorithm you choose, all machine learning models go through a similar process during training. They start with random parameter values and gradually tune them to the training data. Supervised machine learning algorithms, such as those used in classifying images or detecting spam, tune their parameters to map inputs to expected outcomes. For example, say you’re training a deep learning model to classify images into five different categories. The model might be composed of a set of convolutional layers that extract the visual features of the image and a set of dense layers that translate the features of each image into confidence scores for each class. The model’s output will be a set of values that represent the probability that an image belongs to each of the classes. You can assume that the image belongs to the class with the highest probability. For instance, an output might look like this: Cat: 0.90
Dog: 0.05
Fish: 0.01
Tree: 0.01
Boat: 0.01 Before training, the model will provide incorrect outputs because its parameters have random values. You train it by providing it with a collection of images along with their corresponding classes. During training, the model gradually tunes the parameters so that its output confidence score becomes as close as possible to the labels of the training images. Basically, the model encodes the visual features of each type of image into its parameters. A good machine learning model is one that not only classifies its training data but generalizes its capabilities to examples it hasn’t seen before. This goal can be achieved with the right architecture and enough training data. But in general, machine learning models tend to perform better on their training data. For example, going back to the example above, if you mix your training data with a bunch of new images and run them through your neural network, you’ll see that the confidence scores it provides on the training examples will be higher than those of the images it hasn’t seen before. Membership inference attacks take advantage of this property to discover or reconstruct the examples used to train the machine learning model. This could have privacy ramifications for the people whose data records were used to train the model. In membership inference attacks, the adversary does not necessarily need to have knowledge about the inner parameters of the target machine learning model. Instead, the attacker only knows the model’s algorithm and architecture (e.g., SVM, neural network, etc.) or the service used to create the model. With the growth of machine learning as a service (MaaS) offerings from large tech companies such as Google and Amazon, many developers are compelled to use them instead of building their models from scratch. The advantage of these services is that they abstract many of the complexities and requirement of machine learning, such as choosing the right architecture, tuning hyperparameters (learning rate, batch size, number of epochs, regularization, loss function, etc.), and setting up the computational infrastructure needed to optimize the training process. The developer only needs to set up a new model and provide it with training data. The service does the rest. The tradeoff is that if the attackers know which service the victim used, they can use the same service to create a membership inference attack model.  In fact, at the 2017 IEEE Symposium on Security and Privacy, researchers at Cornell University proposed a membership inference attack technique that worked on all major cloud-based machine learning services. In this technique, an attacker creates random records for a target machine learning model served on a cloud service. The attacker feeds each record into the model. Based on the confidence score the model returns, the attacker tunes the record’s features and reruns it by the model. The process continues until the model reaches a very high confidence score. At this point, the record is identical or very similar to one of the examples used to train the model. After gathering enough high confidence records, the attacker uses the dataset to train a set of “shadow models” to predict whether a data record was part of the target model’s training data. This creates an ensemble of models that can train a membership inference attack model. The final model can then predict whether a data record was included in the training dataset of the target machine learning model. The researchers found that this attack was successful on many different machine learning services and architectures. Their findings show that a well-trained attack model can also tell the difference between training dataset members and non-members that receive a high confidence score from the target machine learning model. Membership inference attacks are not successful on all kinds of machine learning tasks. To create an efficient attack model, the adversary must be able to explore the feature space. For example, if a machine learning model is performing complicated image classification (multiple classes) on high-resolution photos, the costs of creating training examples for the membership inference attack will be prohibitive. But in the case of models that work on tabular data such as financial and health information, a well-designed attack might be able to extract sensitive information, such as associations between patients and diseases or financial records of target people. Membership inference is also highly associated with “overfitting,” an artifact of poor machine learning design and training. An overfitted model performs well on its training examples but poorly on novel data. Two reasons for overfitting are having too few training examples or running the training process for too many epochs. The more overfitted a machine learning model is, the easier it will be for an adversary to stage membership inference attacks against it. Therefore, a machine model that generalizes well on unseen examples is also more secure against membership inference. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/28/understanding-when-to-use-a-database-data-lake-or-data-warehouse/,"Understanding when to use a database, data lake, or data warehouse","The “data” part of the terms “data lake,” “data warehouse,” and “database” is easy enough to understand. Data are everywhere, and the bits need to be kept somewhere. But should they be stored in a data warehouse, a data lake, or an old-fashioned database? It all depends on how that data is going to be used. It’s difficult to define the names precisely because they are tossed around colloquially by developers as they figure out the best way to store the data and answer questions about it. All three forms share the goal of being able to squirrel away bits so that the right questions are answered quickly. Still, the terms have evolved and taken on relatively standard meanings. The database now means both the software that stores and manages the information as well as the information stored within the database. Developers use the word database with some precision to mean a collection of data, because the software needs to know that orders are kept on one machine and the addresses on another. Users rarely know where the values are kept and may just call the entire system the database. And that’s fine — most software development is about hiding that level of detail. Among databases, the relational database has become a workhorse for much corporate computing. The classic format arranges the data in columns and rows that form tables, and the tables are simplified by splitting the data into as many tables and sub-tables as needed. Good relational databases add indexes to make searching the tables faster. They can employ SQL and use sophisticated planning to simplify repeated elements and produce concise reports as quickly as possible. Lately, non-relational types of databases have gained traction. These so-called NoSQL databases don’t store the data in relational tables. They are often chosen when developers want the flexibility to add new fields or elements for some entries but not others. But there are use cases where the database is not enough. The data warehouse is a collection of databases, although some may use less structured formats for raw log files. The idea of a data warehouse evolved as a consequence of businesses establishing long-term storage of the information that accumulates each day, and to meet the need to report on and analyze that data. Building a data warehouse is more than just choosing a database and a structure for the tables, as it requires creating retention policies. Data warehouses often include sophisticated analytics to generate statistics to study changes over time. Data warehouses are often tightly integrated with graphics routines that produce dashboards and infographics to quickly show changes in the data. Generally, the term data warehouse has come to describe a relatively sophisticated and unified system that often imposes some order upon the information before storing it. A data lake takes a different approach to building out long-term storage from a data warehouse. In modern data processing, a data lake stores more raw data for future modeling and analysis, while a data warehouse typically applies a relational schema to the information before it’s stored. The data lake may not even use databases to store the information because the extra processing required isn’t worth it. The data is stored in flat files or logs. Lakes are better choices for storing large amounts of records in case someone wants access to a few or many of them in the future. Regulatory compliance is a common use case. Some use both metaphors for the same system. The incoming raw data is stored in the data lake and, after some analysis and aggregation, the information often finds a home in the data warehouse. Databases, warehouses and lakes take many forms because businesses have many different needs for historical record keeping. The choices a business makes for keeping these records affects the architecture and structure. Here are several hypothetical examples: There are two major themes. Some of the companies that make traditional databases are adding features to support analysis and turning the completed product into a data warehouse. At the same time, they’re building out extensive cloud storage with similar features to support companies that want to outsource their long-term storage to a cloud. Microsoft’s Azure has been migrating data warehouse work to live under the umbrella called “Synapse Analytics.” It integrates Microsoft’s cloud storage with the different routines that can include some of the artificial intelligence. The tool is designed to scale to handle petabytes of data using technologies like Apache Spark developed to transform, analyze, and query big data sets. Microsoft also highlights the fact that billing is separate for the storage and computation so users can save money when they can turn off the instances devoted to analytics. Microsoft also bundles some of the same storage and analytical options under the heading of a data lake. It includes both SQL-based options and the more general object storage, and its marketing materials said it’s intended for “data of any size, shape, and speed.” Oracle also offers an Autonomous Data Warehouse for cloud and on-premises that integrates its Autonomous Database with a number of tools with enhanced analytical routines. The service hides all of the work for patching, scaling, and securing the data. It also offers some of the functionality of a data lake, including the classic Big Data tools like Apache Spark, under the “Big Data” product name. Users of IBM’s Db2 can also choose IBM’s cloud services to build a data warehouse. Its tool, which is also available as a Docker container for on-premises hosting, bundles together machine learning, statistical, and parallel processing analytical routines with some migration tools for integrating data sources. Many of the data warehouses and data lake are built on premises by in-house development teams that use a company’s existing databases to create custom infrastructure for answering bigger and more complex queries. They stitch together data sources and add applications that will answer the most important questions. In general, the warehouse or lake is designed to build a strong historical record for long-term analysis. The cloud companies are offering two different solutions. First, they want to help store the data. Amazon, for instance, offers a wide range of storage solutions at different prices where speed can be traded for savings. Some of the tiers are priced below $1 per terabyte per month for storage alone, but there can be additional charges for retrieval. Some of the slower tiers, called Glacier, can also use a basic subset of SQL to find certain data elements, a useful feature that turns the long-term storage into a database of sorts. Amazon also offers a wide range of analytical tools like the RedShift cloud data warehouse, which works with all of its storage options. Second, the cloud companies are also integrating their analytics tools with the storage to turn their racks into data warehouses or data lakes. Google’s BigQuery database, for instance, is also integrated with some of Google’s machine learning tools to make it possible to explore the use of AI with the data that’s already stored on its disks. Some of the upstarts are offering some services and not others. Backblaze, for instances, will store data at rock-bottom prices that may be 60%, 70% or lower than the major clouds. Its API is designed to work just like Amazon’s S3 to make switching easier. Others are designed to work with any data source. Teradata and Snowflake, for instance, are two companies offering sophisticated tools for adding analysis to data. They emphasize a multi-cloud strategy so users can build their warehouse out of many storage options. Is there anything a database can’t do that a data warehouse or data lake can? The terms are not crisp and consistent, but generally databases are more limited in size. Data warehouses and data lakes refer to collections of databases that might be in one, unified product, but often can be a collection built from different merchants. The metaphors are flexible enough to support many different approaches. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/28/eus-plan-to-woo-chip-manufacturers-wont-work-taiwan-says/,"EU’s plan to woo chip manufacturers won’t work, Taiwan says","(Reuters) — Taiwan Economy Minister Wang Mei-hua on Wednesday played down the prospect of Taiwan tech firms making advanced semiconductors in the European Union, noting industry leader TSMC has insisted it will focus its most advanced technology on the island. Wang’s comments come amid overtures from the EU to persuade a global chipmaker to build a major fabrication plant on EU territory that would help the European Commission achieve a strategic goal of securing the most advanced chip production technology over the next decade. Speaking to reporters, Wang pointed to previous comments by TSMC — Taiwan Semiconductor Manufacturing Company — on its production base. “TSMC has said repeatedly that the most advanced technology will definitely be (produced) mainly in Taiwan,” she said. “As for how Taiwan and the EU can cooperate, companies have their arrangements and considerations, and it can be further discussed.” The European Commissioner responsible for the EU’s internal market, Thierry Breton, is scheduled to hold discussions with the chief executive of U.S.-based Intel and the president of TSMC’s Europe division, Maria Marced, on Friday. Sources in Brussels say Breton is keener to reel in TSMC, which is widely regarded as the undisputed industry leader and has a better command of the most advanced manufacturing processes. TSMC did not immediately respond to a request for comment. Breton’s push for technology ‘sovereignty’ for the EU comes as a surge in demand for everything from consumer electronics to cars has disrupted global supply chains and exposed the continent’s reliance on chips made in Asia. Wang added that she had had no direct talks with the EU on the subject “over the last week or two.” “But in our talks with the EU, industrial cooperation has always been an important topic. It’s not just limited to semiconductors, it’s on broader industrial cooperation.” TSMC has said it intends to keep the focus of its production in Taiwan, though it also has major plants in China and announced plans last year to build its own $12 billion factory in the U.S. state of Arizona."
https://venturebeat.com/2021/04/28/cloudcheckr-survey-says-cloud-computing-adoption-is-accelerating/,CloudCheckr survey says cloud computing adoption is accelerating,"Cloud transformation is moving quickly, according to a report released today by cloud visibility management platform CloudCheckr. In a survey of over 300 IT professionals and business stakeholders at companies with more than 500 employees, 57% reported that more than half of their infrastructure is in the cloud and 64% expected they will be fully in the public cloud within five years. The global public cloud computing market is set to exceed $362 billion in 2022, according to Statista. (In 2018 alone, Amazon Web Services earned $26 billion in revenue for parent company Amazon.) IDG reports that the average cloud budget is up from $1.62 million in 2016 to a whopping $2.2 million today. But cloud adoption continues to present challenges for enterprises of any size. A separate Statista survey identified security, managing cloud spend, governance, and lack of resources and expertise as significant barriers to adoption. Indeed, respondents told CloudCheckr that security concerns (44%), compliance and regulations (42%), and lack of application support (41%) remain significant barriers to cloud migration. Ninety-three percent said that their organizations face blockers with budgeting infrastructure cloud costs, and 94% said that they’d experienced unexpected cloud costs. Only 31% reported that they were able to monitor and optimize public cloud costs “effectively,” meanwhile. But on the whole, the professionals surveyed said that their investments in internal cloud strategies and teams were paying off. Those at organizations with a cloud center of excellence, a team tasked with developing a framework for cloud operations, told CloudCheckr that they saw higher benefits than those where cloud expertise wasn’t organized. “While it’s no surprise to anyone how strong cloud adoption is today, this report shows the tremendous growth ahead and how quickly it will happen over the next half decade. Now is the time for IT organizations to define the right strategies to utilize the full potential of the cloud and for cloud service providers to enhance their capabilities to lead their customers through cloud transformations,” CloudCheckr CEO Tim McKinnon said in a press release. “Migrating to the cloud is only the first step. It’s up to organizations to adopt the right technology and form teams — be it internally or externally — to develop and manage cloud strategy, governance, and best practices.” It seems likely that as the pandemic continues, organizations will increasingly rely on remote work — and by extension, cloud services — to help stem the spread of the virus. Gartner projects that this will cause the cloud market to grow 18.4% in 2021, with cloud predicted to make up 14.2% of total global IT spending. “As enterprises increase investments in mobility, collaboration, and other remote working technologies and infrastructure, Gartner expects growth in public cloud to be sustained through 2024,” the firm wrote in a November 2020 study. Cloud providers are reaping the windfall benefits. In its most recent earnings report, Google said that its cloud division brought in $4.047 billion in sales for the first quarter of 2021, up 46% from the year prior. Amazon’s AWS (Amazon Web Services) posted a record $13.5 billion in profits for 2020. And Azure, Microsoft’s cloud business, notched third quarter 2021 revenue growth of 50% year-over-year, beating analyst expectations."
https://venturebeat.com/2021/04/28/atlassians-jira-work-management-encourages-team-collaboration/,Atlassian’s Jira Work Management encourages team collaboration,"At its Team21 conference today, Atlassian unveiled Jira Work Management, a new product built specifically for enterprise teams. The next generation of Jira Core, Jira Work Management is described as a platform that enables marketing, HR, finance, and design employees to connect with their technical counterparts and work together more efficiently. As the pace of change increases, new ways of working are forcing teams to become more agile. Gartner reports that 74% of companies plan to shift some of their employees to remote working permanently. But more than half of remote employees say they feel disconnected from in-office employees, according to an Owl Labs survey, highlighting remote work challenges that must be overcome. With Jira Work Management, which was first announced in March, Atlassian aims to enable business customers to take advantage of the capabilities native to its Jira product family. The architecture that Jira Work Management shares with Jira Software and Jira Service Management lets data flow between projects within organizations. This means a request for a website update can pass through designers using Jira Work Management, as well as developers using Jira Software, for example. “Jira Work Management combines cutting-edge work management capabilities with the power and customizability of Jira to create a new standard for business teams managing projects,” Atlassian product marketing head Chase Wilson said in a blog post. “This is the first Jira built for business teams, by business teams. We crafted this new Jira experience directly with customer design partners of all sizes and industries, as well as internal Atlassian teams of every type — including our own Jira Work Management teams.” Tasks from teams and projects in Jira Work Management can be linked together to reveal dependencies and connections. These links support custom names, allowing employees to create dashboards that report on organization-wide progress. Users can choose from over 35 options, including workload, heat map, filters, and charts, or bring in data with direct integrations. Jira Work Management offers a number of features, including lists, a calendar, and a board that displays tasks from one or more projects. The platform’s timeline feature shows tasks with assignees and statuses, while its forms creator, which is available to licensed Jira Cloud users, lets employees create forms with a drag-and-drop interface. Beyond this, Jira Work Management offers templates of industry-researched workflows and configurations, each with custom issue types, workflows, and fields. Teams see tasks and issues such as “asset” (in design or marketing use cases) and “candidate” (in recruiting use cases) instead of “stories” and “bugs” (software development). And in place of software-specific functionalities like code, backlog, components, and releases, the left navigation menu in the platform highlights views, forms, and a summary tab for insights. Jira Work Management also offers free automation rules and actions within projects. Teams can select premade rules from a business automation library or create rules for use cases and departments. Atlassian says all of the features are available for Jira Work Management project on every instance, across every pricing edition — including the free plan. The company adds that over the next six months, Jira Work Management will gain improvements to all of its views, optimized reporting functionality, approvals for sign-off, and new collaboration features."
https://venturebeat.com/2021/04/28/gartner-says-low-code-rpa-and-ai-driving-growth-in-hyperautomation/,"Gartner says low-code, RPA, and AI driving growth in ‘hyperautomation’","Research firm Gartner estimates the market for hyperautomation-enabling technologies will reach $596 billion in 2022, up nearly 24% from the $481.6 billion in 2020. Gartner is expecting significant growth for technology that enables organizations to rapidly identify, vet, and automate as many processes as possible and says it will become a “condition of survival” for enterprises. Hyperautomation-enabling technologies include robotic process automation (RPA), low-code application platforms (LCAP), AI, and virtual assistants. As organizations look for ways to automate the digitization and structuring of data and content, technologies that automate content ingestion, such as signature verification tools, optical character recognition, document ingestion, conversational AI, and natural language technology (NLT), will be in high demand. For example, these tools could be used to automate the process of digitizing and sorting paper records. Gartner currently anticipates the hyperautomation market reaching $532.4 billion this year. Gartner said process-agnostic tools such as RPA, LCAP, and AI will drive the hyperautomation trend because organizations can use them across multiple use cases. Even though they constitute a small part of the overall market, their impact will be significant, with Gartner projecting 54% growth in these process-agnostic tools. Through 2024, the drive toward hyperautomation will lead organizations to adopt at least three out of the 20 process-agonistic types of software that enable hyperautomation, Gartner said. The demand for low-code tools is already high as skills-strapped IT organizations look for ways to move simple development projects over to business users. Last year, Gartner forecast that three-quarters of large enterprises would use at least four low-code development tools by 2024 and that low-code would make up more than 65% of application development activity. Software automating specific tasks, such as enterprise resource planning (ERP), supply chain management, and customer relationship management (CRM), will also contribute to the market’s growth, Gartner said. Hyperautomation extends the idea of intelligent automation, as it promises end-to-end process automation with minimal human intervention required. The convergence of intelligent process automation technologies and cloud computing, along with the need to process unstructured content, helps make the case for hyperautomation across several industries, including shared services, hospitality, logistics, and real estate. Some day-to-day examples of automation include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Business use cases include applying data and machine learning to build predictive analytics that react to consumer behavior changes and implementing RPA to streamline operations on a manufacturing floor. Gartner earlier included hyperautomation in its Top 10 Strategic Technology Trends for 2021. Gartner said tools that provide visibility to map business activities, automate and manage content ingestion, orchestrate work across multiple systems, and provide complex rule engines make up the fastest-growing category of hyperautomation-enabling software. Organizations will be able to lower operational costs 30% by 2024 through combining hyperautomation technologies with redesigned operational processes, Garner projected. “Hyperautomation has shifted from an option to a condition of survival,” Gartner VP Fabrizio Biscotti said in a statement. “Organizations will require more IT and business process automation as they are forced to accelerate digital transformation plans in a post-COVID-19, digital-first world.”"
https://venturebeat.com/2021/04/28/saas-provider-boostup-ai-nabs-6m-to-support-revenue-operations/,Saas provider BoostUp.ai nabs $6M to support revenue operations,"BoostUp.ai, provider of a software-as-a-service (SaaS) platform for managing revenue operations (RevOps) infused with AI capabilities, today announced it has garnered $6 million in additional series A funding, bringing its total raised to $14 million, after an initial seed round last year. The company is part of a growing group of startups attempting to unify sales, marketing, and customer service processes in a way that enables organizations to boost sales and increase overall profitability. BoostUp claims its revenue increased by more than 1,000% in fiscal 2020, thanks in part to customers such as Udemy, Degreed, Plume, and Windstream. The company’s Connected Revenue Intelligence & Operations platform is usurping customer relationship management (CRM) applications as the single source of truth for organizations that need to tightly integrate sales, marketing, and customer service processes, CEO Sharad Verma told VentureBeat. “The CRM is no longer the system of record,” he added. The Connected Revenue Intelligence & Operations platform differs from rival offerings in that it includes predictive analytics capabilities enabled by machine learning algorithms, Verma explained. The platform ingests unstructured data from sources like emails, phone calls, calendars, meetings, and messaging applications that are then matched to accounts and opportunities found in CRM applications. Natural language parsing, sentiment analysis, and proprietary indexing of spoken and written keywords are applied to better understand sales trends to forecast more accurately whether deals will close. BoostUp claims customers have achieved 95% forecast accuracy, reviewed 5 times the number of opportunities per manager, and increased sales manager and sales representative capacity by over 15%. The latest round of funding was led by Canaan Partners, with participation from Emergent Ventures and BGV Ventures, and will be employed to scale product development and increase customer growth. It’s not clear to what degree providers of CRM applications are responding to a shift toward more integrated RevOps. Salesforce, for example, offers separate CRM, marketing, and customer service applications that are integrated on the same cloud. But BoostUp is making a case for a single platform that aggregates data to make it simpler to apply AI to identify, for example, when the level of customer engagement is misaligned with the sales forecast. Organizations that are shifting toward a RevOps approach to engaging customers have typically appointed a chief revenue officer (CRO) to assume responsibility for all sales channels. At a time when most sales engagements are occurring via some form of digital channel, Verma said organizations must measure and monitor the actual level of engagement with customers alongside historical data that might indicate whether, for example, a customer tends to always wait until the last week of a quarter to sign a purchase order as part of an effort to obtain better pricing. Conversely, sales representatives may simply not be all that good at forecasting, which Sherma noted would signal an opportunity to improve training. Regardless of companies’ motivations for embracing RevOps, it’s clear sales, marketing, and customer service processes are becoming more integrated. Many organizations now realize customer service representatives are often more adept at identifying additional revenue opportunities they can close on their own. And sales teams are in many cases now focusing the bulk of their time and efforts on landing new customers, rather than ensuring every product or service has been delivered to a contract’s precise specifications. Naturally, it may take a while before every organization is able to fully transition to a RevOps model. But the way organizations engage customers is poised to change fundamentally."
https://venturebeat.com/2021/04/28/devops-orchestration-platform-opsera-raises-15m/,DevOps orchestration platform Opsera raises $15M,"Opsera, a continuous orchestration platform for DevOps, today announced it has closed a $15 million series A round led by Felicis Ventures. Opsera says it will put the capital toward growing its engineering team and accelerating its sales, marketing, and customer success initiatives. An estimated 19% to 23% of software development projects fail, with that statistic holding steady for the past couple of decades, according to data compiled by Ask Wonder. Standish Group found that “challenged” projects — i.e., those that fail to meet scope, time, or budget expectations — account for about 52% of software projects. Often, a lack of user involvement, executive support, and clear requirements are to blame for missed benchmarks. Opsera, which was founded in 2020, aims to combat DevOps challenges with a self-service, no-code orchestration platform that lets engineers provision or integrate their CI/CD tools from a common framework. With Opsera, users can build declarative pipelines for a range of use cases, including software delivery lifecycle, infrastructure as code, and software-as-a-service app releases. Opsera correlates and unifies data throughout the development process to provide contextualized diagnostics, metrics, and insights. The DevOps market is projected to reach $14.97 billion by 2026, at a compound annual growth rate of 19.1%, according to a Fortune Business Insights report. Opsera competes directly or indirectly with companies including Harness, a continuous integration and delivery platform for engineering and DevOps teams, and Tasktop, which recently nabbed $100 million. There’s also OpsRamp, which applies AI to DevOps processes. And Productboard offers a product planning interface designed for DevOps orchestration. But Opsera claims to have a growing client roster that includes “several” Fortune 500 customers. “Our mission is to democratize software delivery by abstracting any CI/CD tools into a common framework that can empower engineers to build pipelines in minutes, not days or weeks,” cofounders Chandra Ranganathan and Kumar Chivukula said in a press release. “We offer the only DevOps platform that connects and orchestrates the entire tool stack with complete choice and visibility. Our customers can focus on their core product and will never waste time and resources building and managing toolchains and pipelines in-house or be stuck with single-vendor solutions. Having the support of Felicis and all of our investment partners will accelerate how we help customers along their DevOps journey.” Beyond Felicis Ventures, existing backers Clear Ventures, Trinity Partners, and Firebolt Ventures and new investor HMG Ventures also participated in San Francisco, California-based Opsera’s latest financing round. It brings the startup’s total raised to $19.3 million."
https://venturebeat.com/2021/04/28/ai-powered-construction-project-platform-openspace-nabs-55m/,AI-powered construction project platform OpenSpace nabs $55M,"OpenSpace, a platform that helps construction companies track building projects through AI-powered analytics and 360-degree photo documentation, has raised $55 million in a series C round of funding led by Alkeon Capital Management. The raise comes amid a cross-industry digital transformation boom, spurred in large part by the pandemic. Construction has often lagged behind other sectors in terms of efficiency, but tech such as robotics, artificial intelligence (AI), and remote collaboration tools has helped get the $11 trillion industry back on track. Founded out of San Francisco in 2017, OpenSpace leans on AI to create 360-degree photos of construction sites that are captured by builders or site managers who traverse an area with cameras strapped to their hats. All the imagery is sent to the cloud, where computer vision and machine intelligence tools arrange, stitch, and map the capture visuals to the associated project plans. It’s all about documenting activities on each site so stakeholders can check on progress remotely or resolve discrepancies by reviewing a visual history of the project. OpenSpace also offers AI-powered analytics, including a “progress tracking” feature that uses computer vision to analyze site images and automatically figure out how much of the scheduled work has been completed. Elsewhere, “object search” enables site managers to select an object from a scene and find similar objects elsewhere on the site. Other notable players in the space include SiteAware and Buildots, both of which have raised sizable VC investments over the past nine months, highlighting the growing demand for digital technologies in the construction space. OpenSpace had previously raised around $34 million, including a $15.9 million series B round last July. With its latest cash injection, the company is ready to capitalize on its rapid growth over the past year, when it claims revenue tripled and its customer count rose by 150%. With another $55 million in the bank, OpenSpace said it will double down on its suite of analytics products and expand into areas such as safety management and quality control."
https://venturebeat.com/2021/04/28/viso-trust-assesses-third-party-cybersecurity-risk-with-ai-raises-3m/,"Viso Trust assesses third-party cybersecurity risk with AI, raises $3M","Viso Trust, a  platform that uses AI to perform cyber risk assessments, today announced it has raised $3 million. The company plans to use the funds to support expansion and hiring efforts, as well as sales, marketing, and R&D. It’s estimated that over 65% of security breaches are attributable to third-party failures. The pandemic has heightened the concern among legal and compliance leaders, 52% of whom worry about the risks posed by remote work. While the need for faster vendor security reviews has prompted some companies to use abbreviated questionnaires or outside-in assessments to conduct shorter reviews, security analysts can spend hours every day sending and processing documents. Viso Trust aims to lighten the workload by offering a holistic view of risk, leveraging a “social due diligence” network and AI to deliver continuous reports about third parties. The platform automatically extracts data from source documents and audits to surface key information about third-party relationships. “The goal of third-party risk management ranges from reducing the likelihood of data breaches and costly operational failures to meeting regulatory requirements,” cofounder Paul Valente told VentureBeat via email. “Unfortunately, the tools available for us to manage third-party risk, such as GRC platforms, security ratings, and audit exchanges, were too clunky, overly time-consuming, inaccurate, and most of all, expensive. Adding to the mix was the scale of our operations as a global fintech. We knew there needed to be a better way to run the vendor due diligence process.” One early customer, Ilumio, claims Viso Trust has enabled it to bring the security staff time per third-party relationship down from more than eight hours to 30 minutes. “Leveraging our prior experience and vast networks, we built a solution that solved the problem and validated the core concepts and value proposition with over 300 chief information security officers and security professionals,” Valente said. “Going forward, we believe we can reduce time spent in covering additional major areas of risk, such as business continuity and privacy, to nearly instantaneous.” Kelley Mak, principal at Work-Bench, a Viso Trust investor, says he saw a need in the market due to the proliferation of software-as-a-service tools in the enterprise. While cumbersome processes hamstring security teams attempting to evaluate tools at the speed of business, they face rising security threats and the hidden risk of third parties. Just 35% of organizations rate their third-party risk management program as highly effective, and only 34% have an inventory of their vendors, a 2018 study from Opus and Ponemon Institute found. “Viso Trust [is] building a cyber due diligence platform that leverages intelligence and automation to eliminate all questionnaire-based interactions and deliver continuous automated due diligence accurately across any number of vendors,” Mak told VentureBeat via email. “The founders felt this pain firsthand when they led security at LendingClub and ASAPP and had to onboard and evaluate the risk of hundreds of third parties.” Work-Bench led San Francisco-based Viso Trust’s seed round, with participation from Sierra Ventures and Lytical Ventures."
https://venturebeat.com/2021/04/28/messagebird-acquires-email-data-platform-sparkpost-closes-1b-round/,"MessageBird acquires email data platform SparkPost, closes $1B round","MessageBird, a cloud communications platform that creates APIs for developers and AI-powered contact center tools, is shelling out $600 million to acquire SparkPost, an email delivery, optimization, and analytics platform. Alongside the acquisition, Netherlands-based MessageBird announced that it has significantly extended its series C round of funding from the $200 million it announced in October to $1 billion. Founded in 2011, MessageBird serves up a Twilio-like platform that enables app makers to add WhatsApp messaging, voice, SMS, and email functionality to their software through APIs — saving them from having to develop the infrastructure internally. Last year, MessageBird launched Inbox, an omnichannel contact center platform that centralizes in-bound communications. MessageBird, which has accrued an impressive roster of customers, including Uber, Facebook, and SAP, has been on something of an acquisition spree of late. In December, it snapped up Pusher to help developers integrate real-time communication features into their own software. And last month, MessageBird bought two companies — video meeting platform 24Sessions and customer data platform Hull. With SparkPost under its wing, MessageBird gains access to “predictive email intelligence,” as well as a host of high-profile clients, including JP Morgan, PayPal, Disney, Zillow, Adobe, LinkedIn, and Pinterest. Founded in 2008, Columbia, Maryland-based SparkPost serves businesses with insights and analytics spanning all of their email campaigns, including delivery rate and engagement, bounce rates, spam traps, and ISP responses. With transactional emails, it’s particularly important to know if any are going astray, as this can lead to customer churn and revenue loss. It’s worth noting that the two companies first partnered back in 2019, an integration that enabled MessageBird customers to send emails through the API they were already using. “We made this acquisition to double down on our partnership,” MessageBird CEO Robert Vis told VentureBeat. “In terms of functionality, the SparkPost platform will give our enterprise customers more control over delivery and inbox placement by providing them with the world’s best email analytics platform and optimization tooling.” Despite the hype around communication tools such as Microsoft Teams and Slack, estimates suggest roughly 80% of businesses still use email as their primary communication tool. And email-related technology remains big business, with Exclaimer recently raising $133 million to help companies manage their email signatures, while SparkPost itself locked down $180 million in funding just a few months ago. SparkPost’s acquisition comes in the wake of a handful of similar deals in the email management and analytics space from companies including Twilio, which bought SendGrid for $2 billion, and private equity firm Thoma Bravo, which bought a majority stake in Mailgun. “Ninety-nine percent of interactions are on email — and globally, it remains the largest customer communication channel by volume,” Vis said. “So, whilst it may feel like an older technology to folks like me who live and breathe technology and communications, it’s still as crucial to customers in 2021 as it always has been.” With its huge series C extension, MessageBird is now valued at $3.8 billion post-money equity valuation, including debt. As a result of the acquisition, Sparkpost will continue to be offered as a standalone product."
https://venturebeat.com/2021/04/28/secrets-management-and-authentication-platform-akeyless-raises-14m/,Secrets management and authentication platform Akeyless raises $14M,"Akeyless, a software-as-a-service platform for authentication and digital access, today announced it has raised $14 million in a series A round led by Team8. Akeyless has offices in New York and Tel Aviv and says the round will be put toward hiring and global expansion as it looks to grow its customer base. As the transformation to hybrid and multicloud evolves — with 92% of organizations characterizing their operations as at least “somewhat” in the cloud — secrets like passwords, credentials, certificates, and keys are seeing heavy use in workloads, as well as by privileged users, teams, and IT security software. What was previously solved by key management systems has become much more challenging, given the scale of the cloud-distributed infrastructure. According to a CyberArk survey, 99% of security and DevOps pros fail to identify all the places where privileged accounts or secrets exist. Akeyless aims to solve this with a platform that combines secrets management with zero-trust solutions. In a cybersecurity context, “zero trust” refers to the belief that organizations shouldn’t automatically trust anything and must instead verify everything trying to connect to their systems before granting access. Founded in 2018, Akeyless is the brainchild of Shai Onn, Oded Hareven, and former Intuit senior engineer Refael Angel. Onn is the founder and chair of cybersecurity startup FireGlass, which Symantec acquired in 2017 for $250 million. Hareven, a veteran of the Israeli Defence Forces cybersecurity unit, held a number of product and project management positions, including director of product management at Moovit, which Intel purchased last year for $900 million. Akeyless automatically manages secrets for DevOps tools and cloud platforms using a secure vault for credentials, tokens, API keys, and passwords. The platform supports ephemeral, “just-in-time” access permissions and is built on an architecture that can be deployed on any internal environment, with containerized packages and virtual machines. Twenty-two-employee Akeyless isn’t the only secrets management product in an identity and access market that’s anticipated to be worth $24.12 billion by 2025, according to Grand View Research. Among others, there’s Doppler, as well as 1Password and tech giants like Amazon. But Akeyless claims to uniquely leverage a technique called distributed fragments cryptography to provide a root-of-trust in untrusted multicloud environments. Distributed fragments cryptography performs cryptographic operations using fragments of encryption keys without ever combining them, ensuring third parties can’t access secrets or keys, Akeyless says. “We are witnessing an unprecedented migration of technology development and data to the cloud,” CEO Hareven told VentureBeat via email. “As such, protecting access has never been more challenging or more important. Akeyless’ cloud-native … solution is the first unified vault that allows enterprises to manage credentials, certificates and keys seamlessly, without the need for the lengthy, costly installation of an on-premise solution. The innovative zero-knowledge key management system, where encryption keys never exist as a whole, means neither Akeyless nor any third party can access a customer’s secrets and keys. Our solution offers a seamless onboarding experience, fast time-to-production and ease-of-use. Our solution offers secrets management, zero-trust access and data protection, based on Akeyless… technology.” Jerusalem Venture Partners also participated in Akeyless’ latest funding round."
https://venturebeat.com/2021/04/28/accenture-says-it-investments-are-bearing-fruit/,Accenture says IT investments are bearing fruit,"The divide between organizations that derive maximum financial benefit from IT investments and comparative laggards is widening in the wake of the economic downturn brought on by the COVID-19 pandemic. Accenture today published a report based on a survey of 4,300 business and IT leaders that finds industry leaders are now growing revenue at 5 times the rate of rivals that have not invested as much in IT. That’s twice the growth rate a similar Accenture study found in 2017. Leaders represent the top 10% of that sample, while the bottom 25% are considered laggards. Cloud security and internet of things (IoT) technologies tied for top areas of IT investment among leaders, followed closed by hybrid cloud (68%) and AI and machine learning (59%), according to the survey. Nearly two-thirds (65%) of leaders have also prioritized flexible work arrangements using digital technologies, the survey found. But while Accenture identified many of the same leaders in the two reports, in today’s report the consulting firm also identified 18% of the organizations it surveyed as “leapfroggers,” defined as companies whose aggressive investments in digital transformation initiatives are having a significant impact on revenues. The existence of a leapfrogger category proves it’s not too late for laggards to make strategic IT investments that will enable them to compete more effectively, Accenture Integrated Global Services lead Ramnath Venkataraman told VentureBeat. “It’s become a matter of survival,” he said. “It’s not an option.” Overall, leapfroggers increased their investments in advanced and emerging technologies by 17%, the report finds. For example, 80% of leapfroggers had already adopted some form of cloud technology by 2017, but that figure rose to 98% by 2020. More than two-thirds of respondents (67%) said their organizations are also seeking to aggressively increase revenue from non-core lines of business. It’s not yet clear to what degree industry laggards will be able to remain viable in the face of more aggressive competition, even as the global economy continues to improve, Venkataraman said, adding that leaders and leapfroggers both cited leadership teams’ strategic commitment to investing in IT. In the absence of such commitment, Venkataraman said it will become even more difficult for laggards to close the gap that will grow as AI technologies are employed more widely. Another attribute leaders and leapfroggers share is that they have narrowed the divide between IT and the rest of their business, Venkataraman said. Laggards that want to become more competitive will need to not only become comfortable taking on additional risks, but also migrate as many legacy platforms to the cloud as possible to become more agile, Venkataraman said. As part of that effort, these organizations need to honestly assess their current digital capabilities, he added. Venkataraman said the pandemic has shown how much more resilient organizations that make the right strategic IT decisions can be in the face of great economic uncertainty. Many of those organizations will soon be in an even better position to capitalize on their investments as governments around the world continue to stimulate the economy, he added. While the post-pandemic future is still uncertain, organizations that consistently invest in IT have shown an ability to pivot in the face of adversity that rival organizations can now only envy."
https://venturebeat.com/2021/04/28/salesforce-launches-employee-upskilling-toolkit-for-businesses/,Salesforce launches employee upskilling toolkit for businesses,"Salesforce today announced the launch of Salesforce Learning Paths, which surfaces personalized learning content directly in Salesforce, enabling employees to learn in the flow of work. Salesforce Learning Paths will become generally available and free to all Salesforce customers this summer, when MyTrailhead customers gain the ability to integrate custom content into Salesforce. Workplace learning can be a driver of success for both employees and employers. But with remote work, it’s often harder to find opportunities to develop skills while on the job. According to a Salesforce survey, 59% of employees say they’ve had less access to workplace learning since the start of the pandemic. Salesforce Learning Paths aims to address the training gap with personalized learning content delivered via Salesforce. This allows employees to work and learn in a single environment, tapping into Trailhead, MyTrailhead, and guides that include articles, videos, and quizzes. Salesforce Learning Paths introduces Learning Home in Salesforce, a dashboard where employees can view assignments, track progress, and discover new learning paths. Business leaders and managers can personalize and monitor learning according to an individual, role, team, or for the entire organization. Eighty percent of employees find it easier to retain information they learn on the job, compared to siloed training, according to Salesforce. They’re also more productive (68%), more engaged in their work (70%), and more likely to stay at their job (60%) when their companies invest in continuous learning. Among others, Elekta and United Utilities are already using Salesforce Learning Paths in early access. Salesforce says more than 3 million people have used its Trailhead resources to date, up from 200,000 in October 2016. The launch of Salesforce Learning Paths comes as the global economy inches toward recovery following pandemic headwinds. In the U.S., job growth in March boomed at the fastest pace since summer 2020, as an aggressive vaccination effort contributed to a surge in hospitality and construction jobs. But the labor market simultaneously faces upskilling and wage disparity challenges. It’s estimated that as many as 30 million U.S. workers without college degrees have the skills necessary to earn 70% more, but employer education requirements frequently hold these workers back. The Bureau of Labor Statistics reports that U.S. workers with a bachelor’s degree make $1,248 per week, on average, while workers with only a high school degree earn closer to $746. In 2017, McKinsey said as many as 375 million workers would have to switch occupations or acquire new skills by 2030 because of automation and AI. A recent survey by the firm found that 87% of executives reported experiencing skill gaps in the workforce but less than half of respondents had a clear sense of how to address the problem."
https://venturebeat.com/2021/04/28/sysdig-raises-189m-to-monitor-containers-and-apps-in-the-cloud/,Sysdig raises $189M to monitor containers and apps in the cloud,"Sysdig, a container, Kubernetes, and cloud security company, today announced it has closed a $189 million series F round at a $1.19 billion valuation. The funding brings the company’s total raised to $395 million and will be used to grow its R&D teams, as well as sales and marketing. A key emphasis will be on continuing to build ecosystem and channel partnerships around the world, the company says. Gartner predicts that the public cloud market will reach $304.9 billion in value this year, up from $257.5 billion in 2020.  Modern apps are increasingly built as distributed microservices, leveraging both containers and services. While this shift accelerates innovation, it presents challenges legacy tools are sometimes unable to address. A 2018 Cybersecurity Insiders survey found that 62% of respondents believe misconfiguration is the single biggest threat to cloud security, followed by unauthorized access through the misuse of employee credentials. Sysdig was launched in 2013 as an open source effort to address the security problems facing enterprises adopting cloud apps. The company created projects to leverage visibility as a foundation for security, including Sysdig and Falco, which have become standards for threat detection and incident response. Falco, which was contributed to the Cloud Native Computing Foundation (CNCF) in 2018, is now an incubating hosted-level project with nearly 24 million downloads. “The pandemic accelerated the march to the cloud by a few years, and it exposed to many what we already knew — we are in fact in the midst of a transformative shift in the way applications are developed and secured,” Sysdig founder Loris Degioanni, a cocreator of Wireshark, the open source packet analyzer used for network troubleshooting, threat investigation, and incident response, told VentureBeat via email. “The winner is going to be an innovative open source stack built for secure DevOps, containers, and cloud. The winner will not be a legacy security stack retrofitted for the cloud.”  Sysdig also offers a managed platform that performs image scanning, runtime security, incident response, and continuous monitoring. It delivers alerts on threats, operational issues, and compliance risks with a detailed activity record. Out-of-the-box integrations enable customers to plug into existing workflows. As Degioanni explained, Sysdig uses machine learning to originate tailored profiles for different container images. The technology learns process and file activity, network connections, and system calls to build profiles for images from which policies can be created. Separately, Sysdig is able to auto-tune open source Falco rules. By observing security events, the platform can determine if behavior is expected or a potential security threat — knowledge it uses to add exceptions to the default Falco ruleset. Sysdig’s cloud security posture management incorporates the popular Cloud Custodian, an open source project for configuration checks. Last year, Sysdig announced compatibility and support for Prometheus, the CNCF project second in popularity to Kubernetes. Sysdig competes with rivals like Dome9, Datadog, and Orca Security, which recently raised $210 million to simplify enterprise security with cloud-native tools. Like Sysdig, Orca’s platform helps with compliance across cloud providers and automatically reads a customer’s configuration to detect vulnerabilities, malware, misconfigurations, and authentication risks.  But 2020 was a major growth year for Sysdig, which saw 2.3 times the new annual contract value compared with the previous fiscal year. Sysdig now has “tens of thousands” of users across over 450 enterprise customers, including “dozens” of large global enterprises with an average annual recurring revenue of more than $500,000 across the top 50 purchasing companies. Third Point Ventures and Premji Invest led Sysdig’s latest funding round, with participation from Accel, Bain Capital Ventures, DFJ Growth, Goldman Sachs, Insight Partners, and Next47. The company employs over 300 people and expects to have more than 450 by 2022."
https://venturebeat.com/2021/04/28/amd-bets-on-strong-demand-for-chips-as-revenue-soars-93/,AMD bets on strong demand for chips as revenue soars 93%,"(Reuters) — Advanced Micro Devices (AMD) raised its annual revenue forecast on Tuesday, betting on strong demand for its chips used in data centers and personal computers as its chief executive said she was confident the company could source the chips despite a global supply shortage. The chip designer’s shares rose about 4% in extended trading after it also reported better-than-expected results for the first quarter. CEO Lisa Su said AMD has “good visibility” into being able to secure additional chips from its manufacturing partners. “The entire semiconductor supply chain is very, very tight,” she told analysts on a conference call. “That being said, we’ve been working very closely with our supply chain partners. We have seen improvements that have led to the improved full year guide.” AMD has been prying away central processor chip market share from Intel, whose manufacturing operations have fallen behind contract factories used by AMD, such as Taiwan Semiconductor Manufacturing Co. It has also benefited from a surge in demand for its graphics chips from gamers who have spent more time playing and upgrading their equipment during the COVID-19 pandemic. The company said it now expected 2021 revenue to rise 50% from a year earlier, implying a figure of $14.64 billion, compared with its previous forecast of a 39% jump. Revenue in the first quarter soared 93% to $3.45 billion, beating a Refinitiv IBES estimate of $3.21 billion, thanks in part to higher average selling prices for its chips. “We feel very good about our progress, particularly in notebooks,” Su said. “We’re seeing traction in the premium ultrathin, gaming and commercial” notebook markets. Sales in AMD’s computing and graphics business, which includes graphics and central processor chips for personal computers, rose 46% to $2.10 billion. Its enterprise, embedded and semi-custom segment, the unit that houses data center chips, posted an almost four-fold jump in sales to $1.35 billion. Excluding items, the company earned 52 cents per share, exceeding expectations of 44 cents per share."
https://venturebeat.com/2021/04/27/microsoft-beats-q3-revenue-expectations-spurred-by-strong-cloud-sales/,"Microsoft beats Q3 revenue expectations, spurred by strong cloud sales","(Reuters) — Microsoft on Tuesday met analysts’ quarterly sales expectations and beat profit estimates, but its shares fell slightly as investors hoped for an even stronger performance after a year-long rally to a massive market valuation. The Redmond, Washington company has become one of the world’s most valuable companies, worth close to $2 trillion after its stock jumped 50% over the past year, by entering the booming market for cloud computing. Microsoft has remained a household name during the pandemic through its Teams collaboration software. Sales have even boomed for its Windows operating system for PCs, which had waned for decades as smartphone have proliferated. Microsoft’s Azure cloud service is closing ground on market-share leader Amazon Web Services, and it is doubling down on productivity software used by businesses worldwide. Revenue and adjusted earnings per share for the third quarter ended March 31 were $41.7 billion and $1.95 per share, above analysts’ estimates of $41.03 billion and $1.78 per share, according to data from Refinitiv. Shares initially fell as much as 3.2% after the results were released, but they pared losses to 1.7%, at $257.50, after Microsoft executives gave a better-than-expected forecast during a conference call with investors. “One-off tax and currency advantages have boosted Microsoft’s third-quarter numbers, and as a result the market isn’t being quite as welcoming of expectation-beating numbers as you might expect,” said Nicholas Hyett, equity analyst at Hargreaves Lansdown. “That is the danger of trading on the kind of valuation Microsoft enjoys, 32.8 times next year’s earnings. Disappoint even a little and the market will be unforgiving.” Sales for what Microsoft calls its “commercial cloud” – which contains server infrastructure such as Azure along with cloud-based versions of its Office software – was up 33% at $17.7 billion. Sales for Dynamics 365, which competes directly with Salesforce.com, rose 45% and the business version of Office 365 added 15% more users. “That’s the fourth consecutive quarter of 15% seat growth on a very large base,” Microsoft Chief Financial Officer Amy Hood said of the Office 365 results for commercial customers. Microsoft has continued to double down on cloud-base software and said earlier this month it would buy artificial intelligence software firm Nuance Communications Inc for $16 billion, excluding net debt, to bolster its healthcare business. Microsoft said Azure, its closely watched cloud computing business that competes with Amazon.com Inc’s Amazon Web Services and Alphabet’s Google Cloud, grew 50% in the quarter, or 46% when adjusted for currency variations. This is down from a currency-adjusted 48% the quarter before but in line with analysts’ expectations of 46.3% growth, according to data from Visible Alpha. Overall sales at Microsoft’s “intelligent cloud” unit that contains Azure were $15.1 billion, above analysts’ estimates of $14.92 billion, according to Refinitiv data. Microsoft Teams has 145 million daily users, up from 115 million in October, Microsoft said. Sales for Microsoft’s productivity software unit, which includes Office and Teams, were $13.6 billion, compared with estimates of $13.49 billion, according to Refinitiv. Sales for its LinkedIn social network were up 23% on a currency adjusted basis, slightly above Visible Alpha estimates of 21.9%, as revenue continued to recover from a sharp decline in job listings and hiring at the onset of the pandemic. Microsoft’s personal computing unit, which contains its Windows operating system and Xbox gaming console, had $13.0 billion in sales, compared with analysts’ expectations of $12.57 billion, according to Refinitiv data. Sales of Windows to PC makers were up 10%, compared to a 1% rise the quarter earlier. On a call with investors, Microsoft forecast fiscal fourth-quarter productivity segment revenue with a midpoint of $13.93 billion, above Refinitiv estimates of $13.57 billion. Its sales forecasts for its intelligent cloud and personal computing businesses had midpoints of $16.32 billion and $13.80 billion, respectively, above estimates of $16.0 billion and $13.26 billion, according to Refinitiv data."
https://venturebeat.com/2021/04/27/formation-launches-ai-driven-sales-offer-optimization-platform/,Formation launches AI-driven sales offer optimization platform,"Marketers send out billions of offers each year to engage customers. But succes depends on creating the optimal offer for each customer, a challenge that’s been compounded by changes in customer behaviors during the pandemic. A recent Forrester study found that only 33% of customers believe offers they received from brands were relevant. Marketing tech startup Formation claims it has a solution in Dynamic Offer Optimization, a platform that enables travel, retail, and quick service restaurants to create, deploy, and optimize sales offers. Formation claims Dynamic Offer Optimization can increase customer engagement by helping businesses respond more quickly to market forces and changing customer needs. Dynamic Offer Optimization consists of two components. There’s Offer Builder, which creates and optimizes individual offers, and Offer Engine, which drives real-time delivery, tracking, fulfillment, and measurement of each offer. Offer Builder ingests segments and customer journey data from marketing tech stacks and builds millions of unique offer variants. Offer Engine makes the offers available via APIs to channels and tracks, fulfills, and measures each offer’s performance. Formation applies machine learning to optimize each customer’s offer for subsequent deployments. The company says this process can take less than an hour and that customers — including Starbucks and United Airlines — have used it to deliver over 2 billion unique offers during the pandemic. “We’re really excited to take all the knowledge and experience from working with some of the world’s biggest brands for the past five years and make that breakthrough technology available to all companies,” Formation cofounder and CEO Christian Selchau-Hansen said in a press release. “Automating the creation, deployment, and optimization of billions of offers not only drives material impact to the business but enables brands and marketers to also focus on all the other critical mindset and organizational shifts needed for comprehensive digital transformation.” The release of Dynamic Offer Optimization coincides with the launch of Loyalty Innovators. As Selchau-Hansen explains, Loyalty Innovators’ mission is to connect and support digital, marketing, and loyalty leaders in their journeys to adapt their company’s products to the changing consumer landscape. “We are excited to offer marketers unique technology to help them engage customers with relevant and valuable offers with tremendous speed and agility by better leveraging first-party data through automation and machine learning,” Selchau-Hansen continued. “Our mission is to arm digital and marketing leaders with best-in-class optimization tech, as well as to support them through their digital transformation journey with the Loyalty Innovators community.”"
https://venturebeat.com/2021/04/27/pricefx-launches-ai-powered-market-simulation-tool/,Pricefx launches AI-powered market simulation tool,"AI has the potential to add value to marketing and sales operations. According to a McKinsey survey, 40% of marketing departments using AI achieve 6% or higher revenue growth on average. In sales, respondents most often report revenue increases from AI use in pricing, prediction of likelihood to buy, and customer-service analytics. But AI is also credited with improving the scale and speed of price optimization, or the use of analysis to determine how customers will respond to prices for products across different channels. Against this backdrop, Pricefx today launched what it’s calling AI-powered market simulation. Market simulation, which enables price optimization in the context of a product portfolio, as well as the competition, uses algorithms to predict the impact of pricing on customer purchasing behavior. According to chief product officer Toby Davidson, the goal is to help businesses make better business decisions informed by predicted impact. “Market simulation provides our customers with the ability to evaluate data driven market impact to pricing changes they have, or are looking to implement,” he told VentureBeat via email. “Whether you are looking to grab market share through price or make sure you do not create a price war with your competitors and resulting margin and profit leakage, market simulation lets you evaluate the impact your decisions have on your customer base and product portfolio, and the potential reaction from your competitors — before they go live in market.” Market simulation employs “multiagent” AI modeling of price elasticity among a company’s products and projects the demand impacts of various price changes. Through “what if” scenarios, market simulation guesses at possible competitive counter-moves and reflects customer behavior, in addition to the market response to price adjustments. AI is inevitably subjected to bias and unforeseen confounders, which is why AI-driven hedge funds, for example, don’t reliably outperform the market. Indeed, for enterprises using predictive models to forecast consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the pandemic. Organizations were forced to constantly retrain and update their machine learning models, and 12 months later, many are still wrestling with the challenge. But there’s precedent for economy-simulating AI that works at scale. In August, Salesforce open-sourced the AI Economist, a research environment for exploring global tax policies. During experiments, Salesforce says the AI Economist arrived at a more equitable tax policy than a free-market baseline, the U.S. federal single-filer 2018 tax schedule, and a prominent tax framework called the Saez tax formula. McKinsey forecasts that AI-based price and promotion have the potential to deliver between $259.1 billion and $500 billion in market value. It’s estimated that 55% of marketing decision-makers plan to increase their spending on marketing technology, including AI and machine learning, with one-fifth of the respondents expecting to increase by 10% or more. Pricefx’s newly redesigned Plan, Price, and Profit solution sets include market simulation for subscribers of its Price and Profit packages. The market simulation feature is available beginning this week."
https://venturebeat.com/2021/04/27/red-hat-open-sources-trustyai-an-auditing-tool-for-ai-decision-systems/,"Red Hat open-sources TrustyAI, an auditing tool for AI decision systems","The ability to automate decisions is becoming essential for enterprises that deal in industries where mission-critical processes involve many variables. For example, in the financial sector, assessing the risk of even a single transaction can become infinitely complex. But while the utility of AI-powered, automated decision-making systems is undeniable, utility often plays second fiddle to transparency. Automated decision-making systems can be hard to interpret in practice, particularly when they integrate with other AI systems. In search of a solution, researchers at Red Hat developed the TrustyAI Explainability Toolkit, a library leveraging techniques for explaining automated decision-making systems. Part of Kogito, Red Hat’s cloud-native business automation framework, TrustyAI enriches AI model execution information through algorithms while extracting, collecting, and publishing metadata for auditing and compliance. TrustyAI arrived in Kogito last summer but was released as a standalone open source package this week. As the development team behind TrustyAI explains in a whitepaper, the toolkit can introspect black-box AI decision-making models to describe predictions and outcomes by looking at a “feature importance” chart. The chart orders a model’s inputs by the most important ones for the decision-making process, which can help determine whether a model is biased, the team says. TrustyAI offers a dashboard, called Audit UI, that targets business users or auditors, where each automated decision-making workload is recorded and can be analyzed at a later date. For individual workloads, the toolkit makes it possible to access the inputs, the outcomes the model produced, and a detailed explanation of every one of them. Monitoring dashboards are generated based on model information so users can keep track of business aspects and have an aggregated view of decision behaviors. TrustyAI’s runtime monitoring also allows for business and operational metrics to be displayed in a Grafana dashboard. Moreover, the toolkit can monitor operational aspects to keep track of the health of the automated decision-making system. “Within TrustyAI, [we combine] machine learning models and decision logic to enrich automated decisions by including predictive analytics. By monitoring the outcome of decision making, we can audit systems to ensure they … meet regulations,” Rebecca Whitworth, part of the TrustyAI initiative at Red Hat, wrote in a blog post. “We can also trace these results through the system to help with a global overview of the decisions and predictions made. TrustyAI [relies] on the combination of these two standards to ensure trusted automated decision making.” Transparency is an aspect of so-called responsible AI, which also benefits enterprises. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and punish those that don’t. The study suggests companies that don’t approach the issue thoughtfully can incur both reputational risk and a direct hit to their bottom line."
https://venturebeat.com/2021/04/27/open-source-ai-stack-is-ready-for-its-moment/,Open source AI stack is ready for its moment,"Open source stacks enabled software to eat the world. Now several innovative companies are working to build a similar open source software stack for AI development. Dan Jeffries was there when the LAMP stack kicked this off. LAMP is an acronym representing the key technologies first used in open source software development — Linux, Apache, MySQL, and PHP. These technologies were once hotly debated, but today they are so successful that the LAMP stack has become ubiquitous, invisible, and boring. AI, on the other hand, is hotter than ever. Much as the LAMP stack turned software development into a commodity and made it a bit boring, especially if you’re not a professional developer, a successful AI software stack should turn AI into a commodity — and make it a little boring too. That is precisely what Jeffries is setting out to do with the AI Infrastructure Alliance (AIIA). Jeffries wears many hats. His main role, in theory at least, is chief technical evangelist at data science platform Pachyderm. Jeffries also describes himself as an author, futurist, engineer, systems architect, public speaker, and pro blogger. It’s the confluence of all those things that led him to start the AIIA. The AI Infrastructure Alliance’s mission is to bring together the tools data scientists and data engineers need to build a robust, scalable, end-to-end, enterprise artificial intelligence and machine learning (AI/ML) platform. This sounds like such an obvious goal — one that would be so beneficial to so many — you’d think somebody would have done it already. But asking why we’re not there yet is the first step toward actually getting there. Vendor lock-in is a reason, but not the only one. Vendor lock-in, after all, is becoming increasingly less relevant in a cloud-first, open source-first world, although technology business moats live on in different ways. Jeffries was surprised that he did not see an organization actually trying to capture the energy around AI activity, bring different companies together, and get their integrations teams talking to each other. “Every founder and every engineer I ended up talking to was very excited. I really didn’t have to work very hard to get people interested in the concept. They understood it intuitively, and they realized that the innovation is coming from these small to mid-sized companies,” Jeffries said. “They are getting funded now, and they’re up against giant, vertically integrated plays like SageMaker from Amazon. But I don’t think any of the innovation is coming from that space.” Having spent more than 11 years out of his 20-year career at Red Hat, Jeffries recalls how the proprietary software companies used to come up with all the ideas, and then open source would copy them “in a kind of OK way.” But over time, most of the innovation started flowing to open source and to the smaller companies’ projects, he said. The Amazons of the world have their place, as the cloud is where most AI workloads run. Big vertically integrated proprietary systems serve their own purpose, and they’re always going to make money. But the difference is Kubernetes and Docker don’t become Kubernetes and Docker if they only run on Google, Jeffries said. Innovation is going to come from a bunch of these companies working like little Lego pieces that we stack together, he added. That’s precisely what the AIIA is working on. So, when can we expect to have a LAMP stack for AI? In all likelihood, not very soon, which brings us to the other key reason this has not happened yet. Jeffries expects a LAMP stack, or a MEAN stack, for AI and ML to emerge in the next five to 10 years and to change over time. The LAMP stack itself is kind of passé now. In fact, the cool dev kids these days are all about the MEAN stack, which includes MongoDB, ExpressJS, AngularJS, and NodeJS. Jeffries has described these as canonical stacks, which arise with greater and greater frequency “as organizations look to solve the same super challenging problems.” The kind of momentum that happened with LAMP will occur in the ML space, Jeffries suggested. But he warned against believing that anyone has an end-to-end ML system at this point. This can’t be true because the sector is moving too fast. The space itself and the problems to solve are shifting as the software is being created. That makes sense, but then the question is — what exactly is the AIIA doing at this point? And what does the fact that its ranks include some of the most innovative startups in this space, alongside the likes of Canonical and NewRelic, actually mean? Now some innovators are working to build an open source stack specifically for AI. Enthusiasm is good, but there’s a gap between saying “Hey, that sounds like a good idea, sign me up” and actually coming up with a plan to make it happen. So how are the AIIA and Jeffries going to pull it off? As a writer, Jeffries used George R.R. Martin’s metaphor of gardeners and architects to explain how he sees the evolution of AIIA over time. Architects plan and execute; gardeners plant seeds and nurture them. Jeffries identifies as a gardener and sees a lot of the people in the organization as gardeners. He thinks it’s necessary at this phase and envisions the AIIA evolving over time. Right now, the idea is to get people talking at a lot of different levels, rather than working in their own little silos. Micro-alliances are fair game though: “If you look at 30 logos on the website, you’re not going to build an ML stack with all 30 of those things,” Jeffries said. A concern is the fact that building bridges, and communities, takes time and energy. But Jeffries is enthusiastic about the prospect of helping shape what he sees as the AI revolution, is inspired by the open source ethos, and has the leeway from Pachyderm to run with his ideas. That seems to be what he’s doing with AIIA. Currently, he’s working on turning the AIIA into a foundation, and he’s also in talks with the Linux Foundation. The goal is to get to the point of bringing in some revenue. Jeffries is working on finances and a governance structure for the AIIA. “You get people who are just firmly focused on this, and it becomes a balance of volunteer efforts and people paid to work on different aspects. The next step really is a lot of logistical work — the boring stuff,” Jeffries said. Another metaphor Jeffries uses is that of a strategic board game, where you have to think about everything that can go wrong in advance — a bit like a reverse architect. Inevitably, there is going to be at least some amount of boring work, and somebody needs to do it. But for Jeffries, it’s all worth it. “When I look at AI at this point, I think very few people understand just how important it’s going to be. And I think they have an inkling of it, but it’s usually a fear-based kind of thing,” he said. “They don’t understand fully that in the future, there are two kinds of jobs: one done by AI, and one assisted by AI.” Isn’t it actually three types of jobs, as someone has to build the AI? The people building AI are going to be assisted by AI, so that falls into the second category, Jeffries said. There’s a creative aspect, as someone has to come up with an algorithm. But things like hyper-parameter tuning are already being automated, he added. Jeffries waxed poetic about how “the boring stuff” will be done by AI so people can move up the stack and do more interesting things. Even the creative parts will be a co-creative process between people and AI, in Jeffries’ view. As for the “AI destroys all the jobs” narrative, we’ve heard this one before, but the previous industrial revolutions worked out fine, Jeffries argued. Same goes for the argument that the pace of innovation is so rapid that we don’t have time to create jobs to replace those that are going to be displaced. What even an AI optimist like Jeffries can’t easily dismiss is the fact that innovation may not necessarily be coming from the Big Tech companies, but this is where the data is. This creates a reinforcement loop, where more data begets more AI leading to more data, and so on. Jeffries acknowledges data as a legitimate moat. But he believes ML is progressing in ways that make the dependency on data less vital, such as few-shot learning and transfer learning. This, and the fact that the amount of data the world is creating is not getting any smaller, may spur change. What seems inevitable, however, is the need to do lots of work, often boring work, to be able to chase dreams of creativity."
https://venturebeat.com/2021/04/27/amazon-makes-deepracer-software-available-in-open-source/,Amazon releases DeepRacer software in open source,"In November 2018, Amazon launched AWS DeepRacer, a car about the size of a shoebox that runs on AI models trained in a virtual environment with reinforcement learning techniques. DeepRacer has expanded since then, with a women’s league and new miniature race cars. Starting today, Amazon is making the DeepRacer device software available in open source. The pandemic has boosted automation and robotics in the enterprise. The global market for robots is expected to grow at a compound annual growth rate of around 26% to reach just under $210 billion by 2025, according to Statista. Deloitte anticipates that of the almost 1 million robots sold for business use in 2020, just over half were professional service robots, generating more than $16 billion in revenue — 30% more than in 2019. With the release of DeepRacer, developers can change the behavior of their cars, enabling the prototyping of new robotics apps. As Amazon notes, DeepRacer is essentially an Ubuntu-based computer powered by Robot Operating System, open source robotics middleware originated by Willow Garage and Stanford’s Artificial Intelligence Laboratory that provides low-level device control. Six sample applications — Follow the Leader, Mapping, Off Road, RoboCat, DeepBlaster, and DeepDriver — are available to help with brainstorming, including one that creates visualizations of a home or office. As the DeepRacer community creates projects, Amazon says it will add them to the DeepRacer GitHub organization, as well as featuring them in future blogs. “[W]e want to make it easier for developers of all skill levels to prototype new and interesting uses for their car. By making the AWS DeepRacer device software openly available, now anyone with the car and an idea has the ability to make it a reality,” Amazon wrote in a blog post. “Want to block other cars from overtaking it by deploying countermeasures? Want to deploy your own custom algorithm to make the car go faster from point A to B? You just need to dream it and code it.  We can’t wait to see the ideas you come up with, from new racing formats to new uses for the AWS DeepRacer.” Amazon previously partnered with Udacity to offer machine learning courses and a scholarship based around DeepRacer. The ostensible goal was to educate students on the creation, training, and optimization of reinforcement learning models, or models that employ rewards to achieve goals. In a recent analysis, McKinsey pointed out that reinforcement learning can be applied to solve real-life problems beyond autonomous driving, including classification, continuous estimation, and clustering."
https://venturebeat.com/2021/04/27/red-hat-touts-safety-in-future-linux-os-for-cars/,Red Hat touts safety in future Linux OS for cars,"Red Hat has announced plans to create a new Linux-based operating system for the automotive industry. With this push, the enterprise-focused open source software company is touting safety and continuous certification as core selling points. Red Hat, which IBM acquired for $34 billion in 2018, is already well known for its enterprise-grade Linux distribution, which will serve as the basis of its new platform for road vehicles. For the initiative, Red Hat has partnered with Exida, a company that specializes in functional safety and product certification, to provide ongoing certifications spanning a range of applications, from “infotainment to driver operations,” according to a press release. Red Hat said it’s working with Exida to achieve ISO 26262 certification, the global standard governing functional safety in road vehicles. It’s worth noting that Linux is already being used in the automotive industry. Tesla uses its own flavor of Linux in its vehicles, while the Linux Foundation-led Automotive Grade Linux (AGL) project counts founding members such as Ford, Jaguar Land Rover, Nissan, Denso, and Toyota among its members. The AGL’s ultimate goal is to further develop its Unified Code Base (UCB) Linux distribution to give automakers a “70% to 80%” leg up on their infotainment development projects. The 2018 Toyota Camry became the first production vehicle to use AGL in its infotainment system in the U.S. Open source software is used in most applications these days, as it saves companies having to develop every component internally and allows them to scale more quickly through the power of community-led software. But for mission-critical software — like that found in automobiles — safety is of paramount importance. That’s why Red Hat is setting out to bring a specialized Linux product to market with road-faring vehicles specifically in mind. This is consistent with what Red Hat has done elsewhere, building a multi-billion dollar business through serving enterprises with a hardened product with security and add-on services at its core. For potential customers — likely major automotive companies around the world — the prospect of using open source software (compared with proprietary software from a company such as Google) will be hugely appealing, as it ensures they retain control of everything, including their data. Red Hat hasn’t shared timing for its new automotive Linux product, which will depend on how long it takes to certify Red Hat Enterprise Linux components for vehicles. But the company did note it could eventually expand the product’s scope beyond highways and into robotics and manufacturing equipment."
https://venturebeat.com/2021/04/27/cigent-technology-melds-security-and-storage-to-protect-sensitive-data/,Cigent Technology melds security and storage to protect sensitive data,"To protect data on an endpoint, an enterprise has to get everything right. Otherwise, hackers will find a way to slip in and access sensitive data. But Cigent Technology says it has found a new way to protect data by combining methods from security, storage, and data recovery to make an impenetrable system. The company has its roots in a military project it is now turning into a commercial offering, boosted by a new round of funding led by the CIA’s venture capital firm, In-Q-Tel. “This has been a large development, most of it in stealth, with a lot of moving parts,” said Cigent CEO Brad Rowe. “And it culminates with this release.” The company today announced the commercial availability of its service, as well as a $7.6 million round that included money from CyberJunction, WestWave Capital, and a roster of prominent business angels. The money will be used to commercialize its core product, Cigent Data Defense, a combination of hardware and software designed to prevent data theft even when a hacker has penetrated a network. Following Edward Snowden’s leak of classified information, the U.S. government began seeking ways to toughen its internal and external cyberdefenses. Officials turned to a company called CPR Tools, a security and data recovery company based in Fort Meyers, Florida. The company has been around for about 15 years and has 40 employees, most of them from intelligence backgrounds, including its founder, who previously worked at the National Security Administration. Rowe said a multi-year research project generated dozens of hardware prototypes based on CPR’s storage devices. In early 2018, In-Q-Tel approached some team members about commercializing the technology, and Cigent was officially founded with about $2.8 million in seed funding. Cigent engineering VP Tony Fessel explained that the goal of the security system is to get the security as close to the data as possible — in this case, placing the security inside the storage device. In the most basic version, that could be just using Cigent’s software, which allows users and administrators to designate certain files as extra sensitive. The software applies a multi-factor authorization to specific data or files that are required to open them and access the information. While other solutions offer file-based encryption, Cigent prevents them from even copying the file to crack the encryption. Even if a hacker gets access, they won’t be able to open the files without those credentials, Fessel said. “It’s to stop ransomware and stop insiders from getting and doing things with data that they shouldn’t be doing,” he said. By using a secure storage device, Cigent can extend the protection even further. This storage device creates a hidden drive on a computer that intruders wouldn’t be able to spot, and a user can make it visible when they need to access it. The storage device has additional security built into the firmware and uses machine learning and AI to monitor for attacks. When it detects an attack, it automatically responds and self-defends to protect the drive and the data on it from an attack. While the company is now selling the system, it’s also working with partners like Dell and Microsoft to build the protection capabilities directly into their PCs and software. Rowe said the long-term goal is to focus on licensing the technology to other vendors."
https://venturebeat.com/2021/04/27/2nd-watch-most-enterprises-lack-in-house-skills-for-data-strategy/,2nd Watch: Most enterprises lack in-house skills for data strategy,"Most organizations want to make better use of their data, but most — 70% of enterprises — lack a mature data strategy, according to a recent survey on data management and analytics in large enterprises conducted by 2nd Watch, a cloud migration and managed cloud services provider. These enterprises lack the resources, skills and the vision to be successful A key finding of the 2nd Watch 2021 Data Management & Analytics study is that most organizations do not have a mature data strategy. Only 26% of survey respondents said they have any data strategy at all, and 70% don’t have what they consider to be a mature data strategy. It turns out that legacy systems and IT architecture are impeding the ability of many organizations to optimize their data, and IT can be slow to respond to the analytics needs of end users due to complex data integration and management challenges. Nevertheless, progress is being made. 57% of survey respondents said they are using a cloud data warehouse, and 64% said their data is in the cloud. Why are they doing this? 41% said moving to the cloud has allowed them to be more agile. “Agile data and analytics capabilities are essential to build sense-and-respond capabilities and are leading organizations to unprecedented cycles of rapid innovation to meet the new requirements,” market research firm Gartner said in its IT Roadmap for Data and Analytics. There are multiple steps in the process to becoming a data-driven organization, beginning with having a vision and strategy to establishing an operating framework and governance to “continuous intelligence” and refinement and progress, according to Gartner. The 2nd Watch 2021 Data Management & Analytics study includes responses from over 150 cloud-focused IT professionals in companies with at least 4,000 employees. See the full infographic from 2nd Watch."
https://venturebeat.com/2021/04/27/how-merck-works-with-seeqc-to-cut-through-quantum-computing-hype/,How Merck works with Seeqc to cut through quantum computing hype,"When it comes to grappling with the future of quantum computing, enterprises are scrambling to figure just how seriously they should take this new computing architecture. Many executives are trapped between the anxiety of missing the next wave of innovation and the fear of being played for suckers by people overhyping quantum’s revolutionary potential. That’s why the approach to quantum by pharmaceutical giant Merck offers a clear-eyed roadmap for other enterprises to follow. The company is taking a cautious but informed approach that includes setting up an internal working group and partnering with quantum startup Seeqc to monitor developments while keeping an open mind. According to Philipp Harbach, a theoretical chemist who is head of Merck’s In Silico Research group, a big part of the challenge remains trying to keep expectations of executives reasonable even as startup funding to quantum soars and the hype continues to mount. “We are not evangelists of quantum computers,” Harbach said. “But we are also not skeptics. We are just realistic. If you talk to academics, they tell you there is no commercial value. And if you talk to our management, they tell you in 3 years they want a product out of it. So, there are two worlds colliding that are not very compatible. I think that’s typical for every hype cycle.” Merck’s desire for the dream of quantum computing to become reality is understandable. The fundamental nature of its business — biology and chemistry — means the company has been building molecular or “quantum” level models for more than a century. Part of the role of the In Silico Research group is to develop those models that can solve quantum problems using evolving technologies such as data analytics and AI and applying them to natural sciences to make experimental work less time-consuming. But those models are always limited and imperfect because they are being calculated on non-quantum platforms that can’t fully mimic the complexity of interactions. If someone can build a fully fault-tolerant quantum computer that operates at sufficient scale and cost, Merck could unlock a new generation of efficiencies and scientific breakthroughs. “The quantum computer will be another augmentation to a classical computer,” Harbach said. “It won’t be a replacement, but an augmentation which will tackle some of these problems in a way that we cannot imagine. Hopefully, it will speed them up in a way that the efficacy of the methods we are employing will be boosted.” About 3 years ago, Merck decided it was time to start educating itself about the emerging quantum sector. The company’s venture capital arm, M Ventures, began looking within the company for experts who could help it with due diligence as it began to assess quantum startups. That included mapping out the players and the whole value chain of quantum computing, according to Harbach. That led to the formal creation of the Quantum Computing Task Force, which has roughly 50 members who try to communicate with quantum players large and small as well as peers among Merck’s own competition. “We are basically an interest group trying to understand this topic,” Harbach said. “That’s why we have a quite good overview and understanding on timelines, player possibilities, and applications.” As part of that exploration, M Ventures eventually began investing in quantum-related startups. In April 2020, the venture fund announced a $5 million investment in Seeqc, a New York-based startup that bills itself as the “Digital Quantum Computing” company. “We thought that it might be good to have partners in the hardware part and in the software part,” Harbach said. “Seeqc will partner with us within Merck to really work on problems basically as a hardware partner.” Seeqc is developing a hybrid approach that it believes will make quantum computing useful sooner. The idea is to combine classical computing architectures with quantum computing. It does this through its system-on-a-chip design. This technology was originally developed at Hypres, a semiconductor electronics developer which spun out Seeqc last year. The M Ventures funding for Seeqc followed a previous $6.8 million seed round. Seeqc raised a subsequent round of $22 million last September in a round led by EQT Ventures. According to Seeqc CEO John Levy, the company’s technology allows it to address some of the fundamental challenges facing quantum systems. Despite rapid advancements in recent years, quantum computers remain too unstable to deliver the high-performance computing needed to justify their costs. Part of the reason for that is that qubits, the unit of quantum computing power, need to be kept at near-freezing temperatures to process. Scaling then becomes costly and difficult because a system operating with thousands of qubits would be immensely complex to manage, in part because of the massive heating issue. Levy said Seeqc can address that problem by placing classic microchips over a qubit array to stabilize the environment at cryogenic temperatures while maintaining speed and reducing latency. The company uses a single-flux quantum technology that it has developed and that replaces the microwave pulses being used in other quantum systems. As a result, the company says its platform enables quantum computing at about 1/400 of the cost of current systems in development. “We have taken much of the complexity that you’ve seen in a quantum computer and we’ve removed almost all of that by building a set of chips that we’ve designed,” Levy said. Just as important is a philosophical approach Seeqc is taking. It’s not building a general-purpose quantum computer. Instead, it plans to build application-specific ones that are tailored specifically to the problems a client is trying to solve. Because Seeqc has its own chip foundry, it can customize its chips to the needs of application developers as they create different algorithms, Levy said. In that spirit, Merck’s Quantum Computing Task Force is working closely with Seeqc to create viable quantum computers that can be used by its various businesses. “Their technology is a key technology to scale a quantum computer, which is actually much more important because it will make quantum computers bigger and cheaper,” Harbach said. “And this is, of course, essential for the whole market.” For all this activity, Harbach’s view of quantum’s potential remains sober. He sees nothing on the market that will have any commercial impact, certainly not for Merck. At this point, many of the company’s questions remain academic. “What we are basically interested in is how — or will — the quantum computer hardware ever be scalable to a level that it can tackle problems of realistic size to us,” Harbach said. “And the same question also goes to the software side. Will there ever be algorithms that can basically mimic these problems on a quantum computer efficiently so that they don’t run into noise problems? We are not interested in simulating a molecule right now on a quantum computer. Everything we try to understand is about the timelines: What will be possible and when will it possible.” Harbach has watched the rise in quantum startup funding and various milestone announcements but remains dubious of many of these claims. “They are creating a new market where there’s not even the technology ready for it,” Harbach said. “You have to stay realistic. There’s a lot of money at the moment from governments and VCs. There’s a lot of boost from consultancies because they try to sell the consultancy. And if you talk to experts, it’s the other way around. They tell you not before 15 years.” The questions Merck asks internally are split into 2 fundamental categories: When will there be a quantum computer that can be more efficient at processing its current quantum models? And when will there be a quantum computer that is so powerful that it opens up new problems and new solutions that the company cannot even imagine today? “Quantum will be a thing, definitely,” Harbach said. “The only question is when, and I’m really, really sure it won’t be in the next two years. I wouldn’t even say three years. There will be a quantum winter. Winter is coming.”"
https://venturebeat.com/2021/04/27/legal-management-startup-clio-nabs-110m-to-expand-its-platform/,Legal management startup Clio nabs $110M to expand its platform,"Legal practice management startup Clio today announced it has closed a $110 million series E round, valuing the startup at $1.6 billion post-money. Clio says the funding will be used for strategic acquisitions and partnerships and to grow its workforce, with a focus on product and engineering teams. Contract management is often a time- and money-sucking endeavor for law firms, regardless of their size. According to a 2016 survey conducted by Apptus, 39% of legal departments are forced to rely on people without law degrees to manage their contracts, while 50% take a week or longer to turn out documents like non-disclosure agreements. Moreover, only 40% of firms say they have an automated contract management tool. Jack Newton and Rian Gauvreau cofounded Vancouver, Canada-based Clio in 2007 with the goal of building a legal services orchestration platform. When the company launched its software in 2008, it was among the first cloud-based practice management software products developed for law firms, Clio claims. Clio helps manage cases, organize contacts, and automate documents, as well as keeping track of financials and client legal accounts. The platform supports a range of billing options, including online payments, automated billing, and customized plans. Clio also offers tools that allow clients to create intake forms; automate client communication like emails, reminders, and requests; and organize referrals. According to a recent Gartner survey, the percentage of corporate legal operations leaders responsible for coordinating law firm billing increased 53% in the past two years, while the percentage of those tracking outside counsel spend increased 32%. One report predicts that the global legal operations software market will reach $3.57 billion by 2027, growing from just $1.08 billion in 2018. Clio made its first acquisition in October 2018, snapping up Los Angeles, California-based software provider Lexicata. Lexicata’s product then served as the foundation for Clio Grow, a client intake and invoicing toolkit. By the time Clio purchased Lexicata, the former’s app directory — a portfolio of integrations — had reached 120 connectors. It has now grown to 200, enabling Clio’s platform to sync with other commonly used legal software apps. T. Rowe Price and Omers Growth Equity led Clio’s latest funding round. It brings the company’s total raised to date to over $386 million, following a $250 million series D in September 2019. In recent years, “legaltech” has become one of the most active sectors for investors. Investment activity accelerated as the pandemic spurred legal firms to shift to digital-first business models. As of November 2020, there were 80 investments in legaltech, totaling $490 million for the year."
https://venturebeat.com/2021/04/27/automated-contract-negotiation-platform-pactum-raises-11m/,Automated contract negotiation platform Pactum raises $11M,"Pactum, a platform that leverages AI to automatically negotiate supplier contracts for enterprises, today announced it has raised $11 million in a series A round of funding led by Atomico. The company’s “negotiation-as-a-service” platform hits general availability from today and has already been used by several major enterprises. Clients include Walmart, the world’s largest retailer, which first signed up for a pilot program last year to automate negotiations with its supplier network. Pactum’s software essentially scans a contract, extracts what it believes to be the key priorities, and then sets about negotiating terms with the supplier through a chatbot. At its core, Pactum is all about making contract negotiations less laborious and enabling businesses to go beyond the few key terms that normally revolve around price and payments. “When we work with our customers, we go through a discovery process lasting several weeks,” Pactum cofounder and CTO Kristjan Korjus told VentureBeat. “During that time, we have identified up to 30 items to negotiate about in just one use case alone. Each of these items can be exchanged for value for both sides. Examples include freight, warehousing, contract length, contract cancelation terms, growth rebates, and so on.” The full deployment process can take a few months, including interviews with key stakeholders involved in the contract process. After Pactum’s system is built for the company in question, it’s able to work completely autonomously. This includes making decisions based on data, including current commercial terms, gleaned from enterprise resource planning (ERP) systems and other sources. “The system learns from each negotiation, so every negotiation will make every other negotiation better,” Korjus explained. “This creates a virtuous cycle.” It’s worth noting that Pactum may be best suited to smaller contracts or contracts that involve renegotiating an existing deal. This may be particularly true for larger enterprises that have dozens or hundreds of smaller contracts with different suppliers — and is why Pactum is targeting businesses with more than $1 billion in revenue that are more likely to benefit from this type of technology. Pactum isn’t purely about helping its own customers — it has to benefit both sides, as any typical negotiation would. “This is the quickest way to reach a Pareto optimal outcome — which is a situation where one party (supplier or the enterprise) can be better off without making the other party worse off,” Korjus said. Founded out of Estonia in 2019, Pactum is now headquartered in Mountain View, California. The company had previously raised around $4 million in funding and with another $11 million in the bank is now well-financed to scale its technology beyond the handful of enterprises it was already working with to refine its AI smarts. In other words, Pactum is now officially open for business, and it plans to build a platform that can “simultaneously work with many new enterprise customers on multiple deployments,” according to Korjus. Aside from Atomico, investors in this series A round included Project A, Metaplanet, Checkout.com CTO Ott Kaukver, TransferWise cofounder Taavet Hinrikus, and Teleport cofounder Sten Tamkivi."
https://venturebeat.com/2021/04/27/arms-neoverse-server-chips-generate-at-least-40-better-performance/,Arm’s Neoverse server chips generate at least 40% better performance,"Arm unveiled the performance numbers for its Arm Neoverse V1 and N2 server chip platforms, with processing boosts ranging from 40% to 50% over the previous generation. The demands of datacenter workloads and internet traffic are growing exponentially, and new solutions are needed to keep up with these demands while reducing the current and anticipated growth of power consumption. But  Arm said the variety of workloads and applications being run today means the one-size-fits all approach to computing is no longer the answer. That’s a jab at dominant server vendors Intel and Advanced Micro Devices, which use the x86 architecture. The Arm Neoverse V1 is a server chip microarchitecture that Arm’s customers — the big chipmakers of the world — can design chips around for servers in the big datacenters that power the internet. The V1 supports Scalable Vector Extension (SVE) and delivers more than 50% performance increases for high-performance computing machine learning workloads. “The time for Neoverse across all infrastructure is now,” Chris Bergey, senior VP for the infrastructure line of business at Arm, said in a press briefing. And another chip microarchitecture, the Arm Neoverse N2 platform, uses the new Armv9 architecture that Cambridge, United Kingdom-based Arm recently announced. It can deliver 40% more performance for a variety of workloads. “I think the N2 will pleasantly surprise people how performant designs will be in single-threaded designs,” said Patrick Moorhead, an analyst at Moor Insights & Strategy. “V1 looks to be a strong start in a nichey market, HPC. Overall, Arm is raising its game in the compute market.” Bergey said the journey to producing competitive server chips began a decade ago. Chips based on the designs should be hitting the market either late this year or early next year. Arm said the Arm Neoverse CMN-700 is the industry’s most advanced mesh interconnect to unleash the performance and performance/watt benefits of Neoverse V1 and N2 platforms. This device is a key element for constructing high-performance Neoverse V1 and Neoverse N2-based systems-on-chip (SoCs). It enables higher core counts and cache memory sizes. As Moore’s law comes to an end, solution providers are seeking specialized processing. Enabling specialized processing has been a focal point since the inception of the Neoverse line of platforms, and Arm expects these latest additions to accelerate this trend. Back in September, Arm unveiled the new Neoverse N2 and Neoverse V1 platforms without talking about performance. Now the company is talking about the performance per watt, the total cost of ownership benefits, and partners adopting the designs. “We believe Arm processors are coming to servers in a big way. We believe Arm is actually going to be everywhere, from the edge to the cloud,” Oracle senior VP Bev Crair said in a press briefing. Among the customers: These partners are taking full advantage of what is under the hood of Neoverse platforms. This is just the tip of the iceberg for infrastructure workload benefits and how partners plan to implement and take Neoverse IP to market, Bergey said. Arm argues that innovators shouldn’t have to choose between performance and power efficiency. The chips can target a range of cloud-to-edge uses. “The Neoverse V1 and N2 are huge improvements for Arm,” Tirias Research analyst Kevin Krewell said in an email to VentureBeat. “The V1 with the Scalable Vector Extensions (SVE) [is] powerful enough to be the CPU core for supercomputers. Even though Arm didn’t provide performance numbers against AMD and Intel, it seems to be very competitive, based on Arm’s data. The N2 is not an insignificant improvement over the N1. It’s the core to use for designs with very high core count, trading off some performance and a narrower SVE implementation for a smaller core size and lower power. These improvements are in line with Nvidia’s goals for the Arm architecture in the datacenter, and one of these cores could well be the core used in Nvidia’s Project Grace CPU.” Linley Gwennap, principal analyst at the Linley Group, said in an email that third-party test results are telling. “AMD’s latest Epyc processor outperforms the fastest Neoverse N1 chip on almost every test, often by a wide margin, despite the Arm chip having more cores,” Gwennap said. “Even after adjusting for TDP, the two chips have about the same performance per watt. Arm’s superiority claims rely on synthetic benchmarks that scale ideally across 64 or more cores, which isn’t representative of the real workloads that Phoronix measures. I also estimate that AMD Zen 3 leads the N1 by 60% on single-thread applications.” He added, “If you take this N1 comparison and project based on Arm’s data, the N2 will still be about 20% behind Zen 3 for single-thread (scale-up) workloads. According to Arm, N2 power rises by more than its performance, so power efficiency is actually worse for N2 (and much worse for V1). So if AMD matches N1 on performance per watt, N2 won’t give Arm the lead on that metric. In summary, until Arm achieves parity in single-thread performance, it will be limited to scale-out workloads. And unless it can demonstrate a sizable advantage in performance per watt on real applications, its main selling point is lower prices.” This chip design delivers a 50% uplift, as well as a 1.8 times improvement for a range of vector workloads and a 4 times improvement for machine learning workloads over N1. Neoverse V1 is the first in a new performance-first computing tier for Arm. Neoverse V1 gives silicon partners the flexibility to build compute for applications more reliant on CPU performance and bandwidth while providing system-on-chip (SoC) design flexibility. The performance-first design philosophy behind Neoverse V1 was to build the widest microarchitecture Arm has ever produced to accommodate more instructions in flight in support of markets like high performance and exascale computing. The wide and deep architecture — with the addition of scalable vector extensions (SVE) — gives Neoverse V1 the lead in per-core performance, as well as code longevity with SVE, and provides SoC designers implementation flexibility, Arm said. You can see the benefits of some of these design elements in SiPearl and ETRI’s HPC SoCs, and Arm thinks this is the direction HPC compute is heading, Bergey said. The Neoverse N2 is aimed at cloud-to-edge performance. A few weeks ago, Arm introduced the Armv9 architecture to address global demand for ubiquitous specialized processing. The Neoverse N2 platform is the first based on the Armv9 architecture with improvements to security, power efficiency, and performance. Delivering 40% higher single-threaded performance compared to N1, Neoverse N2 still retains the same level of power and area efficiency as Neoverse N1. The scalability of Neoverse N2 extends from high-throughput computing, such as in hyperscale cloud, where Arm sees 1.3 times improvement on NGINX over N1. The Neoverse N2 platform delivers superior performance per thread and industry-leading performance per watt, driving a reduced total cost of ownership for users. Neoverse N2 is the first platform to feature SVE2, an Armv9 feature that drives a significant uplift in cloud-to-edge performance efficiency. For a broader set of use cases, like machine learning, digital signal processing, multimedia, and 5G systems, SVE2 brings performance and ease of programming, as well as the portability benefits of SVE."
https://venturebeat.com/2021/04/27/campfire-raises-8-million-to-advance-ar-vr-for-product-design/,Campfire raises $8 million to advance AR/VR for product design,"Campfire has raised $8 million in funding for its holographic technology that enables both augmented reality and virtual reality for the purpose of enterprise product design. In stealth until now, Campfire has created a holographic collaboration system for professional 3D designers. The hardware and software headset system is based on patents and technology formerly created by Meta, which ran out of money in 2018. It will come out as a subscription in the fall of this year, after years of research and development. The company has fewer than 15 people and said the funding came from OTV, Kli  Capital, Tuesday Capital, and others. “This was designed for a very specific purpose, for designers and engineers who need to share with other stakeholders inside their company and others,” Campfire CEO Jay Wright told VentureBeat in an interview. “So I’m designing something that I need to show to my colleagues. Paramount for them is the visual experience.” Campfire combines proprietary devices and applications built on a foundation of more than 60 patents. It’s based on the lessons of the past few years of headsets, including the failure of Meta. “The problem with today’s devices is that the experience is not good,” Wright said. “The experience suffers from big problems. They have a small field of view and poor image quality. The devices press on our face, cause pain, get warm, and make us uncomfortable. The users also have to learn new interfaces and tools. And integrating with your regular workflow has been really poor.” With Campfire, Wright said the team had a chance to redo hardware and software and rethink it all. The team focused on creating a full system with good visual performance, ease of use, and an integrated workflow. The Campfire headset delivers a real AR experience with a 92-degree diagonal field of view, Wright said. He added that it doesn’t touch your face and it’s comfortable. The Campfire system is being used by industrial design firm Frog Design. Frog is a Campfire development partner, along with a select group of companies. The Campfire system, which consists of three devices and two applications, is available for preview through Campfire’s Pioneer Program. The Campfire headset can provide VR and AR with one device. Designers can visualize physical products with a natural view of the real world or an environment of their choice using a single headset. You wear the headset on your head, but it doesn’t completely obstruct your view. “You retain your peripheral vision. And by retaining your peripheral vision, you feel a lot more comfortable walking around your desk or table,” Wright said. “It goes a long way for people who get VR sickness because they can still maintain the horizon and see what’s around them.” The Campfire Console is a new device that acts like a holographic projector. And the Campfire Pack transforms a mobile phone into an intuitive controller with tools for working with 3D models. It attaches to the back of the phone and eliminates the learning curve of proprietary controllers and gesture interfaces. “The vision for holographic collaboration has been talked about for decades but not realized in products with any measure of success,” Wright said. “By focusing on specific needs for design and engineering, we’ve reimagined the entire stack to deliver an experience that takes a giant step toward the vision — and more importantly enables a giant step in productivity.” Meanwhile, Campfire Scenes enables users to create scenes from existing 3D models for quick reviews or elaborate presentations. “Campfire Scenes is the tool that solves the workflow gap,” Wright said. “It gets your existing 3D files in a way that can be shared easily.” And the Campfire Viewer enables users to work alone or together during video calls, using a Campfire Headset, tablet, or phone. “The Campfire Viewer is what you use to open the Campfire documents,” Wright said. “You plug the headset into the laptop and you’re in.” Frog Design’s teams have used the device to work together virtually while spread out, Frog venture design lead Graeme Waitzkin said in a statement. “I wanted to make sure we had people on board that were part of the process and were the top of their game for design,” Wright said. “Frog Design was at the top of that list.” Waitzkin said his teams jumped at the chance to test the system as a development partner. “It doesn’t stop with devices,” Wright said. “The Campfire Scenes are like a mashup of Google Docs and Powerpoint. It allows you to take more than 40 different CAD and 3D formats and organize them into a series of three-dimensional scenes that can then be shared with Campfire users. And they can open up, and you can view them with the headset.” You could compose something using a normal computer and then be able to see what it looks like through a live preview. Besides Wright, the team includes chief operating officer Roy Ashok and founding adviser Avi Bar-Zeev. Campfire was started as Meta View in December 2018 by venture capital firm OTV to purchase and commercialize IP developed by the Meta Company, including an AR visor design and a patent portfolio. “Two years ago, I took on a new mission with Campfire, and I couldn’t be more excited to tell you about today,” Wright said. “We really got some rock stars that have been behind the scenes for the last couple of years building this. And the space we’re in is holographic collaboration. It’s been called spatial collaboration. You hear it talked about. It’s the promise we’ve seen in science fiction, the promise we’ve also seen for a lot of companies in the AR/VR space. They have presented compelling visuals but haven’t delivered on them.” OTV recruited Wright, a former Qualcomm executive and president of Vuforia at PTC, as CEO in May 2019 to execute his vision for an integrated hardware and software solution designed from the ground up for collaboration. Campfire’s goal is to bring existing 3D workflows into 3D space for knowledge workers, not to replace the phone or computer, but to extend them. “This is about knowledge workers, specifically people that are working with 3D to build things today, like CAD and similar tools,” Wright said. “So it’s not about full immersion that we need for gaming. And it’s not about full immersion for making somebody feel like they’ve got an emergency situation in a cockpit for training. It’s about visualizing products, and doing it in the easiest and most flexible way.” Campfire utilizes a unique headset to provide holographic views of 3D models and data in AR and VR. The holographic views result from a stereo image generated by two separate displays (left and right) that reflect on the inner surface of the headset visor. The displays are driven by a discrete graphics processing unit (GPU) on a PC connected with a USB-C cable. This is a different operating principle than employed by holographic projectors/displays that seek to generate holograms that are visible with the naked eye. Campfire works with more than CAD/3D file formats directly from a desktop PC. The hardware connects to a PC with a discrete GPU, Windows 10, and a Thunderbolt-3 port with a USB-C connector. The Campfire Pack requires a recent iOS or Android phone."
https://venturebeat.com/2021/04/27/rewind-extends-saas-data-backup-and-recovery-to-trello/,Rewind extends SaaS data backup and recovery to Trello,"Team communication and collaboration software revealed its true worth over the past 12 months, as businesses across the spectrum rapidly transitioned to remote work. From Zoom to Slack and beyond, companies that weren’t already all-in on the digital workforce were given little choice — it was either sink or swim. However, with cloud spending going through the roof in 2020, a trend that’s set to continue in 2021 and beyond, this opens a Pandora’s box of questions for businesses embracing the giant hard-drive in the sky — how safe is all their data? Privacy issues aside, companies that entrust all their mission-critical information to third-party SaaS companies and clouds need a backup plan if disaster strikes. A recent cloud threat report published by Oracle and KPMG found that 75% of organizations in the study had experienced data loss from a cloud service on more than a single occasion. And this is something that Canadian company Rewind is setting out to solve with automated data backup and recovery services for many of the popular SaaS tools of today. Up until now, Rewind offered data backup and recovery for Shopify, BigCommerce, Intuit QuickBooks, and — as of two months ago — GitHub. Today, the company is extending support to Trello, the popular team collaboration and project management platform operated by Atlassian. It’s worth noting that most SaaS platforms offer their own disaster recovery tools for when a systemwide catastrophe occurs, so if a fire rips through one of their datacenters they can restore all the accounts to their former state from an alternative (backup) datacenter. But this doesn’t work at an individual account level, and the SaaS company typically doesn’t enable customers to recover individual data specific to them on-demand. This is what is widely known as a “shared responsibility” model, where the platform owner (e.g. Trello or GitHub) is responsible for infrastructure-level security and disaster recovery, and the customer is responsible for managing password security, permissions, and backing up all the data in their account. There are various existing methods open to Trello users looking to create backups for their data, such as setting reminders to capture screenshots of boards, exporting JSON or CSV files, or manually creating copies of project boards. Ignoring the significant time and resource drain this creates for companies, the process of restoring the data in these scenarios doesn’t bear thinking about. “The main issue with these types of manual backups is the inability to easily restore data,” Rewind CEO and cofounder Mike Potter told VentureBeat. “Manually backing up data means manually restoring it, which tends to be a slow and tedious process. Manual backups are also frequently forgotten and left out-of-date.” And that, essentially, is the role that Rewind fulfills. It not only creates and stores automated backups of each customers’ Trello instance, it restores it all to its former glory with the click of a button. The integration is available via Trello’s Power-Up marketplace, and it requires no real technical prowess — the full backup and recovery service is accessible via a browser. Moreover, Rewind backs up individual items of data and all their dependencies and relationships. This includes each Trello board, as well as all the cards (tasks or ideas), lists (collection of cards), labels, custom fields, checklists, and attachments on that board. At launch, however, users can only back up their boards and all the associated entities as a whole package. In the near future, users will also be able to choose on a more granular level, so they can just back up specific cards, lists, or attachments, for example. This all leads us to one lingering question. Why don’t SaaS companies offer such account-level backup services natively? This would surely be a huge selling point, particularly for enterprise clients. “While backups might seem like basic functionality, the fact is that building and continuously supporting a full-featured, scalable backup and restore solution presents non-trivial technical and usability challenges that tend to lie outside the core capabilities of commonly used SaaS platforms,” Potter hypothesized. Moreover, it’s good practice to house backups away from the host platform. This isn’t purely for reasons related to natural disasters — how do you access your Trello backup if, for example, you’re locked out of your Trello account? “A true backup gives you full access to your data at all times,” Potter said. “Best practices for data security and business continuity call for the 3-2-1 backup method — three total copies of your data, two of which are local but on different mediums or devices, and at least one copy off-site.” This latest launch comes just a few months after Rewind raised its first notable outside funding, securing $15 million in a series A round led by Inovia Capital. In the future, Rewind said that it plans to extend support to other popular SaaS tools such as Jira, GitLab, Xero, Bitbucket, and Zendesk."
https://venturebeat.com/2021/04/27/adobe-extends-ai-infused-customer-analytics-platform-to-offline-data/,Adobe extends AI-infused customer analytics platform to offline data,"Adobe today unfurled an enhanced Adobe Customer Journey Analytics cloud service that enables organizations to apply AI to data from both online and offline sources to gain deeper insights into customer behavior. Available as an extension to the Adobe Experience Platform, the Adobe Customer Journey Analytics service extends the analytics capabilities based on its Sensei AI platform that is already widely employed to track online engagements into the realm of data collected offline. That omnichannel approach to tracking a customer journey is becoming more critical as customers begin to return to retail outlets as more people receive COVID-19 vaccinations, said John Bates, director of product management for Adobe. He spoke during an Adobe Summit event. Adobe Customer Journey Analytics provides a unified view of a complete customer journey using real-time analytics dashboards that can be accessed from anywhere on any type of device, including via a mobile application for accessing Adobe dashboards that was also unveiled today. That approach is intended to democratize the advanced analytics enabled by the Adobe Sensei platform as an alternative to relying solely on data scientists to sift through data. Alerts that are generated by Adobe Customer Journey Analytics will automatically surface insights that would have otherwise gone unnoticed, Bates said. In comparison, legacy analytics applications assume the end user knows what queries should be launched to answer a set of already known questions. Adobe Customer Journey Analytics “will help you identify the unknown unknowns,” said Bates. Over time those alerts, currently available in preview mode, will become more personalized as the Adobe Sensei platform identifies what data is being accessed most often, added Bates. Adobe is also making it easier to collect and process data. A data views feature allows organizations to ingest more of their data in its original format, then apply logic at the time of reporting. That capability will make it easier for organizations to slice and dice data without having to first normalize it into a specific format. The goal is to enable organizations to track customer behaviors more easily whenever they see fit, noted Bates. As part of that effort, data collection tasks that once occurred on a mobile device or in a browser are being shifted to the Adobe server that is part of Adobe Experience Platform Collection Enterprise. The Adobe Experience Platform Edge Network then makes it possible to receive and send an event or individual piece of data in milliseconds in a way that enables privacy and governance controls to be applied. In effect, Adobe is making a case for extending the level of visibility that many organizations have into online customer behavior to include offline activities such as the number of times they visit a mall. Customers may never return to retail outlets in the same numbers they did prior to the pandemic, but it’s also clear many have become more adept at moving back and forth between an online application and a brick-and-mortar experience. It’s not uncommon these days for shoppers to employ a mobile application within a retail outlet to check, for example, inventory availability before deciding whether to make a purchase now at the store or later online. Adobe isn’t trying to replace data scientists. In many instances, Bates noted, data collected by the Adobe cloud offerings will be shared with data scientists that will be using tools from Adobe and others to analyze massive amounts of data. However, it’s also apparent business executives want to be able to identify and track different types of customer journeys in near real time. The challenge and the opportunity now is to enable that capability in the most frictionless way possible."
https://venturebeat.com/2021/04/27/automox-raises-110m-to-help-enterprises-manage-endpoints/,Automox raises $110M to help enterprises manage endpoints,"Endpoint management platform Automox today announced it has closed a $110 million equity round led by Insight Partners. The funds, which come as Automox’s customer base grew 200% from June 2019, bring the startup’s total raised to over $152 million. The company says it will use the funding to support product development. The average cost of a data breach is nearly $4 million, and yet 74% of companies say they can’t patch vulnerabilities quickly enough because they lack the necessary staff. That’s why former HP national account director Jay Prassl founded Automox, which is developing a platform that automates endpoint configuration, patching, management, and inventory. Automox’s platform, which works across operating systems, servers, and PCs, lets security teams automate and conduct cybersecurity actions through policies. From a dashboard, users can orchestrate checks to ensure patch compliance of assets, regardless of location, and perform technical and top-level reporting. Automox also enables critical patches and software updates throughout enterprise computing environments, as well as security configurations and custom scripting. “When we started out, we built the company around the most compelling pain points in the IT Ops space, which was the inability to see and remediate system vulnerabilities across Windows, Mac, and Linux from a single platform,” Prassl told VentureBeat via email. “A lot has changed since those early days, and we have continued to broaden our functionality to become the only fully cloud-native endpoint management platform.” This April, Automox announced the limited release Data Extract, a new report to gather historical patch management insights within a customer-specified time frame. The company also rolled out the Community Worklet Catalog, which allows customers to review, customize, and deploy workflow automations from the wider Automox community. Automox occupies a global cybersecurity market that’s anticipated to be worth $120 billion by 2024 — Israeli startups alone raised $6.32 billion between 2013 and 2019. That’s not surprising, considering Juniper Research pegs the number of digital records that will be stolen in 2023 at 33 billion, compared with the 12 billion stolen in 2018. As recently as 2017, the U.S. outranked all other countries in the volume of ransomware attacks, according to Symantec. And analysts elsewhere estimate that the cybercrime economy has grown to $1.5 trillion in annual profits and that damages will reach $6 trillion by 2021. Indeed, Tel Aviv- and Boston-based CybeReason raised $200 million in August 2019 for its enterprise endpoint protection platform, shortly after SentinelOne nabbed $120 million. CrowdStrike, an AI-powered cybersecurity platform specializing in endpoint protection and threat intelligence, also recently raised $200 million. And AI-powered cybersecurity startup Cylance snagged $120 million in June 2018 to expand its platform globally. For its part, Automox, which has over 160 employees, claims that its nearly 2,000 customers — including Greyhound, NASA, Xerox, and Unicef — have seen a 50% reduction in labor hours to manage patches and an 80% reduction in corporate attack surfaces. That’s in addition to 2-3 times less effort and 15 times faster hardening of infrastructure than with competing solutions. “While there are other players in the IT Ops and endpoint management space, such as Ivanti, ManageEngine, and Tanium, Automox is the only truly cloud-native platform in the market that has the flexibility to solve the problem that modern IT teams face. IT operators don’t need another siloed on-prem tool to manage,” Prassl said. “Many use up to seven different tools, which can leave operators with outdated views of their infrastructure and no single source of truth. As the strategic value of IT Ops continues to rise, so does the demand for a modern cloud-native platform that can interface with security tools they use today, such as CrowdStrike and SentinalOne.” Blackstone and existing investors Koch Disruptive Technologies and TechOperators also participated in Automox’s new series C round."
https://venturebeat.com/2021/04/27/databook-raises-16m-to-automate-key-sales-processes/,Databook raises $16M to automate key sales processes,"Databook, a customer intelligence platform tailored for enterprise applications, today announced that it raised $16 million in series A funding. The company plans to put the funds toward developing new products and expanding its workforce, in addition to acquiring new clients around the world. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Forrester predicts that 57% of business-to-business sales leaders will invest more heavily in tools with automation.  And that’s perhaps why Salesforce anticipates the addressable market for customer intelligence will grow to $13.4 billion by 2025, up from several billion today. Palo Alto, California-based Databook, which was founded in 2017 by Anand Shah and Alex Barrett, offers a dashboard that lets sales teams determine the strengths and weaknesses of their prospects. Each salesperson is able to see how likely prospects are to make a purchase and estimate the impact of sales solutions, as well as view recommendations and suggestions that might help to drive the next steps. Teams using Databook get notifications about account events and news including buyer names and responsibilities. They’re also afforded access to Databook’s automated content creation tools, which can generate documents like executive one-pagers, emails, and benchmarking books. Leveraging algorithms, templates, and big data analytics, Databook can deliver an overview of a customer’s financial and operational performance compared with its industry peer group or an account plan deck for the customer in question. Beyond this, Databook features a module that analyzes publicly available company data across different accounts. It can cross-reference prospect data against a team’s performance and pipeline activities, and highlight deals requiring further inspection and possible adjustments to forecasts. Moreover, the module can show which prospects are at the right stage of their budgeting cycle and outline priorities that rank highest for leadership at the companies. “In the sales and marketing tech space, we are defining a new category of customer intelligence focused on enabling a highly consultative, enterprise selling motion that’s designed to originate and close large opportunities,” a spokesperson told VentureBeat via email. “The vast majority of other tools in sales and marketing tech are designed to accelerate or automate the high volume, transactional selling motion that’s predominant in B2B organizations today. Since this is a necessary motion for companies to maintain in some capacity, we view Databook as additive and a complement to these solutions.” Databook squares off against a number of rivals in a sales enablement market that’s anticipated to be worth $2.6 billion by 2024, according to Markets and Markets. Seismic has raised hundreds of millions of dollars to build out its automated sales and marketing enablement suite. Highspot nabbed $60 million in June 2019 for a sales enablement toolset that taps AI to power features like semantic search. In June 2019, Showpad secured $70 million for its cloud-based sales tools, and that April, Outreach raked in a cool $114 million to grow its semiautomated sales engagement software. But in spite of the competition, Databook claims to have experienced 300% year-over-year growth since its founding. Microsoft’s M12 and Salesforce Ventures led Databook’s latest tranche, which saw participation from Threshold Ventures, Haystack, and Firebolt. The new capital follows a seed round raised in January 2020 and brings Databook’s total investment to $22 million."
https://venturebeat.com/2021/04/27/zoom-brings-alexa-for-business-to-conference-room-calls/,Zoom brings Alexa for Business to conference room calls,"Zoom has announced that Amazon’s business-focused Alexa integration is now available for everyone in Zoom Rooms appliances. The launch comes a day after Zoom launched a new Immersive View feature that allows meeting hosts to arrange participants in a single virtual environment. Amazon launched Alexa for Business back in 2017 to bring the tech titan’s voice assistant to enterprise communication tools such as Cisco, Polycom, and Zoom itself. This meant companies using these tools could say something like, “Alexa, join my meeting” to start their call or “Alexa, find me a free room.” This was made possible due to an array of features Amazon had previously launched to make Alexa more compatible with office environments, including the ability to integrate with video- and audio-call software and integrations with Office 365 and Google Calendar. At its annual Zoomtopia event back in October, Zoom announced that Alexa for Business would eventually arrive for Zoom Rooms Appliances, a program Zoom launched back in 2019 to bring dedicated Zoom hardware to meeting rooms around the world. This launched in beta in January, and now it’s ready for prime time. With this integration, companies can start and stop all their Zoom Rooms meetings using voice commands, as well as finding available rooms in the building to host a meeting by asking something like: “Alexa, find me an available room for 45 minutes.” As the world tries to regain some semblance of normality, businesses will be exploring new contactless ways to run their meetings. With Zoom Rooms and Alexa, a small team in one office can communicate with another team in another office without having to touch any buttons. It’s worth noting that businesses could already set up Zoom Rooms, which is basically Zoom’s software product for physical conference rooms, to work with Alexa on Amazon’s own Echo hardware and other third-party devices such as Logitech’s. But today’s announcement opens Alexa to businesses that don’t have (or don’t want to buy) other Alexa-enabled devices, as admins can now enable Alexa for Business directly through the Zoom management portal."
https://venturebeat.com/2021/04/27/earthquake-monitoring-platform-safehub-raises-9m/,Earthquake-monitoring platform Safehub raises $9M,"Safehub, a startup developing an internet of things platform to monitor the structural integrity of buildings, today announced the closure of a $9 million series A round led by A/O PropTech. The company says it will use the capital to expand its platform and acquire new customers, as well as growing its engineering team. A recent FEMA study pegged U.S. losses from earthquakes at $4.4 billion per year. Each year, there are on average about 15 earthquakes with a magnitude of 7 or greater, strong enough to cause damage in the billions and significant loss of life. In spite of this, more than 60% of U.S. small businesses don’t have a formal emergency-response plan and fail to back up their sensitive data offsite. Safehub, which was founded in 2015, aims to address the risk with a real-time earthquake-monitoring product that leverages motion sensors, analytics, and third-party data to provide building integrity information. Safehub’s cell-connected sensors measure earthquake ground motion and building response, in addition to changes in buildings’ natural and resonant frequencies. The company uses this information to estimate damage to buildings and related business interruption losses. If an earthquake happens, Safehub sends damage alerts and financial loss estimates via text, email, and a web dashboard. In 2019, Safehub teamed up with the Global Earthquake Model (GEM) foundation to model structural robustness directly from sensors installed within buildings. The company used the data to refine the algorithmic predictions of damage from earthquakes that inform its vulnerability estimates and risk and insurance calculations, plus the other planning information it provides to customers. More recently, Safehub released the latest generation of its sensor technology, which includes a rechargeable battery, an accelerometer, and connectivity options like a long-range radio for mesh networking and LTE. The company says 99% of calculations can be performed on the sensor, reducing the need for communication with the cloud. Safehub isn’t the only company intent on tackling the earthquake detection and risk assessment problem. There’s Grillo, an in-home alarm that claims to provide warnings up to two minutes before an earthquake hits. SkyAlert not only provides an early warning for earthquakes, it can also turn off gas and assembly lines. That’s not to mention One Concern, which taps AI and machine learning to advise fire departments how to plan for earthquakes and respond to them. Some experts are skeptical about these systems’ accuracy. In February 2019 and August 2019, SkyAlert’s app issued alerts that overestimated the magnitude of earthquakes and caused tens of thousands of people to unnecessarily evacuate. In response, the Mexico City government adopted a measure preventing private companies from sending alerts to businesses and residents. But studies suggest algorithms can be trained to anticipate earthquakes with reasonable accuracy. Researchers from Google’s AI division and Harvard University created an AI model capable of pinpointing the location of aftershocks up to one year after a major earthquake. And scientists at Stanford developed an AI system — CRED — that can identify seismic signals from both historical and continuous data. Hannover Digital Investments and JLL Spark, the strategic investment arm of commercial real estate services firm JLL, also participated in Safehub’s latest funding round. Existing backers Fusion Fund, Ubiquity Ventures, Promus Ventures, Bolt, Blackhorn Ventures, Maschmeyer Group Ventures, and Team Builder Ventures also contributed. The San Francisco-based company has 12 employees, and this round brings its total raised to $14 million."
https://venturebeat.com/2021/04/26/hashicorp-revoked-private-key-exposed-in-codecov-security-breach/,HashiCorp revoked private key exposed in Codecov security breach,"A private code-signing key was exposed by a compromised Codecov script, open source company HashiCorp said in its discussion forum. Codecov, which makes software auditing tools for developers to see how thoroughly their code is being tested, revealed earlier this month that the script used to upload data to its servers had been modified by unknown actors. The script took advantage of the fact that Codecov’s tools have access to internal accounts and exported those credentials to an unauthorized server. HashiCorp was one of Codecov’s customers affected by the tampered script, HashiCorp product security director Jamie Finnigan wrote on the company’s discussion forum last week. HashiCorp’s Terraform product is an open source infrastructure-as-a-code software tool widely used for automated cloud deployments. “[HashiCorp] found that a subset of HashiCorp CI pipelines used the affected Codecov component,” Finnigan wrote, noting that the GPG [Gnu Privacy Guard] private key used for signing hashes used to validate HashiCorp product downloads had been exposed. The dangerous thing about having a private key exposed is that an attacker can use it to sign anything and the signed file will look like a legitimate file from the owner of the key. In this case, the concern was that someone could have modified one of HashiCorp’s downloads to include malicious code and then re-signed it with the private key. As far as anyone would be able to tell, that file would appear to be an update from HashiCorp that was safe to download and install. Finnigan said the company’s investigation did not show that any of its existing releases had been modified. HashiCorp revoked the exposed key and re-signed its downloadables with a brand-new key. “[The] GPG key used for release signing and verification has been rotated,” Finnigan wrote. “Customers who verify HashiCorp release signatures may need to update their process to use the new key.” While all official downloads on HashiCorp’s website have been signed with the new key, there are still some problems for HashiCorp customers. In environments where HashiCorp product downloads are manually or automatically validated, customers will need to manually update to reflect the key change. Also, Terraform downloads provider binaries and performs signature verification as part of one process during automatic code verification, and that process is still using the revoked key. “HashiCorp will publish patch releases of Terraform and related tooling, which will update the automatic verification code to use the new GPG key,” Finnigan said. Until then, customers can manually verify Terraform has the new key and signatures. This is just one of many disclosures as companies assess whether they were impacted by Codecov’s security breach. More than 29,000 enterprise customers worldwide use Codecov’s tools, and the malicious script was present from January 31 until its discovery on April 1. Codecov discussed the breach and how credentials, tokens, and keys could potentially have been exposed in a blog post on April 15. CircleCI, a continuous integration and delivery platform, confirmed to Cybersecurity Dive that the Codecov breach had impacted its integration with the code testing firm CircleCI Orb. Codecov’s breach is a form of supply chain attack, where attackers target a company’s suppliers or vendors. By compromising Codecov, the attackers got their hands on all kinds of API keys, login credentials, and other security information. In the case of HashiCorp, if the attackers had tampered with the company’s tools, that would be yet another supply chain attack because those tools are widely used within enterprises. It’s possible the attackers used the harvested credentials in other attacks that have not yet been discovered. The fact that HashiCorp’s private key was exposed is bad enough — but the company hasn’t said whether anything else has been stolen or compromised. “HashiCorp has performed additional remediations related to information potentially exposed during this incident,” Finnigan said, but he did not provide details about what else may have been harvested."
https://venturebeat.com/2021/04/26/apple-will-focus-on-machine-learning-ai-jobs-in-new-nc-campus/,"Apple will focus on machine learning, AI jobs in new NC campus","(Reuters) — Apple on Monday said it will establish a new campus in North Carolina that will house up to 3,000 employees, expand its operations in several other U.S. states and increase its spending targets with U.S. suppliers. Apple said it plans to spend $1 billion as it builds a new campus and engineering hub in the Research Triangle area of North Carolina, with most of the jobs expected to focus on machine learning, artificial intelligence, software engineering and other technology fields. It joins a $1 billion Austin, Texas campus announced in 2019. North Carolina’s Economic Investment Committee on Monday approved a job-development grant that could provide Apple as much as $845.8 million in tax reimbursements over 39 years if Apple hits job and growth targets. State officials said the 3,000 jobs are expected to create $1.97 billion in new tax revenues to the state over the grant period. The iPhone maker said it would also establish a $100 million fund to support schools in the Raleigh-Durham area of North Carolina and throughout the state, as well as contribute $110 million to help build infrastructure such as broadband internet, roads, bridges and public schools in 80 North Carolina counties. “As a North-Carolina native, I’m thrilled Apple is expanding and creating new long-term job opportunities in the community I grew up in,” Jeff Williams, Apple’s chief operating officer, said in a statement. “We’re proud that this new investment will also be supporting education and critical infrastructure projects across the state.” Apple also said it expanded hiring targets at other U.S. locations to hit a goal 20,000 additional jobs by 2026, setting new goals for facilities in Colorado, Massachusetts and Washington state. In Apple’s home state of California, the company said it will aim to hire 5,000 people in San Diego and 3,000 people in Culver City in the Los Angeles area. Apple also increased a U.S. spending target to $430 billion by 2026, up from a five-year goal of $350 billion Apple set in 2018, and said it was on track to exceed. The target includes Apple’s U.S. data centers, capital expenditures and spending to create original television content in 20 states. It also includes spending with Apple’s U.S.-headquartered suppliers, though Apple has not said whether it applies only to goods made in those suppliers’ U.S. facilities."
https://venturebeat.com/2021/04/26/apples-new-iphone-privacy-changes-explained/,"Apple’s new iPhone privacy changes, explained","(Reuters) — Apple on Monday will begin rolling out an update of its iOS operating system with new privacy controls designed to limit digital advertisers from tracking iPhone users. For Apple’s more than 1 billion iPhone users, the change will mean a new pop-up notification in some apps seeking their permission to collect data that Apple believes could be used to track their browsing habits across third-party apps and websites. For businesses, the rules could bring seismic changes to the nearly $100 billion mobile advertising market if most iPhone users decline to allow data collection, though the exact impact remains a question, according to industry experts. Apple is requiring app developers who want to collect a digital advertising identifier from iPhone users to show a pop-up saying that the app “would like permission to track you across apps and websites owned by other companies,” along with an explanation from the app developer about why permission is being sought. Some mobile advertising analysts believe that fewer than one in three users are likely to grant permission. IPhone owners also have a “tracking” menu in their phone’s privacy settings where they can opt-out of tracking from all apps on their phone with a single switch, or pick and choose among apps to grant permission to. Both advertisers and app developers who sell ad inventory say if many iPhone users opt-out of tracking, it will make advertising less effective. The ad industry has long gathered data about people’s web browsing behavior in order to serve up ads, such as for clothes or cars, that users might be interested in. A shrinking pool of user data could lead to lower sales for brands and lower ad revenue for mobile apps and publishers. Apple’s move has deepened a rift with Facebook, which has said the change will hurt small businesses because it will impede their ability to cost-effectively find local customers to target with advertisements. Apple has said it wanted to give its customers more control over whether data collected on them by apps is shared with third parties. Yes, data collection is still allowed if it is spelled out in an app’s privacy policy. The changes only affect whether app developers share data they collect with third parties, or mix their data with outside data from third parties, to help target ads. Apple has introduced privacy “nutrition labels” to its App Store to show users what data apps collect. Yes, iPhone users can still see ads even if they decline the new pop-up, as long as those ads are targeted using data the app developer has collected on its own. For example, a social network like Facebook can still target ads based on first-party data such as which groups users join or which posts they like. But if Facebook wants to target ads based on data from which third-party websites users have used their Facebook credentials to log into, it will need to seek permission."
https://venturebeat.com/2021/04/26/toyota-acquires-lyft-self-driving-division-for-550-million/,Toyota subsidiary acquires Lyft self-driving division for $550M,"Woven Planet, a newly established Toyota subsidiary, today announced plans to acquire Lyft’s Level 5 self-driving unit in a deal worth $550 million. The companies say the purchase will bring together roughly 1,200 scientists and software engineers from Level 5; Woven Planet; and Toyota Research Institute, which had researchers already working with Woven Planet. The pandemic and its effects, including testing delays, have resulted in consolidation, tabled or canceled launches, and shakeups across the autonomous transportation industry. Ford pushed the unveiling of its self-driving service from 2021 to 2022. Former Waymo CEO John Krafcik told the New York Times the pandemic delayed work by at least two months. And Amazon acquired driverless car startup Zoox for $1.3 billion. According to Boston Consulting Group managing director Brian Collie, broad commercialization of AVs won’t happen before 2025 or 2026 — at least three years later than originally anticipated. Once the acquisition is complete, Woven Planet says it will have an expanded footprint beyond its Tokyo headquarters, with offices and engineering teams in Palo Alto, California and London. In addition to the purchase of Level 5, Woven Planet has signed commercial agreements with Lyft to use the latter’s system and fleet data to support the commercialization of the driverless technology Woven Planet intends to develop. Lyft will receive approximately $550 million in cash, with $200 million paid up front and $350 million in payments over a five-year period. The transaction is expected to close in the third quarter of 2021. “Today’s announcement launches Lyft into the next phase of an incredible journey to bring our mission to life,” Lyft CEO Logan Green said in a press release. “Lyft has spent nine years building a transportation network that is uniquely capable of scaling autonomous vehicles. This deal brings together the vision, talent, resources, and commitment to advance clean, autonomous mobility on a global scale.” Lyft’s Level 5 R&D division was founded in July 2017 and has developed novel 3D segmentation frameworks, methods of evaluating energy efficiency in vehicles, and techniques for tracking vehicle movement using crowdsourced maps, among other things. In 2019, Lyft announced the opening of a new road test site in Palo Alto, California, near its Level 5 division’s headquarters. That development came after a year in which Lyft expanded access to its employee self-driving service in Palo Alto with human safety drivers on board in a limited area. In November 2019, Lyft said its autonomous cars were driving 4 times more miles on a quarterly basis than they were six months before and that it has about 400 employees dedicated to development globally (up from 300). In May 2020, the company partnered with Google parent company Alphabet’s Waymo to enable customers to hail driverless Waymo cars from the Lyft app in Phoenix. And Lyft has an ongoing collaboration with the self-driving Hyundai-Aptiv joint venture known as Motional, which makes a small fleet of autonomous vehicles available to Lyft customers in Las Vegas. Lyft recently revealed it has begun leveraging data from its ride-hailing network to improve the performance of its autonomous vehicle systems. A subset of drivers’ cars are equipped with inexpensive camera sensors, enabling them to capture challenging scenarios while helping solve problems like generating 3D maps and improving simulation tests. Post-acquisition, Lyft says its Open Platform team, which focuses on the deployment of third-party self-driving technology on the Lyft network, will become the new Lyft Autonomous team. Lyft expects the restructuring to remove $100 million in operating expenses from its books, primarily from reduced R&D spend."
https://venturebeat.com/2021/04/26/iot-development-platform-prescient-devices-nabs-2m/,IoT development platform Prescient Devices nabs $2M,"Prescient Devices, a platform for internet of things (IoT) software and service development, today announced that it raised $2 million in seed funding. The company says it’ll put the proceeds toward product ideation and ramping up its sales and marketing programs. Global IoT revenue hit an estimated $1.7 trillion in 2019, when the number of edge devices connected to the internet exceeded 23 billion, according to CB Insights. But despite the industry’s growth, not all organizations think they’re ready for it. In a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a significant blocker. Prescient offers a low-code programmable platform that allows system integrators, IT engineers, and data scientists to build IoT and edge computing solutions. The platform, which can deploy firmware to fleets of IoT devices, delivers templates that connects sensors to the cloud, enabling remote monitoring and industrial automation. Prescient customers gain access to drag-and-drop graphical programming interfaces, modules, and recipes that they can use to program edge devices, edge and cloud dashboards, and cloud functions. They’re also provided a library of reference solutions for popular sensors and devices. There’s an abundance of tools promising to simplify IoT development and management at the edge including Google’s Cloud IoT Edge, Amazon’s Amazon Web Services (AWS) IoT, Microsoft’s Azure Sphere, and Baidu’s OpenEdge, as well as Zededa, Particle, and Balena. But CEO Andy Wang asserts that Prescient has an advantage in the scalability of its approach.  “We uniquely focus on removing the technology barrier for engineers, integrators, and data scientists to build, and accelerate IoT applications, helping deliver new business applications to the commercial market. The growing interest and active engagement from our users have been amazing,” Wang said in a press release. In what’s been a boon for Prescient, the pandemic has contributed to the growth of the larger IoT market. Microsoft’s 2020 IoT Signals report indicates that 33% of decision makers plan to up their IoT investments, while 41% say their existing investments will remain the same. Meanwhile, a recent Deloitte survey found that respondents believe IoT will have the largest impact on their organizations compared with AI and cloud infrastructure. “Our growing community has already developed active IoT applications for predictive maintenance, machine vision, and test automation within weeks of concept and transforming the entire approach to IoT business automation and edge intelligence applications,” Wang continued. “This round of funding will help accelerate our ability to better support our customers while expanding [the Prescient platform’s] functionality.” Z5 Capital led Boston, Massachusetts-based Prescient’s latest funding round, which had participation from angel investors at MIT and the Harvard Business School."
https://venturebeat.com/2021/04/26/network-security-company-proofpoint-goes-private-in-12-3b-deal/,Network security company Proofpoint goes private in $12.3B deal,"Private equity firm Thoma Bravo has announced plans to acquire cybersecurity company Proofpoint in a deal worth $12.3 billion. The deal serves as further evidence, if any was needed, that demand for cloud-based security is at an all-time high, driven in large part by the continued embrace of cloud computing and the rise of remote work, which necessitates robust network security. Founded in 2002 by former Netscape CTO Eric Hahn, Proofpoint was originally known for an email security product that helped businesses identify spam, viruses, and other electric correspondence that might contravene company policies. In the subsequent years, the Sunnyvale, California-based company has expanded its scope to include an array of cloud-based security products designed to protect enterprises from targeted threats. Proofpoint went public back in 2012, with its shares initially trading at around $13 — these have grown steadily over the past decade, hitting an all-time high of $140 earlier this year and giving it a market capitalization of more than $7 billion. Thoma Bravo has a track record of taking publicly traded cybersecurity companies private, having done just that with network security company Barracuda in a 2017 deal worth $1.6 billion and with Sophos last year for $3.9 billion. The Proofpoint deal, which is expected to close in Q3 2021, sees Thoma Bravo paying a 34% premium on Proofpoint’s closing price at the last full trading day (April 23), with shareholders set to receive $176 for each share they own. It’s worth noting that the $12.3 billion price tag positions this as the biggest cybersecurity acquisition of all time, putting it ahead of the $7.68 billion Intel shelled out for McAfee 11 years ago. And by VentureBeat’s calculations, the Proofpoint acquisition represents one of the biggest overall technology acquisitions ever, putting it in the top 20, alongside megadeals that include Dell’s $67 billion EMC purchase, IBM’s $34 billion Red Hat deal, and Salesforce’s impending $27.7 billion Slack acquisition."
https://venturebeat.com/2021/04/26/zoom-launches-immersive-view-to-unify-participants-in-the-same-virtual-room/,Zoom launches Immersive View to unify participants in the same virtual room,"Zoom has officially launched a new immersive video feature to help businesses create more engaging and collaborative virtual meetings. While a growing number of fledgling startups have adopted remote-first mindsets from the get-go, the transition for larger enterprises is fraught with challenges, given that they may have hundreds of thousands of workers spread across multiple regions and time zones. Despite these hurdles, major businesses — including Salesforce, Microsoft, Shopify, VMware, Dropbox, and Fujitsu — have already confirmed a permanent shift to a remote-first or hybrid working policy. But better and more adaptable virtual collaboration tools will prove vital to the long-term success of these programs — Zoom fatigue is real, after all. Zoom first announced its new Immersive View (then called Immersive Scenes) feature at its annual Zoomtopia event back in October, positioning the technology against Microsoft Teams’ Together Mode, which had launched a few months before. In a nutshell, video call hosts can use Immersive View to arrange participants — anyone from employees to panelists — in a single virtual environment. This deviates from the established norm of displaying participants in a grid-like format with each individual’s personal background showing. Immersive View supports up to 25 participants, and they can be placed in any number of environments, including a boardroom, auditorium, or classroom, depending on the event. Hosts can manually move people around on the screen or let Zoom do it automatically. Immersive View is available now in Zoom’s desktop client (version 5.6.3 or higher) for Windows and MacOS and is activated by default for all free and individual Pro accounts."
https://venturebeat.com/2021/04/26/thetaray-launches-anti-money-laundering-ai-and-analytics-for-the-cloud/,ThetaRay launches anti-money laundering AI and analytics for the cloud,"ThetaRay today announced that its AI-based anti-money laundering (AML) analytics will be available on public and private clouds, including Azure, Google, and AWS. ThetaRay’s AML platform uses unsupervised machine learning to monitor financial transactions, integrating data and triaging alerts in real time. And its new cloud-agnostic version aims to increase the speed at which the cybersecurity company’s clients — banks and fintech firms — can detect potential threats. Under a decades-old treaty called The United Nations Convention against Transnational Organized Crime, nearly 200 countries have pledged to help each other prosecute individuals involved in money laundering schemes. Many domestic financial institutions also take precautions, including closely monitoring clients’ activity for red flags, requiring deposits to stay in a client’s account for a minimum number of days before being transferred, and recording each transaction in detail. Now that nefarious actors can pass money through the internet via cryptocurrencies and digital currency exchanger services to conceal its illegal origins, AML efforts have become even more complicated. Whether complicated means difficult is controversial, however. Internet service providers make anonymity a gamble, and some industry leaders say cryptocurrencies identify and prevent illegal activities better than traditional payment systems do. Either way, big data and AI mechanisms can make it easier for financial institutions to protect themselves. AML software became popular in the early 2000s for customer transactions, identity management, and regulatory compliance. In recent years, providers like ThetaRay, Unit21, C3, and Splunk have developed increasingly intelligent solutions to analyze laundered money’s first two stages: placement (where it’s deposited into a bank) and layering (which involves transferring money before integrating and extracting it). Cloud-based systems became particularly important to banks during the COVID-19 pandemic as they scrambled to understand more data in less time. ThetaRay says its new service is the only cloud-based AML offering that analyzes SWIFT traffic, risk indicators, and client data to detect anomalies indicating “money laundering, terrorism financing, and other criminal activities across complex, cross-border transaction paths.” According to ThetaRay, its platform also helps users reduce operational costs and increase revenue by relying on its “artificial intuition” proprietary machine learning technology that interprets supervised and unsupervised data with fewer false positives. Traditional on-premise AML solutions present several challenges that can make them a poor fit for payments between people in different countries. They require months to put into production and are inefficient for organizations with limited internal resources. ThetaRay’s cloud alternative starts up quickly and does not require additional hardware. Company CEO Mark Gazit claims customers are choosing cloud-based solutions for their stability and security. “Our fully scalable cloud service … empowers the payments ecosystem to enjoy the long-term operational benefits of secure cross-border transactions without having to worry about the maintenance of additional infrastructure,” Gazit said. ThetaRay was founded in 2013 as the brainchild of Tel-Aviv University computer science professor Amir Averbuch and Yale math professor Ronald Coifman. Just one year later, it raised $10 million — its first of multiple rounds — from investors like General Electric to expand in the U.S. When VentureBeat interviewed Gazit in 2018, he said “Human beings are not enough to look at all this data, and we’re lucky to have tens of years of academic research to analyze all those … transactions.” At that time, ThetaRay claimed it had a 100% money laundering detection rate across more than 200 million transactions and that it had flagged novel fraud patterns in millions of ATM sessions across the globe. These numbers will certainly rise with ThetaRay’s shift to cloud AML as it seeks to help more organizations tackle tricky transactions."
https://venturebeat.com/2021/04/26/airehealth-appoints-kien-nguyen-as-chief-executive-officer/,AireHealth Appoints Kien Nguyen as Chief Executive Officer,"ORLANDO, Fla.–(BUSINESS WIRE)–April 26, 2021– AireHealth, a respiratory digital health company based in the U.S., announced today that its Board of Directors has appointed Kien Nguyen as Chief Executive Officer and member of the AireHealth Board of Directors effective immediately. Mr. Nguyen is a proven medical device and technology leader with a distinguished track record of product innovation, market development and commercialization. His career spans nearly three decades leading global teams within medical device and life sciences industries for both start-up and Fortune 500 companies, most recently serving as Chief Commercial Officer at Progenerative Medical. Mr. Nguyen has received multiple advanced degrees including a Master of Business Administration from Columbia University and a Doctorate in Neuroscience from the University of Colorado. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210426005069/en/ “Kien is absolutely the right person to lead AireHealth as we enter this next phase of innovation and growth,” said Stacie Ruth, AireHealth co-founder and Board Director. “Kien’s ability to translate technologies into solutions with high market value and his deep commercialization expertise will accelerate the execution of AireHealth’s mission to improve the lives of millions of people living with chronic lung disease.” AireHealth recently received 510(k) clearance for a connected nebulizer from the FDA to address the growing challenges in chronic respiratory illness that cost more than $130 billion per year and are the cause of nearly 50 deaths per 100,000 people. The portable, connected AireHealth nebulizer is part of a comprehensive digital health platform designed to track medication adherence, symptoms, and behaviors to enable earlier clinician intervention and reduce unnecessary hospitalization. “I am honored to have been chosen by AireHealth’s board of directors to lead the company,” said Mr. Nguyen. “AireHealth is truly revolutionizing respiratory care for people living with chronic respiratory illness and their care teams. My primary focus will be to work with the board and a talented , passionate leadership team to execute on the strategy and vision to bring meaningful respiratory care innovation to market faster.” Learn more about how AireHealth’s digital solution can support chronic care management and remote monitoring programs for healthcare providers. About AireHealth AireHealth is an innovative digital health company empowering and improving healthy living through affordable treatments, symptom tracking and early detection of respiratory conditions. With strong IP and clinically validated products in the pipeline, AireHealth enables more proactive care and early interventions to improve outcomes and reduce costs. The company provides monitored drug delivery through its FDA cleared Class II portable nebulizer, designed to deliver medicine directly to a patient’s lungs where it is most effective. AireHealth’s digital platform allows for earlier detection of respiratory decline through connected devices, the MyAirHealth diary companion app, and provider portal. The result is that patients not only take a more proactive approach to managing their respiratory care but enables faster clinical intervention and fewer hospitalizations, which helps achieve the goal of improving treatment outcomes and lowering costs. For more information, please visit Aire.Health.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210426005069/en/ Media Contact AireHealthCara Collinscara.collins@aire.health 574-376-5528"
https://venturebeat.com/2021/04/25/cisa-warns-of-credential-theft-via-solarwinds-and-pulsesecure-vpn/,CISA warns of credential theft via SolarWinds and PulseSecure VPN,"Attackers targeted both the Pulse Secure VPN appliance and the SolarWinds Orion platform in an organization, the U.S. government said in an incident report last Thursday. Enterprises have been rocked by reports of cyberattacks involving mission-critical platforms over the past year. In the past few months, security teams have been busy investigating a growing list of cyberattacks and vulnerabilities to figure out whether they were affected and to apply fixes or workarounds as needed. The supply chain attack and compromise of the SolarWinds Orion platform reported at the beginning of the year was just the beginning. Since then, there have been reports of attacks against Microsoft Exchange, the Sonicwall firewall, and the Accellion firewall, to name just a few. Defenders also have a long list of critical vulnerabilities to patch, which have been found in multiple widely used enterprise products, including Vmware and F5’s BIGIP appliance. The alert from the U.S. Cybersecurity and Infrastructure Security Agency (CISA) is an unsettling reminder that attackers often chain vulnerabilities in multiple products to make it easier to move around within the victim network, cause damage, and steal information. Compromising the Pulse Secure virtual private network appliance gave attackers initial access to the environment. SolarWinds Orion platform has been used to perform supply chain attacks. In the incident report, CISA said the attackers initially obtained credentials from the victim organization by dumping cached credentials from the SolarWinds appliance server. The attackers also disguised themselves as the victim organization’s logging infrastructure on the SolarWinds Orion server to harvest all the credentials into a file and exfiltrate that file out of the network. The attackers likely exploited an authentication bypass vulnerability in SolarWinds Orion Application Programming Interface (API) that allows a remote attacker to execute API commands, CISA said. The attackers then used the credentials to connect to the victim organization’s network via the Pulse Secure VPN appliance. There were multiple attempts between March 2020 and February 2021, CISA said in its alert. The attackers used the Supernova malware in this cyberattack, which allowed them to perform different types of activities, including reconnaissance to learn what’s in the network and where information is stored, and to move laterally through the network. This is a different method than was used in the earlier SolarWinds cyberattack, which compromised nine government agencies and about 100 private sector companies. “Organizations that find Supernova on their SolarWinds installations should treat this incident as a separate attack [from Sunburst],” CISA wrote in a four-page analysis report released Thursday. It appears the attackers took advantage of the fact that many organizations were scrambling in March 2020 to set up remote access for employees who were suddenly working from home because of the pandemic. It’s understandable that in the confusion of getting employees connected from completely different locations, the security team missed the fact that these particular remote connections were not from legitimate employees. None of the user credentials used in the initial compromise had multi-factor authentication enabled, CISA said. The agency urged all organizations to deploy multi-factor authentication for privileged accounts, use separate administrator accounts on separate administrator workstations, and check for common executables executing with the hash of another process. While CISA did not attribute the combined cyberattack to anyone in its alert, it did note that this cyberattack was not carried out by the Russian foreign intelligence service. The U.S. government had attributed the massive compromise of government and private organizations between March 2020 and June 2020 to the Russian Foreign Intelligence Service (SVR). Security company FireEye last week said Chinese state actors had exploited multiple vulnerabilities in Pulse Secure VPN to break into government agencies, defense companies, and financial institutions in the U.S. and Europe. Reuters said Supernova was used in an earlier cyberattack against the National Finance Center — a federal payroll agency inside the U.S. Department of Agriculture — reportedly carried out by Chinese state actors."
https://venturebeat.com/2021/04/25/how-low-code-platforms-can-aid-intelligent-business-process-management/,How low-code platforms can aid intelligent business process management,"The potential for low-code/no-code platforms is enormous. Low-code increases the productivity of IT developers — sometimes by several orders of magnitude. And no-code empowers experts and subject matter experts primarily on the business or operations side (as opposed to IT) to become “citizen developers.” But as I explained in a previous article, low-code and no-code platforms are not a panacea; they face challenges. Given the broad spectrum of low-code and no-code platforms, how should enterprises find the best options for their specific needs? And what are the use cases for using multiple low-code/no-code platforms? I will address these questions in a series of articles to help you navigate this transformational landscape while avoiding the pitfalls. Specifically, I will be looking at low-code/no-code related to intelligent business process management (BPM), intelligent databases, automated integration, and a number of other areas. In this first installment, I will be focusing on low-code/no-code in the context of intelligent BPM, or iBPM. iBPM’s core value proposition is the collaboration and orchestration of people, applications, connected devices, and trading partners to achieve and continuously improve business objectives. Intelligence and automation are two essential conjuncts for BPM. Intelligence for BPM comes in many forms: digitizing business rules, intelligent virtual assistants, and increasingly process mining. A BPM solution will involve fully automated robotic process automation sub-processes for repetitive tasks that do not need human intervention and automated tasks assigned to human participants. Thus, increasingly RPA is becoming part of the complete intelligent BPM platform. Here is a simple order-to-cash process example:  Some tasks will be performed by humans — for instance, approving the orders. Others could involve automation with RPA — for example, receiving the goods. There will also be tasks accessing systems of record — for instance, preparing and paying the invoice. An iBPM platform will model, execute, monitor, and improve the end-to-end process. Other terms are also often used to denote end-to-end processes. These include “workflow” and “case management.” Intelligent BPM is much more than technology. At its core, it is a transformational management discipline that helps organizations achieve their strategic goals. Automation is a crucial component of iBPM solutions. As a discipline, BPM drives the operations of enterprises. It includes several iterative phases from design to execution to monitoring and continuous improvement. There is a remarkably close affinity between low-code/no-code and BPM. As far as back in 2005 or earlier, BPM suites were touted as platforms for model-driven development, which is akin to what we now call low-code/no-code. What were the “models?” Well, check the next section on how low-code/no-code manifests itself in iBPM platforms. Low-code/no-code iBPM platforms handle: There are other components of a complete iBPM low-code/no-code platform — such as the decisioning (aka business rules), integration, and data model — but I won’t be getting to those in this post. The following is a simple purchase request process model from Bizagi, using shapes from the BPMN graphical notation for business processes (the de-facto standard):  The swim lanes represent the participants in the process. The rectangular shapes are tasks or activities. The diamond shape is for a decision, and the circles represent the start and end of the process. There are many other shapes in the BPMN standard, but these three are the most common. If human participants, such as Boss, Requester, etc, are involved in a particular workflow, the low-code/no-code BPM modeling also supports the creation of UI forms to enable that interaction, and these are pretty easy to model. This “drag and drop” paradigm of building user experience is common and similar across all low-code/no-code platforms that support Web or mobile applications. The following figure illustrates a simple user experience designer from Kissflow. There will be elements such as buttons, input fields, drop-downs, images, etc., that a non-technical developer can use to create the user experience. The elements are then connected to the properties or fields in the business process being modeled and automated.  The interface builder of the iBPM platform is robust enough to allow the designer to build a user experience — preferably without any code. Once the application is deployed, the various participants can then monitor the performance of the activities through interactive analytics. These are actionable analytics dashboards, which means that if there is a bottleneck or issue, the stakeholder can take action, such as escalating or re-assigning a task. The analytics dashboards will typically have pre-built analytics that also support low-code/no-code customization. Here is an example of an actionable business process analytics dashboard from Nintex:  Why is it important for organizations to be able to model, automate, monitor, and improve their business processes without coding? An organization is a collection of business processes for production, marketing, sales, service, and support functions. So any optimizations and improvements of the most critical processes will enhance the bottom line: cost savings, revenue generation, and compliance. These are called operational excellence (OE) improvements. iBPM low-code/no-code platforms are an enabling technology for OE. Here are my recommendations for iBPM low-code/no-code. Prioritize your improvements: There will typically be many mission-critical and support processes that need improvement. By balancing the complexity of implementation with business value, you will identify the low-hanging fruit. (For more details, check out this explanation of four intelligent automation methodologies). The result will be a list of automation and OE business processes that you can optimize through an iBPM low-code/no-code platform. Make sure you start with process mining: To find the top priority low-hanging fruit, you need to know the most common process paths, the bottlenecks, the variations, and improvement opportunities. In other words, you need to understand what processes your transactional data is subject to and then improve them. That is precisely the domain of process mining. Do not automate bad processes. The figure below illustrates the OE reference architecture with iBPM low-code/no-code. At the bottom, you have the systems of record that generate the transactions for specific processes. After aggregating and cleaning the transactional data, a process mining tool — such as Celonis — can then identify the most common process path and the variations and the root causes for the issues. Like data mining, process mining algorithmically “mines” and discovers the processes from the transactional data, including the variations and bottlenecks. Based on these, a iBPM low-code/no-code platform is used to improve, implement, and automate the processes, leveraging workflows with human participants and robotic process automation.  Create and fund an operational excellence competency center: iBPM low-code/no-code — and all other low-code/no-code, for that matter — is technology. As noted above, it is also a management discipline for operational excellence. For organizations that use this approach, it is a good idea to have a competency center that does three things at a minimum: balances innovation through iBPM low-code/no-code with best practices for security and reliability, enables non-technical subject matter experts to leverage iBPM low-code/no-code and become participants in development, and governs the continuous improvement from process mining to automation. Understand the landscape and leverage experts: There is quite a bit of confusion when it comes to classifying what solutions are BPM solutions. Some analysts classify these platforms as “workflow,” “business process,” or “case management” solutions. For example, see these classification schemes: There are also low-code/no-code development platforms that are closely affiliated with the BPM space but that might be classed into other low-code/no-code categories: The low-code/no-code ecosystem is constantly evolving. There are hundreds of platforms — and new ones are entering the market all the time. Sometimes inexpensive and straightforward low-code/no-code tools will be sufficient for your needs. Do not pay for what you will rarely use. Also, avoid vendor lock-in. There are emerging new and innovative low-code/no-code platforms that support plug-ins and add-ons, including those that address process mapping. Dr. Setrag Khoshafian is a cofounder at Startup Assistant and Principal and Chief Scientist at Khosh Consulting. He was previously VP of BPM Technology at Pega, Senior VP of Technology at Savvion, and CTO at Portfolio Technologies and is a member of the Cognitive World Think Tank on enterprise AI."
https://venturebeat.com/2021/04/25/you-need-to-be-constantly-exploring-the-data-in-your-ai-pipeline/,You need to be constantly exploring the data in your AI pipeline,"Poor data quality is hurting artificial intelligence (AI) and machine learning (ML) initiatives. This problem affects companies of every size from small businesses and startups to giants like Google. Unpacking data quality issues often reveals a very human cause. More than ever, companies are data-rich, but turning all of that data into value has proven to be challenging. The automation that AI and ML provide has been widely seen as a solution to dealing with the complex nature of real-world data, and companies have rushed to take advantage of it to supercharge their businesses. That rush, however, has led to an epidemic of sloppy upstream data analysis. Once an automation pipeline is built, its algorithms do most of the work with little to no update to the data collection process. However, creating those pipelines isn’t a one-and-done task. The underlying data must be explored and analyzed over time to spot shifting patterns that erode the performance of even the most sophisticated pipelines. The good news is that data teams can curtail the risk of erosion, but it takes some serious effort. To maintain effective automation pipelines, exploratory data analysis (EDA) must be regularly conducted to ensure that nothing goes wrong. EDA is one of the first steps to successful AL and ML. Before you even start thinking about algorithms, you need to understand the data. What happens in this phase will determine the course of the automation that takes place downstream. When done correctly, EDA will help you identify unwanted patterns and noise in the data and enable you to choose the right algorithms to leverage. In the EDA phase, you need to be actively inquiring about the data to ensure it’s going to behave as expected. As a start, below are 10 important questions to ask for a thorough analysis: These questions may lead to additional questions and even more after that. Don’t think of this as a checklist but as a jumping off point. And at the end of this process, you will be armed with a better understanding of the data patterns. You can then process the data correctly and choose the most appropriate algorithms to solve your problem. The underlying data is constantly changing, which means that a significant amount of time must be spent on EDA to make sure that the input features to your algorithms are consistent. For example, Airbnb found that nearly 70% of the time a data scientist spends on developing models is allocated toward data collection and feature engineering, which requires extensive data analysis to ascertain the structures and patterns. In short, if a company does not invest the time to understand its data, its AI and ML initiatives can easily spin out of control. Let’s look at an example from companies that have used data exploration effectively to develop and build successful data products. One of the most important aspects of digital services is cybersecurity and fraud detection, now a market valued at more than $30 billion and projected to reach more than $100 billion by the end of the decade. While there are tools such as Amazon Fraud Detector and PayPal’s Fraud Management Filters for general detection of online fraud, the only constant in fraud detection is that fraud patterns are always changing. Companies are constantly trying to stay prepared for new kinds of fraud while fraudsters are trying to innovate to get ahead. Every new kind of fraud may have a novel data pattern. For instance, new user sign-ups and transactions may be coming from an unexpected ZIP code at a rapid rate. While new users may come from anywhere, it would be suspicious if a ZIP code that was previously very quiet suddenly started screaming. The more difficult part of this calculus would be knowing how to flag a fraud transaction versus a normal transaction that occurred in that ZIP code. AI technologies can definitely be applied to find a model for fraud detection here, though you as the data scientist must first tell the underlying algorithm which sign-ups and subsequent transactions are normal and which ones are fraud. This can only be done by searching through the data using statistical techniques. You dissect the customer base to ascertain what distinguishes the normal customers from the fraudsters. Next, you would identify information that can help categorize these groups. Details may include sign-up information, transactions made, customer age, income, name, etc. You may also want to exclude information that would introduce significant noise into the downstream modeling steps; flagging a valid transaction as fraud could do more damage to your customer experience and product than the fraud itself. The frustrating (or fun, depending who you ask) part is that this EDA process must be repeated for all products throughout their life cycles. New fraudulent activities mean new data patterns. Ultimately, companies must invest the time and energy into doing EDA so that they can come up with the best fraud detection features to maintain their AI and ML pipelines. Understanding the data is the key to AI and ML success, not a vast repertoire of algorithms. In fact, businesses can easily fail when they force their data to fit their AI and ML pipelines rather than the other way around. Henry Li is Senior Data Scientist at Bigeye."
https://venturebeat.com/2021/04/24/ban-facial-recognition-in-europe-urges-eu-privacy-watchdog/,"Ban facial recognition in Europe, says EU privacy watchdog","(Reuters) — Facial recognition should be banned in Europe because of its “deep and non-democratic intrusion” into people’s private lives, EU privacy watchdog the European Data Protection Supervisor (EDPS) said on Friday. The comments come two days after the European Commission proposed draft rules that would allow facial recognition to be used to search for missing children or criminals and in cases of terrorist attacks. The draft rules, which need to be thrashed out with EU countries and the European Parliament, are an attempt by the Commission to set global rules for artificial intelligence, a technology dominated by China and the United States. The privacy watchdog said it regretted that the Commission had not heeded its earlier call to ban facial recognition in public spaces. “A stricter approach is necessary given that remote biometric identification, where AI may contribute to unprecedented developments, presents extremely high risks of deep and non-democratic intrusion into individuals’ private lives,” it said in a statement. “The EDPS will focus in particular on setting precise boundaries for those tools and systems which may present risks for the fundamental rights to data protection and privacy.” The Commission’s proposals have drawn criticism from civil rights groups, concerned about loopholes that may allow authoritarian governments to abuse AI to clamp down on people’s rights."
https://venturebeat.com/2021/04/24/thistle-tackles-iot-security-by-helping-vendors-update-devices/,Thistle tackles IoT security by helping vendors update devices,"Thistle Technologies emerged this week to tackle the problem of delivering security updates to the internet of things (IoT). The IoT market — which includes printers, edge devices, remote systems, consumer electronics, and automobiles — is booming, and security experts worry about the expanding attack surface. There are ways to update traditional networked devices, such as routers, cameras, and printers, but that isn’t the case for IoT. Each of these devices is now a mini-computer on the network, and a software vulnerability on any one of them means a network compromise. Once in, the attacker can move around looking for other systems to compromise and information to steal. Thistle, led by security veteran Window Snyder, launched on Thursday with $2.5 million in seed funding from True Ventures. The startup plans to address the vulnerability by helping IoT manufacturers securely and reliably deploy updates to their products. Thistle will build a framework for securing printers, ATMs, consumer electronics, and automobiles. The goal is to give embedded device manufacturers the ability to integrate updated mechanisms into their products. “Security-sensitive mechanisms, like updates, should be built and tested by an experienced security team,” the company said in a statement. Snyder has spent over 20 years making some of the biggest brands more secure. She worked in senior cybersecurity positions at Apple, Intel, and Microsoft and was chief security officer at Mozilla, Square, and Fastly. While at Microsoft, she contributed to the Security Design Lifecycle (SDL) and codeveloped the methodology for threat modeling software. She was also part of the effort to reduce Microsoft Windows’ attack surface and make the operating system more resilient to attack. That kind of resiliency is currently missing in the IoT space. If there is a vulnerability in sensors deployed over a large geographic area or in medical devices in a health care setting, the flaws remain unfixed until the system can be replaced. Many of these devices cannot be updated at all, or have a very difficult update mechanism, which means the owners are less likely to bother with the update. These vulnerable devices can cause a lot of problems beyond giving attackers a way to break into a target network. Botnets are networks of hijacked devices used to launch distributed denial-of-service (DDoS) attacks that flood websites and other online services with junk traffic to knock them offline. Last year, BitDefender researchers uncovered the “dark_nexus” botnet, which specifically preys on vulnerable IoT. The botnet compromised more than a thousand connected devices, including home and small office routers, thermal cameras, and video recorders from multiple vendors. Another IoT botnet, Mirai, launched a DDoS attack on internet infrastructure giant Dyn back in 2016 that was devastating enough to knock several major brands — including Shopify — offline and cripple parts of the internet for hours. There are many reasons it is difficult to securely update connected devices. The manufacturer may not know how to build resilience and security updates into its devices. When the goal is speed to market, the developers and engineers often prioritize features over security. Or the device may have limited processing power and memory — just enough to do the task it is designed to do, but not much else. In critical environments, restarting the devices to install updates may not be an option. And in situations where IoT is designed to be deployed over a large geographic area for long periods of time, delivering security updates can be a logistical challenge. Some devices are off-network most of the time and connect only briefly to send data, which may not be enough time to receive and install an update. And it’s a problem that’s just going to get bigger. IoT is well-entrenched in businesses, homes, and industrial plants. Current estimates peg the number of connected devices worldwide at around 25 billion, and that number is expected to explode with the rollout of 5G networks. Data from International Data Corporation (IDC) predicts there will be 55.7 billion connected devices worldwide by the end of 2025, of which 75% will be connected to some kind of IoT platform. “We’re making it easier for device makers to deliver on their security requirements,” Snyder said in a statement. “When the update mechanism is resilient and reliable, the business can leverage that beyond security fixes to provide updates for new features with confidence.”"
https://venturebeat.com/2021/04/24/cisos-must-help-their-boards-manage-cyber-risk-heres-how/,CISOs must help their boards manage cyber risk — here’s how,"In one of the more memorable scenes from the film “Jerry Maguire,” Tom Cruise’s character, a football agent, can be seen pleading with his one client, begging him to just “help me, help you.” Maguire kept repeating the line, hoping to break through to the player, trying to convince him to change his attitude in the hopes it would help him land a big contract from his team. This scene came to mind recently when I was thinking about the relationship between CISOs and their boards of directors. Cyber attacks on a corporation can exact a high price — in money, reputation, and lost business. CISOs battle day and night to prevent their company from suffering a crippling cyber attack, yet too often they don’t receive the help or support they need to properly execute their roles. As a result, CISOs often can’t get enough money to hire staff and purchase the systems that can prevent cyberattacks, can’t raise consciousness among executives to pay attention to cybersecurity issues, and can’t persuade boards of directors to focus more of their attention on cybersecurity needs. For CISOs today to be successful, therefore, their responsibilities must not only include building a robust cyber defense strategy on a limited budget but also convincing their corporate boards of directors — the group eventually responsible for their budget — that cybersecurity needs to be a budgeting priority. Yet, according to a report issued by consulting firm EY, the board is not engaged in the cybersecurity debate. In the report, nearly half of CISOs said their board “does not yet have a full understanding of cybersecurity risk,” and that just 54% of organizations regularly schedule cybersecurity as a board agenda item. How then, can CISOs convince their boards that cybersecurity spending needs to be a priority, and how should they express that need in a way boards can relate to? The first priority for CISOs to advance their objectives is to ensure that board members understand the business issues — and not just the IT issues — involved in cybersecurity, stressing the damage that a cyber attack can have on an organization. Using real-life case studies at quarterly board meetings will help drive the point home — such as the object lesson furnished by Yahoo’s 2013 data breach, perhaps the most expensive in history. That breach cost Yahoo $50 million in damages, paid to customers whose details were revealed; millions of dollars more in fees for free credit monitoring it agreed to supply victims as part of its settlement; and a $350 million discount in its sale price to Verizon. However, it is not enough for CISOs to highlight the potential damage a cyber attack can cause. Working with colleagues from across the company, they must also convincingly demonstrate the benefits that a robust cyber program can have for a business, stressing the opportunity to pursue additional revenue streams, target new customers, and upsell to existing clients. Along with the business aspects of cybersecurity, board members need to both better understand the threats and come to appreciate the steps required to mitigate those threats so they can make informed, strategic decisions for the business. CISO presentations to the board need to include a discussion of the constantly evolving threat landscape, with discussions focused on how hackers choose their victims, how they penetrate networks, which security systems are likely to prevent attacks, and how effective they are. Just as the CEO presents budget and corporate strategy reports to directors, CISOs should present security plans, with details on how security teams plan to defend the company and what they can do to minimize damage if an attack does take place. Once boards understand the technical issues, they will be able to understand the strategies presented to them — and weigh in on whether even more needs to be done. To further make their case to board members, CISOs should propose a formal governance structure — similar to what the board would use for other business objectives — that will allow for effective reporting and analysis of data. That structure should include periodic audits and reviews, assigning ownership, ensuring that funding is adequate to meet challenges and needs, and developing monitoring mechanisms and accountability systems with measurable KPIs. Members of a board of directors usually get to that position because of their business acumen. But in today’s cyber-environment, that business experience must be filtered through the lens of the potential impact a cyber event can have on a company. By helping their board of directors have a “cyber-first” mentality, CISOs will help themselves, allowing their company to develop a healthier and more robust cyber posture. Ronen Lago is CTO at CYE."
https://venturebeat.com/2021/04/24/waymos-leadership-shift-spotlights-self-driving-car-challenges/,Waymo’s leadership shift spotlights self-driving car challenges,"Waymo, Alphabet’s self-driving car subsidiary, is reshuffling its top executive lineup. On April 2, John Krafcik, Waymo’s CEO since 2015, declared that he will be stepping down from his role. He will be replaced by former COO Tekedra Mawakana and Dmitri Dolgov, the company’s former COO and CTO. Krafcik will remain as an advisor to the company. “[With] the fully autonomous Waymo One ride-hailing service open to all in our launch area of Metro Phoenix, and with the fifth generation of the Waymo Driver being prepared for deployment in ride-hailing and goods delivery, it’s a wonderful opportunity for me to pass the baton to Tekedra and Dmitri as Waymo’s co-CEOs,” Krafcik wrote on LinkedIn as he declared his departure. The change in leadership can have significant implications for Waymo, which has seen many ups and downs as it develops its driverless car business. It can also hint at the broader state of the self-driving car industry, which has failed to live up to its hype in the past few years. In 2015, Krafcik joined Google’s self-driving car effort, then called Project Chauffeur. At the time, there was a lot of excitement around deep learning, the branch of artificial intelligence that has made great inroads in computer vision, one of the key components of driverless cars. The belief was that thanks to continued advances in deep learning, it was only a matter of time before self-driving cars became the norm on public streets. Deep learning models rely on vast amounts of training data to develop stable behavior. If the AI algorithms were ready, as it seemed at the time, reaching deployment-level self-driving car technology was only a question of having a scalable data-collection strategy to train deep learning models. While some of this data can be generated in simulated environments, the main training of deep learning models used in self-driving cars comes from driving in the real world. Therefore, what Project Chauffeur needed was a leader who had longtime experience in the automotive industry and could bridge the gap between carmakers and the fast-developing AI sector and deploy Google’s technology on roads. And Krafcik was the perfect candidate. Before joining Google, he was the CEO of Hyundai Motor America, had held several positions at Ford, and had worked in the International Motor Vehicle Program at MIT as a lean production researcher and consultant. With Krafcik at the helm, Project Chauffeur spun off as Waymo under Google parent Alphabet and quickly transformed into a leader in testing self-driving cars on roads. During this time, Waymo struck partnerships with several automakers, integrated Waymo’s AI and lidar technology into Jaguar and Chrysler vehicles, and expanded its test-driving project to more than 25 states. Today, Waymo’s cars have driven more than 20 million miles on roads and 20 billion miles in simulation, more than any other self-driving car company. Like the executives of other companies working on driverless car technology, Krafcik promised time and again that fully autonomous vehicles were on the horizon. In Waymo’s 2020 Web Summit, Krafcik presented a video of a Waymo self-driving car driving in streets without a backup driver. “We’ve been working on this technology a long time, for about eight years,” Krafcik said. “And every company, including Waymo, has always started with a test driver behind the wheel, ready to take over. We recently surveyed 3,000 adults across the U.S. and asked them when they expected to see self-driving vehicles, ones without a person in the driver’s seat, on their own roads. And the common answer we heard was around 2020 … It’s not happening in 2020. It’s happening today.” But despite Krafcik’s leverage in the automotive industry, Google’s crack AI research team, and Alphabet’s deep pockets, Waymo — like other self-driving car companies — has failed to produce robust driverless technology that can run on any road without rigorous testing and tuning. And aside from areas where Waymo’s self-driving technology has been fully tested and approved, the cars still require backup drivers to monitor and take control as soon as the AI starts to act erratically. The AI technology is not ready, and despite the lidar, radar, and other sensor technologies used to complement deep learning models, self-driving cars still can’t handle unknown conditions in the way humans do. They can run thousands of miles without making errors, but they might suddenly make very dumb and dangerous mistakes when they face corner cases, such as an overturned truck on the highway or a fire truck parked at the wrong angle. So far, Waymo has avoided major self-driving incidents like Tesla and Uber’s fatal accidents. But it has yet to deliver a technology that can be deployed at scale. Waymo One, the company’s fully driverless robo-taxi service, is only available in limited parts of Phoenix, AZ. The company is in the process of expanding the service to more crowded and volatile urban areas. The company is also far from becoming profitable. Alphabet’s Other Bets segment, which includes Waymo, had an operating cost of $4.48 billion in 2020, against $657 million in revenue. And Waymo’s valuation has seen a huge drop amid cooling sentiments surrounding self-driving cars, going from nearly $200 billion in 2018 to $30 billion in 2020. Driverless technology has come a long way, but a lot more needs to be done, and the past few years have shown that the “fully self-driving cars are here” narrative is a bit fallacious. It’s clear that just putting more miles on your deep learning algorithms will not make them more robust against unpredictable situations. We need to address some of the fundamental problems of deep learning, such as lack of causality, poor transfer learning, and intuitive understanding of physics. These are active areas of research, and no one has yet provided a definitive answer to them. The self-driving car industry also faces several legal complications. For instance, if a driverless car becomes involved in an accident, how will culpability be defined? How will self-driving cars share roads with human-driven cars? How do you define whether a road or environment is stable enough for driverless technology? These are some of the questions the self-driving car community will have to solve as the technology continues to develop and prepare for mass adoption. In this regard, the new co-CEOs of Waymo are well-positioned to face these challenges. Dolgov, who was Waymo’s CTO before taking on his new role, has a Ph.D. in computer science with a focus on artificial intelligence and a long history of working on self-driving car technology. As a postdoc researcher, he was part of the Stanford University self-driving car team that won second place in DARPA’s 2007 Urban Challenge. He was also a researcher at Toyota’s Research Institute in Ann Arbor, Michigan. And since 2009, he has been among the senior engineers in Google’s self-driving car outfit that later became Waymo. In a nutshell, he’s as good a leader you can have to deal with the AI software, algorithm, and hardware challenges a driverless car company will face in the coming years. Mawakana is a Doctor of Law and had led policy teams at Yahoo, eBay, and AOL before joining Waymo and becoming COO. She’s now well-positioned to tackle the legal and policy challenges Waymo will face as self-driving cars gradually try to find their way into more jurisdictions. The dream of self-driving cars is far from dead. In fact, in his final year as CEO Krafcik managed to secure more than $3 billion in funding for Waymo. And there’s a lot of interest in self-driving cars and their potential value. But Waymo’s new lineup suggests self-driving cars have a bumpy road ahead. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/23/4-reasons-to-learn-machine-learning-with-javascript/,4 reasons to learn machine learning with JavaScript,"In the past few years, Python has become the preferred programming language for machine learning and deep learning. Most books and online courses on machine learning and deep learning either feature Python exclusively or along with R. Python has become very popular because of its rich roster of machine learning and deep learning libraries, optimized implementation, scalability, and versatile features. But Python is not the only option for programming machine learning applications. There’s a growing community of developers who are using JavaScript to run machine learning models. While JavaScript is not a replacement for the rich Python machine learning landscape (yet), there are several good reasons to have JavaScript machine learning skills. Here are four. Most machine learning applications rely on client-server architectures. Users must send their data where the machine learning models are running. There are clear benefits to the client-server architecture. Developers can run their models on servers and make them available to user applications through web APIs. This makes it possible for developers to use very large neural networks that can’t run on user devices. In many cases, however, it is preferable to perform the machine learning inference on the user’s device. For instance, due to privacy issues, users may not want to send their photos, private chat messages, and emails to the server where the machine learning model is running. Fortunately, not all machine learning applications require expensive servers. Many models can be compressed to run on user devices. And mobile device manufacturers are equipping their devices with chips to support local deep learning inference. But the problem is that Python machine learning is not supported by default on many user devices. MacOS and most versions of Linux come with Python preinstalled, but you still have to install machine learning libraries separately. Windows users must install Python manually. And mobile operating systems have very poor support for Python interpreters. JavaScript, on the other hand, is natively supported by all modern mobile and desktop browsers. This means JavaScript machine learning applications are guaranteed to run on most desktop and mobile devices. Therefore, if your machine learning model runs on JavaScript code in the browser, you can rest assured that it will be accessible to nearly all users. There are already several JavaScript machine learning libraries. An example is TensorFlow.js, the JavaScript version of Google’s famous TensorFlow machine learning and deep learning library. If you head to the TensorFlow.js demo page with your smartphone, tablet, or desktop computer, you’ll find plenty of ready examples using JavaScript machine learning. They will run the machine learning models on your device without sending any data to the cloud. And you don’t need to install any additional software. Other powerful JavaScript machine learning libraries include ML5.js, Synaptic, and Brain.js. Privacy is not the only benefit of on-device machine learning. In some applications, the roundtrip of sending data from the device to server can cause a delay that will hamper the user experience. In other settings, users might want to be able to run their machine learning models even when they don’t have an internet connection. In these cases, having JavaScript machine learning models that run on the user’s device can come in very handy. Another important use for JavaScript machine learning is model customization. For example, suppose you want to develop a text generation machine learning model that adapts to the language preferences of each user. One solution would be to store one model per user on the server and train it on the user’s data. This would put extra load on your servers as your users grow and it would also require you store potentially sensitive data in the cloud. An alternative would be to create a base model on your server, create a copy on the user’s device, and finetune the model with the user’s data using JavaScript machine learning libraries. On the one hand, this would keep data on users’ devices and obviate the need to send them to the server. On the other hand, it would free up the resources of the server by avoiding to send extra inference and training loads to the cloud. And users would still be able to use their machine learning capabilities even when they’re disconnected from your servers. Another benefit of JavaScript machine learning is easy integration with mobile applications. Python support in mobile operating systems is still in the preliminary stages. But there is already a rich set of cross-platform JavaScript mobile app development tools such as Cordova and Ionic. These tools have become very popular because they enable you to write your code once and deploy it for iOS and Android devices. To make the code compatible across different operating systems, cross-platform development tools launch a “webview,” a browser object that can run JavaScript code and can be embedded in a native application of the target operating system. These browser objects support JavaScript machine learning libraries. One exception is React Native, a popular cross-platform mobile app development framework that does not rely on webview to run applications. However, given the popularity of mobile machine learning applications, Google has released a special version of TensorFlow.js for React Native. If you have written your mobile app in native code and want to integrate your JavaScript machine learning code, you can add your own embedded browser object (e.g., WKWebView in iOS) to your app. There are other machine learning libraries for mobile applications, such as TensorFlow Lite and Core ML. However, they require native coding in the mobile platform you are developing your app for. JavaScript machine learning, on the other hand, is very versatile. If you have already implemented a version of your machine learning application for the browser, you can easily port it to your mobile application with little or no changes. One of the main challenges of machine learning is training the models. This is especially true for deep learning, where learning requires expensive backpropagation computations over several epochs. While you can train deep learning models on user devices, it could take weeks or months if the neural network is large. Python is better suited for server-side training of machine learning models. It can scale and distribute its load on server clusters to accelerate the training process. Once the model is trained, you can compress it and deliver it on user devices for inference. Fortunately, machine learning libraries written in different languages are highly compatible. For instance, if you train your deep learning model with TensorFlow or Keras for Python, you can save it in one of several language-independent formats such as JSON or HDF5. You can then send the saved model to the user’s device and load it with TensorFlow.js or another JavaScript deep learning library. But it is worth noting that server-side JavaScript machine learning is also maturing. You can run JavaScript machine learning libraries on Node.js, the JavaScript application server engine. TensorFlow.js has a special version that is suited for servers running Node.js. The JavaScript code you use to interact with TensorFlow.js is the same you would use for applications running in the browser. But in the background, the library makes use of the special hardware of your server to speed up training and inference. PyTorch, another popular Python machine learning library, doesn’t yet have an official JavaScript implementation, but the open source community has developed JavaScript bindings for the library. Machine learning with Node.js is fairly new, but it is fast evolving because there is growing interest in adding machine learning capabilities to web and mobile applications. As the JavaScript machine learning community continues to grow and the tools continue to mature, it might become a go-to option for many web developers who want to add machine learning to their skillset. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/23/imperva-bad-bots-polluting-web-traffic-pose-security-risks-to-websites/,Imperva: Bad bots polluting web traffic pose security risks to websites,"Last year, 40.8% of all web traffic requests were not human, underscoring the growing scale of bot traffic across the Internet, Imperva Research Labs, a research group within security software and services company Imperva, said after an in-depth analysis of Internet traffic across every industry. If you couldn’t find a new gaming console to purchase online in late 2020, this report explains why. Through an analysis of billions of bot requests collected from Imperva’s global network, Imperva Research Labs’ 8th annual Bad Bot Report provides an in-depth look at the bot problem that now spans every industry and every region. Whether it’s a bot hoarding gaming hardware or a bot scraping for data as a means to collect and share helpful information with the public, when a site is polluted with automated traffic, it slows web performance and makes it harder for legitimate users to access the information or services they need. While some bot activity may appear benign, Imperva’s report shows that bot activity can be a business risk. Last year, 34% of all login attempts originated from malicious bots – those that closely mimic human behavior and are harder to detect and stop. This particular breed of bots should be a concern for businesses as they are most often responsible for content scraping, account creation, account takeover, denial of service and denial of inventory. The issue of bad bots is unlikely to slow down in the future. In 2020, bad bot traffic reached a record high — 25.6% of all web requests — while human traffic decreased by 5.7%. This is not a traditional security risk; it’s a 24/7 abuse of business’ websites, mobile apps and APIs. Read more in Imperva’s full report Bad Bot Report."
https://venturebeat.com/2021/04/23/gooddata-unveils-analytics-as-a-set-of-microservices-in-data-as-service-platform/,GoodData unveils analytics as a set of microservices in data-as-a-service platform,"GoodData this week unfurled a data-as-a-service platform that employs Docker containers and microservices running on Kubernetes clusters to dynamically scale analytics up and down on demand. The GoodData.CloudNative (GoodData.CN) platform heralds a new cloud-native era that enables easier embedding of analytics within applications. Key to that is a well-defined set of application programming interfaces (APIs), said Roman Stanek, CEO of GoodData. “It makes analytics much more flexible,” he said. Initially available for free via a community edition of the platform that comes in the form of a single Docker container image, GoodData also plans to make GoodData.CN available in Freemium, Growth, and Enterprise editions that come with additional capabilities, along with support from GoodData. Most existing analytics applications are based on monolithic architectures originally created for desktop PCs. These are not designed to dynamically scale up and down on demand. GoodData.CN takes advantage of the orchestration capabilities of Kubernetes to provide application developers with as much compute and storage resources as they can afford to consume, either via a public cloud or in an on-premises IT environment. The ability to deploy GoodData.CN anywhere is crucial because multiple centers of data gravity will always exist in the enterprise, noted Stanek. It’s unlikely any major enterprise is ever going to be able to standardize on a single data warehouse or data lake, he said. The GoodData.CN platform provides all the metadata capabilities required to maintain a single source of truth across what are rapidly becoming highly federated environments, noted Stanek. A programmable API also makes it feasible to deploy a headless data-as-a-service platform for processing analytics that can be readily accessed and consumed as a service by multiple applications. Previously, individual developers had to take the time and effort to embed analytics capabilities directly within their application, noted Stanek. The GoodData.CN platform makes applications more efficient and, as a consequence, smaller. That is because more analytics processing is offloaded to the headless platform, added Stanek. Pressure to embed analytics in every application is mounting as end users seek to make faster and better fact-based decisions. Rather than having to move data into a separate application to analyze it, Stanek said the GoodData.CN platform makes it simpler to infuse real-time analytics within an application. The need to embed analytics within applications is becoming more pronounced with the acceleration of various digital business transformation initiatives. The expectation is that next-generation applications will all provide some type of embedded analytics capability that enables end users to make better decisions in the moment versus long waits for a report prepared by a business analyst, Stanek said. In many cases, the query that was launched by a business analyst is no longer especially relevant by the time that a report can be delivered. GoodData is not likely the last provider of software that will be going cloud-native. A microservices-based application makes it easier to add new features and capabilities to software by ripping and replacing containers. It also makes applications more resilient. That is because, should any microservice become unavailable for any reason, calls are dynamically rerouted to other microservices to ensure redundancy. Most software developers are rapidly moving down the path to microservices as an alternative to monolithic applications that may be easier to build but that are increasingly viewed as being inflexible. In the case of GoodData, it’s not clear to what degree they may be ahead of rivals making similar transitions. However, enterprise IT organizations should expect in the months ahead a wave of headless services based on microservices architectures that will change the way data is consumed and managed."
https://venturebeat.com/2021/04/23/ai-weekly-mit-aims-to-reconcile-data-sharing-with-eu-ai-regulations/,AI Weekly: MIT aims to reconcile data sharing with EU AI regulations,
https://venturebeat.com/2021/04/23/now-is-the-time-for-a-transatlantic-dialog-on-the-risk-of-ai/,Now is the time for a transatlantic dialog on the risk of AI ,"Artificial intelligence is no longer the world’s darling; no longer the “15 trillion dollar baby.” Mounting evidence that AI applications can cause harm and pose risk to communities and citizens has lawmakers under pressure to come up with new regulatory guardrails. While the US government is deliberating on how to regulate big tech, all eyes are on the unbeaten valedictorian of technology regulation: the European Commission. This past Wednesday, April 21, the Commission released wide-ranging proposed regulation that would govern the design, development, and deployment of AI systems. The proposal is the result of a tortuous path that involved the work of a high-level expert group (full disclosure: one of us was a member), a white paper, and a comprehensive impact assessment. The proposal has already elicited both enthusiastic and critical comments and will certainly be amended by the European Parliament and the Council in the coming months, before becoming a final piece of legislation. It is, however, the first of its kind, and marks an important milestone. In particular, it sends a signal to regulators in the US that they will have to address AI as well, especially since the proposal underscores the need for AI risk assessment and accountability for both material and immaterial damage caused by AI — a major concern for both industry and society. The proposed regulation identifies prohibited uses of AI (for example, using AI to manipulate human behavior to circumvent users’ free will, or allowing “social scoring” by governments), and specifies criteria for identifying “high-risk” AI systems, which can fall under eight areas: biometric identification, infrastructure management, education, employment, access to essential services (private and public, including public benefits), law enforcement, and migration and border control. Whether or not an AI system is classified as “high-risk” depends on its intended purpose and its modalities, not just the function it performs. When an AI system is “high-risk,” it will need to undergo a pre-deployment conformity assessment and be registered in a to-be-established EU database. The focus on transparency in the proposed regulation is laudable and will change industry practice. Specifically, the new regulations would emphasize thorough technical documentation and recording a technology’s intentions and assumptions. But the strategy of pre-classifying risk has a blindspot. It leads the Commission to miss a crucial feature of AI-related risk: that it is pervasive, and it is emergent, often evolving in unpredictable ways after it has been developed and deployed. Imposing strict procedures on a subset of AI systems and checking them mostly while they are still “in the lab,” may not capture the evolution of risks emerging from the interaction between AI systems in the real world and the evolution of their behaviour over time. The Commission’s proposal contains provisions for post-market surveillance and monitoring, but these provisions appear weaker than the pre-deployment ones. As it stands, the Commission’s proposal relies heavily on the development of algorithmic auditing practices by so-called “notified bodies” and in the private sector as a whole. Auditing practices, ideally, should be consistent across the markets and geographies where an AI system is deployed; it should also be oriented towards the main requirements of so-called  “trustworthy AI,” and be grounded in principles of equity and justice. The spotlight is now on US regulators, as well as industry leaders. If they aren’t able to promise consistent auditing in US markets as well, that will impact the whole AI ecosystem. Instead of playing regulatory ping-pong across the pond, leaders on both sides of the Atlantic would benefit from initiating a research- and stakeholder-led dialog to create a transnational ecosystem focused on maximizing the impact of AI risk identification and mitigation approaches. At the moment, such a transnational approach is hindered by different cultural approaches to regulation, strong tech lobbying, lack of consensus on what constitutes AI risk assessments and AI auditing, and very different litigation systems. All these barriers can be overcome, and we can reap the real benefits of AI, if the European Commission’s proposal is taken as a cue to harmonize approaches across borders for the maximum protection of citizens. This dialog should focus on equity and impact, outlining optimal procedures for effective risk and audit documentation, and identifying what is needed from governments, civil society, and higher education to build up and maintain a transnational ecosystem of AI risk assessment and auditing. The benefits are obvious. Strong regulation would meet a strong technology research landscape. Rather than reconciling approaches after the fact, co-developing the regulatory approach from the outset and creating the preconditions for mutual learning would be far more effective. The renewed prospects for an  enlightened transatlantic dialog on digital issues are a one-time opportunity to make this happen. Mona Sloane is an Adjunct Professor at NYU’s Tandon School of Engineering and Senior Research Scientist at the NYU Center for Responsible AI. Andrea Renda is Senior Research Fellow and Head of Global Governance, Regulation, Innovation & Digital Economy at the Centre for European Policy Studies."
https://venturebeat.com/2021/04/23/the-5-jobs-you-need-to-consider-applying-for-this-week/,The 5 jobs you need to consider applying for this week,"Are you looking for a new role at the moment? If so, then you’re in luck today folks. We have compiled a list of some of the most exciting open roles all over the U.S. right now, in the hopes that you can land your dream job. You’re just a click away. After participating in our technology boot camp, you’ll get to dive immediately into building a product that is appreciated by hundreds of customers such as: Salesforce.com, Netflix, lululemon athletica, and many others. With every dev project you work on, you’ll create a direct impact on Workday’s product suite that pushes the latest innovations in cloud and mobile technology. Your contributions will be part of what makes Workday unique: the culture, the core values, the company meetings, the commitment to sustainability, the recognition programs, but most importantly, the people. You will collaborate with the Product Manager and QA counterparts on functional design and analysis of requirements, and design and develop products in a metadata-driven development environment. In this role, you will model excellent UX principles and mentor junior staff. The Staff UX Designer is an experienced individual contributor responsible for designing usable, accessible, and engaging user experiences for Visa’s digital product line. Although they would prefer candidates to be located in one of the Design Research hubs a– Austin, San Francisco or New York City — they are open to the possibility of a remote work arrangement should it suit all parties. The EBC Demo, Content and Technical Briefing Manager will manage the planning and execution of briefings, present solutions to customers, ensure the environment reflects Palo Alto Networks cybersecurity leadership through content and relevant demos, and serve as technical team lead. This role will liaise with Palo Alto Networks organizations including Sales, Product Marketing, and Product Management. This individual will maintain expertise on corporate strategy, products, and solutions in order to provide effective counsel to sales teams and effectively message to customers. As the customer advocate, this role ensures that the right message is being delivered to the customers at the right time to solve their most important security challenges. A Sr/Principal Thin Films Equipment Engineer is an integral member of the Technology Development Engineering team at Micron Technology and enables our development of tomorrow’s memory and storage technology! They develop groundbreaking semiconductor processing hardware — both internally with process engineering and externally with vendors — to enable novel process technology. They redefine what is possible within amazing R&D facilities. The Account Sales Manager (ASM) primary function is to perform outside sales of product inventory while driving an assigned route of accounts. The ASM is responsible for increasing product sales and placement of product displays at all large and small format “off premise” stores. ASM’s are responsible for upselling inventory, cooler space, and product displays. The primary role of the ASM is to effectively service all customers in a safe, productive, and professional manner to Red Bull executional standards. The successful candidate will manage the relationships with the customer contact for sale of Red Bull products at assigned RBDC accounts.Manage Red Bull products in assigned RBDC accounts. Good luck!"
https://venturebeat.com/2021/04/23/stolen-identities-sold-in-criminal-marketplace-soared-250-since-2019/,Netacea: Stolen identity sales in criminal marketplace up 250% since 2019,"The number of stolen digital identities available on the Genesis Market has risen from 100,000 in April 2019 to over 350,000 in March 2021, with over 18,000 added each month, Netacea, the bot detection and mitigation specialist, said in new research into the world’s largest invite-only deep web marketplace for stolen information. The Genesis Market is an illegal online marketplace for stolen credentials. While many underground markets for stolen credentials operate from the anonymity of the dark web, Genesis Market is accessible from the open web. Access to the illegal marketplace is closely guarded by a strict invitation system, but once inside, users are presented with a well-organized one-stop-shop of stolen digital identities. This data takes the form of device fingerprints, which allow users to essentially wear the “mask” of their victim online, gaining access to all their online accounts whilst bypassing traditional anti-fraud and cybersecurity defenses. Cybercriminals target victims with malware and account takeover (ATO) bots to infiltrate their devices and harvest login credentials, as well as cookies, form autofill data and device fingerprints. These are then put up for sale on Genesis Market as packaged “bots” which are used to impersonate victims online. The asking price per bot can range from as little as $0.70 up to around $350 depending on the amount and nature of the data. The most expensive will contain financial details to allow access to online banking accounts. Upon purchase, consumers are provided with a custom browser to load the data into and are free to browse the internet masquerading as the hapless victim, use saved logins to access their accounts and – where login cookies exist – continue a victim’s session. All without any access to the original device. Read more in Netacea’s full report Buying Bad Bots Wholesale: The Genesis Market"
https://venturebeat.com/2021/04/23/tackling-the-endpoint-security-hype-can-endpoints-actually-self-heal/,Tackling the endpoint security hype: Can endpoints actually self-heal?,"Imagine that every endpoint on an IT network is self-aware — it knows if it’s under attack and immediately takes steps to thwart the attack. It then shuts itself down and autonomously rebuilds itself with new software patches and firmware updates. This is the promise of self-healing endpoints: endpoints that continually learn about new attack techniques while keeping their configurations optimized for network and security performance. Unfortunately, the reality does not match the hype. A self-healing endpoint is defined by its self-diagnostics, combined with the adaptive intelligence needed to identify a suspected or actual breach attempt and take immediate action to stop the breach. Self-healing endpoints can shut themselves off, complete a recheck of all OS and application versioning, and then reset themselves to an optimized, secure configuration. All these activities happen autonomously, with no human intervention. What differentiates self-healing endpoint offerings on the market today is their relative levels of effectiveness in deploying resilience techniques to achieve endpoint remediation and software persistence to the OS level. Self-healing endpoints with multiple product generations of experience have learned how to create persistence to the firmware, OS, and application layer of endpoint system architectures. This is distinguished from automated patch updates using scripts governed by decision rules or an algorithm. That doesn’t qualify as a true self-healing endpoint and is better described as endpoint process automation. The self-healing endpoint is one of the most overhyped areas of cybersecurity today, with over 100 vendors currently vying for a piece of the market. The anticipated growth of business endpoint security is feeding this frenzy. Gartner predicts the endpoint protection platform (EPP) market will grow 18.5% in 2021 and climb from an estimated $8.2 billion in 2019 to about $18.8 billion by 2024. By the end of 2025, more than 60% of enterprises will have replaced older antivirus products with combined EPP and endpoint detection and response (EDR) solutions that supplement prevention with detection and response capabilities. Taken in total, Gartner’s Top Security and Risk Management Trends for 2021 underscores the need for more effective EDR, including self-healing endpoints. Growth is also being driven by rapidly changing cybersecurity threats. The recent SolarWinds hack forever changed the nature of cyberattacks by exposing how vulnerable software supply chains are as a primary threat vector and showing how easily endpoints could be rendered useless by compromised monitoring systems. The hackers embedded malicious code during DevOps cycles that propagated across customers’ servers. These techniques have the potential to render self-healing endpoints inoperable by infecting them at the firmware level. The SolarWinds attack shows how server, system, and endpoint device firmware and operating systems now form a launchpad for incursions initiated independently of the OS to reduce detection. Endpoints that were sold as self-healing are still being breached, and current gaps in the effectiveness and reliability of endpoints must be addressed. Runtime protection, containment, and fault tolerance-based endpoint security systems were oversold under the banner of self-healing endpoints. In fact, many don’t have the adaptive intelligence to recognize a breach attempt in progress. Fortunately, newer technologies that rely on behavioral analytics techniques found in EDR systems, threat hunting, AI-based bot detection, and firmware-based self-healing technologies have proven more reliable. Further complicating the self-healing endpoint landscape is the speed with which EDR and EPP begin merging to form unified endpoint security stacks. The value of EDR/EPP within an endpoint security stack depends on how well cybersecurity vendors strengthen platforms with new AI and machine learning. EPP offers a prime example of the need for AI and machine learning. The primary role of EPP in an endpoint security stack is to identify and block malicious code that seeks to overtake control of endpoints. It takes a solid combination of advanced threat detection, antivirus, and anti-malware technologies to identify, stop, and then eradicate the endpoint threat. A knowledge base comprising fully documented adversary tactics and techniques provides tooling to truth-test self-healing endpoint claims. Known as MITRE ATT&CK, this knowledge base has captured and cataloged data from actual breach attempts, supplying the verifications teams need to test out self-healing endpoint security claims. The knowledge base for endpoint validation also benefits vendors, as it discloses whether an endpoint is truly self-healing. Using the MITRE dataset, cybersecurity vendors can discover gaps in their applications and platforms. MITRE ATT&CK’s 14 categories of adversarial tactics and techniques form a framework that provides organizations and self-healing endpoint vendors with the data they need to simulate activity cycles. MITRE sponsors annual evaluations of cybersecurity products, including endpoint detection and response (EDR), where vendors can test their solutions against the MITRE ATT&CK datasets. The methodology process is based on a design, execute, and release evaluation process. Simulations of APT29 attacks comprise the 2019 dataset and the Carbanak+FIN7 2020 dataset. Evaluations for 2021 are now open for Wizard Spider and Sandworm. The ATT&CK Matrix for Enterprise serves as the framework for evaluations of each vendor’s EDR capabilities. EDR and self-healing endpoint vendors create test environments that include detection sensors designed to identify, block, and prevent intrusions and breaches from the datasets MITRE provided. Next, MITRE creates a red team comprising emulated adversarial attacks. APT29-based data was the basis of the evaluation in 2019 evaluations and Carbanak+FIN in 2020 and Wizard Spider and Sandworm data. The test involves a simulation of 58 attacker techniques in 10 kill chain categories. MITRE completes attack simulations and relies on detection types to evaluate how effective each EDR solution is in identifying a potential attack. The detection times are classified into alerts, telemetry, or none generated. Microsoft Threat Defender 365 was able to identify all 64 active alerts and successfully identified eight MITRE attack categories from the Enterprise Matrix. The following is an example of the type of data generated based on the simulated MITRE attack scenario. MITRE ATT&CK data has come to influence self-healing endpoint product design. When cybersecurity EDR vendors test their existing self-healing endpoints against MITRE ATT&CK data, they often find areas for improvement and innovation. For Microsoft, 365 Defender’s advances in identifying credential access, initial access, and privilege escalation attack scenarios based on modeled data help improve Threat Defender analytics. Based on the cumulative lessons learned from three years of MITRE ATT&CK data evaluations, the most effective self-healing endpoints are designing in self-generative persistence, resilience, and adaptive intelligence. The three techniques delivering the best results are AI-enabled bots that threat-hunt and remediate self-healing endpoints, behavior-based detections and machine learning to identify and act on threats, and firmware-embedded persistence. Companies across all industries can successfully use automation bots to anticipate security threats, reduce help desk workloads, troubleshoot network connectivity issues, reduce unplanned outages, and self-heal endpoints by continually scanning network activity for any signs of a potential or actual breach. Throughout the pandemic, software vendors have fast-tracked much of their AI and machine learning-based development to help customers improve their service management, asset management, and self-healing endpoint security. In the case of Ivanti, a decision to base its latest IT service management (ITSM) and IT asset management (ITAM) solutions on its AI-based Ivanti Neurons platform reflects the way AI-based bots can contribute to protecting and self-healing endpoints in real time in the “Everywhere Workplace.” The goal with these latest innovations is to improve ITSM and ITAM so IT teams have a comprehensive picture of IT assets from cloud to edge. Ivanti’s product strategy reflects its customers’ main message that virtual workforces are here to stay. They need to proactively and autonomously self-heal and self-secure all endpoints and provide personalized self-service experiences to support employees working from anywhere, anytime. VentureBeat spoke with SouthStar Bank IT specialist Jesse Miller about how effective AI-based bots are at self-healing endpoints. Miller said a major goal of the bank is to have endpoints self-remediate before any client ever experiences an impact. He also said the bank needs to have real-time visibility into endpoint health and have a single pane of glass for all ITSM activity. “Having an AI-based system like Ivanti Neurons allows what I call contactless intervention because you can create custom actions,” Miller said. “We’re relying on Ivanti Neurons for automation, self-healing, device interaction, and patch intelligence to improve our security posture and to pull in asset data and track and resolve tickets.” SouthStar’s business case for investing in a hyper-automation platform is based on hours saved compared to more manual service desk functions and preemptive self-healing endpoint security and management. Below is an example of how self-healing configurations can be customized at scale across all endpoints. Continually scanning every artifact in Outlook 365, Microsoft Defender 365 is one of the most advanced self-healing endpoints for correlating threat data from emails, endpoints, identities, and applications. When there’s a suspicious incident, automated investigation results classify a potential threat as malicious, suspicious, or no threat found. Defender 365 then takes autonomous action to remediate malicious or suspicious artifacts. Remediation actions include sending a file to quarantine, stopping a process, isolating a device, or blocking a URL. The Microsoft 365 Defender suite, which provides autonomous investigation and response, includes a Virtual Analyst. Earlier this month, Microsoft made Microsoft 365 Threat Defender analytics available for public preview. Most recent threats, high-impact threats, and threat summaries are all available in a single portal view. Absolute Software offers an example of firmware-embedded persistence providing self-healing endpoints. The company’s approach to self-healing endpoints is based on a firmware-embedded connection that’s undeletable from every PC-based endpoint. Absolute’s customers say the Persistence technology is effective in remediating endpoints, providing resilience and autonomous responses to breach attempts. Dean Phillips is senior technology director at customer PA Cyber, one of the largest and most experienced online K-12 public schools in the nation, serving over 12,000 students based in Midland, PA. Phillips said it’s been helpful to know each laptop has active autonomous endpoint security running and that endpoint management is a must-have for PA Cyber. “We’re using Absolute’s Persistence to ensure an always-on, two-way connection with our IT management solution, Kaseya, which we use to remotely push out security patches, new applications, and scripts. That’s been great for students’ laptops, as we can keep updates current and know where the system is,” Phillips said. Such an agent enables capable endpoint management on student laptops, which he called “a big plus.” Absolute’s 2021 Q2 earnings presentation reflects how quickly the self-healing endpoint market is expanding today. Cybersecurity vendors all claim to have self-healing endpoints. Absolute Software, Akamai, Blackberry, Cisco, Ivanti, Malwarebytes, McAfee, Microsoft 365, Qualys, SentinelOne, Tanium, Trend Micro, Webroot, and many others attest that their endpoints can autonomously heal themselves. Separating hype from results starts by evaluating just how effective the technologies they’re based on are at preemptively searching out threats and removing them. Evaluating self-healing endpoints using MITRE ATT&CK data and sharing the results with prospects needs to happen more. With every cybersecurity vendor claiming to have a self-healing endpoint, the industry needs better benchmarking to determine how effective threat hunting and preemptive threat assessments are. What’s holding more vendors back from announcing self-healing endpoints is how difficult it is to provide accurate anomaly detection and incident response (IR) results that can autonomously track, quarantine, or remove an inbound threat. For now, the three most proven approaches to providing autonomous self-healing endpoints are AI-enabled bots, behavioral-based detections, and firmware-embedded self-healing technologies."
https://venturebeat.com/2021/04/23/ronovo-surgical-completes-series-a-financing-aims-to-transform-surgery-with-innovations-focused-on-simplicity-precision-and-intelligence/,"Ronovo Surgical Completes Series A Financing, Aims to Transform Surgery with Innovations Focused on Simplicity, Precision and Intelligence","SHANGHAI–(BUSINESS WIRE)–April 23, 2021– Ronovo Surgical, an emerging medtech company focused on innovating minimally invasive (MIS) and digital surgery to address the tremendous needs of the Chinese surgical market, recently announced successful closing of Series A financing. Developed by renowned surgeons, medtech veterans and robotics experts, Ronovo was founded in Shanghai in 2019 to establish a transformative technology platform that democratizes MIS and digital surgery in the vastly underserved Chinese surgical market. By focusing on the three pillars of simplicity, precision, and intelligence, Ronovo is positioning itself as a gateway to China for cutting edge MIS and digital surgery technology companies from around the world. The Series A financing was co-led by Matrix Partners China and Vivo Capital, with strong, continued support from seed investor Lilly Asia Ventures (LAV) and participation from GGV Capital. “With the successful closing of our Series A, we are extremely proud of the strong support for Ronovo’s vision and strategy from leading technology and life science investors, Matrix Partners China, Vivo Capital and GGV Capital, as well as the continued support from our seed investor, LAV,” said Dr. John Ma, Founder, Chairman and CEO of Ronovo. “With our core team already deep in R&D, we are now well-positioned to accelerate our technology development efforts and pursue multiple global strategic partnerships in support of accelerated path to commercialization.” “We firmly believe that our team is the key to strong competitive advantage of Ronovo Surgical,” said Dr. Ying Mao, Co-Founder and CTO of Ronovo. “With world-class talent from top medtech MNCs and industry leaders in robotics, our team is uniquely qualified to take on the mission of enabling China’s hospitals with digital surgery solutions to optimize patient clinical outcome and reduce health economic burden.” “Ronovo Surgical represents our key investment in the surgical robotics space,” said Roger Sun, Director at Matrix Partners China. “We are greatly optimistic about the enormous market potential for endoscopic procedures and surgical robotics in China. Under the experienced leadership of Dr. John Ma, Ronovo has already recruited a world-class R&D team and began working with renowned clinical KOLs. We look forward to seeing Ronovo democratize MIS with innovations that are tailored to clinical needs.” “I have had the pleasure and privilege of working with the Ronovo Surgical founding team since conception and company inception,” said Dr. Hongbo Lu, Managing Partner at Vivo Capital. “Vivo has made significant investments in the space outside of China. We are proud to support Ronovo and its exceptional team to achieve its vision of providing robotic solutions to the surgical market in China through innovative internal development and strategic partnerships.” “Robotics-driven digital surgery is clearly a rising trend for MIS, with great market potential and tremendous room for innovation both globally and in China,” said Dr. Yi Shi, Founding Managing Partner of LAV. “As a highly committed investor in the digital surgery space, we have backed Ronovo Surgical from incubation stage. We are proud of what the company has achieved so far and have great conviction of Ronovo’s team and mission. We are also very excited to work with the prestigious investors from Series A who share the same vision to support Ronovo’s exciting journey to leapfrog existing surgical technologies.” “We believe in the team’s strong focus on customers and global partnership approach, which not only augments their internal capabilities but also accelerates the realization of their technology platform vision to address many unmet clinical needs in surgery,” said Jenny Lee, Managing Partner of GGV Capital. “We would love to continue to support Ronovo Surgical to success.” About Ronovo Surgical Founded in 2019 and headquartered in Shanghai, Ronovo Surgical is built by industry veterans from global leaders in surgical and industrial robotics, such as Intuitive Surgical, Johnson & Johnson, Medtronic and KUKA. Aiming to transform how surgery is performed in China, Ronovo is leveraging robust R&D capabilities and strategic partnerships globally to accelerate the development of a broad portfolio of MIS and digital surgery solutions that exemplify the core themes of simplicity, precision, and intelligence. About Matrix Partners China Matrix Partners China is an early-stage venture capital firm in China that was founded in 2008. With biopharmaceutical and medical technologies as the fund’s most dedicated areas, Matrix Partners China is committed to developing long-term relationships with outstanding entrepreneurs and helping them build significant, industry-leading companies. About Vivo Capital Founded in 1996, Vivo Capital is a leading global healthcare investment firm with a diverse, multi-fund investment platform in venture capital, growth equity, buyout, and public equities. The firm has approximately $5.8 billion in assets under management and has invested in over 280 public and private companies worldwide. Headquartered in Palo Alto, California, the Vivo team consists of more than 50 multi-disciplinary professionals. Vivo invests broadly in healthcare across all fund strategies, including biotechnology, pharmaceuticals, medical devices, and healthcare services, with a focus on the largest healthcare markets globally. About Lilly Asia Ventures Lilly Asia Ventures (LAV) is a leading biomedical venture capital firm founded in 2008, with offices in Shanghai, Hong Kong, and Menlo Park. LAV’s vision is to become the trusted partner for exceptional entrepreneurs seeking smart capital and to build great companies developing breakthrough products that can treat diseases and improve human health. About GGV Capital GGV Capital is a global venture firm that invests in local founders, with investments in the United States, Canada, China, Southeast Asia, India, Latin America, and Israel from offices in Silicon Valley, San Francisco, Singapore, Shanghai, and Beijing. As a multi-stage, sector-focused firm, GGV Capital invests in seed-to-growth stage companies across three sectors: Social/Internet, Enterprise Tech, and Smart Tech. Over the past two decades, the firm has backed more than 400 companies around the world.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210423005102/en/ ZiHan LinVP of Business Developmentzihan.lin@ronovosurgical.com"
https://venturebeat.com/2021/04/23/the-role-open-cloud-plays-in-accelerating-innovation-vb-live/,The role open cloud plays in accelerating innovation (VB Live),"Presented by Supermicro There are three main components for cloud success: the right hardware, the best software stack, and the appropriate network choice. To learn why flexible, innovation-fostering open clouds are the answer, what’s needed to produce the best outcome, and more, join this VB Live event. Register here for free. During the pandemic, cloud has played a major role in helping companies navigate the technological challenges that came in rapid succession. And in response, those companies are realizing the need to accelerate their move to the cloud, pivoting from a long-term IT strategy to an urgent requirement, says Rick Villars, group vice president, worldwide research, at IDC. “Cloud isn’t a substitute for hardware, or SaaS software — in reality, cloud is a foundation that enables people to consume any new technology, as rapidly as possible,” Villars says. “Rather than having to spend five years to move to the next architecture, whether that’s a new chip design or a new software design, cloud is used as a way to take advantage of that new processor, that new algorithm, that new data set immediately, and do that at scale.” Having made that call, IT leaders are faced with some major choices. “The key job of the IT organization in a cloud world is governance, because cloud brings with it so much automation of the delivery of resources,” Villars says. “Administration and classic management can waste time and add complexity, but setting the right rules for governance and making sure they’re being carried out, letting the cloud systems themselves do the work, you can be much faster.” It means being that much more efficient, and gaining the benefits of scale — one of cloud computing’s most valuable benefits. The second major consideration, whether you’re modernizing your applications or developing new applications, is that it all comes down to data, he says. That boils down to three essential considerations: ensuring you have complete control over the data that you have, ensuring you have the ability to make that data available wherever it’s needed, and paying constant attention to new data sources that can improve your business strategy and IT stack. “We like to say that cloud is very data-driven — basically your justification for cloud investment comes down to how it helped you more effectively use your data,” he explains. “If you’re always keeping that in the back of your mind — thinking about any development or strategy in adopting this, and asking is this helping me achieve this goal — then you’re going to be in a real leadership position in making cloud part of your business.” When it comes to accelerating that shift to cloud, there are practical decisions that need to be made immediately. “For those applications that have been a core part of the business, they’re facing three choices,” Villars says. “Do I modernize that application? Do I shift to a re-architected solution, developed in a way that’s more cloud-enabled and more cloud-linked? Or do I just do a simple lift and shift?” In many cases, this kind of lift and shift can appear to be faster, though it doesn’t usually save a lot of money in the long term, primarily because of administrative hurdles, he says. But it does allow you to begin that first part of the transition, getting your data moved into the cloud, and allowing you to start strategizing about the longer term: how to modernize that application to leverage it as part of your future business needs. Once you’ve addressed that urgent need to shift to the cloud, the other big question is how do you begin to use it to innovate? “That’s absolutely where I want to start looking at cloud as a foundation for new application development, new services,” Villars says. “I want a platform that can grow over the next few years to let me accelerate my business transformation.” And for that, open cloud offers the most comprehensive, flexible solution, he says. While public cloud environments offer the types of data, resources, and capacity necessary for innovation, organizations also want to have the option to take advantage of capabilities in another cloud, or keep the ability to develop solutions within their own environments that are strategic or have specific data control sovereignty issues to it. And an open environment layers across all these different cloud areas. By leveraging more open technologies in terms of standard hardware foundation or as-a-service consumption models for these solutions, you accelerate that ability to use new technology as quickly as possible everywhere you need it. “It’s not an either/or choice. It’s more ‘How do we ensure that we get the full advantage of this new and critical resource without sacrificing our ability to innovate and do innovation where we want to in the long term?’” he says. “Is this open option allowing me to get faster access, whether it’s a new processor, new memory, new algorithm, new data set? That’s where the value has to be as IT leaders think about using any of these technologies.” Don’t miss out! Register here for free! Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/23/nokia-and-newcore-wireless-bring-5g-broadband-to-tribal-nations-in-u-s/,Nokia and NewCore Wireless bring 5G broadband to tribal nations in U.S.,"Tribal nations haven’t had the best access to the internet. But Nokia and NewCore Wireless have partnered to bring 5G wireless networking and 4.9G/LTE service to underserved communities in the U.S. The network uses the 117MHz part of the Tribal Educational Broadband Service (EBS) radio band, made available last year by the Federal Communications Commission (FCC), to serve tribal lands with a license for broadband and wireless networking buildouts. More than 400 Native American tribes will have access to this spectrum across tens of thousands of square miles, often in remote rural areas, putting broadband within reach of millions more people. The initial buildouts by Nokia and NewCore Wireless will reach 15,000 tribal members, many living in high-poverty areas. The companies will focus first on North and South Dakota, Oklahoma, and California. The Native American communities involved include the Standing Rock Sioux Tribe, the Cheyenne Tribe, and the Arapaho Tribe. The initiatives will support e-learning, telehealth, and remote work during the pandemic. This band of spectrum is very valuable and can be used to deploy carrier-grade voice or broadband connectivity. The 400 included tribes received this spectrum, and Nokia is using its wireless technologies for rural broadband connectivity, such as 4.9G/LTE and 5G. The company aims to provide rapid and cost-effective wireless connectivity across large areas and the scope for multiple home and business connections from a single base station. The private wireless network, which is based on 4.9G/LTE technology, opens the door to services such as high-speed internet for the home or business (up to 1Gbps); mobile phone options where cellular coverage is not available; and educational enhancements, such as distance learning. John Pretty Bear, councilperson for the Standing Rock Sioux Tribe’s Cannonball District, said in a statement that this will level the technology playing field for the Standing Rock Sioux Tribe. He noted that high-speed internet is coming at a time when such networks are the driving force behind health care, education, and commerce. The 2.5Ghz band of spectrum offered by the Tribal EBS program is mature and can be found in the majority of mobile phones, telephone switching equipment, and add-on devices in the market today. Because current networks are already designed for this spectrum, it allows new carriers to deploy services immediately — using already available hardware. Access to more than 100MHz of that spectrum is good for 4G and offers a viable transition into 5G when communities are ready. “In 2019, the FCC announced a new spectrum opportunity to pull back spectrum that wasn’t being used in the educational band,” NewCore Wireless GM Albert Kangas said in an interview with VentureBeat. “A large part of the country was what we call ‘white space,’ where it was not being leased to either a school or an institution.” The Native American groups applied to take over that EBS band, which the FCC approved. As the spectrum was capable of supporting both 4G and 5G wireless service, this was like giving 400 small companies the chance to become wireless operators. The groups began looking at ways to implement the service and found Nokia. The Nokia Digital Automation Cloud platform offers high-bandwidth, low-latency wireless connectivity, local edge computing capabilities, and applications such as voice and video services. Nokia senior VP Ed Cholerton said the tech was really created to serve enterprise markets such as industrial automation. It turns out it was useful in this particular consumer market, where it could provide a combination of fixed wireless and mobile phone service. It is a compact, easy-to-deploy platform, comprising private cellular network equipment and a cloud-based operation monitoring system. “We felt this was the best way to move forward,” Kangas said. “We proposed this to a few of our tribal partners. And we’ve built three different networks so far. We feel that this would be a great opportunity for other tribal nations across this country to use this solution to build out their wireless network.” One of the first networks approved was Standing Rock Telecommunications, which covers parts of North Dakota and South Dakota, Standing Rock Telecommunications general manager Fred McLaughlin said in an interview with VentureBeat. While other tribal nations are underdeveloped, Standing Rock already had a lot of infrastructure. Many of the areas have standard telecommunications and networks with towers. These were built by the tribes because most commercial companies chose not to develop networks in the sparsely populated areas. “We were in a fortunate situation, where we could utilize what we’ve already invested into ourselves,” McLaughlin said. “For us, it was plug and play. We just had to purchase equipment [like base stations and antennae], put it up on the existing tower, and then use the frequency.” McLaughlin said not many tribes have been aggressively setting up such networks but that the project turned out to be very cost-effective. There are towns with around 200 to 800 people spread across a region the size of the state of Connecticut. Previously, the tribes were using 3G networks for cellphones, getting 5 megabits or 10 megabits a second. Now it’s more like 60 megabits a second, capable of 100 megabits a second, Kangas said. “For tribal people in general, you’re starting to see a lot of people kind of come back home,” McLaughlin said. “For at-home education, it gives them an alternative. The schools are using our equipment. There is a fiber alternative, but some people can’t afford it. Some of these people are living — not paycheck to paycheck, but under the poverty level. They are finally getting connected to the internet. That’s who we are providing this service for.” Institutions such as schools and police departments can set up a private network and not have their traffic go out on the internet, which would slow down the network. “Thanks to the CARES Act, there is money available to build out the networks and there is a lot of greenfield opportunity to provide coverage out there, where none existed before,” Cholerton said. “It serves a good social need as well. Tribal members do not have to rely on moving away from their community to have a good-paying job.” He added, “We started off with this solution for enterprise customers, but it turns out to be really great. And it serves a good social need as well, in this case. Tribal nations are underserved.” “This is another way to achieve coverage in an underserved community,” Kangas said. “We hope to be doing a lot more of this.”"
https://venturebeat.com/2021/04/23/taiwan-predicts-its-chip-industry-will-weather-global-shortage/,Taiwan predicts its chip industry will weather global shortage,"(Reuters) — Taiwan’s key semiconductor industry has years of growth ahead of it with no worries about oversupply despite a massive capital investment program and only a few competitors in the next decade or so, a senior government minister said on Friday. Kung Ming-hsin, the head of Taiwan’s economic planning agency, the National Development Council, told Reuters the business opportunities presented by the global transformation to a digital economy were “very, very enormous”. Kung also sits on the board of Taiwan Semiconductor Manufacturing Co as a representative of the largest shareholder, the government’s National Development Fund, which holds around 6% of the stock of the world’s most valuable semiconductor company. He said between now and 2025, Taiwan companies have planned more than T$3 trillion ($107 billion) in investment in the semiconductor sector, citing expansion plans from chip giants including TSMC and Powerchip Semiconductor Manufacturing. “Once they are built, Taiwan’s competitors in semiconductors in the next decade will be very few,” Kung said in an interview in his office building, which overlooks the presidential office. Taiwan’s semiconductor firms are ramping up production to tackle a global chip shortage, which has affected everything from carmakers to consumer products, and meet booming demand following the work-from-home trend during the COVID-19 pandemic. Soaring demand is set to continue, driven by 5G, artificial intelligence and electric vehicles, Kung said. “In the next decade or even longer there won’t be oversupply for semiconductors,” he added, when asked if the massive investment plans could have a downside. Taiwan is currently in the grip of its worst drought in more than half a century, but Kung said the impact on chip firms was limited at present, citing the amount of water they are able to recycle and the location of their main factories in Hsinchu in northern Taiwan, and in the island’s south. “These two places are okay at the moment. So the impact on semiconductors is not bad.” Still, Taiwan does face other challenges, not least from China where President Xi Jinping has made semiconductors a strategic priority. Kung named Samsung Electronics as Taiwan’s most serious competitor and also able to match TSMC’s advanced chipmaking, but said U.S. tech restrictions had for now blunted the Chinese threat. Intel — both a TSMC client and competitor — last month announced a $20 billion plan to expand its advanced chip making capacity. Kung said there was perhaps room for TSMC to cooperate with Intel, but “what’s important is really how you upgrade yourself”. To that end, the government is helping the industry develop the next generation of semiconductor manufacturing technology like 1 nanometre and beyond with funding support and talent recruitment programmes in the works, he added. ($1 = 28.1070 Taiwan dollars)"
https://venturebeat.com/2021/04/23/analytics-startup-unsupervised-raises-35m-to-spot-patterns-in-enterprise-data/,Analytics startup Unsupervised raises $35M to spot patterns in enterprise data,"Boulder, Colorado-based Unsupervised, a big data analytics company leveraging AI to find patterns in business data, today announced that it raised $35 million in a series B round led by Cathay Innovation and Signalfire. Unsupervised says that it intends to use the funding to hire additional employees as it continues to develop its platform. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Unsupervised claims to accomplish this by analyzing unstructured and structured datasets to arrive at insights “without ignoring the long tail.” The company automates data science processes including preparation and prioritization, making predictions on data in industries spanning transportation, supply chain, ecommerce, and sales and marketing. “We’re seeing a shift in the market where customers are seeking out analytics and AI platforms that don’t just do simple reporting — they reveal opportunities to change the business. BI and traditional AI is great for probing handfuls of known problems, but when you’re really trying to understand what’s happening you need to investigate beyond known issues,” CEO Noah Horton told VentureBeat via email. “This is where unsupervised learning is uniquely valuable. COVID really revealed the need for what we’ve built and this round will help us expand our footprint faster.” Unsupervised says that its AI can identify statistically significant patterns that highlight the differences across subgroups within the data. Using a technique called unsupervised learning or self-supervised learning, Unsupervised’s systems can generate labels from data by exposing the relationships between the data’s parts. That’s as opposed to traditional, supervised AI systems, which require annotated datasets in order to learn patterns and make predictions. For example, in the supply chain domain, Unsupervised’s AI can ostensibly look at the nuances of the local economy, logistics site, employee details, and shipments and inventory to spotlight areas with excess or insufficient supply. On the finance side, Unsupervised can drawn on databases to find fraud schemes and spot financial trends like where people are willing to spend versus save. The technology even has applications in health care, Unsupervised says, where it can reveal opportunities to minimize the time spent on administrative tasks. Unsupervised’s platform presents AI-discovered patterns to customers for review in a web dashboard. Teams can track the performance of these patterns over time, and the AI system learns from what’s prioritized and acted on to continuously improve the insights. Unsupervised isn’t disclosing many customers at this point. That said, the company volunteered that it has “a number” of Fortune 500 customers using the product, including teams at ADP, Disney, and Coatue. “Unsupervised’s customers use the platform for multiple use cases. The average customer is using the platform across three or more use cases. Some customers are supporting as many seven use cases with Unsupervised at one time,” a spokesperson told VentureBeat. In its recent Augmented Analytics Is the Future of Analytics report, Gartner predicts that by 2021, “augmented analytics” like Unsupervised’s will drive new purchases of analytics and business intelligence, as well as data science and machine learning platforms. Assuming this comes to pass, 75-employee Unsupervised’s prospects in the $168.8 billion business analytics market look bright — even in the face of competition from companies like Outlier. “Most companies recognize that data is the new ‘gold’ but still struggle to derive meaningful insights given the deluge of siloed data, both structured and unstructured, across organizations — exasperating teams that are already understaffed and overwhelmed,” Cathay Innovation cofounder and CEO Denis Barrier told VentureBeat. “However, Unsupervised’s unique approach to ‘AI-augmented analytics’ has the potential to be a game-changing tool. It is disrupting the entire process by ingesting data from everywhere and automating the time consuming, tedious portions so users can quickly draw the most interesting insights that are revenue-generating and actionable. We’re honored to support the company on their journey, which very well may usher in a transformation of big data and decision-making in the enterprise.” Eniac Ventures and Coatue also participated in the company’s latest funding round. It brings Unsupervised’s total raised to over $55 million following a $12.8 million series A round in August 2019."
https://venturebeat.com/2021/04/23/alpine-investors-acquires-outdoor-recreation-software-leader-aspira/,Alpine Investors Acquires Outdoor Recreation Software Leader Aspira," Alpine’s investment and partnership will accelerate organic and M&A-driven growth  SAN FRANCISCO & DALLAS–(BUSINESS WIRE)–April 23, 2021– Alpine Investors (“Alpine”), a people-driven private equity firm committed to building enduring software and services companies, announced today it has acquired Aspira (the “Company”), a market-leading software provider for the outdoor recreation industry, from Vista Equity Partners. Alpine’s partnership will accelerate organic growth across Aspira’s business lines and provide capital for strategic acquisitions. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210423005075/en/ Headquartered in Dallas with eight offices worldwide, Aspira’s mission is to cultivate a connected world through shared outdoor experiences. It employs over one thousand team members and provides a comprehensive suite of software used to manage all aspects of outdoor recreation access, including reservation, registration, licensing, and day use entry for public agencies and private entities. Aspira’s enterprise solutions enable its clients to manage multiple mission-critical activities using one centralized platform. Through ReserveAmerica.com-a leading booking site for public and privately-owned campground reservations-Aspira directly connects consumers with outdoor recreation experiences. Fraser Cameron will join Aspira as CEO, leveraging his deep experience leading passion community companies through rapid growth. Most recently serving as CEO of SmartPak Equine, Cameron brings a substantial history of operational, financial, and consumer subscription experience, including eight years leading the team at Velcro Companies in the roles of CEO and CFO. Further expanding the Aspira management team, Marlena Slowik and Graham Ballbach will assist in leading the Company during its next phase. Former CEO Mark Trivette and former President Seth Rosenberg will maintain leadership roles within the Company. Cameron, Ballbach, and Slowik all join Aspira through Alpine’s CEO program which brings supplemental leadership to its portfolio companies. “We are thrilled to welcome such a culturally-aligned and experienced leader in Fraser Cameron to the Aspira team,” said Trivette. “The Alpine team brings a wealth of knowledge and resources that will serve Aspira’s customers and team members exceptionally well throughout this next chapter of growth,” added Rosenberg. “I am excited to work alongside the talented Aspira team to build upon the great work Mark and Seth have done in leading the business. Our first objective is to listen to our team and customers as we implement growth initiatives throughout all lines of business-including significant investments in technology, marketing, and customer support,” said Cameron. “We’ve seen a surge of Americans seeking to explore the outdoors, especially over the past 12–18 months, and believe such growth will continue. Aspira has certainly benefitted from these market tailwinds, and we are delighted to partner with this dynamic Company-a true leader in an industry that brings so much joy to people by leveraging technology to connect them with nature,” said Billy Maguy, partner at Alpine. “We are committed to investing not just in the growth of the business, but also in the hardworking people who make it thrive.” LionTree Advisors acted as exclusive financial advisor to Vista in the transaction, and Kirkland & Ellis LLP served as its legal counsel. Wilson Sonsini Goodrich & Rosati served as legal counsel to Alpine. About Aspira Aspira provides connected experiences for the outdoor recreation industry. Its comprehensive suite of reservation and licensing technology and service solutions support federal, state, provincial, privately-owned, and local government parks, campgrounds, and conservation agencies, conveniently connecting them with outdoor adventure seekers from around the world. Aspira is headquartered in Dallas, Texas, with eight offices worldwide. For more information, visit www.AspiraConnect.com. About Alpine Investors Alpine Investors is a people-driven private equity firm that is committed to building enduring companies by working with, learning from, and developing exceptional people. Alpine specializes in investments in middle-market companies in the software and services industries. Its PeopleFirst strategy includes a CEO-in-Residence program which allows Alpine to bring proven leadership to situations where additional or new management is needed post-transaction. Alpine is currently investing out of its $1 billion seventh fund. For more information, visit http://www.alpineinvestors.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210423005075/en/ MiddleM Creative, on behalf of Alpine InvestorsJoanne Verkuilen, Managing Partnerjoanne@middlemcreative.com | 980.785.4356"
https://venturebeat.com/2021/04/22/challenges-of-applied-machine-learning/,The challenges of applied machine learning,"Every year, machine learning researchers fascinate us with new discoveries and innovations. There are a dozen artificial intelligence conferences where researchers push the boundaries of science and show how neural networks and deep learning architectures can take on new challenges in areas such as computer vision and natural language processing. But using machine learning in real-world applications and business problems—often referred to as “applied machine learning” or “applied AI”—presents challenges that are absent in academic and scientific research settings. Applied machine learning requires resources, skills, and knowledge that go beyond data science, that can integrate AI algorithms into applications used by thousands and millions of people every day. Alyssa Simpson Rochwerger and Wilson Pang, two experienced practitioners of applied machine learning, discuss these challenges in their new book Real World AI: A Practical Guide for Responsible Machine learning. Rochwerger, a former director of product at IBM Watson, and Pang, the CTO of Appen, draw on their personal experience and knowledge to provide many examples of how organizations succeeded or failed in integrating machine learning into their products and business models. Real World AI explains the common challenges and pitfalls of machine learning strategies and how product leaders can avoid repeating the failures of other organizations. Here are four of the key challenges that Rochwerger and Pang highlight in their book. Knowing the problem you want to solve is a challenge that applies to all software engineering tasks. Any experienced developer will acknowledge that “doing the right thing” is different from “doing the thing right.” In applied machine learning, defining the problem plays a crucial role in the choices you make for the technologies, data sources, and people who will be working on your product. “Only 20 percent of AI in pilot stages at major companies make it to production, and many fail to serve their customers as well as they could,” Rochwerger and Pang write in Real World AI. “In some cases, it’s because they’re trying to solve the wrong problem. In others, it’s because they fail to account for all the variables — or latent biases –that are crucial to a model’s success or failure.” Consider image classification problems. Deep neural networks can perform such tasks with stunning accuracy. But if you want to apply them to a real application, a detailed definition of the problem will determine the kind of model, data, talent, and investment you’ll need. For instance, if you want a neural network that can label the files in your image archive, there are plenty of pre-trained convolutional neural networks (e.g., ResNet, Inception) and public datasets (e.g., ImageNet and Microsoft COCO) that you can use out of the box. You can set up the deep learning model on your own server and run your images through it. Alternatively, you can sign up for an API-based service such as Amazon Rekognition or Microsoft Azure Computer Vision. In this case, inference will be done in the service provider’s servers. But suppose you’re working for a large agriculture company and want to develop an image classifier that runs on drones and can detect weeds in crops. Hopefully, the technology will help your company switch to precision application of herbicide to cut down costs, waste, and the negative effects of chemicals. In this case, you’ll need a more specialized approach. You’ll have to consider constraints on the machine learning model and the data. You need a neural network that is light enough to run on the compute resources of edge devices. And you’ll need a special dataset of labeled images of weed vs non-weed plants. In machine learning, defining the problem also includes determining how well you want to solve the problem. For example, in the case of image archive labeling, if your machine learning model mislabels five of every hundred images, you shouldn’t have much of a problem. But if you’re creating a cancer-detection neural network, then you’ll need a much higher standard. Every missed case can have life-impacting consequences. One of the key challenges of applied machine learning is gathering and organizing the data needed to train models. This is in contrast to scientific research where training data is usually available and the goal is to create the right machine learning model. “When creating AI in the real world, the data used to train the model is far more important than the model itself,” Rochwerger and Pang write in Real World AI. “This is a reversal of the typical paradigm represented by academia, where data science PhDs spend most of their focus and effort on creating new models. But the data used to train models in academia are only meant to prove the functionality of the model, not solve real problems. Out in the real world, high-quality and accurate data that can be used to train a working model is incredibly tricky to collect.” In many applied machine learning applications, public datasets are not useful for training models. You need to either gather your own data or buy them from a third party. Both options have their own set of challenges. For instance, in the herbicide surveillance scenario mentioned earlier, the organization will need to capture a lot of images of crops and weeds. For the machine learning model to work reliably, the engineers will need to take the photos under different lighting, environmental, and soil conditions. After gathering the data, they’ll need to label the images as “plant” or “weed.” Data labeling requires manual effort and is a tiring job and has given rise to an entire industry of its own. There are dozens of platforms and companies that provide data labeling services for AI applications. In other settings, such as health care and banking, the training data will contain sensitive information. In such cases, outsourcing labeling tasks can be tricky, and the product team will have to be careful not to run afoul of privacy and security regulations. Yet in other applications, the data might be fragmented and scattered across different databases, servers, and networks. When organizations are drawing data from various sources, they’ll face other challenges too, such as inconsistency between database schemas, mismatching conventions, missing data, outdated data, and more. In such cases, one of the main challenges of the machine learning strategy will be to clean the data and consolidate different sources into a data lake that can support the training and maintenance of the ML models. In cases where the data comes from different databases, verifying data quality and provenance is also crucial to the quality of machine learning models. “It’s incredibly common in an enterprise to find data scattered throughout databases in different departments without any documentation about where it’s from or how it got there,” Rochwerger and Pang warn. “As data makes its way from the point where it’s collected into the database where you find it, it’s very likely that it has been changed or manipulated in a meaningful way. If you make assumptions about how the data you’re using got there, you could end up producing a useless model.” Machine learning models are prediction machines that find patterns in data obtained from the world and forecast future outcomes from current observations. As the world around us changes, so do the data patterns, and models trained on past data gradually decay. “AI isn’t a ‘set it and forget it’ type of system that will keep churning out results without human intervention. It requires constant maintenance, management, and course-correction to continue to provide meaningful, desired output,” Rochwerger and Pang write in Real World AI. A stark example was the COVID-19 pandemic, which caused a worldwide lockdown and changed many living habits, which disrupted many machine learning models. For instance, as shopping transitioned from brick-and-mortar to online stores, machine learning models used in supply chain management and sales forecasting became obsolete and needed to be retrained. Therefore, a key part of any successful machine learning strategy is making sure you have the infrastructure and processes to collect a continuous stream of new data and update your models. If you’re using supervised machine learning models, you’ll also have to figure out how to label the new data. In some cases, you can do this by providing tools that allow users to provide feedback on the predictions made by the machine learning models. In others, you’ll need to label new data manually. “Don’t forget to allocate resources for the ongoing training of your model. Models have to be trained continually, or they’ll become less accurate over time as the real world changes around them,” Rochwerger and Pang write. In applied machine learning, your models will affect people’s work and lives (and your company’s bottom line). That’s why an isolated team of data scientists will seldom implement a successful machine learning strategy. “A business problem that can be solved by a model alone is very unusual. Most problems are multifaceted and require an assortment of skills — data pipelines, infrastructure, UX, business risk analysis,” Rochwerger and Pang write in Real World AI. “Put another way, machine learning is only useful when it’s incorporated into a business process, customer experience or product, and actually gets released.” Applied machine learning needs a cross-functional team that includes people from different disciplines and backgrounds. And not all of them are technical. Subject matter experts will need to verify the veracity of training data and the reliability of the model’s inferences. Product managers will need to establish the business objectives and desired outcomes for the machine learning strategy. User researchers will help to validate the model’s performance through interviews with and feedback from end-users of the system. And an ethics team will need to identify sensitive areas where the machine learning models might cause unwanted harm. “The nontechnical components of a successful AI solution are just as important, if not more important, than the purely technical skills necessary to build a model,” Rochwerger and Pang write. Applied machine learning also needs technical support beyond data science skills. Software engineers will have to help integrate the models into other software being used by the organization. Data engineers will need to set up the data infrastructure and plumbing that feed the models during training and maintenance. And the IT team will need to provide the compute, network, and storage resources needed to train and serve the machine learning models. “Even with a wonderful business strategy, a well-articulated, specific problem, and a great team, it’ll be impossible to achieve success without access to the data, tools, and infrastructure necessary to ingest each dataset, save it, move it to the right place, and manipulate it,” Rochwerger and Pang write. These are just some of the key challenges you’ll face in applied machine learning. You still need more elements to make your machine learning strategy work. In their book, Rochwerger and Pang discuss pilot programs, the “build vs. buy” dilemma, dealing with production challenges, security and privacy issues, and the ethical challenges of applied machine learning. The authors provide plenty of real-world examples that show how you can do things right and avoid botching your machine learning initiative. “There’s no reason to be afraid of AI. It’s not magic, and it’s not even rocket science. With hard work and the right team working together collaboratively, you can do this, and you can do it well,” Rochwerger and Pang write. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/22/intel-signals-aggressive-market-share-push-in-wake-of-improved-q1/,Intel signals aggressive market share push in wake of improved Q1,"Buoyed by strong continuing demand for PC semiconductors, Intel today announced flat non-GAAP revenue of $18.6 billion on a year-over-year basis that exceeded its previous guidance by $1 billion. Overall, Intel reported a net income of $5.7 billion, down 6% year over year. In addition to increased demand for PC semiconductors, which remain in short supply, Intel reported that the decline in demand for semiconductors used in enterprise servers has reached the bottom. Intel is forecasting increased sales of semiconductors in servers in the second half of the year as the COVID-19 pandemic subsides to the point that IT organizations begin investing in datacenters again. Intel launched 3rd Gen Intel Xeon Scalable processors, code-named Ice Lake, this quarter. And the company is banking on driving server refreshes in the second half of 2021. In addition, Intel is expecting to see increased demand from cloud service providers that are currently working through the massive amount of inventory they accumulated in 2020. The bulk of the revenue Intel is forecasting will be generated by 10-nanometer class processors in 2021. Intel is increasing its cadence for transitioning to 7-nanometer processors as part of an effort to regain processing power supremacy over rivals, Intel CEO Pat Gelsinger said. This was Gelsinger’s first call with industry analysts since returning to Intel after several years of leading VMware as its CTO and then CEO. “It’s amazing to be back at Intel, and Intel is back,” Gelsinger said. Intel also launched an Integrated Device Manufacture (IDM) 2.0 initiative this quarter to address the current processor shortage. The company is opening foundries to partners that build substrates and other components it depends on to build processors. Additionally, Intel is codesigning processors with cloud service providers. It expects cloud service providers to begin increasing orders from next quarter. In the meantime, strong demand for notebook PCs, in particular, has enabled Intel to weather the economic downturn brought on by the COVID-19 pandemic, as well as Apple’s decision to abandon Intel in favor of an M1 system-on-chip (SoC) architecture. The new M1 SoC architecture combines ARM CPUs with GPUs and other accelerators to deliver twice as much processing power as an x86 platform. As demand for other classes of processors starts to increase, along with PC components, Gelsinger has promised Intel will be very aggressive at the expense of rivals that can’t match its manufacturing muscle. In addition, Gelsinger notes that Intel processors are now optimized for new classes of workloads based on AI models that need to first be trained by processing massive amounts of data and then deployed using inference engines that require maximum processor performance. It’s not clear to what degree enterprise IT organizations are going to invest in 10-nanometer processor platforms when they know that systems based on next-generation 7-nanometer processors will become increasingly available in the second half of this year. Cloud service providers are also now making greater use of a wide array of processors to run workloads that might previously have been deployed on x86-based servers. Regardless of past missteps, Gelsinger said Intel is now better prepared to fight for control of every processor core being employed. Overall, Intel is now forecasting $17.8 billion in revenue in the second quarter. This is despite the efforts of rivals such as AMD and Nvidia, which are unable to meet demand for processing horsepower now being driven by everything from gaming sites to digital business transformation initiatives that continue to multiply as the global economy improves."
https://venturebeat.com/2021/04/22/bank-of-england-warns-of-potential-risks-from-cloud-data-providers/,Bank of England warns of potential risks from cloud data providers,"(Reuters) — The Bank of England might strengthen its controls on cloud data providers and other technology firms to counter possible risks to the stability of the financial system from the rise of fintech, Deputy Governor Dave Ramsden said. The Bank of England (BoE) has expressed concerns before about the reliance by financial firms, especially fintech startups, on third-party technology companies for key parts of their operations, and Ramsden said this scrutiny would intensify. “We plan to analyse further whether we need even stronger tools to manage the risk that critical third parties, including potentially cloud and other major tech providers, may pose to the Bank’s … objectives,” Ramsden told the Innovate Finance conference on Wednesday. Regulators globally have been tightening scrutiny of outsourced functions as they worry that core services financial firms provide to customers are vulnerable to outages at third parties. Britain’s government is keen to promote fintech as an area of growth and hopes that nimbler regulation will enable it to steal a march over the European Union, where British financial firms now have reduced access due to Brexit. The BoE has said it will not water down regulatory standards, but does see scope for more streamlined regulation of smaller banks and in some areas of insurance. On Monday, finance minister Rishi Sunak asked the BoE to work with the finance ministry on whether the central bank should set up a digital version of sterling to compete with cryptocurrencies, which he dubbed ‘Britcoin’. The government is also consulting over proposals to relax stock market listing rules due to a concern that Britain is less attractive than the United States as a listing venue, especially for tech companies whose founders want to keep an sizeable role. Ramsden said the BoE had taken a step to make life easier for smaller financial companies on Monday by giving firms more direct ways to access its high-value payments system, which is dominated by major banks and processing companies. Other steps included work standardising the identification of businesses involved in financial transactions, and looking at whether artificial intelligence could ease the burden of regulatory compliance."
https://venturebeat.com/2021/04/22/how-walmart-adapted-its-iot-strategy-to-the-pandemic/,How Walmart adapted its IoT strategy to the pandemic,"Walmart made $559 billion in total revenue during the COVID-19 pandemic’s first fiscal year, up from $514.4 billion in fiscal 2019, thanks in part to newly integrated internet of things (IoT) capabilities to improve food quality and lower energy consumption. Walmart claims its systems for IoT deployments are built at a scale unmatched across the retail industry: The company reports that, every day, it takes in approximately 1.5 billion messages and analyzes over one terabyte of data. This proprietary software includes a cloud-based dashboard application to manage volume and detect anomalous events, such as refrigeration failures, so they can ostensibly be fixed more quickly, saving ice cream from melting while driving corporate profit. VP of technology Sanjay Radhakrishnan oversees Walmart’s IoT platforms and applications. Radhakrishnan sat down with VentureBeat to describe the giant retail chain’s long-term data strategy and how it’s changed since last March to accommodate changing store ecosystems across the U.S. This interview has been edited for clarity and brevity. VentureBeat: How would you describe Walmart’s approach to IoT at a high level? Sanjay Radhakrishnan: When we started on this journey, we had three key objectives. One was to address this at the scale of Walmart’s, that Walmart can actually leverage the impact of IoT at Walmart scale. The second objective was to ensure that we are the control plane for our data. So, we control where our data lands, and we have the ability to convert into business insights. And then the last objective was really maintaining that connection to our end customer experience. And then ensuring that we are being good corporate citizens, with respect to our sustainability initiatives. So just want to set the stage that when we started on this IoT journey, those were the three main drivers that we were looking to solve. VentureBeat: I’m really interested in that IoT journey. Could you tell me more about how Walmart has evolved its tech platforms over the past couple of years? And what has that progression has looked like, maybe in the past 5, 10, even 15 years? Radhakrishnan: You know, with those three objectives in the background, we have always had all kinds of devices in our stores. And these devices typically come from vendors or original equipment manufacturers (OEMs) that actually manufactured these devices. Typically this equipment comes with some kind of an HMI human machine interface that’s on the machine, so you can actually go connect to it and collect data out of these devices in a one-off fashion. And we’ve always done that. But with this IoT journey, what we really wanted to do was we wanted to move into the driver’s seat, where we can actually normalize these datasets coming from all these different machines, different devices, and different OEMs. We normalize that data, and we control our data using IoT from these devices and provide those data sets to our business in a way where we can actually convert them into useful information and useful insights and really improve that end customer experience. So our journey really has been, instead of individual point-to-point access from these individual machines, on how we can grow this at scale by being the control plane and getting all this data from equipment, normalize it, and simplify it into our language so that we can do intelligent things things with it, right? And so that focus is really shifted inside Walmart by building our own software that we are using to form that control. VentureBeat: That’s fascinating. And, in building this proprietary software at such a great scale, did Walmart run into any particular kinds of challenges or problems that it then worked to overcome? Radhakrishnan: The biggest challenge we have is just the variety of devices that we have in our ecosystem. They come from different OEMs, they are across different generations of these devices, and they all speak different languages. And what this means to us is, in our world, we are dealing with a wide mix of sensors, a range of protocols, and really a myriad of information models. So our approach has been to look at how we build our software and where we are talking to all these devices. You know, talking to the different protocols. But we have an ability to kind of normalize all of that data into one consistent IoT specification. That’s a Walmart IoT specification. And then we apply the right kind of data quality checks, so that we can certify the data and drop it into our control plane. And then we take it from there. So once we are able to connect the devices to our control plane, then we can land the data, either at the edge or the cloud. And afterward our software engineers can build all kinds of applications for our business customers. And we really kind of looked at this in a cloud-agnostic fashion. So we ensure that we have a dual-pronged strategy with our infrastructure. We leverage infrastructure in our own datacenters, and we also leverage infrastructure at top cloud providers. The focus really has been to ensure that our IoT pipeline software can access the right infrastructure at scale, considering things like latency and connectivity concerns. VentureBeat: Within this group of devices you mentioned, are you including in-store ones like refrigeration systems from the ice cream case study? Radhakrishnan: Yeah, that’s right. So you walk into the store and you see a lot of refrigeration cases. We are talking about sensors that are actually inside these refrigeration cases. And they are connected to what we call controllers in the store. We are actually connecting into those controllers and pulling device telemetry signals. It’s a lot of operating functions that you’re getting out of the equipment, and we are getting it in a consistent manner, in a continuous stream, to do intelligent things. VentureBeat: For the refrigeration IoT tech, could I hear more about how that’s architected in the cloud? Are there any specific foods, like ice cream or frozen pizzas for example, that are easier or more difficult to maintain with the technology? Radhakrishnan: We stream from edge to the cloud, and we have different pathways in the cloud based on data usage patterns. Our IoT applications can access data across the edge and cloud to solve business problems. We are cloud agnostic and leverage a dual-prong strategy that includes access to infrastructure in our own datacenters and top cloud providers. And our focus has been to ensure that our IoT pipeline software can access the right infrastructure at scale, considering connectivity and latency constraints. The type of food in the refrigeration cases does not cause differing complexity of our system. VentureBeat: Do you have any statistics on whether Walmart food quality has been more consistent since IoT tech was implemented? I’m curious if there are any specific stores or products that have seen a particularly measurable difference. Radhakrishnan: What I’ll say is, our focus has been on how you drive operational efficiencies in the store. For example, when things go wrong in the store, technicians actually fix problems with this equipment that is in the store. So the focus has been on how you get the right technician to the right place at the right time so that we can proactively address issues. Because if you don’t, it could impact product quality. Since we have started this journey, just by looking at reference duration, we have been able to improve our refrigeration equipment health by an average of 30%. VentureBeat: On a related note, I remember reading about Walmart’s intention to limit energy consumption. Could you tell me more about how that energy approach is architected in the cloud? Are there any specific frameworks or data strategies that Walmart is using to accomplish that? Radhakrishnan: If you look at our architecture and our frameworks, I would say it’s everything from connecting to the devices to using sophisticated infrastructure and software that runs on the edge and actually knows how to connect to these devices while holding telemetry data. Now, it depends on our use cases. If they’re kind of low latency use cases, then we store data at the edge, and we have logic at the edge to fulfill those use cases. Otherwise, we are streaming data to the cloud. And in the cloud, we have multiple kind of patterns depending on data usage. We might extend the data into kind of a cold path, or a warm path, and our IoT applications have the ability to access the data, either at the edge or in the cloud. They can basically build business applications and solve business problems. So, if you’re talking about frameworks and the technology stack, it’s a mix. We use Walmart homegrown and open standard frameworks like Spring and .NET Core, our device protocols. We can connect to devices all the way from BACnet to Modbus to serial communications to some of the more recent protocols like HTTP and Message Queuing Telemetry Transport (MQTT). If you look at the tech stack itself, typically our device drivers are written in Java, and the applications themselves are all ReactJS, not GS applications that use Linux-type operating systems. VentureBeat: I’m interested in hearing more about how individual elements of Walmart’s tech stack — choices like Spring tools, for example — specifically help with IoT deployments. How and why do specific tools work well for Walmart’s use cases, like scaling large volumes of data? Radhakrishnan: Messages are generated by the equipment (such as HVAC and refrigeration controllers) in the stores and processed by software on edge infrastructure. From the IoT edge infrastructure, messages are then sent to our cloud storage to be processed and consumed by software applications. We use a hybrid approach of edge and cloud computing depending on the type of data. The data is sent over our secured network to our proprietary solution that has multiple architectural components and micro services. We use a mix of Walmart internally developed and open standard frameworks like Spring Boot and .NET Core. Our strategy is to build our software to be cloud agnostic, so we use common frameworks and languages such as Java, Embedded C, React, NodeJS, and Linux technologies. Our focus is really trying to make sure that we map the right technology to solve the right business problem. We always start with the customer in mind. What is the use case? What’s the business? How do our internal customers solve thinking of the end customer in mind, and then work our way back to what does that mean for tech and then what’s the right tech stack to actually fulfill that. So, I mean we are pretty open, and the focus really is on understanding the customer problem, and then marrying it to the right tech stack to solve that problem. VentureBeat: Could you tell me a little bit more about Walmart’s IoT developments in the last year, and how they’ve helped the chain adjust to the COVID-19 pandemic’s challenges? Radhakrishnan: The pandemic has definitely opened new proper business problems and use cases for us where IoT is extremely useful to leverage. For example, when the pandemic hit, we reduced hours in our stores, so our associates could restock inventory and sanitize stores for our customers. We have this system called Demand Response, which is one of the IoT applications that we have built in-house. And we were able to leverage that to a working model, where we can control the temperature settings in the stores to adjust to these new hours, and that  brought a lot of productivity to our associates. Instead of using more constrained and manual approaches, now they have an actual system, where they can do remote deployment of capabilities and really control our high-performance computing (HPC) systems in a remote manner at scale. From a productivity angle, it helped the business, and also from a sustainability angle, we were able to reduce the energy consumption on the grid. So to give you an example, our system was able to execute shedding events. We did it for about 200 sites, and we were able to save enough electricity to roughly power 20-plus U.S. households for a year. That gives you a scale for how we are giving back, both in terms of productivity for our associates and also in terms of sustainability. VentureBeat: How has Walmart’s existing IoT and tech infrastructure allowed for its engineers to create new capabilities, like the COVID-19 responses, so quickly? Radhakrishnan: Over the last few years, we have moved to the driver’s seat, where we built software that will normalize and control our data using IoT from these devices, converting the data to insights that the business can use in decision-making. We apply the right data quality checks to certify the data and bring the data into our control plane. We were able to incorporate real-time data streaming and improve the speed at which issues are identified and resolved in a highly accurate manner. Having this foundation in place has allowed us to quickly respond to external factors, like adjusting store hours overnight during the early response to COVID-19. Another recent example of the IoT technology allowing us to respond quickly would be in February, when the extreme cold weather impacted energy grids in numerous communities. We had the necessary controls in place for demand shedding already, so we were able to apply the tool in a new way that controlled HVAC heating set points and reduced our energy consumption. In less than two days, we used the technology to successfully reduce the HVAC energy consumption in almost 500 stores. VentureBeat: Are there any other digital platform technologies related to ML, blockchain, IoT, or ERP Walmart is deploying? And are there any in particular that Walmart wants to research next? Radhakrishnan: For our IoT use cases, we are looking at ways we can further improve the customer experience and our impact on the communities we serve. Through algorithms, we will continue to update our algorithms as we identify trends between what the data is telling us and how we should respond. Through equipment, we will identify other equipment that we can connect to that would provide a benefit to our customer for remote diagnostics and proactive maintenance."
https://venturebeat.com/2021/04/22/ally-partners-with-microsoft-to-explore-quantum-computing-use-cases-in-fintech/,Ally explores fintech products using quantum computing with Microsoft,"Microsoft today announced that fintech firm Ally will join the former’s Azure Quantum program to explore how quantum computing can create opportunities in the financial sector. The two companies say that they’ll apply research on quantum-inspired algorithms to understand ways they can be tapped to solve complex optimization challenges. Experts believe that quantum computing, which at a high level entails the use of quantum-mechanical phenomena like superposition and entanglement to perform computation, could one day accelerate AI workloads compared with classical computers. Scientific discoveries arising from the field could transform energy storage, chemical engineering, drug discovery, financial portfolio optimization, machine learning, and more, leading to new business applications. Emergen Research anticipates that the global quantum computing market for the enterprise will reach $3.9 billion by 2027. Using optimizers designed to run on classical hardware in the cloud, Ally says it’ll be able to explore quantum use cases without having to invest in specialized hardware. Collaborating with Microsoft as part of its Enterprise Acceleration Program, Ally plans to begin developing quantum subject-matter expertise and fostering relationships with an ecosystem of quantum computing partners. “We have been able to benefit from the deep experience of the Microsoft Quantum research team, learn about the types of problems quantum can help solve within the financial services industry, and begin developing quantum computing skills using Microsoft’s quantum development kit,” Ally wrote in a blog post. “Through this relationship, we are gaining the insight and experience from Microsoft’s leading researchers. Leveraging Microsoft’s Azure capabilities, Ally has access to the software languages, APIs, and infrastructure to build quantum skills. This will open many exciting opportunities and empower us to explore the use of quantum computing technology across the landscape of financial services [applications].” As for the problems Ally hopes to solve with quantum computing, they could include anything from knowing why a customer is likely to contact a call center to portfolio management and streamlining business processes, according to Microsoft. It’s Microsoft Quantum director Julie Love’s belief that future algorithms and quantum hardware might draw on Ally’s financial datasets to help professionals make decisions. For example, Monte Carlo, a popular method for analyzing risk in finance, requires many simulations to achieve high confidence. That’s because Monte Carlo simulations account for uncertainty and randomness by constructing probability distributions over many possible outcomes as opposed to one. In finance, Monte Carlo methods are applied to portfolio evaluation, planning, risk evaluation, and derivatives pricing. Quantum algorithms might improve these computations since quantum computers can run multiple scenarios simultaneously and, through quantum interference, reduce the error in simulation. “Technology has always played an important role in the financial sector, and it has been shown, time and time again, that those who lead with technology make the market … Even though large-scale quantum computers won’t be available in the near term, their future availability is something for which businesses across the board need to prepare,” Love said in a press release. “With access to Azure Quantum, Ally is preparing a quantum-ready workforce that will be able to leverage scalable hardware as it becomes available.” While work progresses toward viable quantum computing hardware, the private sector is increasingly investing in the technology. Last month, IBM announced it would install a quantum computer at the Cleveland Clinic, marking one of the first times the company has physically placed a quantum computer on-premises. And Microsoft previously partnered with global advisory Willis Towers Watson to experiment with ways quantum computing might assist the firm with its work in insurance, financial services, and investing. “Current modelling techniques to quantify risk require a huge amount of computing power, using thousands of computers over many hours,” Willis Towers Watson CEO John Haley said in a whitepaper. “Quantum computing offers us the chance to look at our clients’ problems in a different way. By focusing on how we would model the problems on quantum computers when they become available at scale, we are able to work with Microsoft to redefine the problems and speed up our solutions on existing hardware.” Microsoft launched Azure Quantum in private preview two year ago alongside a developer kit, compilers, and simulators. Partnerships with quantum hardware providers Honeywell, IonQ, or QCI enable developers in the program to use existing Microsoft products — like Visual Studio or the Quantum Development Kit — along with quantum computers. Quantum venture funding dipped 12% in 2020, but quantum investments rose 46%, according to CB Insights. The total amount raised in the sector reached $365 million that year."
https://venturebeat.com/2021/04/22/supervised-vs-unsupervised-learning-whats-the-difference/,Supervised vs. unsupervised learning: What’s the difference?,"At the advent of the modern AI era, when it was discovered that powerful hardware and datasets could yield strong predictive results, the dominant form of machine learning fell into a category known as supervised learning. Supervised learning is defined by its use of labeled datasets to train algorithms to classify data, predict outcomes, and more. But while supervised learning can, for example, anticipate the volume of sales for a given future date, it has limitations in cases where data falls outside the context of a specific question. That’s where semi-supervised and unsupervised learning come in. With unsupervised learning, an algorithm is subjected to “unknown” data for which no previously defined categories or labels exist. The machine learning system must teach itself to classify the data, processing the unlabeled data to learn from its inherent structure. In the case of semi-supervised learning — a bridge between supervised and unsupervised learning — an algorithm determines the correlations between data points and then uses a small amount of labeled data to mark those points. The system is then trained based on the newly-applied data labels. Unsupervised learning excels in domains for which a lack of labeled data exists, but it’s not without its own weaknesses — nor is semi-supervised learning. That’s why, particularly in the enterprise, it helps to define the business problem in need of solving before deciding which machine learning approach to take. While supervised learning might be suited for tasks involving classifying, like sorting business documents and spreadsheets, it would adapt poorly in a field like health care if used to identify anomalies from unannotated data, like test results. Supervised learning is the most common form of machine learning used in the enterprise. In a recent O’Reilly report, 82% of respondents said that their organization opted to adopt supervised learning versus supervised or semi-supervised learning. And according to Gartner, supervised learning will remain the type of machine learning that organizations leverage most through 2022. Why the preference for supervised learning? It’s perhaps because it’s effective in a number of business scenarios, including fraud detection, sales forecasting, and inventory optimization. For example, a model could be fed data from thousands of bank transactions, with each transaction labeled as fraudulent or not, and learn to identify patterns that led to a “fraudulent” or “not fraudulent” output. Supervised learning algorithms are trained on input data annotated for a particular output until they can detect the underlying relationships between the inputs and output results. During the training phase, the system is fed with labeled datasets, which tell it which output is related to each specific input value. The supervised learning process progresses by constantly measuring the resulting outputs and fine-tuning the system to get closer to the target accuracy. Supervised learning requires high-quality, balanced, normalized, and thoroughly cleaned training data. Biased or duplicate data will skew the system’s understanding, with data diversity data usually determining how well it performs when presented with new cases. But high accuracy isn’t necessarily a good indication of performance — it might also mean the model is suffering from overfitting, where it’s overtuned to a particular dataset. In this case, the system will perform well in test scenarios but fail when presented with a real-world challenge. One downside of supervised learning is that a failure to carefully vet the training datasets can lead to catastrophic results. An earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. Another computer vision corpus, 80 Million Tiny Images, was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.” In machine learning problems where supervised learning might be a good fit but there’s a lack of quality data available, semi-supervised learning offers a potential solution. Residing between supervised and unsupervised learning, semi-supervised learning accepts data that’s partially labeled or where the majority of the data lacks labels. The ability to work with limited data is a key benefit of semi-supervised learning, because data scientists spend the bulk of their time cleaning and organizing data. In a recent report from Alation, a clear majority of respondents (87%) pegged data quality issues as the reason their organizations failed to successfully implement AI. Semi-supervised learning is also applicable to real-world problems where a small amount of labeled data would prevent supervised learning algorithms from functioning. For example, it can alleviate the data prep burden in speech analysis, where labeling audio files is typically very labor-intensive. Web classification is another potential application; organizing the knowledge available in billions of webpages would take an inordinate amount of time and resources if approached from a supervised learning perspective. Where labeled datasets don’t exist, unsupervised learning — also known as self-supervised learning — can help to fill the gaps in domain knowledge. Clustering is the most common process used to identify similar items in unsupervised learning. The task is performed with the goal of finding similarities in data points and grouping similar data together. Clustering similar data points helps to create more accurate profiles and attributes for different groups. Clustering can also be used to reduce the dimensionality of the data where there are significant amounts of data. Reducing dimensions, a process that isn’t unique to unsupervised learning, decreases the number attributes in datasets so that the data generated is more relevant to the problem being solved. Reducing dimensions also helps cut down on the storage space required to store datasets and potentially improve performance. Unsupervised learning can be used to flag high-risk gamblers, for example, by determining which spend more than a certain amount on casino websites. It can also help with characterizing interactions on social media by learning the relationships between things like likes, dislikes, shares, and comments. Microsoft is using unsupervised learning to extract knowledge about disruptions to its cloud services. In a paper, researchers at the company detail SoftNER, a framework that Microsoft deployed internally to collate information regarding storage, compute, and outages. They claim that it eliminated the need to annotate a large amount of training data while scaling to a high volume of timeouts, slow connections, and other product interruptions. More recently, Facebook announced SEER, an unsupervised model trained on a billion images that ostensibly achieves state-of-the-art results on a range of computer vision benchmarks. SEER learned to make predictions from random pictures found on Instagram profile pages. Unfortunately, unsupervised learning doesn’t eliminate the potential for bias in the system’s predictions. For example, unsupervised computer vision systems can pick up racial and gender stereotypes present in training datasets. Some experts, including Facebook chief scientist Yann LeCun, theorize that removing these biases might require a specialized training of unsupervised models with additional, smaller datasets curated to “unteach” specific biases. But more research must be done to figure out the best way to accomplish this. Between supervised, semi-supervised, and unsupervised learning, there’s no flawless approach. So which is the right method to choose? Ultimately, it depends on the use case. Supervised learning is best for tasks like forecasting, classification, performance comparison, predictive analytics, pricing, and risk assessment. Semi-supervised learning often makes sense for general data creation and natural language processing. As for unsupervised learning, it has a place in performance monitoring, sales functions, search intent, and potentially far more. As new research emerges addressing the shortcomings of existing training approaches, the optimal mix of supervised, semi-supervised, and unsupervised learning is likely to change. But identifying where these techniques bring the most value — and do the least harm — to customers will always be the wisest starting point."
https://venturebeat.com/2021/04/22/battery-ventures-leads-growth-investment-of-more-than-150-million-in-growing-subscription-management-sector/,Battery Ventures Leads Growth Investment of More Than $150 Million in Growing Subscription Management Sector," Investments in financial-software companies SaaSOptics and Chargify will help SaaS companies better automate, manage financial operations  SAN FRANCISCO–(BUSINESS WIRE)–April 22, 2021– Battery Ventures, a global, technology-focused investment firm, today announced it led a combined growth-equity investment of more than $150 million in two complementary, cloud-software platforms that manage billing and automate related financial functions including payments, revenue recognition and analytics for software-as-a-service (SaaS) companies. The two companies, SaaSOptics and Chargify, together are trusted by more than 2,000 customers and manage more than $10 billion in customer annual recurring revenue. Some of the world’s most high-profile SaaS brands use SaaSOptics and Chargify to power their subscription billing and financial operations. The Battery investment is intended to power growth at both companies and allow them to further invest in their products. Battery has extensive experience making growth-stage, financial-technology investments, including in companies selling cloud software to corporate CFOs, including Avalara, Coupa, Intacct (now part of Sage), AuditBoard, Soldo and Carta. Today, SaaS subscription management and revenue management services are growing rapidly given the broader transition to subscription-based software. The growth also reflects the push by many SaaS CFOs to streamline financial functions like billing, invoicing, and revenue recognition-and tap their internal financial data for key metrics to help them better serve customers and boost revenue. “We are thrilled to partner with both SaaSOptics and Chargify, each of which has built a powerful business in their respective financial-software markets,” said Chelsea Stoner, a Battery Ventures general partner. “Both companies’ growing businesses highlight the fact that today’s software companies require more robust billing and revenue solutions, driven in large part by new, usage-based, cloud models. We are excited to invest in more innovative products that help all software companies better manage their businesses.” SaaSOptics, based in Atlanta, Ga., is a B2B subscription management platform that specializes in automating the financial operations of SaaS companies by streamlining the order-to-revenue process and automating revenue recognition; invoicing and payments; and subscription analytics and metrics. Chargify, based in San Antonio, Tex. and also a leader in billing and subscription management for B2B SaaS, specializes in complex usage and events-based billing, subscription management, payment collections, and data management tools. “We are very excited to partner with Battery Ventures. SaaSOptics is committed to delivering end-to-end financial operations solutions that enable subscription commerce,” said Tim McCormick, CEO of SaaSOptics. “With Battery’s deep expertise and success in partnering with cloud-based financial management software companies, we see a huge opportunity to transform the subscription management market. This vision provides a complete financial operations platform across business models: B2B, B2C; regardless of billing structure: fixed price, tiered, seat-based or events-based and any combination of the above.” Paul Lynch, CEO of Chargify, added: “With Battery’s history of partnering with prominent SaaS businesses, the firm recognizes the massive potential in the subscription-management space. Industries are shifting to subscription, as well as usage and event-based business models, and our solutions allow those industries to monetize and optimize customer value.” *For more information about Battery Ventures and a complete list of Battery Ventures’ investments, click here. About SaaSOptics SaaSOptics is a subscription management platform that automates financial operations for growing B2B SaaS businesses. A cloud-based solution, the SaaSOptics platform allows businesses to pull accurate SaaS metrics and analytics quickly, scale billing and payments smoothly and automate GAAP/IFRS-compliant revenue recognition. Businesses built on SaaSOptics eliminate their risky dependency on spreadsheets and streamline financial operations. SaaSOptics is easy to use, trusted by investors, within reach for early-stage startups and provides a streamlined implementation process. About Chargify Founded in 2009, Chargify has helped thousands of businesses manage millions of offers that drive billions in annual revenue. Chargify removes billing bottlenecks and gives front, corner, and back-office teams the speed and flexibility to grow faster. Over the past decade, Chargify has continued to expand its offerings to address the complexities of the entire subscription lifecycle: recurring billing, subscription management, revenue retention, prepaid subscriptions, revenue operations, and events-based billing. The company has headquarters in San Antonio, Texas and Dublin, Ireland. Learn more about Chargify at www.chargify.com. About Battery Ventures Battery partners with exceptional founders and management teams developing category-defining businesses in markets including software and services, enterprise infrastructure, online marketplaces, healthcare IT and industrial technology. Founded in 1983, the firm backs companies at all stages, ranging from seed and early to growth and buyout, and invests globally from six strategic locations: Boston; San Francisco and Menlo Park, Calif.; Herzliya, Israel; London; and New York. Follow the firm on Twitter @BatteryVentures, visit our website at www.battery.com and find a full list of Battery’s portfolio companies here.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210422005899/en/ Battery VenturesRebecca Buckman, 650-703-0364becky@battery.com"
https://venturebeat.com/2021/04/22/is-poor-data-quality-undermining-your-marketing-ai/,Is poor data quality undermining your marketing AI?,"Marketing’s potential to deliver results relies on data quality, but data accuracy, consistency, and validity continue to be a challenge for many organizations. Inconsistent data quality is holding marketing teams back from converting leads into sales, accurately tracking campaign performance, and taking on the larger challenges of optimizing product mix and product/service revenue forecasts. The latest analytics, Account-Based Marketing (ABM), CRM, marketing automation, and lead scoring tools all provide real-time data capture and analysis. How the tools ensure consistent data quality directly impacts the quality of the AI and machine learning models the tools use. Marketing teams can’t deliver on their goals with bad data quality. For example, inaccurate prospect data clogs sales pipelines by slowing down efforts to turn marketing qualified leads (MQLs) into sales qualified leads (SQLs). Problems with data quality increase the odds of failure for AI initiatives such as predictive audience offers and promotions, personalization, AI-enabled chatbots for advanced service, and automated service recovery. A quarter of organizations attempting to adopt AI report an up to a 50% failure rate, IDC said recently. The leading causes of inconsistent data quality in marketing include problems with taxonomy and meta-tagging, lack of data governance, and loss of productivity. The most common reason AI and ML fail in the marketing sector is that there’s little consistency to the data across all campaigns and strategies. Every campaign, initiative, and program has its unique meta-tags, taxonomies, and data structures. It’s common to find marketing departments with 26 or more systems supporting 18 or more taxonomies, each created at one point in a marketing department’s history to support specific campaigns. O’Reilly’s The State of Data Quality In 2020 survey found that over 60% of enterprises see their AI and machine learning projects fail due to too many data sources and inconsistent data. While the survey was on the organization level, it would not be a stretch to assume the failure rate would be higher within marketing departments, as it’s common to create unique taxonomies, databases, and metatags for each campaign in each region. The larger, more globally based, and more fragmented a marketing department is, the harder it is to achieve data governance. The O’Reilly State of Data Quality Survey found that just 20% of enterprises publish information about data provenance or data lineage, which are essential tools for diagnosing and resolving data quality issues. Creating greater consistency across taxonomies, data structures, data field definitions, and meta-tags would give marketing data scientists a higher probability of succeeding with their ML models at scale. Up to a third of a typical marketing team’s time is spent dealing with data quality issues, which has a direct impact on productivity, according to Forrester’s Why Marketers Can’t Ignore Data Quality study. Inaccurate data makes tactical decisions harder to get right, which could impact revenues. Forrester found that 21 cents of every media dollar have been wasted over the last 12 months (as of 2019) due to poor data quality. Taking the time to improve data quality and consistency in marketing would convert the lost productivity to revenue. Too often, marketers and the IT teams supporting them rely on data scientists to improve inconsistent data. It’s time-consuming, tedious work and can consume up to 80% or more of the data scientist’s time. It is no surprise that data scientists rate cleaning up data as their least-liked activity. Instead of asking data scientists to solve the marketing department’s data quality challenges, it would be far better to have the marketing department focus on creating a single, unified content data model. The department should consolidate diverse data requirement needs into a single, unified model with a taxonomy rigid enough to ensure consistency, yet adaptive enough to meet unique campaign needs. Change management makes the marketer’s job easier and more productive because there is a single, common enterprise taxonomy. Data governance is key to solving this problem, and marketing leaders have to be able to explain how improving metadata consistency and content data models fits within the context of each team member’s role. After that, the marketing organization should focus on standardizing across all taxonomies and the systems supporting them. The bottom line is that inconsistent data quality in marketing impacts the team by jeopardizing new sales cycles and creating confusion in customer relationships. The ability to get AI and ML pilots into production and provide insights valuable enough to change a company’s strategic direction depends on reliable data. Companies will find their marketing campaigns’ future contributions to growth are defined by how the team improves data quality today."
https://venturebeat.com/2021/04/22/microsoft-details-the-latest-developments-in-machine-learning-at-gtc-21/,Microsoft details the latest developments in machine learning at GTC 21,"This article is part of the VB Lab Microsoft / NVIDIA GTC insight series. With the rapid pace of change taking place in AI and machine learning technology, it’s no surprise Microsoft had its usual strong presence at this year’s NVIDIA GTC event. Representatives of the company shared their latest machine learning innovations in multiple sessions, covering inferencing at scale, a new capability to train machine learning models across hybrid environments, and the debut of the new PyTorch Profiler that will help data scientists be more efficient when they’re analyzing and troubleshooting ML performance issues. In all three cases, Microsoft has paired its own technologies, like Azure, with open source tools and NVIDIA’s GPU hardware and technologies to create these powerful new innovations. Much is made of the costs associated with collecting data and training machine learning models. Indeed, the bill for computation can be high, especially with large projects — into the millions of dollars. Inferencing, which is essentially the application of a trained model, is discussed less often in the conversation about the compute costs associated with AI. But as deep learning models become increasingly complex, they involve huge mathematical expressions and many floating point operations, even at inference time. Inferencing is an exciting wing of AI to be in, because it’s the step at which teams like Microsoft Azure are delivering an actual experience to a user. For instance, the Azure team worked with NVIDIA to improve the AI-powered grammar checker in Microsoft Word. The task is not about training a model to offer better grammar checking; it’s about powering the inferencing engine that actually performs the grammar checking. Given Word’s massive user base, that’s a computationally intensive task — one that has comprised billions of inferences. There are two interrelated concerns: one is technical, and the other is financial. To reduce costs, you need more powerful and efficient technology. NVIDIA developed the Triton Inference Server to harness the horsepower of those GPUs and marry it with Azure Machine Learning for inferencing. Together, they help you get your workload tuned and running well. And they support all of the popular frameworks, like PyTorch, TensorFlow, MXNet, and ONNX. ONNX Runtime is a high-performance inference engine that leverages various hardware accelerators to achieve optimal performance on different hardware configurations. Microsoft closely collaborated with NVIDIA on the TensorRT accelerator integration in ONNX Runtime for model acceleration on NVIDIA GPUs. ONNX Runtime is enabled as one backend in Triton Server. Azure Machine Learning is a managed platform-as-a-service platform that does most of the management work for users. This speaks to scale, which is the point at which too many AI projects flounder or even perish. It’s where technological concerns sometimes crash into the financial ones, and Triton and Azure Machine Learning are built to solve that pain point. Creating a hybrid environment can be challenging, and the need to scale resource-intensive ML model training can complicate matters further. Flexibility, agility, and governance are key needs. The Azure Arc infrastructure lets customers with Kubernetes assets apply policies, perform security monitoring, and more, all in a “single pane of glass.” Now, the Azure Machine Learning integration with Kubernetes builds on this infrastructure by extending the Kubernetes API. On top of that, there’s native Kubernetes code concepts like operators and CI/CDs, and an “agent” runs on the cluster and enables customers to do ML training using Azure Machine Learning. Regardless of a user’s mix of clusters, Azure Machine Learning lets users easily switch targets. Frameworks that the Azure Machine Learning Kubernetes native agent supports include SciKit, TensorFlow, PyTorch, and MPI. The native agent smooths organizational gears, too. It removes the need for data scientists to learn Kubernetes, and the IT operators who do know Kubernetes don’t have to learn machine learning. The new PyTorch Profiler, an open source contribution from Microsoft and Facebook, offers GPU performance tuning for popular machine learning framework PyTorch. The debugging tool promises to help data scientists and developers more efficiently analyze and troubleshoot large-scale deep learning model performance to maximize the hardware usage of expensive computational resources. In machine learning, profiling is the task of examining the performance of your models. This is distinct from looking at model accuracy; performance, in this case, is about how efficiently and thoroughly a model is using hardware compute resources. It builds on the existing PyTorch autograd profiler, enhancing it with a high-fidelity GPU profiling engine that allows users to capture and correlate information about PyTorch operations and detailed GPU hardware-level information. PyTorch Profiler requires minimal effort to set up and use. It’s fully integrated, part of the new Profiler profile module, new libkineto library, and PyTorch Tensorboard Profiler plugin. You can also visualize it all Visual Studio Code. It’s meant for beginners and experts alike, across use cases from research to production, and it’s complementary to NVIDIA’s more advanced NSight. One of PyTorch Profiler’s key features is its timeline tracing. Essentially, it shows CPU and GPU activities and lets users zoom in on what’s happening with each. You can see all the operators that are typical PyTorch operators, as well as more high-level Python models and the GPU timeline. One common scenario that users may see in the PyTorch Profiler is instances of low GPU utilization. A tiny gap in the GPU visualization represents, say, 40 milliseconds when the GPU was not busy. Users want to optimize that empty space and give the GPU something to do. PyTorch Profiler enables them to drill down and see what the dependencies were and what events preceded that idle gap. They could trace the issue back to the CPU and see that it was the bottleneck; the GPU was sitting there waiting for data to be read by another part of the system. Examining inefficiencies at such a microscopic level may seem utterly trivial, but if a step is only 150 milliseconds, a 40-millisecond gap in GPU activity is a rather large percentage of the whole step. Now consider that a project may run for hours, or even weeks at a time, and it’s clear why losing such a large chunk of every step is woefully inefficient in terms of getting your money’s worth from the compute cycles you’re paying for. PyTorch Profiler also comes with built-in recommendations to guide model builders for common problems and possible. In the above example, you may simply need to tweak DataLoader’s number of workers to ensure the GPU stays busy at all times. Don’t miss these GTC 2021 sessions. Watch on demand at the links below: VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/22/nhost-is-an-open-source-firebase-rival-backed-by-githubs-founders/,Nhost is an open source Firebase rival backed by GitHub’s founders,"In a world where technical talent is at a premium, businesses have to tap into the broader technology ecosystem to help build and scale their digital products. In truth, most companies probably don’t care much how their software is constructed — as long as it has all the features and functionality needed to satisfy their target market. Against this backdrop, Swedish startup Nhost is setting out to expedite the development process with an open source backend-as-a-service (BaaS) platform that lets developers forget about the infrastructure and focus purely on the customer-facing frontend. With Nhost, companies can automate their entire backend development and cloud infrastructure spanning file storage, databases, user authentication, APIs, and more. “We remove a considerable amount of ongoing effort, time, and resources for tasks that are not directly related to the product our customers want to build,” Nhost CEO and cofounder Johan Eliasson told VentureBeat. “With Nhost, they can start building their customer-facing products after only one minute.” To help fund its next stage of growth, Nhost today announced it has raised $3 million in a round of funding led by Nauta Capital, with participation from some prominent angel investors, including GitHub founders Scott Chacon and Tom Preston-Werner and Netlify founders Christian Bach and Mathias Biilmann Christensen. Existing investor Antler also participated in the round. Even the biggest technology giants with the deepest pockets look externally to boost their technology stack. Open source software, for example, allows them to benefit from the scalability of community-driven projects. And using third-party APIs (application programming interfaces) also saves them having to develop every component of their application internally. Nhost and its backend infrastructure are a different proposition, but the idea is the same — to help companies offload some of their requirements to a third party with domain-specific expertise. The Stockholm-based startup was created in late 2019 with Eliasson as the sole founder, though he soon realized that building what is effectively an open source alternative to Google’s Firebase would be a tall order. After he met software engineer Nuno Pato at a startup accelerator program in early 2020, the duo officially became cofounders. The global backend-as-a-service market was pegged at $1.6 billion in 2020, according to some estimates, a figure that’s projected to rise to nearly $8 billion by 2027. Existing players such as Firebase claim major clients like Alibaba, the New York Times, Duolingo, Venmo, and Trivago, highlighting that it’s not just cash-strapped startups that want to outsource their backend management. One of Nhost’s major selling points is that it’s an open source project, meaning companies can do with it as they please, though Eliasson notes that the main benefits of its open source status are around “collaboration and transparency.” There are, of course, other players in the open source BaaS space, such as Back4App, Parse, Kinvey, and Kuzzle. But Nhost considers itself distinct on a number of grounds, chief among them the scope of its single-platform offering. Nhost offers all the required building blocks for modern software, including a PostgreSQL database, real-time GraphQL API (which is available for most major front-end frameworks, such as React, Flutter, and Vue), authentication, storage, and serverless functions that allow companies to deploy custom code. On top of that, Nhost offers a managed cloud incarnation with plans spanning hobbyists, professionals, and enterprises. “Our tech stack offers a unique combination of open source tools we haven’t seen anywhere else, plus a tremendous focus on the developer experience,” Eliasson said. “We believe that building robust and highly scalable applications should be fun, fast, and easy for everyone.” For now, Eliasson said most Nhost customers are “indie-hackers, startups, and agencies,” and it has around 110 paying customers. However, it has aspirations on the enterprise front, something its seed round should help support. “Our approach is bottom-up — indies, developers, startups, small and medium-sized teams first,” Eliasson explained. “Enterprise will have its own sales channel when the right time comes.” Nhost is in the process of rolling out enterprise-grade features, including support for single sign-on (SSO), audit logs, and ISO certificates, which have “already been requested by larger customers,” according to Eliasson. It’s easy to see why Nhost could prove popular for developer teams looking to spin up a quick prototype or minimal viable product (MVP), given that it removes much of the friction involved in launching even a semi-functional app. However, it’s worth noting that prototypes or MVPs are how most modern software starts out — which puts Nhost in a favorable position when the time comes for developers to ramp things up. “Nhost really shines for MVPs because the stack we chose makes that easy,” Eliasson explained. “That is important for us because there is very low friction for developers to start building, while the platform is scalable, flexible, and performant enough for when their apps get successful and need to scale.”"
https://venturebeat.com/2021/04/22/hubspot-adds-operations-hub-to-drive-revenue-ops-shift/,HubSpot adds Operations Hub to drive revenue ops shift,"HubSpot kicked off an ambitious effort to unify sales, marketing, and service processes with the launch of Operations Hub on Wednesday. The offering is directed at companies that are beginning to unify various operations teams within sales, marketing, and services groups in order to create a single revenue operations (Revenue Ops) organization, HubSpot VP Andy Pitre said. Operations Hub is an extension of the HubSpot customer relationship management (CRM) application through which those cross-functional teams can manage business workflows on an end-to-end basis for each customer, Pitre added. It’s available in Free, Starter, and Professional versions. In addition, the platform bi-directionally synchronizes data across applications such as Microsoft Dynamics and NetSuite. Citizen developers and power users can also extend those processes using low-code tools based on a JavaScript framework that HubSpot makes available. It also supports a Lambda serverless computing framework from Amazon Web Services (AWS) to orchestrate event-driven workflows. As those processes evolve, however, professional developers will have the option of working with their preferred development tool to further customize them, Pitre said. In effect, Operations Hub makes it possible to unify a flywheel of processes that all tie into the same content management system, Pitre added. “We’re opening up the hood of HubSpot,” he said. HubSpot is also working toward adding additional analytics and data management capabilities within each of the marketing, sales, and service hubs it provides. In the longer term, it may open up AI models it has created to end users that might wish to extend them, Pitre noted. Historically, sales, marketing, and customer service have been managed by organizations in isolated silos. But that is changing. Now it has become more common for customer service representatives to upsell customers. Sales representatives are also increasingly crafting their own content marketing campaigns to drive leads. Meanwhile, marketing teams are running ecommerce sites that enable end customers to acquire products and services without ever engaging a sales representative. Revenue operations break down the silos to work with a single pipeline. The HubSpot CRM application provides a system of record that enables all those customers to be engaged in a consistent fashion, Pitre said. Just about every provider of a CRM platform is now trying to drive some level of unification across sales, marketing, and service functions. As a provider of a CRM platform that many organizations already rely on to drive marketing automation, HubSpot sees an opportunity to expand its reach. Many organizations are just coming to terms with Revenue Ops as a concept, and it’s not always clear who is in charge of the area. However, the title chief revenue officer is starting to supplant more mundane titles, such as vice president of sales. Regardless of how or even to what degree organizations embrace Revenue Ops, the whole sales and marketing motion within organizations is being transformed. Much of the impetus for that shift is being driven by the need to either launch or accelerate digital business transformation initiatives in the wake of the economic downturn brought on by the COVID-19 pandemic. It wasn’t always easy to get an appointment with customers prior to the pandemic. These days, it’s often next to impossible. The HubSpot operations effort comes as enterprises continue moving to online channels. Organizations of all sizes now depend on online engagements with customers in place of sales teams that travel to visit a customer. Not only are many of the individuals that sales teams used to regularly visit in person now working from home, such visits are discouraged due to the pandemic."
https://venturebeat.com/2021/04/22/yseop-recognized-as-best-ai-product-enterprise-at-the-technical-analyst-awards-2021/,Yseop Recognized as “Best AI Product: Enterprise” at The Technical Analyst Awards 2021," Yseop has won the Best Enterprise AI Product award, courtesy of The Technical Analyst Awards. Founded in 2008, Yseop is a pioneer in Natural Language Generation (NLG), intelligent report automation and a world-leading AI software company.  PARIS–(BUSINESS WIRE)–April 22, 2021– The Technical Analyst Awards are the only awards devoted to technical analysis research, data and trading software for the institutional market. Now in its thirteenth year, the Awards are highly regarded within the financial markets, attracting participation from hundreds of banks, research houses and software companies across the globe. Award categories are assessed by an independent panel of judges, who identify the winners based on excellence. “We are delighted to receive the recognition from these awards, which is testament to the hard work of our colleagues and best in class intelligent report automation solution – Augmented Financial Analyst,” said Emmanuel Walckenaer, CEO of Yseop. Financial controllers and analysts spend 45% of their time interpreting and analyzing data, 48% building and writing reports, and the remaining 7% communicating and interacting with the business. Yseop has over 50,000 users who use its no-code and AI-based Intelligent Report Automation, Augmented Financial Analyst (AFA) to save time. AFA automatically transforms complex data sets into high-quality narrative reports with actionable insights – all reliably, efficiently and at scale. With just a few clicks, analysts can seamlessly create and automate text-based reporting from all structured financial data such as balance sheets, profit and loss, financial statements, and more. Thanks to Yseop’s solution that removes the risk of human error or any room for interpretation by providing automated written and explained reports on achieved results, companies can gain greater productivity and lower their costs. Augmented Financial Analyst is designed for large-scale enterprise-level deployment, making it easy to generate hundreds of financial reports on demand. With an immediate and high return on investment (ROI), businesses use Yseop’s solution to solve many pain points across different departments. ENDS About Yseop: Founded in 2008 and based in North and South America, and Europe, Yseop specializes in artificial intelligence (AI) and is a recognized pioneer in Natural Language Generation (NLG) technology. Yseop is rapidly expanding globally, providing enterprise-level automation solutions for some of the world’s largest companies in a variety of industries including finance (Credit Agricole, Factset, BNP Paribas), pharmaceuticals (Sanofi) and computer software company (Oracle). Yseop also partners with strategic consulting firms and system integrators including CapGemini, Accenture and LTI, who support the adoption and deployment of Yseop’s NLG solution. With its multi award-winning Language AI technology, Yseop is revolutionizing the way analysis and reporting is done. Yseop’s powerful and user-friendly Augmented Analyst platform allows business users to seamlessly and quickly build and automate the generation of text reports from any structured data. At Yseop, we exist to support companies through this digital transformation. We believe that our cutting-edge artificial intelligence technology allows businesses to increase the efficiency of their operations and enables people to accomplish less tedious tasks and allows them to use that saved time to do more added-value and creative work. Find out more at https://yseop.com Find out more about Augmented Financial Analyst here: https://www.yseop.com/solutions/augmented-financial-analyst  View source version on businesswire.com: https://www.businesswire.com/news/home/20210422005578/en/ Press contact – Lise Grant – +33 6 99 65 71 91 – lgrant@yseop.com"
https://venturebeat.com/2021/04/22/deep-instincts-neural-networks-for-cybersecurity-attract-110m/,Deep Instinct’s neural networks for cybersecurity attract $100M,"The increasingly rich data companies are collecting makes them a more tantalizing target for attacks. But Deep Instinct wants to turn that same data into an enterprise’s greatest defensive asset. Deep Instinct is applying end-to-end deep learning to cybersecurity, an approach that allows it to predict and prevent cyberattacks across a company’s network, according to CEO Guy Caspi. Today, Deep Instinct announced it has raised $100 million in a round led by BlackRock. Other investors include Untitled Investments, The Tudor Group, Anne Wojcicki, Millennium, Unbound, and Coatue Management. The company has now raised a total of $200 million. The New York-based company is part of a growing wave of startups turning to machine learning and artificial intelligence to combat the rising number of cyberattacks. The industry is optimistic that this ability to automate defenses will help companies gain an edge against increasingly sophisticated and well-funded hackers. But Deep Instinct is trying to go a step beyond the way others are using AI and machine learning for security. The company has created deep neural networks that allow it to avoid using feature processing that can add an additional step and slow reaction time. With traditional machine learning, Caspi explained, executable files cannot be processed directly. Instead, they must be converted into a list of features that are then fed into a machine learning model. Deep Instinct’s end-to-end deep learning system uses the raw data as input without needing to convert it. The company trains its model in its own labs, rather than on the customer’s premises, by feeding it hundreds of millions of malicious and legitimate files. This huge-scale training workload relies on Nvidia GPUs. Once the training is finished, Deep Instinct creates a standalone neural network that can be deployed to an organization, where it starts protecting every device connected to the network. Because the system doesn’t require agents, it can be rapidly installed, including covering all applications currently running. And it can recognize previously unknown types of attacks without needing to be constantly updated. As a result, Deep Instinct claims it can identify and stop attacks within 20 milliseconds while reducing false positives by 99%. Caspi said he wants to use the latest funding to accelerate growth with an eye toward an IPO in the next couple of years. For now, that means ramping up sales and marketing, with about 30% of the money being reserved for product development."
https://venturebeat.com/2021/04/22/software-product-planning-platform-productboard-raises-72m/,Software product planning platform Productboard raises $72M,"Productboard, a startup developing a DevOps orchestration system for enterprises, today announced that it raised $72 million in a series C round. The company says that the funds will be put toward expanding its team and customer base while supporting product research and development. An estimated 19% to 23% of software development projects fail, with that statistic holding steady for the past couple of decades, according to data compiled by Ask Wonder. Standish Group found that “challenged” projects — i.e., those that fail to meet scope, time, or budget expectations — account for about 52% of software projects. Often, a lack of user involvement, executive support, and clear requirements are to blame for missed benchmarks. Productboard was founded in 2014 by Hubert Palan and Daniel Hejl, who sought to build a service that could tackle some of the most common DevOps challenges. The platform enables companies to create single product feedback repositories and prioritize what to build next, ostensibly helping to mitigate bottlenecks in the creation process. “Product teams often use an assortment of PowerPoint, email, Post-It notes, Slack, and other generic task management and engineering tools,” Palan told VentureBeat via email. “While a lot of these tools are free, they fall short when it comes to delivering the structure, best practices, and flexibility provided by tools built specifically for product management. There are a few other roadmapping tools out there in the market, but they don’t offer a full-fledged, customer-centric product management platform. Productboard is the first enterprise-ready, customer-centric product management platform that organizes the product development process around customer insights, creates alignment between product, and go-to-market teams.” With Productboard, companies can consolidate support tickets, Slack messages, and sales conversations in a single dashboard.  Moreover, they can categorize product ideas, requests, and feedback to route back to product teams for resurfacing down the road. Insights can be highlighted in each piece of user research or feedback and linked to a related feature idea. Once approved by a manager, developers can indicate the importance of these features, rating them on a 0-3 scale. Productboard also lets teams share product roadmaps to which they can add features and custom filters. Each roadmap can be tailored to different audiences with leadership-, company-, delivery-, and customer-focused views. And roadmaps can be integrated with existing workflows in Trello, GitHub, Jira, and other DevOps platforms. Productboard’s other headlining feature is its product portal, which lets companies show which features are planned, what’s been launched, and user feedback. Product boards hosted by Productboard can surface top-requested features and update customers about features that they requested, or serve as an internal brainstorming board for developers. “Productboard uses its own product … to showcase planned features and receive feedback from customers and prospects, which includes ways the company is incorporating AI and machine learning into the product,” Palan said. “For example, Productboard is currently working on machine learning algorithms to auto-suggest existing features relevant to the customer feedback a user is processing within the platform. The keywords found in the features’ names and descriptions or in other insights and notes already linked to these features can drive these suggestions and the algorithm is constantly trained by product managers using the platform and users submitting structured feedback via the customer portal.” Productboard, which employs over 200 people, has indirect competition in a number of startups vying for a slice of the more than $3.42 billion DevOps tools market. For example, there’s Tasktop, which recently nabbed $100 million to turn DevOps metrics into visualizations at scale. Meanwhile, Harness coordinates DevOps and cloud spending across multiple platforms. But Productboard claims to have over 4,000 customers including Zoom, Microsoft, UiPath, and 1-800 Contacts. “In today’s incredibly fast-paced world, companies are in a race to bring their customers the best products possible. Yet, most product teams still don’t have a system of record that brings together customer insights to inform and drive their product strategy, prioritization, and roadmapping process,” Palan continued. “Productboard is building the first customer-centric product management platform and working to help companies worldwide deliver better digital product experiences.” Tiger Global Management led Productboard’s latest funding round with participation from existing investors Sequoia Capital, Kleiner Perkins, Index Ventures, and Bessemer Venture Partners. It brings the San Francisco-based company’s total raised to date to over $137 million."
https://venturebeat.com/2021/04/22/rapid7-bolsters-open-source-security-with-velociraptor-acquisition/,Rapid7 bolsters open source security with Velociraptor acquisition,"Cybersecurity company Rapid7 yesterday announced it has acquired Velociraptor, an open source platform focused on endpoint monitoring, digital forensics, and incident response. Terms of the deal were not disclosed. Founded in 2000, Rapid7 provides a range of security-focused tools spanning applications and the cloud, including vulnerability management, orchestration and automation, and detection and response. With clients such as Autodesk, First Republic Bank, Kimberly-Clark, Hilton, and Univision and the pandemic driving digital transformation across industries, Rapid7 has been on a tear over the past 12 months. In fact, its share value has nearly doubled. Australian company Velocidex developed Velociraptor as an open source endpoint visibility tool in 2018. It’s designed to help digital forensics and incident response (DFIR) security teams proactively search for malicious activities across all devices and entry points to a network. With this deal, Velociraptor will be better positioned to receive direct and continued investment from a billion-dollar cybersecurity giant. Velocidex founder Mike Cohen added that Velociraptor will also receive greater exposure through conference and community events, which should increase participation in the project globally. “Rapid7 will enable Velociraptor to graduate to the ‘next level’ in terms of scale, development velocity, stability, and capability by drawing on a wide range of capable and experienced people to support the project,” Cohen wrote in a blog post. Boston-based Rapid7 has something of a track record in the open source security sphere, having acquired Metasploit back in 2009. There are benefits to pursuing a community-driven ethos in cybersecurity — essentially, the more eyeballs tethered to a piece of software, the more chances flaws or vulnerabilities will be found promptly. And the threat is urgent. In the past few months alone, at least two prominent security software providers have fallen victim to exploits. Fireye was reportedly hacked in a state-sponsored attack, and just this week cybersecurity company Sonicwall confirmed that some of its customers were targeted using a previously undisclosed vulnerability in its email security product. In truth, all software — open source or otherwise — can become vulnerable if it’s neglected. But open source holds greater potential for robust security, given that it draws on the collective wisdom of a community. This is why companies invest significant resources in supporting and maintaining mission-critical open source software. The Linux Foundation, for example, has set up the The Core Infrastructure Initiative (CII) with support from Amazon, Google, Microsoft, Intel, and others to ensure open source projects are sufficiently supported. And earlier this year, Google announced it would be funding the salaries for two developers to improve Linux’s security. In cybersecurity, specifically, attackers only need to get lucky once when searching for a weakness to exploit, whereas defenders have to cover all entry points to a network at all times. The fact that new vulnerabilities come to light on a daily basis highlights why a community-led (i.e. open source) approach to cybersecurity makes sense. With Velociraptor on board, Rapid7 said it will continue to build and work with the community around it, and — as you might expect — “leverage its technology and insights” to improve Rapid7’s own incident response abilities. According to Cohen, who now joins Rapid7 to continue leading the Velociraptor project, there are no immediate plans to commercialize Velociraptor directly."
https://venturebeat.com/2021/04/22/cluedin-raises-15m-to-grow-its-data-prep-and-analytics-platform/,CluedIn raises $15M to grow its data prep and analytics platform,"Data management startup CluedIn today announced the closure of a $15 million series A funding round led by Dawn Capital, which brings its total raised to over $16 million. The company says that the proceeds will be used to build out its platform, expand its sales and marketing team, and drive partnerships and expansion, particularly in the U.S. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Gartner says that data integration and preparation are among the top three technologies organizations seek to automate by the end of 2022. Copenhagen-based CluedIn, which was founded in 2015 by former Sitecore engineers Tim Ward, Pierre Derval, and Martin Hyldahl, aims to streamline the process of making data ready for insights. The company leverages a graph database that sits between data sources and applications, offering solutions for data integration, governance, and management.  CluedIn integrates with existing systems and delivers a view of what data needs fixing at the global, source, entity, and property level. Data arbitration is built into the platform — CluedIn automatically reconciles records like addresses, company names, and more via reconfigurable rules. CluedIn’s data prep engine removes duplicate entries from databases, leveraging AI and machine learning for metadata management system. According to a Forbes survey, data scientists spend 80% of their time on data preparation, and 76% view it as the least enjoyable part of their work. It’s also expensive. Trifacta pegs the collective data prep cost for organizations at $450 billion. Beyond time and cost savings, CluedIn says that its platform, which can share data with third-party applications, enables enterprises to meet core regulatory and compliance requirements. For example, CluedIn can track data lineage and mask any personally identifiable information that it encounters. It also features templates for retention policies designed to help customers align with business- and government-level compliance rules. Thirty-employee CluedIn has customers in a range of verticals including Nordea, SAP, and Ticketmaster. For Pfizer and Coca-Cola, it’s providing customer insights analytics, according to Ward. “For some sectors, for example retail and consumer industries, the shift to online sales channels during the pandemic added urgency to the need to know the customer and sales patterns through data,” a spokesperson told VentureBeat via email. “In terms of master data management use cases, the top vendors in the space are the likes of Informatica, SAP and IBM, however the disruptive nature and unique approach of CluedIn’s product mean it offers a very different experience for the end user than these providers.” In addition to Dawn Capital, Collibra cofounder Stijn Christiaens and existing investor Nordic Makers participated in CluedIn’s latest funding round."
https://venturebeat.com/2021/04/21/automation-software-maker-uipath-shares-rise-over-23-in-nyse-debut/,Automation software maker UiPath shares rise over 23% in NYSE debut,"(Reuters) — Shares of automation software provider UiPath jumped 23.21% in their New York Stock Exchange debut on Wednesday, underscoring investors’ appetite for high-growth tech stocks. The stock closed its first day on the stock market at $69 a share, up from its $56 IPO price on Tuesday, giving the company a market capitalization of $35.82 billion. “This is just a milestone,” CEO and cofounder Daniel Dines told Reuters in an interview. “Starting from tomorrow, our focus is posting a good quarter, and we’re really marching on our vision of empowering everyone through automation.” Backed by the likes of Accel, Dragoneer, and Coatue Management, UiPath uses artificial intelligence and low-code tools to help large corporations and government agencies automate repetitive and routine tasks in areas such as accounting and human resources. Several richly valued startups, including cryptocurrency exchange operator Coinbase Global and South Korean ecommerce startup Coupang, have already cashed in on the record run in U.S. capital markets this year. Unicorns such as electric-vehicle startup Rivian and Microsoft-backed DataBricks are also set to go public later in 2021. Started in 2005 in Romania by Dines, a former Microsoft executive, UiPath recorded a surge in demand for its services during the COVID-19 pandemic from businesses shifting to remote working and digitalizing workflows. The New York-based company reported $607.6 million in revenue in the year ended January 31, 2021, an 81% jump year over year. “It took them 10 years to go from zero to a couple million in revenue. And then in five years, they went from a couple to 600 million,” said Rich Wong, partner at Accel who first invested in UiPath in 2017. The company partnered with Cleveland Clinic, one of the largest hospitals in the United States, to cut the waiting time at drive-thru COVID-19 testing sites from three minutes to around 15 seconds, Dines said. Morgan Stanley and J.P. Morgan were the lead underwriters for the IPO."
https://venturebeat.com/2021/04/21/adversarial-machine-learning-underrated-threat-data-poisoning/,Adversarial machine learning: The underrated threat of data poisoning,"Most artificial intelligence researchers agree that one of the key concerns of machine learning is adversarial attacks, data manipulation techniques that cause trained models to behave in undesired ways. But dealing with adversarial attacks has become a sort of cat-and-mouse chase, where AI researchers develop new defense techniques and then find ways to circumvent them. Among the hottest areas of research in adversarial attacks is computer vision, AI systems that process visual data. By adding an imperceptible layer of noise to images, attackers can fool machine learning algorithms to misclassify them. A proven defense method against adversarial attacks on computer vision systems is “randomized smoothing,” a series of training techniques that focus on making machine learning systems resilient against imperceptible perturbations. Randomized smoothing has become popular because it is applicable to deep learning models, which are especially efficient in performing computer vision tasks. But randomized smoothing is not perfect. And in a new paper accepted at this year’s Conference on Computer Vision and Pattern Recognition (CVPR), AI researchers at Tulane University, Lawrence Livermore National Laboratory, and IBM Research show that machine learning systems can fail against adversarial examples even if they have been trained with randomized smoothing techniques. Titled “How Robust are Randomized Smoothing based Defenses to Data Poisoning?,” the paper sheds light on previously overlooked aspects of adversarial machine learning. One of the known techniques to compromise machine learning systems is to target the data used to train the models. Called data poisoning, this technique involves an attacker inserting corrupt data in the training dataset to compromise a target machine learning model during training. Some data poisoning techniques aim to trigger a specific behavior in a computer vision system when it faces a specific pattern of pixels at inference time. For instance, in the following image, the machine learning model will tune its parameters to label any image with the purple logo as “dog.” Other data poisoning techniques aim to reduce the accuracy of a machine learning model on one or more output classes. In this case, the attacker would insert carefully crafted adversarial examples into the dataset used to train the model. These manipulated examples are virtually impossible to detect because their modifications are not visible to the human eye. Research shows that computer vision systems trained on these examples would be vulnerable to adversarial attacks on manipulated images of the target class. But the AI community has come up with training methods that can make machine learning models robust against data poisoning. “All previous data poisoning methods assume that the victim will use the standard training procedure of minimizing the empirical error on the training data,” Akshay Mehra, Ph.D. student at Tulane University and lead author of the paper, told TechTalks. “However, the adversarial robustness community has highlighted that minimizing the empirical error is not suitable for model training since models trained with it are vulnerable to adversarial attacks. Several works have been published that try to improve the adversarial robustness of the models. Of these works, training procedures that can produce certifiably robust models are of the most interest due to the adversarial robustness guarantees of the models, trained using these methods.” Random smoothing is a technique that cancels out the effects of data poisoning by establishing an average certified radius (ACR) during the training of a machine learning model. If a trained computer vision model classifies an image correctly, then adversarial perturbations within the certified radius will not affect its accuracy. The larger the ACR, the harder it becomes to stage an adversarial attack against the machine learning model without making the adversarial noise visible to the human eye. Experiments show that deep learning models trained with random smoothing techniques maintain their accuracy even if their training dataset contains poisoned examples. In their research, Mehra and his coauthors assumed that a victim has used random smoothing to make the target robust against adversarial attacks. “In our work, we explored three popular training procedures (Gaussian data augmentation, smooth adversarial training, and MACER) which have been shown to increase certified adversarial robustness of the models as measured by the state-of-the-art certification method based on randomized smoothing,” Mehra says. Their findings show that even when trained with certified adversarial robustness techniques, machine learning models can be compromised through data poisoning. In their paper, the researchers introduce a new data poisoning method called Poisoning Against Certified Defenses (PACD). PACD uses a technique known as bilevel optimization, which achieves two goals: create poisoned data for models that have undergone robustness training, and pass the certification procedure. PACD produces clean adversarial examples, which means the perturbations are not visible to the human eye. “A few previous works have shown the effectiveness of solving the bilevel optimization problem to achieve better poisoning data,” Mehra says. “The difference in the formulation of the attack in this work is that instead of using the poison data to reduce the model accuracy we are targeting certified adversarial robustness guarantees obtained from state-of-the-art certification procedure based on randomized smoothing.” The bilevel optimization process takes a set of clean training examples and gradually adds noise to them until they reach a level that can circumvent the target training technique. The ingenuity behind this data poisoning technique is that researchers were able to create a machine learning algorithm that optimizes the adversarial noise for the specific type of robustness training method used in the target model. The algorithm that creates the adversarial example is called ApproxGrad, and it can be adjusted for different robustness training methods. Once the target model is trained on the tainted dataset, its ACR will be reduced considerably, and it will be highly vulnerable to adversarial attacks. “In our approach, we explicitly generated poison data that when used for training, will lead to models with low certified adversarial robustness,” Mehra says. “To do this we used the training procedures that produce models with high certified adversarial robustness as our lower-level problem. The attacker’s objective (upper-level problem) is to lower the guarantees produced by the certification procedure. By approximately solving this bilevel optimization problem we were able to generate poison data that could significantly hurt the certified adversarial robustness guarantees of the models. The lowered guarantees lead to a loss of trust in the model’s prediction at test-time.” The researchers applied PACD to the MNIST and CIFAR datasets and tested it on neural networks trained with all three popular adversarial robustness techniques. In all cases, PACD data poisoning resulted in a considerable decrease in the average certified radius of the trained model, making it vulnerable to adversarial attacks. The AI researchers also tested to see whether a poisoned dataset targeted at one adversarial training technique would prove to be effective against others. Interestingly, their findings show that PACD transfers across different training techniques. For instance, even if a poisoned dataset has been optimized for gaussian data augmentation, it will still be effective on machine learning models that will go through the MACER and smooth adversarial training processes. “We demonstrate, through transfer learning experiments, that the generated poison data works to reduce the certified adversarial robustness guarantees of models trained with different methods and also models with different architectures,” Mehra says. But while PACD has proven to be effective, it comes with a few caveats. Adversarial attacks that assume full knowledge of the target model, including its architecture and weights, are called “white box attacks.” Adversarial attacks that only need access to the output of a machine learning model are “black box attacks.” PACD stands somewhere in between the two ends of the spectrum. The attacker needs to have some general knowledge of the target machine learning model before formulating the poisoned data. “Our attack is a grey box attack since we are assuming knowledge of victim’s model architecture and training method,” Mehra says. “But we don’t assume knowledge of the particular weights of the network.” Another problem with PACD is the cost of producing the poisoned dataset. ApproxGrad, the algorithm that generates the adversarial examples, becomes computationally expensive when applied to large machine learning models and complicated problems. In their experiments, the AI researchers focused on small convolutional neural networks trained to classify the MNIST and CIFAR-10 datasets, which contain no more than 60,000 training examples. In their paper, the researchers note, “For datasets like ImageNet where the optimization must be performed over a very large number of batches, obtaining the solution to bilevel problems becomes computationally hard. Due to this bottleneck we leave the problem of poisoning ImageNet for future work.” ImageNet contains more than 14 million examples. A machine learning model that can perform well on the ImageNet dataset requires a convolutional neural network with dozens of layers and millions of parameters. Accordingly, creating PACD data would require large resources. “Solving bilevel optimization problems can be computationally expensive, especially when using very large datasets and deep models,” Mehra says. “However, in our paper, we show that attacks generated against moderately deep models transfer well to much deeper models. It would be interesting to see if attacks generated against a portion of the large training data also work well on the entire training data.” Today, machine learning applications have created new and complex attack vectors in the millions of parameters of trained models and the numerical values of image pixels, audio samples, and text documents. Adversarial attacks are presenting new challenges for the cybersecurity community, whose tools and methods are centered on finding and fixing bugs in source code. The PACD technique shows that poisoned data can render proven adversarial defense methods ineffective. Mehra and his coauthors warn that data quality is an underrated factor in assessing adversarial vulnerabilities and developing defenses. For instance, a malicious actor can develop a tainted dataset and deploy it online for others to use in training their machine learning models. Alternatively, the attacker can insert poisoned examples into crowdsourced machine learning datasets. The adversarial perturbations are imperceptible to the human eye, which makes it extremely difficult to detect them. And automated tools that vet software security can’t detect them. PACD has important implications for the machine learning community. Machine learning engineers should be more careful about the datasets they use to train their models and make sure the source is trustworthy. Organizations that curate datasets for machine learning training should be more careful about the provenance of their data. And companies such as Kaggle and GitHub that host datasets and machine learning models should start thinking about ways to verify the quality and security of their datasets. We still don’t have complete tools to detect adversarial perturbations in training datasets. But securing the pipeline for accessing and managing machine learning training datasets can be a good first step in preventing the kind of data poisoning measures Mehra and his coauthors describe in their paper. The Adversarial ML Threat Matrix, introduced last October, provides solid guidelines on finding and fixing possible holes in the training and deployment pipeline of machine learning models. But a lot more needs to be done. Another useful tool is a series of deep learning trust metrics developed by AI researchers at the University of Waterloo, which can find classes and areas where a computer vision system is underperforming and might be vulnerable to adversarial attacks. “Through this work, we want to show that advances in certified adversarial robustness are dependent on the quality of the data used for training the models,” Mehra says. “Current methods for detecting data poisoning attacks may not be sufficient when attacker adds imperceptibly distorted data. We need more sophisticated methods to deal with this and is a direction for our future research.” Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/21/proglove-promotes-worker-well-being-with-human-digital-twin-technology/,ProGlove promotes worker well-being with human digital twin technology,"ProGlove, the company behind an ergonomic barcode scanner, has developed new tools for analyzing human processes to build a human digital twin. “We have always been driven to have our devices narrate the story of what is really happening on the shop floor, so we added process analytics capabilities that allow for time-motion studies, visualization of the shop floor, and more,” ProGlove CEO Andreas Koenig told VentureBeat. The company’s newest process analytics tools can complement the typical top-down perspective of applications by adding a process-as-seen view to the conventional process-as-wanted view. Most importantly, it can also provide insights that improve well-being. Koenig said, “We are building an ecosystem that empowers the human worker to make their businesses stronger.” The market for barcode scanning is still going strong and is often taken for granted, given how old it is. “You have technologies like RFID that have been celebrated for being the next big thing, and yet their impact thus far hasn’t been anywhere near where most pundits expected it,” Koenig said. Companies like Zebra, Honeywell, and Datalogic have lasted for decades by building out an ecosystem of tools to address industry needs. “What sets us apart is that we looked beyond the obvious and started with the human worker in mind,” Koenig said. Not only is the company providing a form factor designed to meet requirements for rugged tools, this shift to analytics could further promote efficiency, quality, and ergonomics on the shop floor. ProGlove’s cofounders participated in Intel’s Make It Wearable Challenge, with the idea of designing a smart glove for industries. Today, ProGlove’s MARK scanner can collect six-axis motion data, including pitch, yaw, roll, and acceleration, along with timestamps, a step count, and camera data (such as barcode reading speed and the scanner ID). Koenig’s vision goes beyond selling a product to establish the right balance between businesses’ need for profits and their obligation to ensure worker well-being. Koenig estimates that human hands deliver 70% of added value in factories and on warehouse floors. “There is no doubt that they are your most valuable resource that needs protection. Even more so since we are way more likely to experience a shortage of human workers in the warehouses across the world than having them replaced by robots, automation, or AI.” ProGlove Insight contextualizes the collected data and lets users compare workstations and measure the workload and effort necessary to complete the tasks. Users can also visualize their shop floor, look at heatmaps, and identify best practices or efficiency blockers. After a recent smart factory lab experiment with users, DPD and Asics realized efficiency gains by as much as 20%, Koenig said. ProGlove’s vision of the human digital twin is built on three pillars: a digital representation of onsite workers, a visualization of the shop floor, and an industrial process engineer. “The human digital twin is all about striking the right balance between businesses’ needs for profitability, efficiency, and worker well-being,” Koenig said. At the same time, it is important that the human digital twin complies with data privacy regulations and provides transparency to frontline workers around what data is being transmitted."
https://venturebeat.com/2021/04/21/koch-industries-embraces-networking-as-a-service/,Koch Industries embraces networking as a service,"Koch Industries has joined a growing list of enterprises where the IT organization is evolving into becoming a general manager of IT that is consumed as a service rather than via platforms built and maintained by an internal IT team. That shift in the case of the privately held conglomerate not only encompasses cloud platforms but now also network services delivered via Alkira, a provider of a wide-area network (WAN) service that Koch Industries is starting to rely on to interconnect a highly distributed computing environment that spans the globe. Rather than deploy, secure, and maintain routers, switches, and firewalls, along with all the hardware and software required to deliver enterprise-class networking services, it’s becoming more economically efficient to simply consume networking services much like any other cloud service, said Koch Industries CTO Matt Hoag. Alkira constructed an Alkira Cloud Services Exchange (CSX) using $76 million it raised from Koch Disruptive Technologies (KDT), the venture capital arm of Koch Industries, along with Sequoia Capital, Kleiner Perkins, GV (formerly Google Ventures), and others. That WAN aggregates multiple points-of-presence that can be provisioned via a console that Alkira customers access as a cloud service. The security capabilities of the Alkira WAN are provided by firewalls and other offerings from Palo Alto Networks that Alkira also manages on behalf of customers. The Alkira approach eliminates both the need to acquire networking infrastructure and the need to hire the specialists required to maintain it, noted Hoag. The decision by Koch Industries to employ the Alkira networking service is part of a larger series of initiatives to reduce the amount of IT that needs to be directly managed by Koch Industries personnel, said Hoag. “We don’t have own or manage all the underlying infrastructure,” he said. “It’s not our business.” Koch Industries has made a strategic decision to focus its internal resources on building applications that make a strategic difference to the business, added Hoag. Everything else going forward will be consumed as a service as much as possible. The plan is to elevate the role of the internal IT team organization to one focused on the management of services rather than, for example, installing networking equipment, noted Hoag. The conglomerate is joining the ranks of an increasing number of enterprise IT organizations that have grown more comfortable consuming IT as a service. A recent report published by Information Services Group (ISG) noted demand for technology and business services continues to rise sharply as the economy recovers from the downturn brought on by the COVID-19 pandemic. In the first quarter of 2021 the annual contract value (ACV) for as-a-service and managed services offerings that exceed $5 million reached a record $17.1 billion, up 11% over last year and 4% from the previous quarter. Overall, the cloud-based as-a-service market rose 15% to a record $9.9 billion in the first quarter. Managed services reached $7.2 billion in the first quarter, up 7% year over year. It’s not clear at what rate IT will be consumed as a service by enterprises that have historically preferred to deploy and maintain their own infrastructure. Consuming IT as a service not only provides more flexibility to scale services up and down as required, but it enables organizations to finance IT as an operational rather than capital expense. The financing option frees up more capital that can be invested in, for example, a manufacturing plant rather than servers, switches, and storage systems. For the time being, the bulk of IT systems still reside in on-premises IT environments that are managed by internal IT teams. However, even on-premises IT environments are being increasingly managed by vendors such as Hewlett-Packard Enterprise (HPE) and Dell Technologies or third-party managed service providers (MSPs). It’s not clear how the rank and file that make up IT teams will ultimately be impacted by this shift. Many of them will wind up working for IT services providers. Others will evolve into managers of services provided by those third-party providers. However, as IT services become more automated, it’s clear the number of IT professionals required to manage IT may not be as large as it is today."
https://venturebeat.com/2021/04/21/eu-proposes-strict-ai-rules-with-fines-up-to-6-for-violations/,"EU proposes strict AI rules, with fines up to 6% for violations","(Reuters) — The European Commission on Wednesday announced tough draft rules on the use of artificial intelligence, including a ban on most surveillance, as part of an attempt to set global standards for a technology seen as crucial to future economic growth. The rules, which envisage hefty fines for violations and set strict safeguards for high-risk applications, could help the EU take the lead in regulating AI, which critics say has harmful social effects and can be exploited by repressive governments. The move comes as China moves ahead in the AI race, while the COVID-19 pandemic has underlined the importance of algorithms and internet-connected gadgets in daily life. “On artificial intelligence, trust is a must, not a nice to have. With these landmark rules, the EU is spearheading the development of new global norms to make sure AI can be trusted,” European tech chief Margrethe Vestager said in a statement. The Commission said AI applications that allow governments to do social scoring or exploit children will be banned. High risk AI applications used in recruitment, critical infrastructure, credit scoring, migration and law enforcement will be subject to strict safeguards. Companies breaching the rules face fines up to 6% of their global turnover or 30 million euros ($36 million), whichever is the higher figure. European industrial chief Thierry Breton said the rules would help the 27-nation European Union reap the benefits of the technology across the board. “This offers immense potential in areas as diverse as health, transport, energy, agriculture, tourism or cyber security,” he said. However, civil and digital rights activists want a blanket ban on biometric mass surveillance tools such as facial recognition systems, due to concerns about risks to privacy and fundamental rights and the possible abuse of AI by authoritarian regimes. The Commission will have to thrash out the details with EU national governments and the European Parliament before the rules can come into force, in a process that can take more than year. ($1 = 0.8333 euros)"
https://venturebeat.com/2021/04/21/gartner-predicts-public-cloud-spending-to-reach-332b-in-2021/,Gartner predicts public cloud spending to reach $332B in 2021,"Worldwide spending on public cloud services is expected to reach $332.3 billion in 2021, an increase of 23.1% from 2020, according to the latest Gartner forecast. Public cloud spending in 2020 was $270 billion. Cloud spending is getting a boost because emerging technologies such as containerization, virtualization, and edge computing are becoming more mainstream, Sid Nag, research vice president at Gartner, said in the forecast. Software-as-a-service (SaaS) remains the largest market segment and is forecast to reach $122.6 billion in 2021, spurred by the demand for composable applications. “The events of last year allowed CIOs to overcome any reluctance of moving mission critical workloads from on-premises to the cloud,” Nag said. There was an expectation that some workloads had to stay on-premises, but the past year showed those concerns were unfounded. But even if there hadn’t been a pandemic, Nag said there was a “loss of appetite” for datacenters. As organizations mobilize for a massive global effort to produce and distribute COVID-19 vaccinations, SaaS-based applications that enable essential tasks such as automation and supply chain are critical. The fact that these applications demonstrate reliability in scaling vaccine management will help CIOs validate the ongoing shift to cloud, Gartner said. Desktop-as-a-service (DaaS) will see the highest growth in 2021, growing 67.7% to reach $2 billion, followed by infrastructure-as-a-service (IaaS) at 38.5% to reach $82 billion. CIOs will boost IaaS and DaaS spending as they continue to face pressure to scale infrastructure that supports complex workloads and meets the demands of a hybrid workforce, Gartner said. Growth in IaaS and DaaS will slow down 30% in 2022, with spending to reach $106.8 billion and $2.7 billion, respectively. Cloud spending will grow across all areas in 2021, Gartner said in its forecast. Business process services (business-product-as-a-service, or BPaaS) is forecast to reach $50.1 billion, and application infrastructure services (platform-as-a-service, or PaaS) will reach $59.4 billion. Cloud management and security services will reach $16 billion.  The public cloud market fared well over the past year as enterprises relied on cloud services to maintain business continuity in light of business disruptions and shift to a remote workforce. However, cloud spending will look different in 2021 and 2022 as enterprises shift away from infrastructure and application migration and toward innovative applications combining cloud with technologies such as AI, internet of things, and 5G. “Cloud will serve as the glue between many other technologies that CIOs want to use more of, allowing them to leapfrog into the next century as they address more complex and emerging use cases,” Nag said. Companies depend on the cloud to adopt emerging technologies at scale, and the resulting applications and workloads encourage more cloud spending. For example, a company building an IoT application could use virtual machines and containers to build at scale, and then buy more cloud services to manage the data that is generated. Gartner is not the only one noting that emerging technologies such as edge computing is growing rapidly. International Data Corporation (IDC) said the worldwide edge computing market is expected to reach $250.6 billion in 2024. The services market should account for 46.2% of all edge spending by 2024, followed by hardware at 32.2% and edge software at 21.6%, IDC said. Earlier this month, Gartner predicted worldwide IT spending will reach $3.8 trillion in 2021, an increase of 4% from 2020. The biggest growth is expected in enterprise software, fueled by enterprises accelerating their digital transformation plans to deliver virtual services such as distance learning, telehealth, and automation. Other areas of spending include datacenter systems, communications, IT services, and devices. PC spending is also up this year."
https://venturebeat.com/2021/04/21/google-rolls-out-new-ai-powered-features-for-meet/,Google rolls out new AI-powered features for Meet,"Google today announced a number of updates to Google Meet, including a refreshed user interface, features powered by AI, and tools that aim to make meetings more engaging. Among the highlights is a data saver mode that limits data usage on mobile networks, and Autozoom, which uses AI to zoom in and position participants in front of their cameras. The pandemic has supercharged video chat usage. Eighty-three percent of businesses with over 250 employees are likely to purchase video calling tools in the near future, according to a survey by Commercial Integrator. Underlining the trend, Meet had over 50 million users as of May 2020, a 900% increase from last March. Meet’s new Data Saver feature, which launches this month, automatically limits data usage on mobile and other bandwidth-constrained networks. It will arrive alongside improvements to low-light mode for Meet, a capability that leverages AI to brighten video in poorly lit environments. Previously only available for smartphones and tablets, low-light mode will soon work on the web, detecting when users are underexposed and enhancing quality to improve their visibility. Meanwhile, Autozoom, which will arrive for paid Google Workspace customers in the coming months, is a feature that taps AI to zoom in and position users squarely in-camera. Autozoom intelligently readjusts as users move, ensuring they remain in the frame, Google says. Beyond Data Saver, low-light mode on the web, and Autozoom, Meet will soon allow users to replace their background with an image of a classroom, party, or forest — with more on the way. In the future, Meet for web will also offer greater control over how users see themselves in meetings. For example, they’ll be able to shrink their video feed to a tile in a grid or a floating picture, with the latter resizable, as well as moveable. And if they prefer not to see themselves at all, users will be able to minimize their feed and hide it from view, optionally disabling their self-feed across calls. Meet will also offer a way to pin and unpin content that will shrink the presentation tile down to the size of the other tiles. And the bottom bar will become easier to navigate — meeting dial-in codes, attachments, the participants’ list, chat, and other activities will move to the bottom right to create more vertical space for participants and content. The “leave call” button will also move away from the camera and microphone buttons to prevent accidental hang-ups, Google says. This week, Google also updated Google Maps with over 100 AI-powered improvements, like upgraded Live View navigation that enables users to get turn-by-turn directions indoors. Other enhancements include a new routing model that optimizes for lower fuel consumption based on factors like road incline and traffic congestion, as well as a layer that shows current and forecasted weather and temperature conditions."
https://venturebeat.com/2021/04/21/workflow-automation-platform-aisera-raises-40m/,Workflow automation platform Aisera raises $40M,"Aisera, a company developing a platform that automates operations and support tasks across IT, sales, and customer service, today announced it has raised $40 million in a series C round led by Icon Ventures. The startup says the funds, which bring its total raised to $90 million, will support product expansion and deployment, as well as go-to-market, marketing, and software development efforts. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. According to market research firm Fact.MR, small and medium-sized enterprises are expected to adopt business workflow automation at scale, creating a market opportunity of more than $1.6 billion between 2017 and 2026. Aisera offers products that auto-complete actions and workflows by integrating with existing enterprise apps, like Salesforce and ServiceNow. The company was founded in 2017 by Muddu Sudhakar, who previously launched e-discovery vendor Kazeon (which was acquired by EMC in 2009), big data startup Cetas (acquired by VMware in 2012), and cybersecurity firm Caspida (acquired by Splunk in 2015). Aisera claims its platform can continuously learn to resolve issues through a combination of conversational AI, robotic process automation, and reinforcement learning. For example, Aisera can predict outages and send notifications to DevOps teams and customers. Moreover, the company claims its platform can detect patterns to predict service disruptions.  Aisera customers can choose from a library of prebuilt workflows and intents built for IT, HR, facilities, sales operations, and customer service applications. The platform offers out-of-the-box reports and dashboards for auditing, including auto-resolution metrics and the ability to discover the most-requested knowledge articles. Aisera has a number of competitors in a global intelligent process automation market that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere and UiPath have secured hundreds of millions of dollars in investments at multibillion-dollar valuations. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. That’s not counting newer startups like WorkFusion, Indico, Tray.io, Tonkean, AirSlate, Workato, Camunda, and Automation Hero. But the funding comes at a time of significant expansion for Aisera. In addition to achieving year-over-year growth of 300% and a base of over 65 million users, the company says it has secured a number of new enterprise customers, including Autodesk, Dartmouth College, McAfee, and Zoom. Aisera’s success is perhaps unsurprising, given the value proposition of automation. Ninety-five percent of IT leaders are prioritizing automation, and 70% of execs are seeing the equivalent of over four hours saved per employee, per week, according to Salesforce’s recent Trends in Workflow Automation report. Moreover, market research firm Fact.MR says the adoption of business workflow automation at scale could create a market opportunity of over $1.6 billion between 2017 and 2026. Palo Alto, California-based Aisera’s latest funding round saw participation from new investor World Innovation Lab, as well as existing backers True Ventures, Menlo Ventures, Norwest Venture Partners, Khosla Ventures, First Round Capital, Webb Investment Network, and Sherpalo."
https://venturebeat.com/2021/04/21/security-operations-platform-cyrebro-raises-15m-to-expand-its-customer-base/,Cyrebro raises $15M to expand its security operations platform,"Cyrebro, a cloud-based security operations center (SOC), today announced it has raised $15 million in a series B round led by Prytek. CEO Nadav Arbel says the capital, which brings the company’s total raised to $22 million, will be used to support development of the Cyrebro platform and strengthen the startup’s reach in the small and medium-sized business (SMB) market. Email messages and files have become top attack vectors for hackers looking to steal data, particularly as the pandemic necessitates novel work-from-home arrangements. Cybersecurity Ventures anticipates that the damage related to cybercrime will hit $6 trillion annually this year. Corresponding with this rise, Gartner reports that worldwide spending on cybersecurity is expected to reach $133.7 billion in 2022. Cyrebro aims to address this with a platform that integrates cybersecurity tools in a single dashboard. Cyrebro shows critical incidents across business operations and security solutions, allowing response teams to conduct investigations by type, severity, and status. With Cyrebro, security analysts can drill down into events and see which assets were impacted, as well as recommended actions and real-time status. Beyond this, Cyrebro delivers insights about where most alerts are generated and where attention should be focused for preemptive steps. “Global businesses of all sizes are facing the new threats of cyberattacks brought on by the pandemic, and companies now need simplified solutions to see, understand, and respond to their cybersecurity needs. With this understanding in mind, we created Cyrebro, a real-time, live security operation platform to enable online operations of the entire security stack,” Arbel told VentureBeat via email. “It is the only platform to address the full scope of cybersecurity needs in the most effective, powerful, and cost-efficient way. More than ever, organizations need holistic detection and response mechanisms while covering the complete suite of security solutions, including monitoring, threat hunting, response, and compliance.” Cyrebro collects and processes data from clouds, networks, and endpoints, including laptops, desktops, and servers. A feature shows the geographic location of hosts with their coverage, connectivity state, and related alerts, and reporting functions enable users to generate audits with visualizations like pie charts. Competition is fierce in a cybersecurity market that’s anticipated to reach $199.98 billion in value by 2025, according to Market Research Future. Securiti.ai, a developer building a platform designed to automate cybersecurity and compliance processes, recently emerged from stealth with $31 million. There’s also Swimland and Tines, a cybersecurity startup that helps enterprise security teams automate repetitive workflows. But Arbel, which has 80 employees, says it has “hundreds” of paying customers, including casinos, global retailers, banks, insurance companies, and other Fortune 500 companies. “Over the past year, our customer base has grown by 100% and our employees by 20%. Since the start of COVID, SMBs experienced new challenges within their IT and cybersecurity departments. We realized the critical need to develop an accessible platform that simplifies proactive cyber operations for the average small to mid-sized business owner, allowing them to gain the posture of Fortune 500 enterprises,” he added. “Our business has grown exponentially, and we took on this round of funding to meet this new market demand for comprehensive online security solutions.” InCapital, Mizrahi Bank, and previous investor Mangrove also participated in Tel Aviv, Israel-based Cyrebro’s latest round of funding."
https://venturebeat.com/2021/04/21/pipedrives-smart-docs-automates-sales-workflows-and-helps-close-deals/,Pipedrive’s Smart Docs automates sales workflows and helps close deals,"Pipedrive, a customer relationship management (CRM) platform for salespeople, has launched new smart documentation features designed to automate the process of creating, sending, and tracking sales documents. The rollout comes as much of the world has retreated to remote work, which means teams across industries need new tools to carry out their day-to-day tasks. Founded in 2010, Pipedrive helps teams plan all their sales activities, including recording their customer conversation history and tracking their deals from inception. It also integrates with more than 200 third-party apps, such as Zoom, Microsoft Teams, Trello, and MailChimp. The company, which counts Salesforce and Hubspot as competitors, has amassed a number of notable clients, including the mighty Amazon, leading to Pipedrive’s acquisition by Vista Equity Partners a few months back. Pipedrive quietly launched Smart Docs earlier this month as a rebrand of a beta product called Sales Docs it launched last year. Smart Docs enables teams to send trackable quotes, contracts, and proposals to clients and prospects so they can know when a customer opens a document and follow up at a suitable time. More than that, sales teams can use Smart Docs to create templates that automatically pull in Pipedrive field data to reduce the amount of manual work required to generate new documents from scratch. For example, rather than creating largely identical sales quotes every day, companies can use the Smart Docs template editor to construct a standardized document by adding the relevant Pipedrive fields (such as company name, job titles, and addresses) from a panel on the right. Then all a salesperson has to do is choose the template to auto-fill the document with data from Pipedrive — it’s all about being able to send quotes faster, reducing the need to copy/paste from other documents. Smart Docs is actually split into two components — Smart Docs Basics, which is available to subscribers on the Advanced pricing plan, and Smart Docs Pro, which is available to Professional and Enterprise users. With Smart Docs Basics, users can create document templates and quote tables that automatically pull in product data related to the field, receive notifications when a third party views their document, and automatically share new document links with recipients if they update that document. On the Pro plan, users can remove the Pipedrive logo from the document before it’s shared with a third party. The plan also offers support for e-signatures, which is particularly useful in a remote-working world. This plan includes a feature that automatically requests signatures from all relevant stakeholders, who receive an emailed request for their signature — as well as a transparent audit trail that shows the sender who has signed and who hasn’t. Smart Docs Pro also ships with granular permission settings so that only specified individuals at a company can access specific templates. While Smart Docs Pro is currently restricted to Pipedrive’s Professional and Enterprise subscribers, it will also be made available for Advanced plan users as an add-on later this year."
https://venturebeat.com/2021/04/21/google-launches-ai-powered-document-processing-services-in-general-availability/,Google launches AI-powered document processing services in general availability,"Google today announced that several of its cloud-based, AI-powered document processing products have become generally available after launching in preview last year. DocAI platform, Lending DocAI, and Procurement DocAI, which have been piloted by thousands of businesses to date, are now open to all customers and include new features and resources. Companies spend an average of $20 to file and store a single document, by some estimates, and only 18% of companies consider themselves paperless. An IDC report revealed that document-related challenges account for a 21.3% productivity loss, and U.S. companies waste a collective $8 billion annually managing paperwork. With the launch in general availability, Lending DocAI, which processes loan applicants’ asset documents, now offers a set of specialized AI models for paystubs and bank statements. The service also now benefits from DocAI platform’s Human-in-the-Loop AI capability, which provides a workflow to manage human data review tasks. As Google explains, Human-in-the-Loop AI enables human reviewers to verify data captured by Lending DocAI, Procurement DocAI, and other offerings in DocAI platform. The system shows a percentage score of how “sure” it is that the AI ingested the document correctly, and it’s customizable, with the flexibility to set different thresholds and assign groups of reviewers to stages of a workflow. Developers can choose reviewers to assign to tasks either from within their own company or from partner organizations. Lending institutions like banks and brokers process hundreds of pages of paperwork for every loan. It’s a heavily manual process that adds thousands of dollars to the cost of issuing a loan. While hardly flawless, automated processing can give customers a degree of confidence they can afford the property they’re interested in, and some lenders are able to complete the ordeal within minutes, as opposed to the weeks it once took. Procurement DocAI, which performs document processing for invoices, receipts, and more, has gained an AI parser for electric, water, and other utility bills. The latest release taps Google’s Knowledge Graph to validate information, a system that understands over 500 facts about 5 billion entities from the web, as well as from open and licensed databases. Google claims that Knowledge Graph can help increase document parsing accuracy by identifying, for example, that “Angelina” correlates to “Angelina Paris,” a bakery identified using geodata. Google also today announced a partnership with mortgage servicing firm Mr. Cooper, following a collaboration with home loan company Roostify last October. Google says Mr. Cooper will offer its customers greater automation and workflow tools by connecting them with the DocAI platform. “Over the last few years, we have made substantial investments in our proprietary servicing technology and core mortgage platform that have revolutionized the customer experience while providing dramatic efficiencies in operating cost. By joining forces with Google … we are able to build on those advances and help make these technologies available for the mortgage industry to deploy through Google Cloud,” Mr. Cooper CEO Jay Bray said in a press release. The general release of the DocAI platform comes after Google launched PPP Lending AI, an effort to help lenders expedite the processing of applications for the since-exhausted U.S. Small Business Administration’s (SBA) Paycheck Protection Program. As Google explained at the time in a whitepaper, AI can automate the handling of volumes of loan applications by identifying patterns that would take a human worker longer to spot."
https://venturebeat.com/2021/04/21/saas-app-management-and-security-startup-appomni-nabs-new-funding/,SaaS app management and security startup AppOmni nabs $40M,"AppOmni, a provider of software-as-a-service (SaaS) security management, today announced the closing of a $40 million round led by Scale Venture Partners. The company says it will use the funding to grow its footprint and bolster product R&D as it looks to acquire new customers. Estimates show that 92% of web apps have security flaws that can be exploited, and some 16.2% of U.S. companies have two or more web apps that don’t securely store personally identifiable information. The problem is often less negligence than ignorance, as companies lack the expertise required to lock down their services and platforms. A recent survey by ESG and the Information Systems Security Association found that 70% of organizations believe they’re impacted by a cybersecurity skills shortage. AppOmni’s SaaS platform allows customers to scan, triage, and monitor apps using firewalls and access controls. With AppOmni, teams can define security rules and use continuous monitoring for alerts of data exposures. They’re also able to apply role-based policies across clouds, business units, environments, and apps. “While some companies have taken steps to secure their SaaS environments, they commonly use a cloud access security broker (CASBs) or a penetration test. Unfortunately, neither of these tools is sufficient to fully secure today’s SaaS platforms,” AppOmni cofounder and CEO Brendan O’Connor told VentureBeat via email. “CASBs focus on network traffic between employee devices and the cloud. Pentests can be helpful but only measure a single moment in time and often are limited in scope. Alternatively, AppOmni scans cloud APIs and implements data access policies within the SaaS application, governs third-party access integrations, and continuously monitors SaaS applications and activities.” IT teams using AppOmni get a configuration management dashboard that lets them snapshot security policies, access profiles, and review changes before they’re promoted to production. Beyond this, teams can export access reviews and reports or classify data according to type and business requirements. AppOmni isn’t alone in the app management services market, which is anticipated to be worth $32.5 billion by 2024. Among others, it competes with DoControl and Sqreen, which raised $14 million in April 2019 for its cybersecurity suite that helps developers monitor and protect web apps from vulnerabilities and attacks. And Productiv, which develops software that helps enterprises manage SaaS apps, secured $45 million in March. The sector’s growth is partly due to the pandemic, which accelerated the adoption of SaaS applications and remote work. According to Gartner, enterprises are investing in SaaS at a record rate, with 95% of new enterprise app purchases cloud-based. And cloud spending rose 37% to $29 billion during the first quarter of 2020, a May 2020 report from Synergy Research Group revealed. AppOmni, which employs over 50 people, claims to have an advantage over rivals in its momentum. The company has several Fortune 500 organizations as clients, including Dropbox and Accenture, and backing from Salesforce Ventures and ServiceNow Ventures via its recent series B round. And in 2020, AppOmni’s annual recurring revenue increased by over 900%. “The COVID pandemic accelerated the move to cloud and SaaS … Unfortunately, we anticipate that companies’ rush to install and expand SaaS platforms over the past year — without making the accompanying investment in SaaS security — has created a significant security gap,” O’Connor said. “What makes AppOmni unique, aside from its technology, is that it was founded by security practitioners who have real-world experience securing SaaS applications at scale.” San Francisco-based AppOmni has raised over $53 million to date, after closing a $10 million series A in January 2020 and a $3 million seed round in April 2019. Existing backers, including ClearSky, Costanoa Ventures, Inner Loop Capital, and Silicon Valley Data Capital, also participated in AppOmni’s series B announced today."
https://venturebeat.com/2021/04/21/rapidapi-capitalizes-on-booming-api-economy-with-60m-in-funding/,RapidAPI capitalizes on booming API economy with $60M in funding,"RapidAPI, a platform that helps developers find, manage, and test application programming interfaces (APIs), has raised $60 million in a series C round of funding. The raise comes amid a boom in the API economy, as organizations strive to improve their efficiency (and profitability) by moving away from tightly woven, monolithic applications to microservices-based applications. By splitting apps into smaller, function-based components, companies are afforded more agility, and they gain domain-specific expertise from third parties and circumvent the need to develop everything from scratch themselves. Founded out of San Francisco in 2014, RapidAPI helps developers discover and connect to thousands of public APIs, covering everything from file storage and currency conversion to flight search and COVID-19 statistics. The company also offers Enterprise Hub, which is essentially a white label version of RapidAPI that businesses use to build private marketplaces for internal and external APIs. RapidAPI has amassed a number of notable enterprise clients, including SAP, Cisco, Tata, and Hyatt. RapidAPI had raised around $63 million after closing a $50 million series B last year from lead investors that included Microsoft’s M12 and Andreessen Horowitz. With today’s $60 million cash injection, spearheaded by Green Bay Ventures, the company plans to invest in several key areas “based on direct feedback from our enterprise customers,” according to RapidAPI founder and CEO Iddo Gino. This will include the continued platformization of the RapidAPI Enterprise Hub to include additional tooling spanning the entire API development lifecycle. This is evidenced by the company’s recent acquisition of Paw, a platform that helps developers design and test web APIs. “Over the course of this year, we will continue to build out the platform to add other key capabilities, including things like API Security, and increase the integration between our API Marketplace and Enterprise Hub, testing, and design products,” Gino told VentureBeat. RapidAPI intends to continue opening up to new API types beyond SOAP and REST, having recently extended support to Kafka and GraphQL APIs. “We will continue to add support for other API types, such as asynchronous APIs and webhooks,” Gino added. The company is also setting out to solve the problem of “multicloud, multi-gateway integration” for its customers. “As customers continue to deploy more API gateways across a variety of hybrid cloud environments, we will continue to invest in becoming the aggregation layer across multiple gateways and clouds, providing customers with the ability to integrate their workflows across different platforms and cloud environments,” Gino explained. Other investors in the company’s series C round include existing backers M12, Andreessen Horowitz, DNS Capital, Viola Growth, and Grove Ventures, in addition to new investor Stripes. There has been a flurry of activity across the API realm of late. MessageBird, SendBird, Postman, and Kong have all pulled in large sums of cash at multi billion-dollar valuations, not to mention investments for smaller API-focused companies such as Nylas and Skyflow. Twilio, meanwhile, acquired Segment for $3.2 billion, and Idera acquired Apilayer, a cloud-based API provider for big-name companies like Amazon, Apple, and Facebook. The API onslaught is nothing new — over the past five years, Google shelled out $625 million for Apigee and Salesforce snapped up Mulesoft for a cool $6.5 billion. The API economy is unquestionably thriving. But with the proliferation of APIs, many methods companies previously used to manage them all have become outdated, according to Gino. “One of our customers is a major financial institution that, prior to RapidAPI, used a spreadsheet to manage their APIs,” he said. “This spreadsheet was updated by distributing it manually to various executives throughout the company once a year. They had very lagging visibility into their APIs.” Internal tools such as Google Docs and Excel spreadsheets offer little in the way of transparency or collaboration in terms of being able to share code and documentation. “They lack key features such as deep search, tagging, and so on — in fact, in some cases, teams could be developing the same API in different organizations or subscribe to the same APIs without ever realizing it,” Gino said. “This leads to inefficiencies and lowered productivity.” The pandemic has also played a part in the continued rise of the API. Many businesses that relied on more traditional offline mechanisms to drive revenues were forced to embrace digital alternatives. And even major companies with long-established digital components have seen those efforts boosted massively by COVID-19 — for example, Starbucks’ mobile orders increased from 17% to 24% of its total transactions in 2020.  RapidAPI itself has benefited greatly from this surge in demand, adding more than 750,000 new developers to its platform over the past 12 months and doubling the number of available APIs to 35,000. Digitalization has “gone from being a nice-to-have to an imperative to survival,” according to Gino. “To react this quickly and accelerate development, companies have to rely on APIs in their software development. It really puts the onus on companies to get APIs right — and fast.” Put another way, the fact that every company is now a software company is one reason APIs are in such hot demand."
https://venturebeat.com/2021/04/21/perception-point-raises-28-million-in-series-b-funding-to-advance-messaging-and-collaboration-protection-for-enterprise-users/,Perception Point Raises $28 Million in Series B Funding to Advance Messaging and Collaboration Protection for Enterprise Users," Company triples recurring revenue amid increased customer demand for advanced cyber threat protection  TEL-AVIV, Israel–(BUSINESS WIRE)–April 21, 2021– Perception Point, a leading email and collaboration security company, offering fast interception of content-based attacks as a service, announced today it has raised $28 million in Series B, bringing the total funding to $48 million. The new funding round was led by Red Dot Capital Partners and joined by global investor NGP Capital along with existing investors Pitango Venture Capital and State of Mind Ventures (SOMV). Yoram Oron and Atad Peled from Red Dot Capital Partners, and Bo Ilsoe from NGP Capital will be joining the company’s board of directors. Funds from this round will be used to fuel rapid growth, expand to new markets, accelerate product innovation and grow the team to support customer demand. In 2020, Perception Point recorded its most successful year since its inception. The company tripled its recurring revenue and expanded its customer portfolio to include users from multiple industries, such as telecom, tech, retail, food and beverage, healthcare, financial services, and more. The company also doubled its number of Fortune 500 customers and developed a strong network of partners, including global resellers and MSSPs. The need for enterprises to be agile, data savvy, and responsive to any change has only intensified since the COVID-19 pandemic outbreak. As a result, companies now deploy on average 6 different communication and collaboration solutions to support their business operations, including email, cloud storage platforms, messaging apps, CRM, and more to help them communicate with internal and external stakeholders. These growing numbers of channels are fertile ground for cybercriminals to launch content-based attacks. Perception Point has formulated a comprehensive three-pillar approach for protecting businesses from content-based attacks, which provides the most effective detection, supports admins and end-users with full Incident Response services, and covers all communication channels. In addition, the solution is offered through easily implementable API technology, enabling clients to get immediate protection. This combination enables Perception Point to not only intercept more incidents prior to compromising end-users, but also ensures that all events are fully and rapidly contained and remediated. It is a cybersecurity service that works for its users, and not the other way around. “Perception Point is perfectly positioned to capture the rapidly growing yet untapped messaging and collaboration market. We are pleased to join forces with the company and the other investors to help the company expand to new verticals and other parts of the world,” said Yoram Oron, chairman and managing partner at Red Dot Capital Partners. “The beauty of Perception Point is that it addresses challenges that many companies encounter today, offering a 360-degree, SaaS solution that enhances enterprise security and allows users to become more agile and responsive.” “We are strong believers in the Israeli cybersecurity ecosystem and its vision,” commented Bo Ilsoe, Partner at NGP Capital. “Perception Point is a great example of a startup solving complex problems with innovative ‘hacks’ in the development of its core IP, leveraging rapid API-based integration and the cloud to provide instant value to customers while continuing to gain strong commercial and technological momentum. We are glad to be part of the company’s journey.” “Our prevention-as-a-service approach solves the customers’ most advanced messaging and collaboration security challenges, showing value immediately,” said Yoram Salinger, CEO of Perception Point. “We combine a 7-layer platform that easily and quickly integrates with any application along with a comprehensive Incident Response service to ensure flawless prevention, monitoring, and remediation of any attack.” About Perception Point: Perception Point is a Prevention-as-a-Service company, offering fast interception of any content-based attack across all collaboration channels, including email, cloud storage, CRM apps, and messaging platforms. The company prevents phishing, BEC, spam, malware, Zero-days, and N-days well before it reaches enterprise users. Deployed in minutes with no change to the enterprise’s infrastructure, the Perception Point solution confirms with any policy and requires zero fuss from IT teams. The company’s Incident Response team serves as a force multiplier to the enterprise’s SOC team. To learn more about Perception Point, visit our website, or follow us on LinkedIn, Facebook, and Twitter.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210421005370/en/ Georgiana ComsaSilicon Valley Public Relationsgeorgiana@siliconvalleypr.com (650) 800-7084"
https://venturebeat.com/2021/04/21/salesforce-introduces-new-sales-cloud-features-to-boost-automation-and-remote-collaboration/,Salesforce introduces new Service Cloud features to boost automation and remote collaboration,"Salesforce today rolled out the next generation of Service Cloud, including enhancements to Cloud Voice and Einstein Bots. The company says that the features and products are intended to address the new reality brought about by the coronavirus pandemic. Over the past year, service agents moved quickly to work from home but were forced to rely on legacy technology that wasn’t designed to manage distributed workforces. Today, parts of the world are beginning to reopen, but these reopenings are raising questions around updated policies, protocols, and safety measures. This adds a new level of challenge for agents, who are already contending with increased workloads. Service Cloud Voice, Salesforce’s service that brings together phone, digital channels, and customer relationship management data, can now connect with existing phone systems via a new product called Service Cloud Voice for Partner Telephony. With Service Cloud Voice for Partner Telephony, enterprises with landlines can benefit from features including real-time call transcription and AI-powered guidance on recommended next steps. Beyond Service Cloud Voice for Partner Telephony, Salesforce is introducing Service Cloud Workforce Engagement, a workforce planning product that uses AI to predict how many requests will come into a contact center and on which channels (for example, phone, email, web chat, text, and social). First announced in December, Service Cloud Workforce Engagement provides agents with a workspace that integrates data, as well as real-time coaching and on-demand training with Salesforce’s MyTrailhead online learning portal. Salesforce also today previewed Pre-built Einstein Bots, a collection of chatbots powered by Einstein Bots, the company’s platform for conversational bots that resolve issues like processing a return or checking a flight. And the company furthered detailed Visual Remote Assistant, a service launched last year that helps companies deliver service without coming on site by walking customers through solutions remotely. Visual Remote Assistant, an offshoot of Salesforce Field Service, provides tools like annotations, a live pointer, and more. Leveraging AI-powered character recognition and scalable video, the service launches sessions in a browser, integrating customer service data from Service Cloud, Field Service, and third-party systems. Service Cloud Workforce Engagement and Service Cloud Voice for Partner Telephony are expected to be generally available in June 2021. Pricing information will be made available at general availability. Visual Remote Assistant is generally available today, while Pre-Built Einstein Bots are currently in beta and are expected to be generally available in October 2021. According to Kate Leggett, VP and principal analyst at Forrester, the new Service Cloud features align with trends accelerated by the pandemic. For example, according to Salesforce, 61% of salespeople believe their roles have changed since the pandemic began. Even when salespeople are able to return to the road and in-person workplaces, 51% expect to travel less than they did before the pandemic — and fewer than half expect to go back to an office. “Customer service leaders must stay abreast of three megatrends in 2021 as they weather the storm,” Leggett wrote in a recently published report. “They are: AI-fueled digital experiences underpin great customer service, modern agent desktops empower agents to best serve customers, and customer service technology enables resilience and sustainability.”"
https://venturebeat.com/2021/04/21/hives-cloud-hosted-machine-learning-models-draw-85m/,Hive’s cloud-hosted machine learning models draw $85M,"While cloud computing continues to gain favor, only a limited number of companies have embraced machine learning based in the cloud. Hive wants to change this by allowing enterprises to access hosted machine learning models via APIs. Hive has had particular success in the area of content moderation, thanks to its deep learning models that help companies interpret unstructured data, like images, videos, and audio. But it’s also expanding into areas like advertising and sponsorship measurement as it seeks to find other areas that would benefit from intelligent automation. In an interview with VentureBeat, Hive CEO Kevin Guo said the company kept relatively quiet as it sought to prove its models work. But its growth has started to accelerate, and the company is getting ready to make more noise. “Now that we have enough tracking points and we have over 100 customers, we are quite confident what we have in the market does actually work,” Guo said. “Now we’re ready to scale up.” Investors are also excited by what they see. Today, Hive announced it has raised $85 million in funding over two rounds that put its total raised at $121 million and bring its valuation to $2 billion. Glynn Capital led a $50 million round, which followed a $35 million round the company had not previously disclosed. Guo said that when the company was founded in 2014, he and cofounder Dmitriy Karpman were at Stanford studying computer vision. They originally began building consumer apps that used AI to improve things like content recommendations. But along the way, they encountered issues around content moderation and couldn’t find models that solved them. As they started building a solution, other companies heard about it and asked if they could try it. By the end of 2017, the company had become enterprise-focused and the current incarnation of Hive was born. Even then, Guo said the founders took a slow and steady approach. They continued to deploy the service to partners who are now using it to monitor every piece of content shared by users. If Hive spots a problem, something like a video stream can be instantly shut down. Guo said Hive has fewer false positives than some alternatives on the market, which lowers the risk of blocking a non-violating piece of content. “If your models are inaccurate and you’re banning 30% of your users’ content incorrectly, that’s a real problem,” Guo said. Guo said the key to Hive’s accuracy is the massive amount of data fed into the models as they have been developed. To do that, Hive has built a distributed workforce of data labelers. “They have fed in now billions of human judgments,” Guo said. “And that’s what makes this model work so well. At this point, our clients basically view that our models are pretty much at or even above human accuracy, which is why they can trust [them] 100% and use [them] in real-time in production.” Hive claims its models have been trained on 1 billion pieces of human-labeled data, the largest such public dataset. That allows Hive to screen for 40 classes of content categories, such as sexual content, violence, and hate speech. This work has put the company in the middle of the debate over supervised versus semi-supervised and unsupervised learning. Guo said the right answer really depends on the nature of the company’s service. But for Hive, the human element is essential. “There’s nothing quite like humans, truthfully,” he said. “Humans are really good at generating labeling data, finding patterns, and solving hard problems. And so that’s ground truth data for models. We’ve always believed that human training is necessary. Our approach has been generally to stick with [the] tried and true path of using humans to train our models, first and foremost.” The ability to offer the service as an API means clients just have to drop a few lines of code into their service to be up and running, Guo said. That ease of use has helped adoption. According to Guo, the company has seen 300% revenue growth over the past year. Customers include NBCUniversal, Interpublic Group, Reddit, Walmart, Visa, Anheuser-Busch InBev, Comscore, and Cognizant. Hive intends to use the funding to continue development of the company’s cloud-based deep learning models. It also plans to invest in building out its sales and marketing teams. “Up until now, we’ve really been operating with a bit more of a conservative mindset,” Guo said. “We didn’t want to over-invest in sales and marketing until we knew our product [worked]. It took a while. It takes time to prove these models out.”"
https://venturebeat.com/2021/04/21/blockchain-startup-digital-asset-nabs-120m-to-knock-down-data-silos/,Blockchain startup Digital Asset nabs $120M to knock down data silos,"Blockchain-based app development platform Digital Asset today announced that it raised $120 million in series D funding led by 7Ridge and Eldridge. The company plans to use the funding to expand its team and product portfolio with a protocol that enables data to interface across blockchains and databases. Digital Asset was cofounded 2014 by Sunil Hirani, Don Wilson, Yuval Rooz, Shaul Kfir, and Eric Saraniecki, and its flagship product is a distributed ledger technology for banks and other financial, health care, and insurance institutions. The company asserts that the blockchain can transform disparate data silos into synchronized networks, minimizing latency and errors by ensuring that data remains consistent. “I’ve always been passionate about technology and innovation. While I was working at DRW, I had the great opportunity to get out from behind the trading desk and work side by side with Don,” Rooz told VentureBeat via email. “He challenged me to find interesting companies in which to invest on behalf of DRW. At that time, Bitcoin and cryptocurrency were gaining traction and we were able to make some rewarding investments in that space. On the technology side, we developed an investment thesis and spent nearly two years trying to find companies that fit the bill so to speak, but no such company existed. In October 2014, with the support of Don, I left DRW to start Digital Asset.” Digital Asset developed an open source smart contracting language called DAML that’s designed to run on various ledgers. The company hosts a marketplace featuring premade DAML scripts from Accenture, Wipro, and other companies, addressing uses cases like repackaging mortgage-backed securities and reducing credit risk in repo markets. DAML can also run on top of non-blockchain solutions like Amazon Aurora, which works with MySQL, Postgres, and other conventional database technologies. Digital Asset also offers DABL, a hosting platform for developers to deploy their DAML scripts into production. DABL helps to abstract away the deployment, management, and scaling of distributed apps, functioning as a live environment for apps that involve flows between people, companies, and markets. Digital Asset’s highest-profile project is perhaps the distributed ledger system scheduled to be adopted by the Australian Securities Exchange (ASX) in 2023. The system, which was built using DAML and the ISO 20022 protocol, a messaging standard adopted by the Reserve Bank of Australia, is expected to offer better performance, resilience, and security as well as new functions compared with ASX’s existing CHESS system. “We continue to build our customer base,” CFO and COO Emnet Rios said. “We are working with 5 of the 10 global stock exchanges, including Hong Kong Exchanges and Singapore Exchange, as well as companies across financial services, like Broadridge and BNP Paribas, health care, like Change Healthcare, and a variety of customers across multiple industries, including insurance, that have not yet been announced.” Buoyed by recent events, blockchain adoption is on the rise across industries. Thirty-nine percent of respondents to a 2020 Deloitte survey said they’d already put blockchain into production, compared to 23% in 2019. The production figure was even higher, at 46%, for organizations with more than $1 billion in revenues. Gartner forecasts that blockchain will generate an annual business value of more than $3 trillion by 2030. “Our vision is for a world of countless systems, each powered by infrastructure that suits their own unique requirements,” Rios said. “We call this vision the global economic network. To date, there is nothing of its kind that exists today. With the additional funding we can take Daml to the next level, creating a global ecosystem of interconnected, heterogeneous technologies tied together with a common protocol. We are trailblazing a path for the future of how disparate systems will interoperate regardless of the underlying technology.” Digital Asset’s latest round of funding brings the New York-based company’s total raised to over $300 million."
https://venturebeat.com/2021/04/21/marketing-automation-startup-activecampaign-raises-240m/,Marketing automation startup ActiveCampaign raises $240M,"Customer experience automation startup ActiveCampaign today announced it has raised $240 million at an over $3 billion valuation. The capital comes as the startup surpasses $165 million in annual recurring revenue, up from $90 million a year ago. CEO Jason VandeBoom created the company in 2003 to help businesses orchestrate email marketing, customer relationship management, and ad campaign processes. ActiveCampaign’s automations are powered in part by AI and machine learning algorithms and customized with flows that inform hundreds of unique experiences generated dynamically for each customer. The flows live within a visual automation map that reveals which are connected to each other, active, or in need of adjustment. ActiveCampaign’s platform enables brands to broadcast emails or configure triggers that send messages based on purchase intent, site visits, and engagement. Segmenting tools let managers group audiences and orchestrate email autoresponders, funnels, and scheduled emails. Meanwhile, an integrated drag-and-drop email designer enables features like revision histories, geotracking, analytics, image hosting, and social sharing. ActiveCampaign supports split testing — up to five emails with different subject lines, content, images, and calls to action can be tested at once — and records metrics like conversion rate, opens, and more to bubble the best-performing options to the top. A split actions feature allows customers with a certain number of products to make offers until they sell out, for example, before sending other contacts down a different automation path. And extensive event tracking saves logins, plays, clicks, video views, orders, and more across apps, membership sites, and online portals. Beyond email, ActiveCampaign features a text message marketing platform that supports scheduling, notifying, and automating, as well as social media audience targeting. Companies can target people with Facebook Ads and draw on data to automatically retarget based on visits to websites, product interests, form submissions, custom fields, and other collected information. A brand can set up a welcome series and other automations and view them from within a single dashboard using ActiveCampaign. Or they can tap migration services that move emails, forms, and contacts from other platforms to ActiveCampaign’s own. They’re also able to implement site tracking that follows up on customer interactions with triggered messages and tracks goals and conversions to evaluate the effectiveness of ongoing campaigns. And they can employ lead scoring to pick out top engaged contacts and offer what they want. “[We] rely on machine learning to help our customers better understand their own customers,” a spokesperson told VentureBeat via email. “One example is sentiment analysis, which looks at the emails of a customer or prospect and analyzes the sentiment of that communication to trigger automations. This means we can easily automate a happy customer receiving a thank you note, while a frustrated customer can receive a support call — all without manual intervention.” ActiveCampaign says it achieved record usage in 2020, including 4 billion weekly automated experiences, 150 million monthly automated campaigns, and 2 million daily predictions. The company also launched functionality like pages and web personalization, plus a predictive feature that helps to navigate its over 500 automation recipes. The platform’s ecosystem grew to over 850 technology partners, up from 200 as of January 2020. And ActiveCampaign announced plans to grow to over 1,000 employees by the end of 2021, after adding 300 in 2020. The global campaign management software market was valued at approximately $1.85 billion in 2017 and is anticipated to grow at 15.65% from 2018 to 2025. Competitors abound. There’s Pyze, which raised $4.6 million in July 2019 for its AI-driven analytics and marketing campaign orchestration tool, and Clari, which recently nabbed $60 million to further develop its AI sales pipeline toolset. That’s not to mention TapClicks, which raised $10 million in August 2019; Boomtrain; Albert Technologies; 6Sense; Appier; Highspot; and Panoramic. But VandeBoom notes that ActiveCampaign’s client list is swiftly growing, with over 145,000 customers in more than 170 countries — 50,000 of which were added in the last year. “The need for enhanced customer experiences is fueling growth at ActiveCampaign and the growth of businesses around the globe. Companies are becoming increasingly frustrated with legacy marketing automation tools and customer relationship management platforms that only solve for one part of the customer experience,” he told VentureBeat via email. “Our customer experience automation solution helps companies grow by automating 1:1 communication during the entire customer lifecycle, from awareness to acquisition to advocacy, and creates experiences that turn customers from first-time purchasers into long-term brand advocates.” Tiger Global led the series C round announced today, which included new investor Dragoneer. Existing investors Susquehanna Growth Equity and Silversmith Capital Partners also participated. The round brings the Chicago, Illinois-based company’s total raised to date to $360 million."
https://venturebeat.com/2021/04/21/hackers-exploit-sonicwall-email-security-vulnerability/,Hackers exploit SonicWall email security vulnerability,"(Reuters) — Hackers have targeted customers of California-based network services firm SonicWall via a previously undisclosed vulnerability in its email security product, the company and cybersecurity firm FireEye said Tuesday. In a statement, SonicWall said that the vulnerability had been “exploited in the wild,” meaning hackers had already used the flaw to break into target systems. SonicWall urged customers to “immediately upgrade” to a version that patched the hole. The intrusions are the latest in a string of hacks using third-party-provided software and hardware in the United States. The most notable — the compromise of SolarWinds by alleged Russian hackers last year — has raised concerns about the ability of end users to vet the security of their devices and their programs. Last month, it was disclosed that an unknown number of Microsoft customers had been compromised after an allegedly Chinese hacking group made use of serious vulnerabilities in the company’s email server software. Just last week, a breach with potentially serious knock-on consequences was reported at San Francisco-based software auditing firm Codecov. Earlier on Tuesday, hackers were outed for exploiting a serious vulnerability in VPN devices made by Utah-based IT firm Ivanti. In SonicWall’s case, hackers could have used the weakness to easily gain “a pretty significant foothold” in their targets’ networks, said Charles Carmakal, a senior VP of Mandiant, an arm of FireEye. He said his firm didn’t have a clear idea of who the hackers were and said that he was aware of “fewer than five” victims. SonicWall did not immediately respond to a Reuters’ call for comment."
https://venturebeat.com/2021/04/20/only-6-of-organizations-have-adopted-ai-powered-solutions-study-finds/,"Only 6% of companies have adopted AI, study finds","In a new survey of over 700 C-suite executives and IT decision-makers examining AI adoption in the enterprise, Juniper Networks found that 95% of respondents believe their organization would benefit from embedding AI into their daily operations. However, only 6% of those respondents reported adoption of AI-powered solutions across their business. The findings agree with other surveys showing that, despite enthusiasm around AI, companies struggle to deploy AI-powered services in production. Enterprise use of AI grew a whopping 270% over the past several years, Gartner recently reported, while Deloitte says 62% of respondents to its corporate October 2018 study adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of businesses that have seen half their AI projects fail will tell you. According to the Juniper survey, top challenges around AI remain ingesting, processing and managing data. In other tech stack trends, 39% of respondents said that they were likely to collect telemetry data to improve “user experience” AI embedded in products, while 34% noted that AI tool capabilities are the most critical in order to enable AI adoption. Juniper’s is the second recent report to peg data issues as the reason organizations fail to successfully deploy AI. Data scientists spend the bulk of their time cleaning and organizing data, according to a 2016 survey by CrowdFlower. And respondents to Alation’s latest quarterly State of Data Culture Report said that inherent biases in the data being used in their AI systems produce discriminatory results that create compliance risks for their organizations. A majority (73%) of Juniper survey respondents said that their organizations were struggling with expanding their workforce to integrate with AI systems. C-level executives reported that they feel it’s more of a priority to hire people than to develop AI capabilities within their business. Laments over the AI talent shortage have become a familiar refrain from private industry. O’Reilly’s 2021 AI Adoption in the Enterprise paper found that a lack of skilled people and difficulty hiring topped the list of challenges in AI, with 19% of respondents citing it as a “significant” barrier. In 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers globally working on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” Tencent says that there are about 300,000 AI professionals worldwide but “millions” of roles available. And a 2019 Gartner survey found that 54% of chief information officers view this skills gap as the biggest challenge facing their organization. On the topic of AI governance, 87% of executives told Juniper that they believe organizations have a responsibility to implement policies that minimize the negative impacts of AI. Despite this, executives ranked establishing AI governance, policies, and procedures as one of their lowest priorities. And only 7% of survey takers said their organizations have established a company-wide leader who oversees AI strategy and governance. Growing evidence suggests that organizations are implementing AI less responsible than they internally assume. According to a recent Boston Consulting Group survey of 1,000 enterprises, less than half that achieved AI at scale had fully mature, responsible AI implementations, according to the same report. The lagging adoption of responsible AI belies the value that these practices can bring to bear. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and in turn, punish those that don’t. The study suggests that there’s both reputational risk and a direct impact on the bottom line for companies that don’t approach the issue thoughtfully."
https://venturebeat.com/2021/04/20/why-a-cedars-sinai-hospital-and-bp-use-facial-recognition-exclusive/,Why a Cedars-Sinai hospital and BP use facial recognition,"(Reuters) — Deployments of facial recognition from Israeli startup AnyVision show how the surveillance software has gained adoption across the United States even as regulatory and ethical debates about it rage on. The technology finds certain faces in photos or videos, with banks representing one sector that has taken interest in systems from AnyVision or its many competitors to improve security and service. Organizations in other industries are chasing similar goals. Los Angeles hospital Cedars-Sinai and oil giant BP are among several previously unreported users of AnyVision. Cedars-Sinai’s main hospital uses AnyVision facial recognition to give staff a heads-up about individuals known for committing violence or drug fraud or using different names at the emergency room, three sources said. Cedars said it “does not publicly discuss our security programs” and could not confirm the information. Meanwhile, BP has used facial recognition for at least two years at its Houston campus to help security staff detect people on a watchlist because they had previously trespassed or issued threats, two sources said. BP declined to comment. AnyVision declined to discuss specific clients or deals. Gaining additional clients may be difficult for AnyVision amid mounting opposition to facial recognition from civil liberties advocates. Critics say the technology compromises privacy, targets marginalized groups, and normalizes intrusive surveillance. Last week, 25 social justice groups, including Demand Progress and Greenpeace USA, called on governments to ban corporate use of facial recognition in an open letter. AnyVision CEO Avi Golan, a former SoftBank Vision Fund operating partner who joined the startup in November, sees a bright future. He told Reuters that AnyVision has worked with companies across retail, banking, gaming, sports, and energy on uses that should not be banned because they stop crime and boost safety. “I am a bold advocate for regulation of facial recognition. There’s a potential for abuse of this technology both in terms of bias and privacy,” he said. “[But] blanket bans are irresponsible.” The startup has faced challenges in the past year. AnyVision laid off half of its staff, with deep cuts to research and sales, according to people who have worked for the company, as well as customers and partners, all speaking on the condition of anonymity. The slashing followed the onset of COVID-19 shrinking clients’ budgets, sources said, with investor Microsoft in March 2020 saying it would divest its stake over ethical concerns. AnyVision announced raising an additional $43 million last September. Macy’s installed AnyVision in 2019 to alert security when known shoplifters entered its store in New York’s Herald Square, five sources said. The deployment expanded to around 15 more New York stores, three sources said, and if not for the pandemic would have reached an additional 15 stores, including on the West Coast. Macy’s told Reuters it uses facial recognition “in a small subset of stores with high incidences of organized retail theft and repeat offenders.” Menards, a U.S. home improvement chain, has used AnyVision facial recognition to identify known thieves, three sources said. Its system has also alerted staff to the arrival of design center clients and reidentified them on future visits to improve service, a source said. Menards said its current face mask policy has rendered “any use of facial recognition technology pointless.” In an online video, and without naming Menards, AnyVision has touted its results, and two sources said the companies struck a deal for 290 stores. In 2019, Menards apprehended 54% more potential threats and recovered over $5 million, according to the video. The U.S. financial services unit of automaker Mercedes-Benz said it has used AnyVision at its Fort Worth, Texas offices since 2019 to authenticate about 900 people entering and exiting daily before the pandemic, adding a layer of security on top of building access cards. Such employee-access applications are a common early use of AnyVision, including at Houston Texans’ and Golden State Warriors’ facilities, sources said. The sports teams declined to comment. Several deals have failed to materialize, however. Among organizations that considered AnyVision early last year were Amazon’s grocery chain Whole Foods to monitor workers at stores, Comcast to enable ticketless experiences at Universal theme parks, and baseball’s Dodger Stadium for suite access, sources said. Talks with airports in the Dallas and San Francisco areas referenced in public records have not led to contracts either. Universal Parks, the Los Angeles Dodgers, and the airports all declined to comment on their interest. And Whole Foods did not respond to a request for comment. Government requirements for surveillance at casinos have made the gaming industry a big purchaser of facial recognition. Las Vegas Sands, for instance, is using AnyVision, three sources said. Sands declined to comment. MGM Resorts International and Cherokee Nation Entertainment also use AnyVision, representatives of the casino operators said last month in an online presentation seen by Reuters. Ted Whiting of MGM said the software, deployed in 2017 and used at 11 properties, including the Aria in Las Vegas, has detected vendors not wearing masks and helped catch patrons accused of violence. MGM said its “surveillance system is designed to adhere to regulatory requirements and support ongoing efforts to keep guests and employees safe.” Cherokee’s Joshua Anderson said in addition to security uses, AnyVision has accelerated coronavirus contact tracing as the Oklahoma company rolls out the technology across 10 properties."
https://venturebeat.com/2021/04/20/shine-phoenix-merger-focused-on-advancing-fusion-technology/,"SHINE, Phoenix merger focused on advancing fusion technology","JANESVILLE & FITCHBURG, Wis.–(BUSINESS WIRE)–April 20, 2021– SHINE Medical Technologies LLC and Phoenix LLC today announced that the companies have completed a merger under which Phoenix has become a wholly owned subsidiary of SHINE. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420006038/en/ SHINE is a next-generation nuclear technology company focused on unlocking the power of fusion technologies to benefit the planet and humankind. The company’s goal is to deliver on the long-term promise of clean fusion energy by advancing fusion technology starting with the commercialization of medical isotopes. Phoenix designs and manufactures the world’s strongest steady-state fusion neutron generators used for advanced industrial imaging and other applications for improving safety and quality in the aerospace, defense, medical and energy sectors. The combined company represents the first two phases of the long-term vision of Greg Piefer, the founder of both companies, for producing clean energy from fusion (see “SHINE’s Four-Phase Progression to Clean Energy Production” below). The goal of each phase of SHINE’s approach is to build additional capacity and capability, and deepen scientific understanding of fusion technology as it progresses to clean fusion energy production. Each step through the four phases is expected to provide further proof of the technology’s robustness, a foundation for ongoing innovation in the next phase and the creation of value for the company, its customers, and shareholders. “SHINE and Phoenix have shared a common long-term vision and operated in close collaboration during the past 11 years, but it’s always been inefficient to operate as separate companies,” said Greg Piefer, CEO of SHINE. “Coming together will enable us to advance fusion technology more quickly by aligning interests and combining complementary core competencies. Through the four phases, we are taking a deliberate approach to building a company that can ultimately deliver cost-effective, clean fusion energy to billions, while serving important near-term market needs like advanced industrial imaging and medical isotopes, along the way.” For a video of additional comments from Greg Piefer, please click here (:46 broadcast-quality available for the media). Phoenix has developed a strong track record of commercialization and revenue generation by applying its fusion-based technology to applications such as advanced industrial imaging, which can image modern materials in great detail, addressing quality assurance and safety needs in the aerospace, defense, energy, and other industries. These applications are part of Phase 1 of the four-phase approach. The second phase of the approach involves applications of nuclear fusion to replace nuclear reactors used in the production of life-saving medical isotopes for diagnostic imaging, like molybdenum-99 (Mo-99), and with potential use as cancer therapeutics like lutetium-177 (Lu-177). This month, SHINE kicked off Phase 2 commercialization when it began producing Lu-177. In 2022, SHINE expects to commence production of up to 20 million doses of Mo-99 per year in its fusion-powered production facility in Janesville, Wis. The facility is expected to be the world’s largest-capacity medical isotope production plant. “This merger is a natural evolution of our strong existing partnership with SHINE, rooted in our common origin and shared mission,” said Evan Sengbusch, general manager of SHINE’s Phoenix division. “Phoenix’s track record of successfully deploying our core neutron generation technology across multiple demanding market sectors has provided important commercial validation and risk reduction for critical technologies that underpin execution in Phase 2. We are excited to join with SHINE and leverage our complementary nuclear capabilities to advance towards clean fusion energy production.” For a video of additional comments from Evan Sengbusch, please click here (1:24 broadcast-quality available for the media). Phoenix was founded in 2005 by Piefer to develop and commercialize a unique technology that generated neutrons through fusion. He spun SHINE out of Phoenix in 2010 to apply that technology to medical isotope production and other applications through the four-phase approach. Evercore Group L.L.C. served as exclusive financial advisor to SHINE. Foley & Lardner served as lead legal counsel to SHINE. SVB Leerink served as exclusive financial advisor to Phoenix. Godfrey & Kahn S.C. served as lead legal counsel to Phoenix. SHINE’s Four-Phase Progression to Clean Energy Production About SHINE Medical Technologies SHINE is a nuclear technology company committed to improving the lives of people and the planet. The company is focusing its fusion-based technology initially on advanced industrial imaging and the production of diagnostic and therapeutic isotopes. These isotopes include molybdenum-99, a diagnostic isotope used to diagnose heart disease, cancer, and other conditions, and lutetium-177, a therapeutic isotope that holds the promise of significantly improving the outcome of some cancer patients. SHINE has a long-term strategy to solve some of humanity’s biggest problems, including nuclear waste recycling and the production of clean fusion energy, in addition to advanced industrial imaging and medical isotopes, by pursuing our vision for progressively broad and impactful uses of fusion technology. For more information about SHINE, please visit our website at www.shinemed.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420006038/en/ MALLORY PROUTYCORPORATE COMMUNICATIONS PROJECT MANAGER, MBA P: 608-530-5606 | M: 630-945-2379mallory.prouty@shinemed.com ROD HISEDIRECTOR OF STRATEGIC COMMUNICATIONS P 608-530-5659 | M 608-770-7850rod.hise@shinemed.com"
https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/,Cerebras launches new AI supercomputing processor with 2.6 trillion transistors,"Cerebras Systems has unveiled its new Wafer Scale Engine 2 processor with a record-setting 2.6 trillion transistors and 850,000 AI-optimized cores. It’s built for supercomputing tasks, and it’s the second time since 2019 that Los Altos, California-based Cerebras has unveiled a chip that is basically an entire wafer. Chipmakers normally slice a wafer from a 12-inch-diameter ingot of silicon to process in a chip factory. Once processed, the wafer is sliced into hundreds of separate chips that can be used in electronic hardware. But Cerebras, started by SeaMicro founder Andrew Feldman, takes that wafer and makes a single, massive chip out of it. Each piece of the chip, dubbed a core, is interconnected in a sophisticated way to other cores. The interconnections are designed to keep all the cores functioning at high speeds so the transistors can work together as one. In 2019, Cerebras could fit 400,000 cores and 1.2 billion transistors on a wafer chip, the CS-1. It was built with a 16-nanometer manufacturing process. But the new chip is built with a high-end 7-nanometer process, meaning the width between circuits is seven billionths of a meter. With such miniaturization, Cerebras can cram a lot more transistors in the same 12-inch wafer, Feldman said. It cuts that circular wafer into a square that is eight inches by eight inches, and ships the device in that form. “We have 123 times more cores and 1,000 times more memory on chip and 12,000 times more memory bandwidth and 45,000 times more fabric bandwidth,” Feldman said in an interview with VentureBeat. “We were aggressive on scaling geometry, and we made a set of microarchitecture improvements.” Now Cerebras’ WSE-2 chip has more than twice as many cores and transistors. By comparison the largest graphics processing unit (GPU) has only 54 billion transistors — 2.55 trillion fewer transistors than the WSE-2. The WSE-2 also has 123 times more cores and 1,000 times more high performance on-chip high memory than GPU competitors. Many of the Cerebras cores are redundant in case one part fails. “This is a great achievement, especially when considering that the world’s third largest chip is 2.55 trillion transistors smaller than the WSE-2,” said Linley Gwennap, principal analyst at The Linley Group, in a statement. Feldman half-joked that this should prove that Cerebras is not a one-trick pony. “What this avoids is all the complexity of trying to tie together lots of little things,” Feldman said. “When you have to build a cluster of GPUs, you have to spread your model across multiple nodes. You have to deal with device memory sizes and memory bandwidth constraints and communication and synchronization overheads.” The WSE-2 will power the Cerebras CS-2, the industry’s fastest AI computer, designed and optimized for 7 nanometers and beyond. Manufactured by contract manufacturer TSMC, the WSE-2 more than doubles all performance characteristics on the chip — the transistor count, core count, memory, memory bandwidth, and fabric bandwidth — over the first generation WSE. The result is that on every performance metric, the WSE-2 is orders of magnitude larger and more performant than any competing GPU on the market, Feldman said. TSMC put the first WSE-1 chip in a museum of innovation for chip technology in Taiwan. “Cerebras does deliver the cores promised,” Patrick Moorhead, an analyst at Moor Insights & Strategy. “What the company is delivering is more along the lines of multiple clusters on a chip. It does appear to give Nvidia a run for its money but doesn’t run raw CUDA. That has become somewhat of a de facto standard. Nvidia solutions are more flexible as well as they can fit into nearly any server chassis.” With every component optimized for AI work, the CS-2 delivers more compute performance at less space and less power than any other system, Feldman said. Depending on workload, from AI to high-performance computing, CS-2 delivers hundreds or thousands of times more performance than legacy alternatives, and it does so at a fraction of the power draw and space. A single CS-2 replaces clusters of hundreds or thousands of graphics processing units (GPUs) that consume dozens of racks, use hundreds of kilowatts of power, and take months to configure and program. At only 26 inches tall, the CS-2 fits in one-third of a standard datacenter rack. “Obviously, there are companies and entities interested in Cerebras’ wafer-scale solution for large data sets,” said Jim McGregor, principal analyst at Tirias Research, in an email. “But, there are many more opportunities at the enterprise level for the millions of other AI applications and still opportunities beyond what Cerebras could handle, which is why Nvidia has the SuprPod and Selene supercomputers.” He added, “You also have to remember that Nvidia is targeting everything from AI robotics with Jenson to supercomputers. Cerebras is more of a niche platform. It will take some opportunities but will not match the breadth of what Nvidia is targeting. Besides, Nvidia is selling everything they can build.” And the company has proven itself by shipping the first generation to customers. Over the past year, customers have deployed the Cerebras WSE and CS-1, including Argonne National Laboratory; Lawrence Livermore National Laboratory; Pittsburgh Supercomputing Center (PSC) for its Neocortex AI supercomputer; EPCC, the supercomputing center at the University of Edinburgh; pharmaceutical leader GlaxoSmithKline; Tokyo Electron Devices; and more. Customers praising the chip include those at GlaxoSmithKline and the Argonne National Laboratory. Kim Branson, senior vice president at GlaxoSmithKline, said in a statement that the company has increased the complexity of the encoder models it generates while decreasing training time by 80 times. At Argonne, the chip is being used for cancer research and has reduced the experiment turnaround time on cancer models by more than 300 times. “For drug discovery, we have other wins that we’ll be announcing over the next year in heavy manufacturing and pharma and biotech and military,” Feldman said. The new chips will ship in the third quarter. Feldman said the company now has more than 300 engineers, with offices in Silicon Valley, Toronto, San Diego, and Tokyo."
https://venturebeat.com/2021/04/20/study-finds-that-detoxified-language-models-might-marginalize-minority-voices/,"‘Detoxified’ language models might marginalize minorities, says study","AI language models like GPT-3 have an aptitude for generating humanlike text. A key factor is the large datasets, scraped from the web, on which they’re trained. But because the datasets are often too large to filter with precision, they contain expletives, slurs, and other offensive and threatening speech. Language models unavoidably learn to generate toxic text when trained on this data. To address this, research has pivoted toward “detoxifying” language models without affecting the quality of text that they generate. Existing strategies employ techniques like fine-tuning language models on nontoxic data and using “toxicity classifiers.” But while these are effective, a new study from researchers at the University of California, Berkeley, and the University of Washington finds issue with some of the most common detoxification approaches. According to the coauthors, language model detoxification strategies risk marginalizing minority voices. Natural language models are the building blocks of apps including machine translators, text summarizers, chatbots, and writing assistants. But there’s growing evidence showing that these models risk reinforcing undesirable stereotypes, mostly because a portion of the training data is commonly sourced from communities with gender, race, and religious prejudices. Detoxification has been proposed as a solution to this problem, but the coauthors of this latest research — as well as research from the Allen Institute — found that the technique can amplify rather than mitigate biases. In their study, the UC Berkeley and University of Washington researchers evaluated “detoxified” language models on text with “minority identity mentions” including words like “gay” and “Muslim,” as well as surface markers of African-American English (AAE). AAE, also known as Black English in American linguistics, refers to the speech distinctive to many Black people in the U.S. and Canada. The researchers — who used GPT-2, the predecessor to GPT-3, as a test model — showed that three different kinds of detoxification methods caused a disproportionate increase in language model perplexity on text with African-American English and minority identity mentions. In machine learning, perplexity is a measurement of the quality of a model’s outputs — lower is generally better. Using a curated version of English Jigsaw Civil Comments for training, a dataset from Alphabet-owned anti-cyberbullying firm Jigsaw, the researchers found that perplexity increased by a factor of 2.1 on nontoxic “white-aligned English” data and a factor of 4.3 on minority identity mention data. Increasing the strength of the detoxification worsened the bias. Why might this happen? The coauthors speculate that toxicity datasets like English Jigsaw Civil Comments contain spurious correlations between the presence of AAE and minority identity mentions and “toxic” labels — the labels from which the language models learn. These correlations cause detoxification techniques to steer models away from AAE and minority identity mentions because the models wrongly learn to consider these aspects of language to be toxic. As the researchers note, the study’s results suggest that detoxified language models deployed into production might struggle to understand aspects of minority languages and dialects. This could force people using the models to switch to white-aligned English to ensure that the models work better for them, which could discourage minority speakers from engaging with the models to begin with. Moreover, because detoxified models tend to avoid certain topics mentioning minority identity terms, like religions including Islam, they could lead to ostracization and a lack of informed, conscious discussion on topics of identity. For example, tailoring an language model for white-aligned English could stigmatize AAE as incorrect or “bad” English. In the absence of ways to train accurate models in the presence of biased data, the researchers propose improving toxicity datasets as a potential way forward. “Language models must be both safe and equitable to be responsibly deployed in practice. Unfortunately, state-of-the-art debiasing methods are still far from perfect,” they wrote in the paper. “We plan to explore new methods for debiasing both datasets and models in future work.” The increasing attention on language biases comes as some within the AI community call for greater consideration of the role of social hierarchies like racism. In a paper published last June, Microsoft researchers advocated for a closer examination and exploration of the relationships between language, power, and prejudice in their work. The paper also concluded that the research field generally lacks clear descriptions of bias and fails to explain how, why, and to whom that bias is harmful."
https://venturebeat.com/2021/04/20/is-nvidias-arm-deal-really-a-u-k-national-security-threat/,Is Nvidia’s Arm deal really a U.K. national security threat?,"A British government official added a new wrinkle to United Kingdom’s investigation of Nvidia’s proposed $40 billion Arm acquisition by directing the agency to probe the national security implications of the buyout. The U.K.’s Competition and Markets Authority, which scrutinizes mergers and acquisitions on anti-competition and monopoly grounds, launched an investigation into the deal’s impact on competition back in January. On Monday, the U.K.’s Secretary of State for Digital, Culture, Media and Sport directed the CMA to consider the national security component. The directive is ostensibly because the deal would transfer ownership of Arm, a crown jewel of the U.K.’s high-tech portfolio, to a foreign-owned (in this case, American) entity. The Secretary asked the CMA, whose role is similar to that of the U.S. Federal Trade Commission, to report its findings by July 30. Arm ecosystem partners I’ve spoken with hope the latest development is a sign regulators are waking up to the notion that the buyout is bad for just about everybody but Nvidia. It’s an understandable sentiment. The fortunes of hundreds of licensees, developers, and others are so inexorably enmeshed with the Arm platform that they have no choice but to accept the deal and stick with the relationship post-acquisition. Many of them view the acquisition — a transfer of ownership from a neutral third party that benefits when everyone is successful to an aggressive competitor that stands to gain at their expense — as an existential threat. But they are unwilling to speak on the record for fear of retribution from Nvidia, should regulators approve the deal over their objections. The FTC is reportedly taking a second, more intense look at the deal as well. Which it should. Arm is a foundational building block for myriad electronics markets, including smartphones, wearables, automobiles, industrial robotics, IoT, and — increasingly — the datacenter, as I said in today’s Feibus Tech report, Nvidia and Arm: The Perils of Technology Platform Acquisitions. If Nvidia owned Arm, it could, for example, focus resources on the market it cares most deeply about — specifically, the datacenter, where its high-profit GPUs are a leading source of processing power for artificial intelligence applications. It could forcibly link its own GPUs to Arm cores — whether applications needed them or not — and rationalize much higher licensing fees as a result. To allow any company with a vested interest to take the reins of an organization that has to date functioned as a neutral standards-setting body is blatantly anti-competitive. The company underscored its interest in the datacenter last week at its annual GTC developer conference, when it announced a new Arm-based datacenter processor that will feature its own proprietary high-speed link to its GPUs. Project Grace, which is based on a new Arm datacenter platform unveiled last September, won’t hit the market before 2023. Grace is widely seen as a shot across the bow at Intel, which dominates the datacenter CPU market. But it is far more than that. Indeed, given that Nvidia hopes to be the eventual owner of Arm, the news of Project Grace should raise questions for regulators. For example, why does Nvidia feel it needs to buy Arm, when it can develop a new CPU as a licensee for a fraction of the cost? Nvidia also raised eyebrows at GTC 21 when it revealed that Project Grace would feature NVLink, the company’s proprietary high-speed data connection between the CPU and its market-leading GPUs for AI in the datacenter. This means none of the growing number of AI alternatives would work with the new CPU. As an Arm licensee, Nvidia’s design decision makes absolute sense. But coming from the hopeful owner of the platform, the move smacks of anticompetitive behavior. Once it owned Arm, would Nvidia carry the design choice over to that side of the house, thereby locking AI alternatives out of Arm-based datacenter systems? Would it weave NVLink into an Arm license, raising the price as a result? Might the company also cut competitive graphics designs out of the smartphone market? All legitimate questions, and with potentially unsettling answers. The U.S. government should follow Britain’s lead on this investigation. When you get right down to it, allowing a company to buy what is effectively a standards-setting body that so many companies entrust with their livelihoods is a national security issue. We usually evoke national security concerns when a foreign company acquires an American company. But the concerns are just as valid when it’s the reverse, when the target is a foreign-based asset being acquired by a U.S. company. Mike Feibus is president and principal analyst of FeibusTech, a Scottsdale, Arizona-based technology market research and consulting firm. Reach him at mikef@feibustech.com. Follow him on Twitter @MikeFeibus."
https://venturebeat.com/2021/04/20/charm-embraces-open-source-to-make-command-line-interfaces-glamorous/,Charm embraces open source to make command line interfaces ‘glamorous’,"Before the slick bells and whistles of the modern graphical user interface (GUI), the humble command line interface (CLI) ruled the roost — purely text-based commands typed via a keyboard to communicate with the operating system. But even in a world now dominated by fancy GUIs, command lines remain a popular option for developers. They can be a lot quicker (for those who know the commands, at least) and also consume less memory and computing resources than GUIs. Microsoft even launched a new application for command line users last year called Windows Terminal. Against this backdrop, Charm is setting out to apply “modern product thinking to one of the original forms of human-computer interaction” and make the command line “glamorous.” Charm was founded in 2019 by Toby Padilla, a former engineer at Apple, Last.fm, and TweetDeck; and Christian Rocha, formerly head of voice at Snap-acquisition Zenly. In a nutshell, the company is developing next-gen CLIs fit for the 21st century, with tools to improve visual appearance and store data, such as user profiles. To advance that effort, the company today announced it has raised $3 million in a round of funding led by Cavalry Ventures. Charm currently has three products on the market, including an open source terminal-based markdown reader it calls Glow that enables developers to view documentation such as readme files directly on the command line. It also renders the markdown to make the text easier to read and allows users to build a private, encrypted library of documentation. A typical use case might involve a developer who has cloned a project from GitHub and can run Glow in the project directory to discover all the documentation for that project. “The developer can read that documentation and/or stash it for later reference on this machine, or any other machine linked to their Charm account,” Padilla explained. As an open source tool, Charm tacitly acknowledges that most of the tools that contain markdown documentation are open source. Command line aficionados have a “strong desire to use open source software,” Padilla said. “It also allows Glow to be bundled and distributed with other open source tools, like Linux distributions and package managers. Since the majority of our users are developers, being open source also helps with adoption.” Elsewhere, Charm also offers a command-line-first account management tool called Charm, which leans on secure shell (SSH) keys to create an invisible account system. “This means that on first run of any software built with Charm accounts, users are not prompted to create an account,” Padilla said. “It happens invisibly behind the scenes, thus lowering the barrier of entry to using any tool built with Charm.” And then there’s Charm Cloud, which offers a suite of online services spanning identity, authentication, data storage, and encryption. It is a little like Firebase or Parse, except “focused on the command line as a platform,” Padilla said. Charm launched Glow in late 2019, though in its original guise it only displayed documentation — it didn’t allow a user to stash or build a private library. In October, the company introduced Charm Cloud, alongside a new version of Glow, replete with Charm Cloud integration. Additionally, Charm has open-sourced a bunch of libraries and tools it used to make Glow and Charm, such as Bubble Tea, Lip Gloss, Termenv, Glamour, and Bubbles, each designed to encourage an ecosystem of products built on Charm’s technologies. Glamour, for example, offers stylesheet-based markdown rendering for CLI apps, and GitHub has already integrated this into its official command line tool, according to Padilla.  Charm’s suite of products is currently free to use, and the company plans to keep it that way for “basic non-commercial use.” However, it intends to charge for enterprise-grade solutions and perhaps resource usage on individual accounts — “for example, if you need to store more than the free limit of data,” Padilla said. “Our business model is selling developer experience solutions built on our suite of products to enterprises.” Charm is also planning to later launch an open source self-hosted version of Charm Cloud that will be free for non-commercial use."
https://venturebeat.com/2021/04/20/datarails-nabs-18-5m-to-automate-financial-reporting-for-excel-users/,"DataRails, which automates financial reporting for Excel users, nabs $18.5M","DataRails, an Israeli startup that wants to help businesses understand their financial data better — and more quickly — has raised $18.5 million in funding as it looks to double down on its enterprise integrations and invest in its AI capabilities. The raise comes amid a flurry of activity across the financial planning and analytics sphere, with OneStream this month raising $200 million at a $6 billion valuation, shortly after Jedox locked down more than $100 million. “Businesses typically spend between 10 to 14 days every month on manually gathering data from different sources and bringing it together to understand the current status of the organization and try to predict future performance,” DataRails cofounder and COO Eyal Cohen told VentureBeat. “Despite their efforts, the results tend to be difficult to analyze, error-prone, and lacking insights. DataRails shortens the time spent on this to a few hours and allows organizations to get better insights into their business.” Founded in 2015, DataRails has entered a space that includes legacy players such as Anaplan; Hyperion, which Oracle bought for more than $3 billion in 2007; and Adaptive Insights, which Workday acquired for north of $1.5 billion in 2018. But the biggest incumbent DataRails is up against is arguably trusty old Excel. This is particularly true for small to medium-sized businesses, according to Cohen, as they use Microsoft’s omnipresent spreadsheet software for all their month-close and management reports, budgets, forecasts, and more. “Although there are well-established solutions in the FP&A [financial planning and analysis] market, more than 80% of small and medium-sized organizations conduct their routine processes manually, using Microsoft Excel and PowerPoint,” Cohen said. “The reason that this persists as the status quo is due to the fact that existing solutions require redesigning existing models and processes, leaving behind years’ worth of invested time in analyses, models, and reporting templates.” Excel’s persistence is one of the reasons we’ve seen a slew of newer players enter the FP&A market — such as Cube Software and Vena Solutions — taking a more modern approach that works on top of familiar spreadsheets. DataRails takes a similar path insofar as it seeks to supplement — rather than replace — Excel. It has created what it calls an “elastic database technology,” one that can transform spreadsheets into a structured database. Underpinning this are AI and machine learning algorithms that can take both structured and unstructured spreadsheet data (e.g. cell values, formulas, formats, and macros) to develop a “logical, centralized database.” “With this technology, DataRails automates existing Excel-based processes by leveraging existing models and templates to create one unified database,” Cohen said. “DataRails combines the flexibility of Excel with the power of a cloud-based database and a web-based dashboard.” DataRails’ customers, which include businesses from across the medical, transport, manufacturing, and cybersecurity spheres, can access the platform as a layer directly on top of Microsoft Excel and PowerPoint, which perhaps is how a financial analyst is most likely to use it. But it can also be accessed through a dedicated web interface, where management, executives, and board members would be more inclined to go to access data and insights. It’s worth noting that DataRails can also glean data from sources such as enterprise resource planning (ERP), customer relationship management (CRM), and human resource information systems (HRIS), including Netsuite, Quickbooks, SAP, Salesforce, and Microsoft Dynamics. And given that its interface is based on Excel, DataRails can connect with pretty much any tool capable of exporting data as a CSV file. “DataRails has very strong analysis capabilities, thanks to the fact that cross-organizational data, from all financial and operational systems, is housed under one roof in our unified database,” Cohen said. “With all organizational data centralized in one place, customers can conduct variance analyses [and] drill-downs for full-scale granularity and quickly design ad-hoc reports.” DataRails had previously raised $10 million, and with another $18.5 million from Zeev Ventures Fund, Vertex Ventures Israel, and Innovation Endeavors, the company is well-financed to add more data and analytics tooling to its platform. “We’re looking to add stronger analysis capabilities, as well as newer prediction capabilities and better insights,” Cohen said."
https://venturebeat.com/2021/04/20/bushel-closes-47-million-investment-round-led-by-lewis-clark-agrifood-and-continental-grain-company/,Bushel® Closes $47 Million Investment Round Led by Lewis & Clark AgriFood and Continental Grain Company," Investment captures significant market share of grain supply chain and accelerates digital infrastructure for sustainability initiatives, fintech offerings and expanded product suites  FARGO, N.D.–(BUSINESS WIRE)–April 20, 2021– Bushel, an independently owned software company and leading provider of software technology solutions for growers, grain buyers, ag retailers, protein producers and food companies, announced today closings on its $47 million Series C investment round. The oversubscribed round was led by Lewis & Clark AgriFood, a St. Louis-based food and agriculture focused investment firm, and Continental Grain Company, a global holding company focused on agriculture, food and protein production. Participation from new and existing investors include Cargill, Scoular, Germin8 Ventures and others. In addition, Consolidated Grain and Barge Co. is expected to close their investment in the round in the coming weeks. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420005377/en/ The infusion of growth capital and continued partnership of the syndicate will introduce payments, credit, trading offerings and increased data services, including capabilities to help consumer brands sustainably source commodities in the global grain supply chain. The funding will also accelerate Bushel’s current product offerings that currently deliver value to growers, commodity buyers, ag retailers and consumer packaged goods companies (CPGs). “We are excited to invest further into Bushel alongside some of our high-value partners like Cargill and Scoular,” said Chris Abbott, Co-Head of Continental Grain Ventures. “Bushel is one of the leading independent software companies in agriculture, and serves as the critical link connecting growers to grain buyers, processors, brands and consumers. This funding accelerates deployment of new offerings and embedded capabilities throughout the food and ag value chain. We believe it results in better outcomes for growers and the supply chain.” Bushel’s platform now reaches 40% of grain origination in the United States, resulting in inarguably the largest technology network effect among growers and grain buyers in the U.S. today. $22 billion of grain is contracted annually within Bushel’s ecosystem. Bushel’s collaborative ethos will continue to accelerate adding new participants throughout the value chain, particularly CPGs and consumer brands, that will benefit from upstream and midstream data insights and help deliver on sustainability promises. “Bushel is here to win for our customers and their growers,” said Jake Joraanstad, CEO and Co-founder of Bushel. “We are here to build on the agriculture industry’s century of infrastructure investment. We’re here not to disrupt but instead, to collaborate and help lead the industry into the digital age with strategies that make sense and provide value for all stakeholders in the ag and food value chain. We serve as the independent integration hub that saves businesses significant manual effort, time and expense, while also strengthening the relationships between growers and agribusinesses through secure, proven and easy to use software products and services.” “As Food and Agriculture specialist investors, we see Bushel as the leader in connecting disparate data systems in the ‘messy middle’ of the food and agriculture supply chain,” said Larry Page, Managing Director of Lewis & Clark Agrifood. “Digital innovation has landed on the farm, but few companies are focused on the digital side of agriculture (or lack thereof) from the farm gate and beyond. We hear so much noise in the space with very little real traction. As Bushel has consistently gained market share, it has become clear that the ability to connect data with physical grain has significant value for many stakeholders in our food system.” Monthly, 60,000 growers use Bushel’s products and services and nearly 2,000 grain buying locations across the U.S. and Canada trust Bushel to power their grower-facing and internal software products. Bushel’s platform integrates into multiple grain accounting systems, trading desks, farm management systems, insurance companies and market feeds to allow different software systems to work with each other. Current grain data-sharing processes are manual and fragmented, resulting in lost productivity and revenue potential. Bushel’s technologies aim to solve some of the industry’s biggest challenges and bring the grain supply chain together through connected, standardized and properly-permissioned data. “As a grower myself, and a daily user of Bushel’s products and services, it’s been rewarding to have a front row seat to the work our team is doing to create a great platform for the existing food industry,” said Ryan Raguse, President and Co-founder of Bushel. “This also comes with a deep responsibility to be stewards of the data that flows through the digital infrastructure we’re building. In addition to the responsibility of carrying the data of our customers and their growers, Bushel’s platform also carries the data of many of our employees’ farming operations, including my own. We will continue to set high standards for data security, ownership and transparency in order to protect and empower our customers, our families and the broader agriculture industry.” In addition to their investment, Cargill has committed to using the Bushel platform to power select internal and external digital tools. Scoular has been using the Bushel platform since 2018. Consolidated Grain and Barge Co. is currently exploring opportunities for utilization of Bushel’s suite of services. Bushel’s product suite includes its flagship mobile app, websites, trading tools, market feeds, API services and a custom software division focused on agriculture. Bushel has been focused on building software since the company was founded in 2011. Learn more: To learn more about how Bushel works, view demo videos, read testimonials from customers or to get started, visit bushelpowered.com. About Bushel Bushel is an independently owned software company and leading provider of software technology solutions for growers, grain buyers, ag retailers, protein producers and food companies, headquartered in Fargo, N.D. Since launching in 2017, Bushel’s platform has grown rapidly, now powering nearly 2,000 grain facilities across the U.S. and Canada with real-time business information for their producers. Monthly, 60,000 producers utilize Bushel products and services. Bushel is focused on bringing innovative software products and solutions to the agriculture industry.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005377/en/ Bushel contact:Camille Grade, Chief Market Officercgrade@bushelpowered.com 701.730.0694"
https://venturebeat.com/2021/04/20/synthesia-raises-12-5m-for-ai-that-generates-avatar-videos/,Synthesia raises $12.5M for AI that generates avatar videos,"Synthesia, a startup using AI to create synthetic videos of avatars for marketing, today announced it has raised $12.5 million. In a press release, the company said the funding will be put toward expanding its workforce as it invests in product R&D. As the pandemic makes virtual meetups a regular occurrence, the concept of “personal AI” is rapidly gaining steam. Startups creating virtual beings, or artificial people powered by AI, have collectively raised more than $320 million in venture capital to date. As my colleague Dean Takahashi points out, these beings are a kind of precursor to the metaverse, a universe of virtual worlds that are all interconnected, as in novels such as Snow Crash and Ready Player One. Synthesia’s immediate goals are less ambitious. Like rivals Soul Machines, Brud, Wave, Samsung-backed STAR Labs, and others, the company employs a combination of machine learning techniques to create visual chatbots, product videos, and sales videos for clients without actors, film crews, studios, or cameras. “We’ve still only scratched the surface of the video economy. In 10 years, we believe most of our digital experiences will be powered by video in some way or form,” CEO Victor Riparbelli told VentureBeat. Riparbelli cofounded Synthesia in 2017 alongside Steffen Tjerrild and computer vision professors Lourdes Agapito and Matthias Niessner, who is behind some of the better-recognized research projects in the field of synthetic media, such as Deep Video Portraits and Face2Face.  “Today, video production is costly, complex, and unscalable. It requires studios, actors, cameras, and post-production. It’s an incredibly long and multidisciplinary process, rooted in physical space and sensors,” Riparbelli continued. “To truly realize the video-first internet, we need a more scalable and accessible way to make video.” Synthesia customers choose from a gallery of in-house, AI-generated presenters or create their own by recording voice clips and then uploading them. After typing or pasting in a video script, Synthesia generates a video “in minutes,” making it available for translation into dozens of languages. As pandemic restrictions make conventional filming tricky and risky, the benefits of AI-generated video have been magnified. According to Dogtown Media, an education campaign under normal circumstances might require as many as 20 different scripts to address a business’ worldwide workforce, with each video costing tens of thousands of dollars. Synthesia’s technology can pare the expenses down to a lump sum of around $100,000. Synthesia says that client CraftWW used its platform to ideate an advertising campaign for JustEat in the Australian market featuring an AI-manipulated Snoop Dogg. The company also worked with director Ridley Scott’s production studio to create a film for the nonprofit Malaria Must Die, which translated David Beckham’s voice into over nine languages. And it partnered with Reuters to develop a prototype for automated video sport reports. “We’re building an application layer that turns code into video, allowing for video content to be programmed with computers rather than recorded with cameras and microphones. Once video production is abstracted away as code, it has all the benefits of software: infinite scale, close to zero marginal costs, and it can be made accessible to everyone,” Riparbelli said. “This is now quickly becoming a reality. We launched our software-as-a-service product just six months ago … [and we] have essentially reduced the entire video production process to a single API call or a few clicks in our web app.” In the near future, Synthesia plans to make generally available a product that personalizes videos to specific customer segments. It’s called Personalize, and Synthesia says it can automatically translate videos featuring actors or staff members into over 40 languages. “We have been overwhelmed by the response in the last six months since our beta launch: We now have thousands of users, and our customers range from small agencies to Fortune 500 companies,” Riparbelli said. “They use Synthesa primarily for internal training and corporate communications. But now we are seeing more and more companies starting to use it for external communications, incorporating personalized video into every step of the customer journey through our personalized video API.” Some experts have expressed concern that tools like Synthesia’s could be used to create deepfakes, or AI-generated videos that take a person in an existing video and replace them with someone else’s likeness. The fear is that these fakes might be used to do things like sway opinion during an election or implicate a person in a crime. Deepfakes have already been abused to generate pornographic material of actors and defraud a major energy producer. For its part, Synthesia has posted ethics rules online and says it vets its customers and their scripts. It also requires formal consent from a person before it will synthesize their appearance and refuses to touch political content. “We are trying to solve a very complex and technical problem,” Riparbelli recently told the Telegraph. “We are not releasing any software to the public … There is a wider discussion to be had about the malevolent use of this kind of stuff.” Synthesia’s series A funding round announced today was led by FirstMark Capital, with participation from Christian Bach; Michael Buckley; and existing investors, including Mark Cuban. The London, U.K.-based company has 30 employees, and its total raised is now over $16.6 million."
https://venturebeat.com/2021/04/20/ci-security-closes-first-tranche-of-series-b-financing-and-announces-record-q1/,CI Security Closes First Tranche of Series B Financing and Announces Record Q1," MDR Leader Accelerates Presence in Healthcare as American Hospital Association Names Company Preferred Provider  SEATTLE–(BUSINESS WIRE)–April 20, 2021– CI Security (CI), the Seattle-based company mission-focused on defending critical services from cyberattack, has announced the initial close of a Series B extension funding round led by Alan Frazier’s East Seattle Partners. San Francisco technology-focused investment bank Capital Clarity is advising CI. Due to strong investor interest, CI and Capital Clarity will continue accepting investments into the round. CI will use these funds to continue developing its vertical-specific cybersecurity and MDR platform and to support its sales and marketing efforts. The company’s announcement comes after recording record-breaking sales in Q1. The first quarter results doubled the Company’s prior quarterly sales record, demonstrating significant positive momentum in sales velocity and closed sales. Additionally, CI just announced it is an American Hospital Association preferred provider to protect and defend hospitals. The AHA vetted cybersecurity providers in a nationwide search and chose CI among the select group. CI is the only AHA preferred provider for Managed Detection and Response as well as the Critical Insight Healthcare Security Program, which includes HIPAA Risk Assessments, Vulnerability scanning along with MDR. “The exciting Q1 results, along with the news from the AHA means we are fulfilling our mission to defend critical service organizations from cyberattack,” said Garrett Silver, CI Security’s CEO. “Our combination of Managed Detection and Response, coupled with our suite of Assessment and Testing services, support small security teams in building out their total security program.” “CI’s investors are enthusiastic about supporting and growing a cybersecurity company with this kind of focus,” said John Cooper, Managing Partner of Capital Clarity. “We’re keeping the round open to support significant additional investment interest.” About CI Security CI Security provides Managed Detection and Response services, combining purpose-built technology with expert security analysts to perform full-cycle threat detection, investigation, response, and recovery. CI Security is focused on defending critical systems in healthcare, the public sector, and other industries while helping customers gain critical insight into their security posture through the MDR platform and Information Security Consulting Services. Find out more at https://ci.security. About Capital Clarity Capital Clarity offers a refined approach to investment banking that emphasizes long-term partnership with investors and management teams. Our leadership team has a combined experience of 80 years in financial advisory, mergers & acquisitions, corporate development and private equity. We combine advisory expertise with deep industry knowledge and long-standing buyer and investor relationships to create successful outcomes for our clients. Cybersecurity is one of Capital Clarity’s many areas of technology focus and expertise.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005203/en/ Susan BlancoCapital Clarity(415) 320-1582susan@capital-clarity.com John CooperCapital Clarity(415) 683-0339coop@capital-clarity.com Jake MilsteinCI Security206-347-0588jake.milstein@ci.security"
https://venturebeat.com/2021/04/20/cape-privacy-raises-20m-to-enable-data-science-operations-on-encrypted-data/,Cape Privacy raises $20M to enable data science operations on encrypted data,"Cape Privacy, which is developing a privacy-preserving platform for collaborative data science, today announced that it closed a $20 million series A led by Evolution Equity Partners. CEO Ché Wijesinghe says that the proceeds will be used to support growth as Cape Privacy develops new technologies for secure machine learning. AI promises to transform — and has transformed — entire industries, from civic planning and health care to cybersecurity. But privacy remains an unsolved challenge, particularly where compliance and regulation are concerned. Banks, health providers, and even retailers can run into problems when collaborating on AI and machine learning research involving sensitive or proprietary data, like patient records, financial documents, and supply chain details. Cape was founded in 2018 by Gavin Uhma, the cofounder and CTO of GoInstant, which was acquired by Salesforce in 2012. Cape’s combination of privacy, machine learning, and cryptography enables encrypted data-sharing, helping teams in compliance, legal, and risk management work with each other and third-party vendors. “Today many financial institutions access the same publicly available data from data providers like Nielsen and Bloomberg — but they all want a better edge,” a spokesperson told VentureBeat via email. “Non-public data sources such as those from retail and credit card companies would greatly enrich their models. Yet concerns around confidentiality on both sides have prevented this collaboration. Many data providers are interested in finding new channels to monetize their data, but they can rarely get it past their internal legal and compliance teams.” Cape’s open source software integrates with data science and AI infrastructure to provide a workflow guiding contributors toward building custom projects and policies. Cape enables developers to decide on the placement of tools in relation to data storage and pipelines, ensuring data access, privacy, and monitoring meet each product’s requirements. Moreover, it allows stakeholders to set monitoring and auditing configurations so that all parties receive logs for review, approval, or amendment. “Cape Privacy’s platform … ensures privacy by default. With Cape as the broker, data providers are only renting data instead of selling it. This is a significant point because companies that lose control of their data can get in trouble,” the spokesperson said. “Once a data model is enriched using encrypted data on the Cape cloud, the transaction between buyer and seller ends and the data is returned. Now the data subscriber can enrich its data for better business outcomes, and the data provider can securely monetize its data.” Cape’s platform is underpinned by tf-encrypted, the company’s suite for experimenting with private machine learning on top of Google’s TensorFlow framework. Tf-encrypted enables training, validation, and prediction over encrypted data. The data remains encrypted during the workflow, meaning that AI models can be hosted in the cloud without decrypting the training data or outputs. Seventeen-employee Cape, which claims to have two major clients and “half a dozen” in the pipeline, isn’t the first to advance a privacy-preserving data science approach. Companies including Enveil, Cosmian, Duality Technologies, and Intel are investigating homomorphic encryption, a form of cryptography that enables computation on file contents encrypted using an algorithm so that the generated encrypted result exactly matches the result of operations that would’ve been performed on unencrypted file. Using homomorphic encryption, a “cryptonet” can perform computation on data and return the encrypted result back to a client, which can then use the encryption key to decrypt the returned data and get the actual result. Homomorphic encryption libraries don’t yet fully leverage modern hardware and are at least an order of magnitude slower than conventional methods. That said, newer projects like the accelerated encryption library cuHE claim speedups of 12 to 50 times on various encrypted tasks over previous implementations. And HE-Transformer, a backend for nGraph (Intel’s neural network compiler), delivers leading performance on some cryptonets. New investors Tiger Global Management, Ridgeline Partners, and Downling Lane participated in Cape Privacy’s series A together with existing investors Boldstart Ventures, Version One Ventures, Haystack, Radical Ventures, and Jevon MacDonald. Additional investment came from Coinbase cofounder and board member Fred Ehrsam, the Tokyo Black Fund, and Sand Hill East. To date, New York-based Cape Privacy has raised over $25 million. “We are excited at reaching this company milestone,” Wijesinghe told VentureBeat via email. “Cape’s technology will be a defacto standard for privacy preserving machine learning. Building on our success in the financial services industry, we have already had great interest from Health and Life Sciences companies for potential drug discovery and genomics research use cases. In addition, there is clear demand for this technology for collaboration on machine learning model development across government agencies for counter-terrorism programs.”"
https://venturebeat.com/2021/04/20/grip-security-aims-to-simplify-and-automate-saas-endpoint-security/,Grip Security aims to simplify and automate SaaS endpoint security,"As SaaS platforms explode in popularity, developers are finding it easier than ever to write applications that plug in and reach big user bases. But this has created a growing security headache as the number of applications running on enterprise SaaS platforms soars. Idan Fast, Lior Yaari, and Alon Shenkler joined forces last year when they saw an opportunity to address this problem. Their new startup, Grip Security, is the latest member of the red-hot Israeli cybersecurity ecosystem. In an interview with VentureBeat, Yaari said Grip aims to revolutionize SaaS security with an enforceable endpoint-centric approach that secures all SaaS application access, regardless of device or location. “SaaS is becoming a problem, with the ever-growing landscape of SaaS within enterprises,” he said. “Hundreds of applications are being used, and there is a lack of tools to control that.” Grip is just at the start of that journey. Today, the company officially unveiled itself and announced it has raised $6 million in a funding round led by YL Ventures. It also boasts an impressive roster of business angels, including CrowdStrike CEO George Kurtz, former Akamai CSO Andy Ellis, former Zscaler CISO Michael Sutton, and former Bank of America chief security scientist Sounil Yu. According to Yaari, existing SaaS security tools have failed to offer comprehensive solutions and lack visibility and control for users. Because an enterprise may not see all their SaaS applications, they remain vulnerable. He puts secure access service edge (SASE) in this category of approaches with limitations. That’s big talk, considering SASE has been showing tremendous momentum with both users and investors. In this respect, Grip will be competing against fellow Israeli startup Cato Networks, which pioneered SASE and raised rounds of $130 million and $77 million last year. Boston-based SASE startup Iboss raised $145 million in January. Endpoint security company Lookout in March acquired CipherCloud, a cloud-native cybersecurity startup focused on SASE. And Cisco recently announced its own SASE offering. But Yaari said such SaaS security solutions tend to approach the problem by trying to secure the network or by securing the applications. “Both of them have blind spots in either the availability to cover the vast amount of applications that exist or the ability to cover things that are coming out of the corporate network,” he said. Yaari and his cofounders are veterans of Israel’s security ecosystem, and their fast start is a testament to just how robust this beehive of security startups has become. Despite facing some well-funded rivals, they are betting Grip can build a superior solution by creating a platform that automatically tracks all applications, no matter which device they are running on or where they’re located. By mapping data flows, the Grip platform will be able to apply security policies across a SaaS network and all applications. Eventually, the Grip platform will work alone or in tandem with a cloud access security broker. “The platform itself does two things,” he said. “It discovers what applications are being used, which is a very big problem in the security space. And then for each application it discovers, it adds an additional security layer, allowing for access and data governance to those applications.” Among the features the company is promising are greater visibility into all applications and risk profiles and protection against sensitive data flows. Grip has already deployed an initial product for medium-sized tech companies that it’s learning from to further develop its platform. The product is currently only available to a limited number of companies as an early-access version. Grip will use the funding to increase its customer count in order to accelerate development while expanding up to 30 employees by next year."
https://venturebeat.com/2021/04/20/log-management-database-startup-era-software-raises-15-25m/,Log management database startup Era Software raises $15.25M,"Era Software, creator of the EraDB time-series database architecture for log management, today announced it has raised $15.25 million in a round led by Playground Global. The funding, which brings the company’s total raised to over $22 million, will be put toward supporting product development and bolstering the launch of its EraSearch Cloud product in private beta. A 2018 report from Domo estimated that humans are creating 2.5 exabytes of data per day, a number that’s increasing. Perhaps unsurprisingly, some companies are tapping the deluge more effectively and efficiently than others. A recent PricewaterhouseCoopers survey of over 1,800 business leaders found that 43% obtained little tangible benefit from their data. And over 20% reported deriving no benefit whatsoever. While Seattle, Washington-based Era has a number of rivals in a database market estimated to be worth tens of billions (e.g., Splunk, ChaosSearch, and Sumo Logic), the company differentiates itself with decoupled storage and compute, indexing powered by machine learning, and schema-free data storage. Era automatically indexes on every dimension in logs and works with arbitrarily shaped data, with support for third-party frontend and backend systems, as well as query-based performance optimizations. Era can adjust to different log data rates dynamically, without fine-tuning. And the architecture can ingest data that isn’t timestamped, allowing developers to filter and aggregate pieces of information. Todd Persen, who cofounded Era with Robert Winslow, previously launched InfluxData, originator of the InfluxDB time-series database. He started Era to tackle challenges around managing large volumes of data in logs, with a focus on data that’s becoming increasingly complex. High-dimensionality data is emerging at a rate faster than traditional databases can keep up, he says, leaving organizations to pay exponentially increasing costs for ill-designed solutions. “While Elasticsearch became the de facto open source choice for storing logs, the world was moving to a place where the average log volumes exploded from gigabytes per day to terabytes per day,” an Era spokesperson told VentureBeat via email. “What began as simple, single-node deployments of Elasticsearch grew into complex clusters that had skyrocketing hardware costs and burdensome operational demands. Elasticsearch was never designed to be an optimal solution for the high-volume demands of the log management use case. While still hugely popular, users are battling the costs of running it at scale — but there are not many alternatives, hence the creation of Era’s log management solution.” Persen claims Era’s fully managed offering, EraSearch Cloud, allows customers to deploy log management infrastructure that’s less complex and performant but more cost-effective. EraSearch Cloud ensures copies of data live within object storage, like on Amazon’s Simple Storage Service, Google Cloud Storage, or Microsoft Azure Blob Storage, ostensibly without sacrificing performance. Beyond Playground Global, existing investors Foundation Capital, Array Ventures, Global Founders Capital, and angel backers also participated in Era’s series A. This follows a $7 million seed round in 2020 that was led by Foundation Capital."
https://venturebeat.com/2021/04/20/xilinx-launches-kria-chips-to-handle-ai-for-edge-applications/,Xilinx launches Kria chips to handle AI for edge applications,"Xilinx has introduced its Kria programmable chips and boards for holding AI applications at the edge of the network. This should come in handy for visual applications like smarter cameras. San Jose, California-based Xilinx, which is in the process of being acquired by Advanced Micro Devices (AMD) for $35 billion, has a group of products dubbed the Kria portfolio of adaptive system-on-module offerings for AI at the edge. These are production-ready small form factor embedded boards that enable rapid deployment in edge-based applications. Coupled with a complete software stack and prebuilt, production-grade accelerated applications, Kria adaptive modules are a new method of bringing adaptive computing to AI and software developers. The first product available in the Kria SOM portfolio, the Kria K26 SOM, specifically targets vision AI applications in smart cities and smart factories. The Xilinx SOM roadmap includes multiple products, from cost-optimized SOMs for size- and cost-constrained applications to higher-performance modules that will offer developers more real-time compute capability per watt. Xilinx said industry reports put market growth at 11% per year and total market revenue at $2.3 billion by 2025. “I think there’s solid potential here,” HotTech Vision and Analysis analyst Dave Altavilla said in an email to VentureBeat. “Similar to the way Nvidia enabled its graphics processing unit (GPU)-powered edge AI solutions with Jetson and Jetson Nano, Xilinx’s Kria SOM is a complete shrink-wrapped solution for developers, engineers, and makers to get their feet wet with adaptable field-programmable gate array (FPGA)-based acceleration, with little to no experience required in programmable logic (FGPAs).” He added, “In addition, Xilinx’s embedded app store for edge AI will help foster accelerated solutions with faster time to market in anything from facial recognition to natural language processing applications and more. In short, it’s what the company needs to demystify the FGPA for the masses, and in conjunction with Xilinx Vitis software development tools, will allow engineers to work in their own native frameworks and programming languages like PyTorch, TensorFlow, Caffee, C++, OpenCL, and Python.” He added, “It’s a clear indication that the adaptive nature of FPGAs doesn’t need to be relegated to just the power user-programmable logic engineer anymore. And with Ubuntu support on the way, these dev kits could go mainstream in a hurry.” Kria SOMs use Xilinx adaptable hardware, delivered as production-deployable, adaptive modules. Kria SOMs can be deployed rapidly using end-to-end board-level solutions with a prebuilt software stack. By allowing developers to start at a more evolved point in the design cycle compared to chip-down design, Kria SOMs can reduce time to deployment by up to nine months, Xilinx said. The Kria K26 SOM is built on top of the Zynq UltraScale+ MPSoC architecture, which features a quad-core ARM Cortex A53 processor, more than 250,000 logic cells, and an H.264/265 video codec. The SOM also features 4GB of DDR4 memory and 245 input-output paths that allow it to adapt to virtually any sensor or interface. With 1.4 teraflops of AI compute, the Kria K26 SOM enables developers to create vision AI applications offering more than 3 times higher performance at lower latency and power compared to GPU-based SOMs. This is critical for smart vision applications, including security cameras, city cameras, traffic cameras, retail analytics, machine vision, and vision-guided robotics. Xilinx has invested heavily in its tool flows to make adaptive computing more accessible to AI and software developers who lack hardware expertise. The Kria SOM portfolio takes this accessibility to the next level by coupling the hardware and software platform with production-ready vision accelerated applications. These applications eliminate all the FPGA hardware design work and only require software developers to integrate their custom AI models and application code. They can optionally modify the vision pipeline — using their familiar design environments, such as TensorFlow, Pytorch, or Caffé frameworks, as well as C, C++, OpenCL, and Python programming languages — enabled by the Vitis unified software development platform and libraries. Xilinx offerings are open source accelerated applications, provided at no charge, and range from smart camera tracking and face detection to natural language processing with smart vision. The Kria KV260 Vision AI starter kit is priced at a low $199. When customers are ready to move to deployment, they can seamlessly transition to the Kria K26 production SOM, including commercial and industrial variants priced at $250 or $350, respectively. The KV260 Vision Starter Kit is available immediately, with the commercial-grade Kria K26 SOM shipping in May of 2021 and the industrial-grade K26 SOM shipping this summer. Ubuntu Linux on Kria K26 SOMs is expected to be available in July."
https://venturebeat.com/2021/04/20/how-vertical-location-improves-operational-efficiency/,How vertical location improves operational efficiency,"Presented by NextNav From food delivery services to safety apps, 3D location technology is about to transform the 2D technology we’ve long accepted as the limit of location services. Join VB’s Dean Takahashi and others to learn what it means for any app relying on location services — and how it’s now possible to enable accurate vertical location at scale in vertical environments such as skyscrapers, apartment buildings, or malls. Register here for free. Mobile apps of all kinds use location to increase efficiency. From deliveries to ridesharing, family safety to parking, mobile apps depend on location infrastructure to deliver the just-in-time services that we all depend on — the services which power billions of dollars a day in revenue. While location services play a key role in driving operational efficiency, there’s a significant missing piece that keeps most location-enabled services from reaching their full potential. Today’s location services are all constrained by 2D technology. They can optimize for flat environments, but that’s it. Buildings, underground locations, and complex urban landscapes are a significant challenge. For a delivery application, an app can get you to an address, but can’t identify which floor the recipient ordered from. Parking apps can’t direct you to your car in a four-story underground garage. Family safety apps can tell you that your child is in a skyscraper somewhere, but you’d have to search fifty floors to find out exactly where. A quick-serve restaurant on the ground floor of a building can’t predict when someone from the twentieth floor above them will arrive to pick up their food. Unfortunately, most companies don’t realize what they’re missing. We’re all so used to the flat, 2D technology we rely on today that we don’t think about how much better it could be with a whole new dimension of data. Or we think about vertical location as a neat feature, but not something that actually has an impact on the bottom line. We discount that “last mile” of the user experience, not making the connection to increased revenue, higher LTV, and improved customer retention. In fact, vertical location is a major step forward in operational efficiency, with downstream effects that have the potential to revolutionize a wide range of location-based applications. If the 2D applications we have today made us this efficient, just imagine what we could accomplish with a whole new data set? It’s worth noting that the efficiency gains of vertical location are likely to be felt the most by the very customers that most app companies crave:  urban residents.  Dense cities — the places where people live, work, and play in multi-story buildings — are both the largest market for mobile apps and the places where 2D location falls down on the job.  The companies that fill this gap first will naturally reap outsize dividends. NextNav’s vertical location technology helps companies increase operational efficiency in the urban markets where it matters most. We’ve built our Pinnacle service for vertical location to focus specifically on built-up urban areas — 4,400 cities at last count.  We cover over 90% of buildings three stories and above, knowing that these are the places where deliveries, directions, and other precise location-based services are the most critical. How can vertical location improve the operational efficiency of your location app? Don’t miss this VB Live event. Register for free here. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/20/korean-ai-based-diagnostic-software-dr-answer-helps-countries-with-a-shortage-of-medical-professionals/,Korean AI-based Diagnostic Software Dr. Answer Helps Countries With a Shortage of Medical Professionals," -Saudi Arabia and Africa are in clinical verification of Dr. Answer while discussing its adoption  SEOUL, South Korea–(BUSINESS WIRE)–April 20, 2021– Dr. Answer, an innovative artificial intelligence (AI) based diagnostics application, is expected to significantly help countries with insufficient healthcare professionals make more efficient diagnosis. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420005568/en/ Dr. Answer is a suite of AI-powered precision diagnostic software that supports healthcare. The suite is comprised of 21 AI-based software programs that assist diagnosis of 8 major diseases selected in consideration of mortality, the proportion of medical costs and public interest. 26 medical organizations developed Dr. Answer in collaboration with 22 information and communication technology companies in Korea for three years, and 38 medical organizations verified its stability and effectiveness in improving accuracy in diagnosis and reducing diagnosing time. “Healthcare professionals at large general hospitals can easily make reference to opinions of other professionals when they involve in diagnosis and therapeutic decisions as a large number of professionals in diverse disciplines are working within the same hospitals. But a few medical professionals diagnose patients in many small hospitals or clinics where Dr. Answer can help professionals make an accurate diagnosis,” explained Kim Jong-jae, Director of Asan Institute for Life Sciences. “Dr. Answer can be usefully used in Europe, and countries with insufficient medical professionals such as Africa may achieve substantial outcomes when they use Dr. Answer,” added Kim. Recognizing the excellence of Dr. Answer, hospitals affiliated with the Ministry of National Guard-Health Affairs of Saudi Arabia are currently conducting its clinical verification while discussing the possibility of its adoption.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005568/en/ Dr. AnswerLee Junyoung+82-43-931-5500leejy@nipa.kr"
https://venturebeat.com/2021/04/20/codecov-hackers-gained-access-to-hundreds-of-restricted-customer-networks/,Codecov hackers gained access to hundreds of restricted customer networks,"(Reuters) — Hackers who tampered with a software development tool from a company called Codecov used that program to gain restricted access to hundreds of networks belonging to the San Francisco firm’s customers, investigators told Reuters. Codecov makes software auditing tools that allow developers to see how thoroughly their own code is being tested, a process that can give the tool access to stored credentials for various internal software accounts. The attackers used automation to rapidly copy those credentials and raid additional resources, the investigators said, expanding the breach beyond the initial disclosure by Codecov on Thursday. The hackers put extra effort into using Codecov to get inside other makers of software development programs, as well as companies that themselves provide many customers with technology services, including IBM, one of the investigators said on condition of anonymity. The person said both methods would allow the hackers to potentially gain credentials for thousands of other restricted systems. IBM and other companies said their code had not been altered but did not address whether access credentials to their systems had been taken. “We are investigating the reported Codecov incident and have thus far found no modifications of code involving clients or IBM,” an IBM spokesperson said. The FBI’s San Francisco office is investigating the compromises, and dozens of likely victims were notified on Monday. Private security companies were already beginning to respond to assist multiple clients, employees said. Codecov did not respond to Reuters’ request for comment on Monday. Security experts involved in the case said the scale of the attack and the skills needed to execute it compared to last year’s SolarWinds attack. The compromise of that company’s widely used network management program allowed hackers inside nine U.S. government agencies and about 100 private companies. It is unclear who is behind the latest breach or if they are working for a national government, as was the case with SolarWinds. Others among Codecov’s 19,000 customers, including big tech services provider Hewlett Packard Enterprise, said they were still trying to determine if they or their customers had been affected. “HPE has a dedicated team of professionals investigating this matter, and customers should rest assured we will keep them informed of any impacts and necessary remedies as soon as we know more,” said HPE spokesperson Adam Bauer. Even Codecov users who had seen no evidence of hacking were taking the breach seriously, a corporate cybersecurity official told Reuters. He said his company was busy resetting its credentials and that his counterparts elsewhere were doing the same, as Codecov recommended. Codecov earlier said hackers began tampering with its software on January 31. The hack was only detected earlier this month, when a customer raised concerns. Codecov’s website says its customers include consumer goods conglomerate Procter & Gamble, web hosting firm GoDaddy, the Washington Post, and Australian software firm Atlassian. Atlassian said it had not yet seen any impact or signs of a compromise. The Department of Homeland Security’s cybersecurity arm and the FBI declined to comment."
https://venturebeat.com/2021/04/20/devops-automation-platform-octopus-deploy-nabs-172-5m/,DevOps automation platform Octopus Deploy nabs $172.5M,"Octopus Deploy, a Brisbane, Australia-based company developing a continuous integration and delivery (CI/CD) platform, today announced that it closed a $172.5 million round. The funding, led by Insight Partners for a minority stake in the company, will be used to expand Octopus Deploy’s footprint and accelerate its go-to-market efforts. In software engineering, CI/CD ideally bridges gaps between development and operations by enforcing automation in building, testing, and deployment. But execution remains a challenge. In a recent survey, when asked whether their organizations have software release problems, 77% of respondents said that they did. Lack of expertise, poor training, and other challenges can lead to the inefficient implementation of CI/CD pipelines. Octopus Deploy began in 2011 as a nights-and-weekends project for software developer Paul Stovell and his wife, Sonia. It became a profitable business in 2012, and until now, it’s remained entirely self-funded. Octopus Deploy is designed to integrate with existing source control systems and build servers, orchestrating the DevOps automation that happens after a software build completes. By reusing configurations, API keys, connection strings, permissions, and automation logic, Octopus Deploy lets teams work together from a single platform.  From a dashboard, developers can use Octopus Deploy to facilitate releases, approvals, and notes across environments. Over 400 step templates help to ensure processes remain consistent across dev, test, and production environments. Runbook automation, meanwhile, affords control over infrastructure and apps. With runbooks, developers can automate tasks like maintenance and emergency incident recovery. Each automation includes permissions for the infrastructure it runs on so that on a team can be granted permission to execute a runbook with an audit trail. Octopus Deploy, which employs over 100 people, says that more than 25,000 organizations (7,200 of which are paying customers) use its platform, including Disney, NASA, Microsoft, Xero, and Stack Overflow. With offices in the U.S. and U.K., the company plans to open a larger headquarters in Brisbane in the near future. “Octopus Deploy makes it easy for software teams to automate complex software deployment processes. One of the big four accountancy firms uses the solution to deploy over 50 different internal products,” Stovell said. “Prior to using Octopus Deploy, its deployments used to be manual or semi-automated and took 3 to 4 hours to complete, and failure was common because of human error; now, deployments take 10 to 15 minutes, and humans are only involved for approvals. With less downtime and with their confidence increased, the firm can now do thousands of deployments even during the busy tax season.” According to Markets and Markets, the CI tools market alone is anticipated to reach $1.1393 billion in value by 2023. Among other rivals, Octopus Deploy competes with CI/CD platform Harness, which in January raised $115 million at a $1.7 billion valuation."
https://venturebeat.com/2021/04/20/mantl-raises-40m-to-help-legacy-banks-transition-to-digital/,Mantl raises $40M to help legacy banks transition to digital,"Mantl, a New York-based digital account-opening solution for legacy banks and credit unions, today announced that it closed a $40 million series B funding round. The company says it’ll use the capital to hire new talent and expand its product suite, with a particular eye toward developing solutions that improve and digitize the onboarding experience for businesses. It’s estimated that community banks and credit unions make up 95% of all banking institutions. Currently, they rely on third-party technology providers whose systems sometimes lack the capabilities deployed by larger, established banks. This impedes them when competing online and limits the options available to customers who prefer banking digitally. For example, one study found most millennials prefer to have interactions with financial brands through social media, and millennials make up the highest percentage of mobile banking users. Founded in 2016, Mantl provides a platform that integrates with core banking systems to enable customers to open accounts through white-labeled web and mobile portals. The system automates application decisioning for over 90% of cases while ostensibly reducing fraud, leading to deposit growth while eliminating the need to build new branches. “We originally set out to build a challenger bank, but we realized that the bigger opportunity was in helping existing banks modernize,” CEO Nathaniel Harley, who cofounded Mantl with Benjamin Conant and Raj Patel, told VentureBeat via email. “We enable banks and credit unions to open deposit accounts online, often for the first time. This includes products like checking accounts, savings accounts, certificates of deposit, and money market accounts. We estimate that less than half of all banks in the US give their customers the ability to open deposit accounts online today.”  With Mantl, banks can customize the look, feel, and messaging on the platform using a no-code editor. A console aggregates data points and turns them into insights to show which marketing channels are driving conversions. Mantl’s campaign management tool, meanwhile, automates deposit operations including payment settlement. And the company’s data connectors work with over 50 systems including Plaid, Stripe, SendGrid, and Twilio. “Currently, 43% of legacy banks are still running their core banking services on platforms that were designed with COBOL, a programming language that’s now over 60 years old,” Harley said. “This is why the gap keeps widening between the community banks who rely on these legacy players and the big banks who don’t: the top 15 banks hold 56.2% deposit market share today versus 16% 25 years ago. MANTL is fixing the legacy infrastructure problem, which is the biggest obstacle limiting modernization in the U.S. banking system today.” Mantl says that its clients, which include Cross River Bank, Quontic, and Midwest BankCentre, have attracted hundreds of thousands of new customers and raised billions in core deposits to date. In the past year, revenue increased by 213%. And banks on the platform saw 300% growth in deposit volumes. For Midwest BankCentre, Mantl helped raise more than $180 million through a digital-only branch called Rising Bank. And in the case of Flushing Bank, Mantl estimates that 20% of all new accounts are now coming from digital channels that didn’t previously exist. In what might be a boon for Mantl, eMarketer predicts that banks will increasingly partner with companies to offer “banking-as-a-service” products. For example, in August 2019, HSBC partnered with startup Amount to launch a digital lending product — and more partnerships like that could be on the way.  A PricewaterhouseCoopers survey of banks worldwide found that among those that wanted to collaborate with other sectors for growth, 47% were likely to work with a fintech firm. “As the world prepares to fully open again, MANTL has rounded out our account opening suite with a fully omni channel platform that improves the customer experience across all bank channels — in-branch, online, mobile and through relationship managers in the field. The next step is to bring similar efficiencies to business onboarding, and we’re excited to introduce a first-of-its-kind business account opening solution to market,” Harley said. “We are launching it later this year and it will be met with significant demand as community institutions are urgently looking for ways to transition their [short-term] relationships into long-term deposit relationships.” Mantl’s series B investment was led by Google parent company’s Alphabet’s growth fund, CapitalG, with participation from D1 Capital Partners, BoxGroup, and existing investors Point72 Ventures, Clocktower Technology Ventures, and OldSlip Group. It brings Mantl’s total raised to date to over $60 million following a $19 million series A round that closed in July 2020."
https://venturebeat.com/2021/04/20/hypr-raises-35m-to-grow-its-passwordless-authentication-platform/,Hypr raises $35M to grow its passwordless authentication platform,"Hypr, a cloud multifactor authentication platform, today announced it has raised $35 million in a series C round led by Advent International, doubling the company’s total funding to over $72 million. Hypr says it will leverage the funds to bolster its go-to-market strategy and grow its support organization globally. According to a recent study, passwordless authentication — which Hypr provides — is seeing heightened interest among enterprises. A survey of executives commissioned by LogMeIn’s LastPass found that 92% believe passwordless authentication is the future of their organization. That’s perhaps because users prefer it. The Ponemon Institute reports that 57% of users would would choose a passwordless method of protecting their identity if given the choice. But the rise of passwordless authentication is also likely attributable to an increased understanding of the risks associated with passwords. It’s estimated that 51% of people use the same passwords for both work and personal accounts and that 53% have a password that’s easy enough to memorize. Hypr offers a platform that’s powered by “open standards,” the company claims, with a “mobile-first” and public-key cryptography-based login experience for workstations. Using a branded smartphone app, customers can deploy a passwordless solution to users whether they’re online or not. Once an employee registers their smartphone, they gain access to their PCs via Hypr’s desktop client for Windows, macOS, and Linux. From Hypr’s web dashboard, admins can manage, provision, and deploy passwordless authentication policies across upwards of millions of users. The platform integrates with existing online fraud detection and risk engines, offering plugins and extensions for third-party identity and single sign-on providers, including Azure AD, Okta, and ForgeRock. And with an SDK, companies can integrate Hypr’s login flow with existing mobile and web apps. Hypr CEO George Avetisov notes that an estimated 300 billion passwords are used worldwide. This being the case, passwords remain the top cause of cybersecurity breaches — over 80% of which are due to compromised credentials. The pandemic and a swift transition to work-from-home arrangements expanded the attack surface for cybercriminals, highlighting the security risk, high costs, and user friction caused by passwords and password-based multifactor authentication. “It’s 2021 and passwords are still the top cause of breaches. And despite millions of dollars invested in multi-factor authentication (MFA), the whole world still relies on passwords,” Avetisov told VentureBeat via email. “As if the password problem wasn’t loud enough, the pandemic has turned the volume up to an 11. Businesses of all sizes have been dealing with credential reuse, fraud, phishing, and MFA bypass attacks at a scale never seen before. Just last year, the number of remote login attacks jumped more than 700%. Passwordless MFA is a new way to solve this problem that combines the security of strong encryption with the speed and convenience of the smartphone that users love.” Since announcing its previous funding round in October 2019, Hypr says its annual recurring revenue grew by more than 300%, driven by the doubling of the company’s customer base to brands including Norwegian Airlines, Point72, Rakuten, Otis Worldwide, Takeda Pharmaceuticals, CVS Health, Fiserv, and City National Bank. To date, Hypr has sold more than 100 million licenses, and it now serves organizations with up to “millions” of customers in more than 20 verticals, primarily financial services, energy, automotive, health care, education, and government. Hypr is headquartered in New York City, with teams in California, as well as Boston, London, and Tokyo."
https://venturebeat.com/2021/04/20/chargebee-boosts-subscription-and-recurring-revenue-tools-with-125m-round/,Chargebee boosts subscription and recurring revenue tools with $125M round,"Subscription and recurring billing tools provider Chargebee today said that it raised $125 million in series G funding co-led by Sapphire Ventures, Tiger Global, and Insight Venture Partners. With the fresh round of capital, which values Chargebee at $1.4 billion post-money, the company says that it plans to increase its investments in  expansion and partnerships, setting the stage for an IPO. The pandemic accelerated the shift to software-as-a-service (SaaS) and subscription-based business models, with companies expecting a 12% compound annual growth rate from recurring revenue over the next five years. By some estimates, 40% of ecommerce revenue comes from repeat purchasers — streaming music subscriptions alone generated $19.1 billion in 2018. But only 32% of online bills are made on a recurring basis, while the remaining 68% are one-time payments. Chargebee’s platform automates things like funneling users toward plans and collecting payment information, as well as executing upgrade or downgrade billing adjustments and facilitating recurring subscription renewals. For tasks it can’t handle automatically, Chargebee enlists human agents through customer relationship management dashboards from Zendesk, Salesforce, and NetSuite while guiding customers through a customizable checkout experience. Courtesy of integrations with Stripe, Braintree, WorldPay, and PayPal payment products, customers using Chargebee can pay with digital wallets like Amazon Pay and Apple Pay; with credit or debit cards; or directly through their bank accounts. Chargebee supports over 480 recurring billing use cases, with more than 20 payment gateways across over 50 countries. And the platform is available for upwards of 120 currencies and payment methods in dozens of languages. Chargebee features a range of pricing schemes including variable and usage-based pricing, and the platform is able to renew billing cycles based on sign-up or other dates. Chargebee can also selectively route payments and currencies in keeping with predefined rules. The platform’s optional Smart Dunning feature algorithmically susses out retry logic across days and times for failed payments. On Chargebee’s backend, managers get a visual customer organizational chart that allows them to define payment and invoicing responsibilities. They can also access templatized reports and KPI dashboards with metrics such as subscription revenue, discount, bad debt, and add-on metrics, all of which feed into accounting platforms like NetSuite, Intacct, and Xero. Teams receive real-time notifications if any tracked goals meet or exceed expectations or the team is in danger of falling behind. “Chargebee is the only solution in the space to include a ‘simulation engine’ as part of the product called TimeMachine. TimeMachine allows a business to simulate their subscription and billing configurations and analyze the potential response to external conditions,” a spokesperson told VentureBeat via email. “With capabilities like this businesses are able to assess the impact of their pricing, revenue recovery, and billing logic proactively. Chargebee also offers features like smart dunning capabilities which automate revenue recovery processes to maximize collections, while minimizing the cost of payment failures due to hard declines.” Chargebee competes with publicly traded Zuora and with ReCharge and Recurly, which create and maintain software subscription-based service solutions for businesses. But Chargebee is evidently doing something right — its customers include Okta, Freshworks, Calendly, and Study.com. And Chargebee claims to have a net retention rate exceeding $150. In fact, Chargebee claims to have the largest footprint of any revenue management provider in its segment, with businesses in 160 countries across North America, Europe, Asia, and Australia processing billions of dollars in revenue. The company’s user base has grown to more than 3,000 paying customers and over 20,000 companies across SaaS, direct-to-consumer ecommerce, over-the-top streaming, elearning, and publishing, and Chargebee says it is continuing to benefit from a “global surge” in subscription services deployments. A report from Zuora suggests 22% of companies have seen their subscription growth accelerate since the start of the pandemic, particularly in categories like video streaming and digital news and media. “The pandemic has opened up opportunities for us as a business.  Some of our customers have even been successful in pivoting their business models and offerings to the market, [and] we are proud to have enabled these shifts in decisions and support our customers through these turbulent times, only opening more doors of opportunity,” the spokesperson said. Chargebee’s latest investment round was led by Insight Venture Partners, with participation from existing investors. It brings Chargebee’s total funding to date to $230 million, following a $55 million round in October 2020."
https://venturebeat.com/2021/04/19/mastercard-bets-on-security-and-digital-identity-with-850m-ekata-deal/,Mastercard bets on security and digital identity with $850M Ekata deal,"(Reuters) — Mastercard said on Monday it had agreed to buy digital identity verification company Ekata in a deal valued at $850 million, as the global payments processor bets on a boom in demand for companies in the digital security space. Ekata’s products allow businesses to separate fraudsters from legitimate customers during digital interactions like opening an online account or making digital payments. It operates in three industries: ecommerce, payments, and financial services, according to its website. “The acceleration of online transactions has thrust global digital identity verification to the forefront as one of the biggest opportunities to build digital trust and combat global fraud,” Ekata CEO Rob Eleveld said in a statement. Seattle-headquartered Ekata counts more than 2,000 companies as its partners, including credit reporting company Equifax and software firm Intuit, its website showed. Its products, which include Ekata Identity Graph and Ekata Identity Network, allow companies to combat online fraud, it said. In February, Ekata said its revenue had surged in 2020, as the COVID-19 pandemic accelerated the adoption of ecommerce, boosting demand for services to safeguard against cyber fraud. Ekata added 300 new customers last year, including food and grocery-delivery startup Postmates, which was acquired by Uber Technologies last year. The payments processor said the deal is expected to close in the next six months, adding that it does not expect the deal to be a drag on its business for more than two years."
https://venturebeat.com/2021/04/19/cloud-strength-pushes-ibm-to-sales-growth-after-a-year-of-declines/,Cloud strength pushes IBM to sales growth after a year of declines,"(Reuters) — International Business Machines returned to sales growth in the first quarter after a year of declines and beat Wall Street targets on Monday, boosted by its bets in the high-margin cloud computing business. Shares of the Dow component, which have gained nearly 6% so far this year, were up more than 4% in extended trading. Finance chief James Kavanaugh said cloud spending by clients in retail, manufacturing, and travel industries in the United States was picking up after the initial pandemic-driven slump. Sales from its cloud computing services jumped 21% to $6.5 billion in the quarter. The 109-year-old firm is preparing to split itself into two public companies, with the namesake firm narrowing its focus on the so-called hybrid cloud, where it sees a $1 trillion market opportunity. Big Blue recorded a sales decline in global technology services, its largest unit, but that was largely offset by a rise in revenue in the remaining three units, including a surprise growth in the business that hosts mainframe computers. Mainframe saw strong traction from the financial services industry, where its banking clients shopped for more capacity as trading volumes soared during the retail trading frenzy, Kavanaugh said. Total revenue rose nearly 1% to $17.73 billion in the quarter, beating analysts’ average estimate of $17.35 billion, according to IBES data from Refinitiv. Net income fell to $955 million, or $1.06 per share, in the quarter ended March 31, from $1.18 billion, or $1.31 per share, a year earlier. Excluding items, the company earned $1.77 per share, beating market expectations of $1.63."
https://venturebeat.com/2021/04/19/the-ai-arms-race-has-us-on-the-road-to-armageddon/,The AI arms race has us on the road to Armageddon,"It’s now a given that countries worldwide are battling for AI supremacy. To date, most of the public discussion surrounding this competition has focused on commercial gains flowing from the technology. But the AI arms race for military applications is racing ahead as well, and concerned scientists, academics, and AI industry leaders have been sounding the alarm. Compared to existing military capabilities, AI-enabled technology can make decisions on the battlefield with mathematical speed and accuracy and never get tired. However, countries and organizations developing this tech are only just beginning to articulate ideas about how ethics will influence the wars of the near future. Clearly, the development of AI-enabled autonomous weapons systems will raise significant risks for instability and conflict escalation. However, calls to ban these weapons are unlikely to succeed. In an era of rising military tensions and risk, leading militaries worldwide are moving ahead with AI-enabled weapons and decision support, seeking leading-edge battlefield and security applications. The military potential of these weapons is substantial, but ethical concerns are largely being brushed aside. Already they are in use to guard ships against small boat attacks, search for terrorists, stand sentry, and destroy adversary air defenses. For now, the AI arms race is a cold war, mostly between the U.S., China, and Russia, but worries are it will become more than that. Driven by fear of other countries gaining the upper hand, the world’s military powers have been competing by leveraging AI for years — dating back at least to 1983 — to achieve an advantage in the balance of power. This continues today. Famously, Russian President Vladimir Putin has said the nation that leads in AI will be the “ruler of the world.” According to an article in Salon, diverse and ideologically-distinct research organizations including the Center for New American Security (CNAS), the Brookings Institution, and the Heritage Foundation have argued that America must ratchet up spending on AI research and development. A Foreign Affairs article argues that nations who fail to embrace leading technologies for the battlefield will lose their competitive advantage. Speaking about AI, former U.S. Defense Secretary Mark Esper said last year, “History informs us that those who are first to harness once-in-a-generation technologies often have a decisive advantage on the battlefield for years to come.” Indeed, leading militaries are investing heavily in AI, motivated by a desire to secure military operational advantages on the future battlefield. Civilian oversight committees, as well as militaries, have adopted this view. Last fall, a U.S. bipartisan congressional report called on the Defense Department to get more serious about accelerating AI and autonomous capabilities. Created by Congress, the National Security Commission on AI (NSCAI) recently urged an increase in AI R&D funding over the next few years to ensure the U.S. is able to maintain its tactical edge over its adversaries and achieve “military AI readiness” by 2025. In the future, warfare will pit “algorithm against algorithm,” claims the new NSCAI report. Although militaries have continued to compete using weapon systems similar to those of the 1980s, the NSCAI report claims: “the sources of battlefield advantage will shift from traditional factors like force size and levels of armaments to factors like superior data collection and assimilation, connectivity, computing power, algorithms, and system security.” It is possible that new AI-enabled weapons would render conventional forces near obsolete, with rows of decaying Abrams tanks gathering dust in the desert in much the same way as mothballed World War II ships lie off the coast of San Francisco. Speaking to reporters recently, Robert O. Work, vice chair of the NSCAI said of the international AI competition: “We have got … to take this competition seriously, and we need to win it.” Work to incorporate AI into the military is already far advanced. For example, militaries in the U.S., Russia, China, South Korea, the United Kingdom, Australia, Israel, Brazil, and Iran are developing cybersecurity applications, combat simulations, drone swarms, and other autonomous weapons.  Caption: The Russian Uran-9 is an armed robot.  Credit: Dmitriy Fomin via Wikimedia Commons. CC BY 2.0. A  recently completed “global information dominance exercise” by U.S. Northern Command pointed to the tremendous advantages the Defense Department can achieve by applying machine learning and artificial intelligence to all-domain information. The exercise integrated information from all domains including space, cyberspace, air, land, sea, and undersea, according to Air Force Gen. Glen D. VanHerck. Gilman Louie, a commissioner on the NSCAI report, is quoted in a news article saying: “I think it’s a mistake to think of this as an arms race” — though he added, “We don’t want to be second.” West Point has started training cadets to consider ethical issues when humans lose some control over the battlefield to smart machines. Along with the ethical and political issues of an AI arms race are the increased risks of triggering an accidental war. How might this happen? Any number of ways, from a misinterpreted drone strike to autonomous jet fighters with new algorithms. AI systems are trained on data and reflect the quality of that data along with any inherent biases and assumptions of those developing the algorithms. Gartner predicts through 2023, up to 10% of AI training data will be poisoned by benign or malicious actors. That is significant, especially considering the security vulnerability of critical systems. When it comes to bias, military applications of AI are presumably no different, except that the stakes are much higher than whether an applicant gets a good rate on car insurance. Writing in War on the Rocks, Rafael Loss and Joseph Johnson argue that military deterrence is an “extremely complex” problem — one that any AI hampered by a lack of good data will not likely be able to provide solutions for in the immediate future. How about assumptions? In 1983, the world’s superpowers drew near to accidental nuclear war, largely because the Soviet Union relied on software to make predictions that were based on false assumptions. Seemingly this could happen again, especially as AI increases the likelihood that humans would be taken out of decision making. It is an open question whether the risks of such a mistake are higher or lower with greater use of AI, but Star Trek had a vision in 1967 for how this could play out. The risks of conflict had escalated to such a degree in a “Taste of Armageddon” that war was outsourced to a computer simulation that decided who would perish.  Source: Star Trek, A Taste of Armageddon. There is no putting the genie back in the bottle. The AI arms race is well underway and leading militaries worldwide do not want to be in second place or worse. Where this will lead is subject to conjecture. Clearly, however, the wars of the future will be fought and determined by AI more than traditional “military might.” The ethical use of AI in these applications remains an open-ended issue. It was within the mandate of the NSCAI report to recommend restrictions on how the technology should be used, but this was unfortunately deferred to a later date. Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/04/19/on-the-hunt-for-a-new-job-then-you-need-to-check-these-roles-out/,On the hunt for a new job? Then you need to check these roles out,"There are so many incredible jobs available right now in the tech space, and we want you all to see them! We all deserve to have jobs we love, so we’re going to help you on your way. From Software engineering to designers, there are so many brilliant opportunities. We’ve compiled a list of some really exciting roles available at the moment. As a member of this team, you’ll be joining during exciting times as the company architects, designs, and engineers a distributed streaming pipeline to process billions of market data events each day while producing accurate and repeatable pricing. The successful candidate will take ownership for the full software development life-cycle, from understanding the needs of the business through to coding to deployment and maintenance. They will be thoughtful in testing and making sure robust systems are being developed from the ground up. The ideal candidate will have at least 5 years of software development experience in Java or Scala along with expertise in building high volume, high availability distributed systems. In this role, the successful candidate will work alongside a team of talented software developers that work to make the data engineering experience productive and high quality. The Data Engineering paved path is still taking shape, and Airbnb wants passionate engineers to develop this to support the entire company. They’re looking for someone who has 4+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks, along with a strong coding ability in one of the following — Scala, Java, Python. This role requires a multi-disciplined Senior Designer to join the Creative Team and execute a range of graphic assets across many areas of the business. Your design work will be viewed by millions of sports fans across North America on items such as homepage takeovers, static advertisements, landing pages, and print campaigns. You will also create assets for major partners across NFL, MLB, NBA, and NHL. You will work closely with Copywriters, Creative Directors, Project Managers, and other Designers in Fanduel’s New York office to ensure projects meet goals and objectives, while also maintaining a high level of quality and brand consistency. This role will have the ability to translate business and marketing objectives into designs that are clear, compelling, and engaging to drive the business forward. Outbrain is seeking a highly motivated, business oriented, self-reliant Product Analyst to join their BI group. In this role, you’ll serve as an analytics domain expert, work closely with the PM team and help drive the adoption, growth, and success of the products. If you’re excited to work in a cutting-edge big data environment, if you’re eager to expand a data-driven culture and drive actionable insights to meet changing business needs, this might be the role for you! The company wants someone who is resourceful, bright, proactive, works well independently and as part of a team, and who will be passionate about what she or he does. For this position, they’re looking for a person with exceptional analytical skills, and who is also a leader with the ability of self-learning and delivering results. As a full-stack engineer at 3Data, you will be responsible for building the future of the 3Data WebXR platform. You’ll work closely with other members of the 3Data Product team to reduce technical debt, ideate on potential features, and implement new features and bug fixes into the platform. Since they’re a startup, you’ll have the opportunity to directly impact the direction of this exciting platform, the creation of an inclusive culture, and will get in at the ground floor of a burgeoning industry. For even more exciting roles, head over to VentureBeat Jobs now."
https://venturebeat.com/2021/04/19/health-care-api-adoption-is-stymied-by-security-concerns-and-skills-gap/,Health care API adoption is slowed by security concerns and skills gap,"APIs may be the cornerstone of a digital transformation strategy, but the health care industry has not really benefited from widespread API adoption because of concerns over interoperability, patient data exchange, and infrastructure challenges, according to a new study. While 9 in 10 health care executives said APIs were important or mission-critical, only 24% of leading organizations currently use APIs, according to the study from Change Healthcare and Engine Group. One reason many health care organizations are not using APIs may be because the main driver for adoption has been regulatory compliance, Change Healthcare VP Gautam Shah told VentureBeat. In contrast, the primary driver in other industries was improving business processes and customer experience. Digital transformation in health care focuses on better patient care, differentiated patient and provider experiences, and increased efficiency. For health care organizations to see the kind of success enjoyed by industries such as banking, fintech, and retail, they need to address differences in API usage, misaligned priorities, and a skills gap. Health care providers (hospitals, health systems, and facilities) and payers (insurance companies, government programs) are not currently aligned on their priorities. Providers want to ensure the security of the data and its appropriate use, so security (52%) and cost (47%) are their biggest barriers to adoption. Payers want to get and operate on health care data in the fastest and most efficient manner possible, but they are hampered by a lack of high-performance technology infrastructure and standards. Payers in the study were concerned about technical infrastructure (45%), privacy (43%), and lack of industry standards (43%). There is also a disconnect between how providers and payers use APIs. Providers use APIs tactically, for eligibility verification and collecting payments. Payers are more strategic, using APIs for insights and engagements. The health care industry is undergoing a third wave of digital transformation, driven in part by changes mandated by the American Restoration and Recovery Act (2014), the 21st Century Cures Act (2016), and the Interoperability and Patient Access Rule (2020). Providers and payers need to implement electronic health record systems, improve data portability and interoperability, and enhance the use of data to make the cost and quality of care more visible and transparent. Making, supporting, and operating these technologies and models requires new or specialized skillsets. There is a steep learning curve because of the unique challenges associated with health care data and operations. There is also a skills gap, and 35% of payers and 29% of providers in the study said knowledge of how to create or use APIs was a barrier to adoption. As the industry consolidates around data interoperability, enhanced patient experiences, and operation experiences, there will be a shift to value-based care, increased consumerization, and adoption of digital technologies in health care, Shah said. New Fast Healthcare Interoperability Resources (FHIR) requirements take effect in July. Providers need to increase their adoption and use of FHIR APIs to power their digital and virtual patient experiences. Payer organizations will need to adopt FHIR APIs to enable greater data portability and transparency for their members. Digital Health companies will need to move quickly to embrace FHIR APIs to enable a wide range of applications, from wellness initiatives to chronic disease management, in order to meet patient and member expectations for richer applications driven by their data. Shah predicted, “Over the next two years, as APIs become more prevalent, as their usage matures, and as the data becomes more liquid, we stand to see real change and tangible benefit to patient care, patient and provider experience, payer operations, and ultimately to a health care system that we, our families and community, interact with daily.”"
https://venturebeat.com/2021/04/19/randori-probes-likely-attack-targets-for-cyberdefense/,Randori probes likely attack targets for cyberdefense,"Organizations spend a lot of time and money on penetration tests and “red team” exercises to identify which vulnerabilities attackers will use to get into the network and to figure out what the attackers will do afterward. Gaining insights into what attackers are most likely to do helps defenders adapt their security decisions appropriately, said Brian Hazzard, CEO and cofounder of security startup Randori. Randori provides red teaming-as-a-service via its Randori Attack Platform so enterprises can test their defenses with real exploits and attack techniques in a safe environment. The company’s Recon product helps organizations find vulnerabilities in their environments, while Attack tests real exploits against production systems to see how they would fare in a real attack. Randori’s Target Temptation engine, which launched last week, helps identify the assets attackers are most likely to target. The Target Temptation engine tells an organization how the attacker sees their infrastructure, which encompasses internet-accessible systems, as well as other services — including third-party services and tools. The assets are ranked by “attackability,” or the likelihood an attacker would want to try to compromise them. This is different from asset inventory, which provides the organization with an internal view of what it has, and vulnerability management, which identifies what is vulnerable. “For every 1,000 exposed assets, there is often only one that’s truly interesting to an attacker,” Hazzard told VentureBeat. It’s important to realize that asset management and attack surface management look at the infrastructure from different directions. “Customers have one view of their infrastructure, but the reality is that the attackers are seeing a totally different view,” Hazzard said. Attackers also care about their return on investment (ROI). They don’t want to waste time targeting systems that are well-defended or won’t lead to anything worth stealing. They look for published proofs-of-concept and exploits because that is cheaper than developing their own attack tools. They will also put more effort into targeting platforms that are widely used. Attackers typically don’t go where the organization is defending, Hazard said. Lionbridge, a company delivering AI-powered translation and localization solutions, uses Randori to help its security team prioritize which security alerts to work on. “First thing you want to do when you get your hands on a security program is to know what you have,” Lionbridge chief trust officer Doug Graham told VentureBeat. “It’s important to know, ‘What does the world see when they look at Lionbridge?'” There are certain properties attackers look for in a potential target: what useful information the attacker would be able to see about the target (enumerability), how valuable the asset is (criticality), whether there are any known vulnerabilities or published proofs of concept (weakness), how well-defended the asset is (post-exploitation), how long it would take to develop an exploit (research), and the ROI for doing so (applicability). “Things that make a software interesting are not always related to vulnerabilities,” Randori cofounder and CTO David Wolpoff told VentureBeat. This is what makes Target Temptation useful. Instead of defenders using severity scores to decide which vulnerability to fix or trying to figure out the firewall rules to protect all their internet-facing systems, organizations can focus on assets that are most likely to be compromised. “The combination helps [the organization] figure out what to do next,” Wolpoff said. Some targets are juicier than others because of where they lead. “I am always going to be interested in a VPN,” Wolpoff said, noting that he also pays attention to remote access technology, credential stores, and authentication systems. Compromising these types of components potentially opens up paths to go deeper into the network. “There is a thing in your perimeter that draws my interest. All things equal, you better have defenses around it,” Wolpoff said. Graham can see everything Randori found in Lionbridge’s environment, and any asset the security team doesn’t recognize is treated as a “risk event” to investigate. The unknown asset may be the result of an incomplete asset inventory, a case of shadow IT, or an unknown system set up by an attacker. Lionbridge found vulnerable assets shortly after signing on with Randori and was able to promptly address the issue, Graham said. The value of a platform like Randori should not be measured in terms of time saved or breaches prevented, but rather reducing attack surface, Graham said. “We measure our attack surface. What’s the vulnerability? What’s the target temptation? What is the priority?” he said. “Can I find a way to shrink that target?” Graham said he has three questions when hearing about a new vulnerability or an attack: “Do I have it [the affected system]? Is it vulnerable? And is it accessible to the internet?” The answers to those questions shape how he would respond to the executive team when they inevitably ask what is being done. “When [the CEO] sends me an email and he says, ‘What are we doing about this?’ I can say, ‘We know about it, and we’re not vulnerable’ or ‘We’re going to jump on it, and we’re going to solve it immediately,'” Graham said. For example, if a vulnerability becomes public, Randori would notify Lionbridge of the fact that it’s present in the company’s environment. But because Randori uses real exploits to test the environment to find weaknesses in the network and provides mitigation controls to fix those issues, the company may already have the controls in place and not actually be vulnerable. Or Randori may provide information about additional controls needed to address the issue. “The difference for me is between a ruined day chasing the latest brand-name vulnerability and just another routine day,” Graham said."
https://venturebeat.com/2021/04/19/meroxas-change-data-capture-service-works-with-apache-kafka-others/,"Meroxa’s change data capture service works with Apache Kafka, others","Meroxa has launched a platform-as-a-service (PaaS) environment with a control plane that leverages machine learning algorithms to manage real-time data. This comes after the company raised a fresh $15 million in its series A round. Meroxa has developed a PaaS platform through which IT organizations invoke a control plane that provides a change data capture service integrated with platforms such as Apache Kafka. That core capability is then extended through a set of rule engines that make it possible to automate repetitive engineering tasks, Meroxa CEO DeVaris Brown told VentureBeat. IT teams will be able to access that control plane via a visual interface or programmatically invoke it through a set of application programming interfaces (APIs) Meroxa has exposed. As organizations look to accelerate digital business transformation initiatives, many have discovered those projects require an ability to regularly shift massive amounts of data between the applications that enable a given process. This has necessitated hiring data engineers with the programming skills to orchestrate the movement of that data. Meroxa is making it possible for the average IT administrator or developer to now orchestrate data flows between applications. Beyond reducing the total cost of digital business initiatives, the rate at which those projects can be completed can now be significantly accelerated, Brown said. “Anybody can be a data engineer,” he said. Meroxa is applying many of the DevOps automation principles that were first applied to application development to the engineering of data pipelines. This has typically been viewed as an IT maintenance task that requires an individual to master the nuance of extract transform and load (ETL) tools. But people with data engineering skills are now among the most sought-after IT specialists, and the number of IT professionals with those skills is limited. The need to orchestrate data pipelines faster is becoming more acute because most digital business transformation initiatives typically require data to be processed and analyzed in near real time. Batch-oriented application processes are being replaced by applications that are capable of consuming stream data directly from platforms such as Kafka. That transition, however, will become extended if every organization needs to find, hire, and retain data engineering specialists. The ability to automate the constriction of data pipelines will also make it feasible for a larger number of organizations to successfully re-engineer processes. Many smaller organizations simply can’t afford to hire a dedicated data engineering specialist or contract an IT services firm to provide one. It may be a little while yet before IT teams are routinely creating data pipelines between applications and processes. However, the history of IT is littered with examples where the domain of an IT specialist has been subsumed into a function that can be handled by an IT generalist using some form of platform that automates a task. The engineering of data pipelines will ultimately be no different. In total, Meroxa has now raised $19.2 million in funding from investors that include Drive Capital, Root, Amplify, Hustle Fund, Village Global, Meritech Capital, Sequoia, Kleiner, Addition, Menlo, and Index Ventures. Other investors include former Heroku CEO Adam Gross; GitHub CTO Jason Warner; former Segment CTO Calvin French-Owen; and Nick Caldwell, VP of engineering at Twitter."
https://venturebeat.com/2021/04/19/zoom-boosts-its-apps-ecosystem-with-100m-fund/,Zoom boosts its app ecosystem with $100M venture fund,"Zoom has announced a new $100 million venture fund designed to “stimulate growth” of its burgeoning ecosystem of third-party app integrations. The announcement comes after a whirlwind 12 months for the video-communications platform company. Zoom has more than doubled in value over the past year, with businesses forced to embrace cloud-based tools as they rapidly transitioned to remote work. Back in October, Zoom launched its new Zoom Apps platform for third-party developers to integrate their apps into Zoom. This is designed to make it easier for teams to collaborate and access data while on video calls — integrations include everything from whiteboarding to cloud storage services. And this is essentially what the new $100 million fund will support. Zoom said it will invest between $250,000 and $2.5 million in growth-stage companies looking to develop tools and products that “will become core to how Zoom customers meet, communicate, and collaborate,” according to a statement. In many ways, Zoom is following the Salesforce playbook in terms of how it’s pushing to develop a vibrant ecosystem built around its core product — first through embracing third-party integrations and then through investing in them directly. Zoom has invested in startups before — in 2019 it backed hardware startup Neat — but this latest fund goes some way toward establishing Zoom as a more formal investor. To qualify for funding, companies must have a market-ready product with evidence of at least some early traction. Perhaps more importantly, their product must be focused on helping improve the Zoom experience in some way, either through Zoom Apps, SDKs, APIs, or even hardware products."
https://venturebeat.com/2021/04/19/affogata-which-helps-brands-manage-their-online-reputation-raises-5m/,"Affogata, which helps brands manage their online reputation, raises $5.5M","Customer intelligence platform Affogata today announced it has raised $5.5 million in series A funding co-led by Mangrove Capital Partners and PICO Venture Partners. Cofounder Sharel Omer said the funds will enable Affogata to invest in its tooling, strengthen its online presence, and support its customer service initiatives. No matter how exceptional a company’s customer service, meeting expectations is a challenge. According to American Express, the U.S. consumers’ board report shows more people talk about poor service than good experiences. Affogata, which was founded in 2019 by Omer, Ran Margaliot, and Itamar Rogel, offers AI-driven technology that provides clients with data related to their brand and competitors from forums, the web, and internal data sources. The platform lets stakeholders — including marketers, product managers, customer success teams, and data analysts — derive insights based on this data, enabling proactive actions to ostensibly ensure better customer experiences. The company surfaces what customers are searching for across various channels to provide a picture of preferences and trends, offering the ability to perform customer segmentation with automated competitor keyword and sentiment tracking. Affogata clients can receive alerts about sentiment changes and connect and engage with customers through the aforementioned channels. And they can keep track of reviews while controlling their messaging by removing bots and spammers with automated, AI-powered moderation.  For example, Affogata can highlight mentions of a company name in app store reviews, on Twitter and Facebook, in online communities such as Discord, and from internal systems like Zendesk and Wix Answers. Users can funnel these mentions into customized reports and dashboards or export the data for further analysis and ingestion by analytics and business intelligence systems. “The unique challenge faced by our platform is how to fuse data from structured, unstructured, and half-structured sources, coming from sources as diverse as support conversation, reviews, surveys, social media posts, and more,” Omer told VentureBeat via email. “This means we had to build a proprietary analysis pipeline, leveraging a broad set of AI approaches that we developed in-house, which is able to look at very different ‘signals’ from a vast array of sources. After those signals are taken into account, we then come up with a sensible, unified view of what customers care about, what warrants attention, and what is relevant to which stakeholder, and what requires immediate action.” Omer believes that there’s a vital need for brands like Wix, eToro, MyHeritage, Playtika, and Lemonade, which Affogata counts among its customers, to gain visibility and take action regarding sentiment. A company’s ability to implement customer feedback, monitor, and respond to online communities is increasingly a determinant in their success or failure, he claims. “Our technology shines when customers integrate it as a key part of their ops. For example, gaming companies have really short feedback loops — for example, overly aggressive game mechanics are introduced, leading to immediate churn and user complaints about ‘pay to play,'” Omer said. “Our tool helps them get instant feedback and adapt their games accordingly, reducing churn and increasing engagement, both directly contributing to their bottom-line revenues.” One study found that nearly 3 out of 4 consumers trust a company to a greater degree if the reviews for that company are positive. And according to research from Reputation X, consumers read an average of seven reviews before trusting a business. “The criteria for the insights we produce is whether they’re actionable. Our insights are relevant for customer success, product managers, marketing, developers, executives, crisis managers, security, and more,” Omer said. “That cross-organizational relevancy enables strategic initiatives that involve multiple teams at the company, giving them the power to utilize customer insights to execute and adjust initiatives.” Wix Capital and Fiverr Founder and CEO Micha Kaufman also participated in Affogata’s funding round — its first. the Tel Aviv, Israel-based company currently has 25 employees. Updated at 7:35 a.m. Pacific: This article has been updated to note that the funding round totaled $5.5 million, not $5 million, as was earlier reported. We regret the error."
https://venturebeat.com/2021/04/19/survey-finds-talent-gap-is-slowing-enterprise-ai-adoption/,Survey finds talent gap is slowing enterprise AI adoption,"AI’s popularity in the enterprise continues to grow, but practices and maturity remain stagnant as organizations run into obstacles while deploying AI systems. O’Reilly’s 2021 AI Adoption in the Enterprise report, which surveyed more than 3,500 business leaders, found that a lack of skilled people and difficulty hiring topped the list of challenges in AI, with 19% of respondents citing it as a “significant” barrier — revealing how persistent the talent gap might be. The findings agree with a recent KPMG survey that revealed a large number of organizations have increased their investments in AI to the point that executives are now concerned about moving too quickly. Indeed, Deloitte says 62% of respondents to its corporate October 2018 report adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of companies that have seen half their AI projects fail will tell you. The O’Reilly report suggests that the second-most significant barrier to AI adoption is a lack of quality data, with 18% of respondents saying their organization is only beginning to realize the importance of high-quality data. Interestingly, participants in Alation’s State of the Data Culture Report said the same, with a clear majority of employees (87%) pegging data quality issues as the reason their organizations failed to successfully implement AI. The percentage of respondents to O’Reilly’s survey who reported mature practices (26%) — that is, ones with revenue-bearing AI products — was roughly the same as in the last few years. The industry sector with the highest percentage of mature practices was retail, while education had the lowest percentage. Impediments to maturity ran the gamut but largely centered around a lack of institutional knowledge about machine learning modeling and data science (52%), understanding business use cases (49%), and data engineering (42%). Laments over the AI talent shortage in the U.S. have become a familiar refrain from private industry. According to a report by Chinese technology company Tencent, there are about 300,000 AI professionals worldwide but “millions” of roles available. In 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers globally working on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” And a 2019 Gartner survey found that 54% of chief information officers view this skills gap as the biggest challenge facing their organization. While higher education enrollment in AI-relevant fields like computer science has risen rapidly in recent years, few colleges have been able to meet student demand, due to a lack of staffing. There’s evidence to suggest the number of instructors is failing to keep pace with demand due to private sector poaching. From 2006 to 2014, the proportion of AI publications with a corporate-affiliated author increased from about 0% to 40%, reflecting the growing movement of researchers from academia to corporations. One curious trend highlighted in the survey was the share of organizations that say they’ve adopted supervised learning (82%) versus more cutting-edge techniques like self-supervised learning. Supervised learning entails training an AI model on a labeled dataset. By contrast, self-supervised learning generates labels from data by exposing relationships between the data’s parts, a step believed to be critical to achieving human-level intelligence. According to Gartner, supervised learning will remain the type of machine learning organizations leverage most through 2022. That’s because it’s effective in a number of business scenarios, including fraud detection, sales forecasting, and inventory optimization. For example, a model could be fed data from thousands of bank transactions, with each transaction labeled as fraudulent or not, and learn to identify patterns that led to a “fraudulent” or “not fraudulent” output. “In the past two years, the audience for AI has grown but hasn’t changed much: Roughly the same percentage consider themselves to be part of a ‘mature’ practice; the same industries are represented, and at roughly the same levels; and the geographical distribution of our respondents has changed little,” wrote Mike Loukides, O’Reilly VP of content strategy and the report’s author. “[For example,] relatively few respondents are using version control for data and models … Enterprise AI won’t really have matured until development and operations groups can engage in practices like continuous deployment; until results are repeatable (at least in a statistical sense); and until ethics, safety, privacy, and security are primary rather than secondary concerns.”"
https://venturebeat.com/2021/04/19/salesforce-launches-toolkits-and-guides-to-help-businesses-embrace-digitization/,Salesforce launches toolkits and guides to help businesses embrace digitization,"Salesforce today launched Digital 360 for Industries, a set of services built to help companies in market segments like consumer goods, financial services, nonprofit, and the public sector embrace digitization. With prebuilt templates, industry-specific developer toolkits, and customer guides, Salesforce says that Digital 360 is designed to enable businesses to overcome challenges involving data capture, systems integration, and compliance requirements. In a changing, all-digital pandemic and post-pandemic world, consumers are spending 54% more time on digital channels, according to Salesforce — creating major surges across all industries. This has required many businesses to change their operating models, even in industries historically reluctant to make the leap to digital. According to a recent Prophet report, digital transformation is still often perceived as a cost center, and data to prove return on investment remains hard to come by. Digital 360 for Industries aims to ease the transition with a library of webpage layouts, prebuilt portals, and integrations with other technologies. The components address use cases including emergency response management, insurance agents, licensing, permitting and inspections, nonprofit fundraising, and more. For example, the insurance agent template delivers a portal for independent agents to manage their list of clients. The template enables agents to view client policy claims, life events, business milestones, and other key moments to help keep agents organized and deepen client relationships. Digital 360 for Industries also includes the aforementioned toolkits, which contain code and app samples, documentation links, and best practice data for incorporating commerce into digital experiences. The toolkits come with guides for retail, discrete manufacturing, and retail banking that offer implementation blueprints, certifications, and industry best practices. Among other topics, the guides recommend ways to build ecommerce features for companies in the insurance, grocery, communications, and media segments. Digital 360 for Industries is Salesforce’s latest play in the growing digital transformation market, which is projected to grow from $469.8 billion in 2020 to $1 trillion  by 2025, according to Research & Markets. Based on a recent survey of 1,200 mid- to large-scale US companies, the average digital transformation budget was $14 million in 2018, and it is expected to increase. And digital transformation is expected to add $100 trillion to the world economy by 2025, with platform-driven interactions enabling two-thirds of value at stake from digitalization, a report by the World Economic Forum found. Salesforce says that clients including Big Brothers and Big Sisters of America, Deluxe, Fanalca, and Foodstuffs are already using Digital 360 for Industries to deliver goods and services online via tools, websites, and portals. “With changing consumer trends and business models as a result of the COVID-19 pandemic, Foodstuffs North Island, New Zealand’s biggest grocery distributor, was able to use digital channels to extend wholesale relationships,” Foodstuffs chief digital officer Simon Kennedy said in a press release. “We were able to launch a new consumer storefront in only a few weeks, pivoting fast with Salesforce tools for grocery and digital experience.” Digital 360 for Industries is generally available starting this week."
https://venturebeat.com/2021/04/19/the-democratization-of-esports-how-any-size-developer-can-reap-the-benefits-vb-live/,The democratization of esports: How any size developer can reap the benefits (VB Live),"Presented by Xsolla Right now, esports has a 495 million-strong global audience. In this VB Live event, you’ll learn how to leverage the opportunity for an engaged fanbase, including best practices on creating your own online esports and skill-based gaming platform and more. Register for free right here. Esports and competitive gaming isn’t limited to the large events and big cash prizes of an Unreal Tournament. It’s what Niccolo Maisto, co-founder and CEO of FACEIT, calls the ultimate representation of a healthy ecosystem of communities around the game — and it’s where the real opportunity of the esports boom lays. “It’s a strategic product and marketing direction that every game developer of any size should consider, to increase both engagement and reach for their game titles,” Maisto says. Offering a truly competitive experience not only unlocks a number of new sources for engagement and retention through the application of more powerful feedback and reward systems, but also stronger social network effects. When a game title invests in the development of a competitive community that’s healthy and open, players become part of something that goes beyond the in-game experience. The interactions within  this structured ecosystem, created by players, teams, communities, and tournament organizers, create endless storylines and user-generated content for a game title, he explains. “There are many examples of game titles in the market that are 10-plus years old, and have remained almost unchanged in their core game mechanics,” says Maisto, “but since they provide a solid competitive experience and a strong esports ecosystem, they keep generating engagement, growth, and revenues. Unlocking new and more powerful reward and feedback systems within the player experience generates higher levels of engagement and retention, and therefore higher monetization opportunities overall. In particular, no matter the size of a game, a true competitive experience is the most powerful way to effectively engage that cohort of hardcore players who typically tend to play more, spend more, and who tend to act as the backbone of a strong community around every game title. The organic creation and distribution of content of a competitive game title becomes a powerful activation and reactivation strategy for new and existing players with a low, almost null, marginal cost. As a title succeeds and increases its audience, events become the icing on the cake — a great way to not only engage your existing player base but also to accelerate organic growth through the exposure the game gets from the event. “Designing a great multiplayer game is always an incredible challenge that requires the perfect mix of game mechanics, physics, design, technology, psychology, and many other components that many talented game developers deal with every single day,” Maisto says. “Taking a great multiplayer game to the point at which it can become a successful competitive esports title adds even more layers of complexity.” That includes considerations like server latency, ranking systems, matchmaking balancing, tournaments formats, anti-cheat, moderation, skill systems features, and more. “My advice is to address all those considerations early on in the development lifecycle and be open to onboarding partners that can bring product and technical solutions that focus on those specific parts of the experience for the players,” he says. In part that’s because both the provisioning of a solid competitive experience and the development of the esport ecosystems require a set of products and technologies that are usually out of the scope of the typical game design and production cycle. “In the past, this was a luxury of a few large titles supported by deeper pockets,” Maisto says. “They were the only ones accessing those technologies, an issue that we decided to address as FACEIT.” Since 2012, the company has focused on building a platform to support game developers and their communities in creating competitive experiences, and developing stable esport ecosystems. The depth of integration between the FACEIT platform and a game title can vary, from the creation and development of competitive ecosystems within an experience that is entirely served outside the game client, or an integrated competitive engine inside the game client. Their most popular game on FACEIT is Counter-Strike: Global Offensive, which currently hosts nearly a million daily active users. From there, once a robust competitive experience is built, it’s a matter of supporting and empowering the creation of that ecosystem where content creators, teams, communities and players can interact with the game title to generate other forms of engagement through events and content generation, which drive more and more organic awareness for the game. “The activation of this ecosystem can take very different routes and it depends on multiple factors including the type of game, and the game-mode being played competitively,” he says. “However, providing the community with a set of accessible and effective tools to create and manage different types of competitive experiences, including tournaments, in-house leagues, and clans, is always a good way to start a snowball effect.” To learn more about the acquisition, engagement, and retention opportunities an esports strategy holds, how to design an effective platform for your game and more, don’t miss this VB Live event! Register for free here. Attendees will learn about: Speakers:"
https://venturebeat.com/2021/04/19/backup-firm-druva-protects-data-in-the-cloud-with-147m-in-new-funding/,Backup firm Druva protects data in the cloud with $147M in new funding,"Druva, a cloud data protection and backup startup based in Sunnyvale, California, today announced it has raised $147 million, pushing the company’s valuation to over $2 billion post-money. Druva says the capital will bolster a range of initiatives spanning product development and geographic expansion, as well as hiring, delivery, and customer support. Enterprises are managing nearly 40% more data than a year ago, and the stakes have arguably never been higher. Gartner predicts that at least 75% of IT operations will face one or more cyberattacks by 2025, and the University of Texas found 94% of companies suffering from a catastrophic data loss do not survive. Those statistics are more alarming in light of high-profile outages like that of OVHCloud earlier this year, an attack that took down 3.6 million websites ranging from government agencies to financial institutions and computer gaming companies. Druva, which was founded in 2008 by Jaspreet Singh, Milind Borate, and Ramani Kothandaraman, provides software-as-a-service-based data protection and management products for over 4,000 organizations, including Zoom, NASA, and Pfizer. In 2008, Singh, Borate, and Kothandaraman, who met working together at Veritas Software, formally launched Druva in Pune, India. (In Sanskrit, “druva” translates to “North Star.”) The company initially focused on providing management software to financial companies before shifting to general enterprise data management. In 2018, Druva acquired Letterkenny-based CloudRanger, a backup and disaster recovery company. The following year, Druva purchased CloudLanes to supplement its on-premises to cloud performance. Today, Druva offers services that aggregate enterprise data from endpoints, datacenters, and cloud workloads for backup and restore, disaster recovery, archival and retention, compliance monitoring, data forensics, and other uses. For example, Druva’s InSync product supports data backup on endpoint devices like laptops, smartphones, and tablets, in addition to platforms such as G Suite and Office 365. Druva Phoenix is the company’s solution for physical and virtual file servers, while Druva CloudRanger addresses Amazon Web Services  (AWS) environments and workloads. All of Druva’s offerings run on the Druva Cloud Platform, a cloud-native backup platform built on AWS that provides a centralized backup repository. Druva occupies a data backup and recovery market anticipated to be worth $11.59 billion by 2022, according to Markets and Markets. It competes to a degree with San Francisco-based Rubrik, which has raised hundreds of millions in venture capital for its live data access and recovery offerings. There’s also Cohesity and Clumio, which raked in $51 million for its cloud-hosted backup and recovery tools, as well as data recovery companies Veeam, Acronis, and HYCU. But Druva, which has over 800 employees, believes it can continue to stand out in a crowded field. In December 2019, the company surpassed $100 million in annual recurring revenue, and it claims to have grown since then, with a 26% uptick in a customer base of thousands of companies over the last year. In March, Druva crossed 2.5 billion annual backups, experiencing a 40% increase in daily backup activity over the last 12 months alone. Singh says the platform now performs over 7 million backups per day. “The global pandemic and unprecedented events of 2020 have ushered in a generational cloud transformation for businesses, with data’s increasing value at the heart of it,” Singh told VentureBeat via email. “Businesses today need a new approach to data protection, which can be deployed from anywhere, protect data everywhere, and securely scale on-demand. Only solutions built natively in the cloud are able to deliver all this functionality.” Caisse de dépôt et placement du Québec led Druva’s latest round of fundraising, with a significant investment by Neuberger Berman. It brings the company’s total raised to date to $475 million, following a $130 million series G round in June 2019."
https://venturebeat.com/2021/04/19/u-s-banks-deploy-visual-ai-tools-to-monitor-customers-and-workers/,U.S. banks deploy visual AI tools to monitor customers and workers,"(Reuters) — Several U.S. banks have started deploying camera software that can analyze customer preferences, monitor workers, and spot people sleeping near ATMs, even as the banks remain wary about possible backlash over increased surveillance, more than a dozen banking and technology sources told Reuters. Previously unreported trials at City National Bank of Florida and JPMorgan Chase, as well as earlier rollouts at banks such as Wells Fargo, offer a rare view into the potential U.S. financial institutions see in facial recognition and related artificial intelligence systems. Widespread deployment of such visual AI tools in the heavily regulated banking sector would be a significant step toward their becoming mainstream in corporate America. Bobby Dominguez, chief information security officer at City National, said smartphones that unlock via a face scan have paved the way. “We’re already leveraging facial recognition on mobile,” he said. “Why not leverage it in the real world?” City National will begin facial recognition trials early next year to identify customers at teller machines and employees at branches, aiming to replace clunky and less secure authentication measures at its 31 sites, Dominguez said. Eventually, the software could spot people on government watch lists, he said. JPMorgan said it is “conducting a small test of video analytic technology with a handful of branches in Ohio.” Wells Fargo said it works to prevent fraud but declined to discuss how. Civil liberties issues loom large. Critics point to arrests of innocent individuals following faulty facial matches, disproportionate use of the systems to monitor lower-income and non-white communities, and the loss of privacy inherent in ubiquitous surveillance. Portland, Oregon, as of January 1 banned businesses from using facial recognition “in places of public accommodation,” and drugstore chain Rite Aid shut down a nationwide face recognition program last year. Dominguez and other bank executives said their deployments are sensitive to the issues. “We’re never going to compromise our clients’ privacy,” Dominguez said. “We’re getting off to an early start on technology already used in other parts of the world and that is rapidly coming to the American banking network.” Still, the big question among banks, said Fredrik Nilsson, vice president of the Americas at Axis Communications, a top maker of surveillance cameras, is “what will be the potential backlash from the public if we roll this out?” Walter Connors, chief information officer at Brannen Bank, said the Florida company had discussed but not adopted the technology for its 12 locations. “Anybody walking into a branch expects to be recorded,” Connors said. “But when you’re talking about face recognition, that’s a larger conversation.” JPMorgan began assessing the potential of computer vision in 2019 by using internally developed software to analyze archived footage from Chase branches in New York and Ohio, where one of its two Innovation Labs is located, said two people — including former employee Neil Bhandar, who oversaw some of the effort at the time. Chase aims to gather data to better schedule staff and design branches, three people said, and the bank confirmed. Bhandar said some staff even went to one of Amazon’s cashierless convenience stores to learn about its computer vision system. Preliminary analysis by Bhandar of branch footage revealed more men would visit before or after lunch, while women tended to arrive mid-afternoon. Bhandar said he also wanted to analyze whether women avoided compact spaces in ATM lobbies because they might bump into someone, but the pandemic halted the plan. Testing facial recognition to identify clients as they walk into a Chase bank, if they consented to it, has been another possibility considered to enhance their experience, a current employee involved in innovation projects said. Chase would not be the first to evaluate those uses. A bank in the Northeast recently used computer vision to identify busy areas in branches with newer layouts, an executive there said, speaking on the condition the company not be named. A Midwestern credit union last year tested facial recognition for client identification at four locations before pausing over cost concerns, a source said. While Chase developed custom computer vision in-house using components from Google, IBM Watson, and Amazon Web Services, it also considered fully built systems from software startups AnyVision and Vintra, people including Bhandar said. AnyVision declined to comment, and Vintra did not respond to requests for comment. Chase said it ultimately chose a different vendor, which it declined to name, out of 11 options considered, and began testing that company’s technology at a handful of Ohio locations last October. The effort aims to identify transaction times, how many people leave because of long queues, and which activities are occupying workers. The bank added that facial, race, and gender recognition are not part of this test. Using technology to guess customers’ demographics can be problematic, some ethics experts say, because it reinforces stereotypes. Some computer vision programs are also less accurate on people of color, and critics have warned that could lead to unjust outcomes. Chase has weighed ethical questions. For instance, some internally called for reconsidering planned testing in Harlem, a historically Black neighborhood in New York, because it could be viewed as racially insensitive, two of the people said. The discussions emerged about the same time as a December 2019 New York Times article about racism at Chase branches in Arizona. Analyzing race was not part of the eventually tabled plans, and the Harlem branch had been selected because it housed the other Chase Innovation Lab for evaluating new technology, the people said, and the bank confirmed. Security uses for computer vision have long stirred banks’ interest. Wells Fargo used software from the company 3VR over a decade ago to review footage of crimes and see if any faces matched those of known offenders, said John Honovich, who worked at 3VR and founded video surveillance research organization IPVM. Identiv, which acquired 3VR in 2018, said banking sales were a major focus, but it declined to comment on Wells Fargo. A security executive at a mid-sized Southern bank, speaking on condition of anonymity to discuss secret measures, said over the last 18 months the bank has rolled out video analytics software at nearly every branch to generate alerts when doors to safes, computer server rooms, and other sensitive areas are left open. Outside, the bank monitors for loitering, such as the recurring issue of people setting up tents under the overhang for drive-through ATMs. Security staff at a control center can play an audio recording politely asking those people to leave, the executive said. The issue of people sleeping in enclosed ATM lobbies has long been an industry concern, said Brian Karas, vice president of sales at Airship Industries, which develops video management and analytics software. Systems that detected loitering so staff could activate a siren or strobe light helped increase ATM usage and reduce vandalism for several banks, he said. Though companies did not want to displace people seeking shelter, they felt this was necessary to make ATMs safe and accessible, Karas said. City National’s Dominguez said the bank’s branches use computer vision to detect suspicious activity outside. Sales records from 2010 and 2011 reviewed by Reuters show that Bank of America purchased iCVR cameras that were marketed at the time as helping organizations reduce loitering in ATM lobbies. Bank of America said it no longer uses iCVR technology. The Charlotte, North Carolina-based bank’s interest in computer vision has not abated. Its officials met with AnyVision on multiple occasions in 2019, including at a September conference during which the startup demonstrated how it could identify the face of a Bank of America executive, according to records of the presentation seen by Reuters and a person in attendance. The bank said, “We are always reviewing potential new technology solutions that are on the market.”"
https://venturebeat.com/2021/04/19/national-grid-partners-raises-150-million-to-invest-in-energy-and-information-crossovers/,National Grid Partners raises $150 million to invest in energy and tech crossovers,"National Grid Partners, the venture capital arm of a big utility provider in the northeastern U.S., has raised $150 million in fresh capital to invest in startups at the intersection of energy and information technology. The funding comes a couple of years after the Los Gatos, California-based investment division raised $250 million to try to disrupt the energy business. Of that amount, National Grid Partners has invested more than $227 million into 29 startups in about 30 months. Now the utility industry’s first Silicon Valley-based corporate venture and innovation group will invest even more money in the cause of energy transition amid greater overall investment activity. Just last week, Energy Transition Ventures raised $75 million to invest in getting the U.S. energy industry to move to renewable energy. Lisa Lambert, chief technology and innovation officer of National Grid and president of National Grid Partners, said in a statement that the new money is a vote of confidence from National Grid’s senior leadership. She noted that this week — which is Earth Week — National Grid is kicking off its sponsorship of the United Nations’ COP26 climate conference. COP26 is the biggest assembly of global environmental policy and industry leaders since the 2015 Paris Agreement. As part of its COP26 activities in the United Kingdom this fall, NGP plans to hold its first annual summit of the Next Grid Alliance. This consortium of more than 60 worldwide utility companies provides senior-level executives a platform to share industry best practices and solve some of the biggest challenges facing the energy sector. I asked NGP how it measures the success of its investments, and Lambert responded via email. “We measure ourselves on both strategic success and financial success,” she said. “Almost 78% of our portfolio companies are strategic — which means they have a proof-of-concept, pilot, or deployment with National Grid’s business units. These are technologies that are helping keep our networks more secure and reliable for customers, helping National Grid become more efficient and cost-effective, and helping onboard more clean energy to the grid.” She added, “On the financial side, we’ve seen significant valuation increases throughout the portfolio. With our exits, Pixeom and Aporeto, we saw an internal rate of return of over 150% for each company. And we expect more exits this year. Our top eight portfolio companies right now are expected to produce minimum combined proceeds of nearly half a billion dollars in the next one to three years. If you think about a venture capital fund’s lifecycle, it typically takes 10 years to return a fund. In another three years, we’ll likely have returned our initial $250 million fund twice over.” Lambert also announced today that NGP has invested $7.5M in two Silicon Valley companies that help enterprise customers protect physical and cyber infrastructure. These are: These companies will help National Grid serve its customers more safely and securely, Lambert said. While more than 70% of NGP’s portfolio companies have strategic engagements with National Grid — such as LineVision, which also joined the portfolio this month — seed stage startups like AccuKnox and Pathr reflect NGP’s plan to invest in every stage of the innovation ecosystem, Lambert said. This stage-agnostic approach includes NGP’s Incubation office in San Francisco and an Innovation team to turn ideas into prototypes that can be deployed in National Grid’s business units or spun out as standalone companies. NGP has led more than 60% of its startup investment rounds, with two merger and acquisition exits. Lambert spent 19 years at Intel Capital, investing in a variety of software and other kinds of tech companies. She was a leader in the company’s investments in diversity and was directly involved in the creation of a $125 million diversity fund at Intel. Asked how far along the company is in terms of disrupting the energy industry, Lambert said:"
https://venturebeat.com/2021/04/18/cambridge-quantum-pushes-nlp-quantum-computing-new-head-ai/,Cambridge Quantum pushes into NLP and quantum computing with new head of AI,"Cambridge Quantum Computing (CQC) hiring Stephen Clark as head of AI last week could be a sign the company is boosting research into ways quantum computing could be used for natural language processing. Quantum computing is still in its infancy but promises such significant results that dozens of companies are pursuing new quantum architectures. Researchers at technology giants such as IBM, Google, and Honeywell are making measured progress on demonstrating quantum supremacy for narrowly defined problems. Quantum computers with 50-100 qubits may be able to perform tasks that surpass the capabilities of today’s classical digital computers, “but noise in quantum gates will limit the size of quantum circuits that can be executed reliably,” California Institute of Technology theoretical physics professor John Preskill wrote in a recent paper. “We may feel confident that quantum technology will have a substantial impact on society in the decades ahead, but we cannot be nearly so confident about the commercial potential of quantum technology in the near term, say the next 5 to 10 years.” CQC has been selling software focused on specific use cases, such as in cybersecurity and pharmaceutical and drug delivery, as the hardware becomes available. “We are very different from the other quantum software companies that we are aware of, which are primarily focused on consulting-based revenues,” CQC CEO Ilyas Khan told VentureBeat. For example, amid concerns that improvements in quantum hardware will make it easier to break existing algorithms used in modern cryptography, CQC devised a method to generate quantum-resistant cryptographic keys that cannot be cracked by today’s methods. CQC partners with pharmaceutical and drug discovery companies to develop quantum algorithms for improving material discovery, such as working with Roche on drug development, Total on new materials for carbon capture and storage solutions, and CrownBio for novel cancer treatment biomarker discovery. The addition of Clark to CQC’s team signals the company will be shifting some of its research and development efforts toward quantum natural language processing (QNLP). Humans are good at composing meanings, but this process is not well understood. Recent research established that quantum computers, even with their current limitations, could learn to reason with the uncertainty that is part of real-world scenarios. “We do not know how we compose meaning, and therefore we have not been sure how this process can be carried over to machines/computers,” Khan said. QNLP could enable grammar-aware representation of language that makes sense of text at a deeper level than is currently available with state-of-the-art NLP algorithms like Bert and GPT 3.0. The company has already demonstrated some early success in representing and processing text using quantum computers, suggesting that QNLP is within reach. Clark was previously senior staff research scientist at DeepMind and led a team working on grounded language learning in virtual environments. He has a long history with CQC chief scientist Bob Coecke, with whom he collaborated 15 years ago to devise a novel approach for processing language. That research stalled due to the limitations of classical computers. Quantum computing could help address these bottlenecks, and there are plans to continue that research program, Clark said in a statement. “The methods we developed to demonstrate this could improve a broad range of applications where reasoning in complex systems and quantifying uncertainty are crucial, including medical diagnoses, fault-detection in mission-critical machines, and financial forecasting for investment management,” Khan said."
https://venturebeat.com/2021/04/18/is-boston-dynamics-becoming-boring-robotics-company/,Is Boston Dynamics becoming a boring robotics company?,"Boston Dynamics has made a name for itself through fascinating videos of biped and quadruped robots doing backflips, opening doors, and dancing to “Uptown Funk.” Now, it has revealed its latest gadget: a robot that looks like a huge overhead projector on wheels. It’s called Stretch, and it doesn’t do backflips, it doesn’t dance, and it’s made to do one task: moving boxes. It sounds pretty boring. But this could, in fact, become Boston Dynamics’ most successful commercial product and turn it into a profitable company.  Stretch has a box-like base with a set of wheels that can move in all directions. On top of the base are a large robotic arm and a perception mast. The robotic arm has seven degrees of freedom and a suction pad array that can grab and lift boxes. The perception mast uses computer vision–powered cameras and sensors to analyze its surroundings. While we have yet to see Stretch in action, according to information Boston Dynamics provided to the media, it can handle boxes weighing up to 23 kilograms and make 800 displacements per hour, and its battery can last eight hours. The video Boston Dynamics posted on its YouTube channel suggests the robot can reach the 800-cases-per-hour speed if everything remains static in its environment. Traditional industrial robots must be installed in a fixed location, which puts severe limits on the workflows and infrastructure of the warehouses where they are deployed. Stretch, on the other hand, is mobile and can be used in many different settings with little prerequisite beyond a flat ground and a little bit of training (we still don’t know how the training works). This could be a boon for many warehouses that don’t have automation equipment and infrastructure. As Boston Dynamics’ VP of business development Michael Perry told The Verge, “You can take this capability and you can move it into the back of the truck, you can move it into aisles, you can move it next to your conveyors. It all depends what the problem of the day is.” At first glance Stretch seems like a step back from the previous robots Boston Dynamics has created. It can’t navigate uneven terrain, climb stairs, jump on surfaces, open doors, and handle objects in complicated ways. It did manage to do some amusing feats on its intro video, but we can’t expect it to be as entertaining as Spot, Atlas, and Handle. But that’s exactly what real-world applications of robotics and artificial intelligence are all about. We still haven’t figured out how to create artificial general intelligence, the kind of AI that can mimic all aspects of the cognitive and physical abilities of humans and animals. Current AI systems are robust when performing narrow tasks in stable environments but start to break when they’re forced to tackle various problems in unpredictable settings. Therefore, the success of AI systems is to find the right balance between versatility and robustness, especially in physical settings where safety and material damage are major concerns. And Stretch exactly fits that description. It does a very specific task (picking up and displacing boxes) in a predictable environment (flat surfaces in warehouses). Stretch might sound boring in comparison to the other things that Boston Dynamics has done in the past. But if it lives up to its promise, it can directly result in reduced costs and improved production for many warehouses, which makes it a viable business model and product. As Perry told The Verge last June, “[A] lot of the most interesting stuff from a business perspective are things that people would find boring, like enabling the robot to read analogue gauges in an industrial facility. That’s not something that will set the internet on fire, but it’s transformative for a lot of businesses.” Boston Dynamics is not alone in working on autonomous mobile robots for warehouses and other industrial settings. There are dozens of companies competing in the field, ranging from longstanding companies such as Honeywell to startups such as Fetch Robotics. And unloading boxes is just one of the several physical tasks that are ripe for automation. There’s also a growing market for sorting robots, order-picking robots, and autonomous forklifts. What would make Boston Dynamics a successful contender in this competitive market? The way I see it, success in the industrial autonomous mobile robots market will be defined by versatility/robustness threshold on the one hand and cost efficiency on the other. In this respect, Boston Dynamics has two factors working to its advantage. First, Boston Dynamics will leverage its decades of experience to push the versatility of its robots without sacrificing their robustness and safety. Stretch has inherited technology and experience from Handle, Atlas, Spot, and other robots Boston Dynamics has developed. It also contains elements of Pick, a computer vision­–based depalletizing solution mentioned in the press release that declared Hyundai’s acquisition of Boston Dynamics. This can enable Stretch to work in a broader set of conditions than its competitors.  Second, the company’s new owner, Hyundai, is one of the leading companies in mobile robot research and development. Hyundai has already made extensive research in creating autonomous robots and vehicles that can navigate various environments and terrains. Hyundai also has a great manufacturing capacity. This will enable Boston Dynamics to reduce the costs of manufacturing Stretch and sell it at a competitive price. Hyundai’s manufacturing facilities will also enable Boston Dynamics to deliver new parts and props for Stretch at a cost-efficient price. This will further improve the versatility of the robot in the future and allow customers to repurpose it for new tasks without making large purchases. Stretch is the second commercial product of Boston Dynamics, the first one being the quadruped robot Spot. But Spot’s sales were only covering a fraction of the company’s costs, which were at least $150 million per year when Hyundai acquired it. Stretch has a greater potential for making Boston Dynamics a profitable company. How will the potential success of Stretch affect the future of Boston Dynamics? Here’s an observation I made last year after Hyundai acquired Boston Dynamics: “Boston Dynamics might claim to be a commercial company. But at heart, it is still an AI and robotics research lab. It has built its fame on its advanced research and a continuous stream of videos showing robots doing things that were previously thought impossible. The reality, however, is that real-world applications seldom use cutting-edge AI and robotics technology. Today’s businesses don’t have much use for dancing and backflipping robots. What they need are stable solutions that can integrate with their current software and hardware ecosystem, boost their operations, and cut costs.” How will Stretch’s success affect Boston Dynamics’ plans for humanlike robots? It’s hard to remain committed to long-term scientific goals when you’re owned by a commercial enterprise that counts profits by the quarter. But it’s not impossible. In the early 1900s, Albert Einstein worked as an assistant examiner at the Swiss patent office in Bern because physics research didn’t put food on his family’s table. But he remained a physicist at heart and continued his research in his idle time while his job as patent clerk paid the bills. His passion eventually paid off, earning him a Nobel prize and resulting in some of the greatest contributions to science in history. Will Stretch and its successors become the norm for Boston Dynamics, or is this the patent-clerk job that keeps the lights on while Boston Dynamics continues to chase the dream of humanoid robots that push the limits of science? This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/17/why-microsofts-new-ai-acquisition-is-a-big-deal/,Why Microsoft’s new AI acquisition is a big deal,"Microsoft’s recent shopping spree reached a new climax this week with the announcement of its $19.7 billion acquisition of Nuance, a company that provides speech recognition and conversational AI services. Nuance is best known for its deep learning voice transcription service, which is very popular in the health care sector. The two companies had already been working together closely before the acquisition. Nuance had built several of its products on top of Microsoft’s Azure cloud. And Microsoft had been using Nuance’s Dragon service in its Cloud for Healthcare solution, which launched last year in the midst of the pandemic. The acquisition is Microsoft’s biggest since the $26 billion purchase of LinkedIn. And it tells a lot about Microsoft’s AI strategy. Most of the focus in the announcement was on AI in health care, which makes sense because Nuance is a leading provider of AI services in the sector. “Nuance provides the AI layer at the health care point of delivery and is a pioneer in the real-world application of enterprise AI,” Microsoft CEO Satya Nadella said. “AI is technology’s most important priority, and health care is its most urgent application.”  One thing I like about Nuance is its laser focus, which is in line with the current limits and capabilities of deep learning algorithms. Deep learning might not be very good at general problem-solving or causal inference, but it can be extremely efficient at narrow tasks. Nuance has chosen one application (voice transcription) and has narrowed down its focus to one domain (clinical settings). This has enabled the company to train its machine learning models on tons of data in that specific field and make sure that its AI solutions have peak performance and reliability. Nuance has a series of AI products tailored for clinical settings, including a virtual assistant for electronic health records, a multi-party conversation transcription service, and a deep learning language model that converts clinical conversations into structured notes for integration into health records. Documentation is one of the main pain points for clinics and one of the lowest-hanging fruits for AI in health care. Nuance’s AI technology is helping save time and improve the patient experience. According to the acquisition announcement, Nuance’s AI solutions are currently used by more than 55% of physicians and 75% of radiologists in the U.S. and used in 77% of U.S. hospitals. The company has also seen a 37% year-over-year growth in the revenue of its cloud service, though that is probably due to the shifts caused by the COVID-19 pandemic. “The acquisition will double Microsoft’s total addressable market (TAM) in the health care provider space, bringing the company’s TAM in health care to nearly $500 billion,” according to Microsoft’s announcement. Nuance’s reach in the health care market suggests Microsoft will recoup its $19.7 billion investment in a relatively short term. But being able to address this market is not a simple feat. Other big tech companies, such as Apple and Google, already have health care initiatives that are much older than Microsoft’s. But Microsoft is especially well-positioned to take advantage of this new acquisition because of its business model. Google and Apple are consumer companies. Microsoft, on the other hand, gets most of its revenue from enterprise customers. Its Office suite and its collaboration tools were already being used in many hospitals even before it announced its health care solution. That’s why it was already in a good spot to penetrate the market. And if you look over at the Cloud for Healthcare page, the company has done a great job of integrating its health solution into tools that many health care workers were already used to working with, such as Outlook, Teams, Office, and messaging apps. The real advantage is the infrastructure Microsoft has built, the integration of all these services with clinical applications, and terrific data engineering that makes it possible to deploy machine learning models and data analytics tools that span various data sources. This is the perfect infrastructure on top of which Microsoft can build an AI factory, where it creates machine learning models that provide ways to improve existing products and build new ones. The acquisition will enable Microsoft to accelerate its growth by leveraging Nuance’s reach in the health care sector. Now every Nuance customer will also be a Microsoft customer. Before the acquisition, Microsoft was already using Nuance’s Dragon AI technology in its health care solution, transcribing virtual visits, taking notes, and integrating information into patients’ health records. Now, with the acquisition of Nuance, Microsoft will also have full access to its technology and will be able to take its new AI transcription power beyond health care. “Beyond health care, Nuance provides AI expertise and customer engagement solutions across Interactive Voice Response (IVR), virtual assistants, and digital and biometric solutions to companies around the world across all industries,” Microsoft says in its blog. It will be interesting to see how Nuance’s technology will be integrated into other Microsoft enterprise products. One thing that is also worth watching is how Microsoft will be able to combine Nuance’s AI with other technologies it’s experimenting with. For instance, Microsoft already has an exclusive license to OpenAI’s GPT-3 language model. Nuance’s transcription technology and GPT-3 might become a powerful combination for the enterprise. Microsoft might not be able to predict which company will be successful in five years’ time, especially in a field as volatile as AI. But it’s banking on the one constant that is always needed in the field: compute power. Microsoft uses its huge Azure platform to develop ties with companies, often providing them with subsidized access to its cloud-based machine learning tools. It also makes many of its investments in Azure credits, ensuring companies it invests in will be locked into its platform. This puts Microsoft in a position to both help those companies grow and learn from them. And the investment pays off when the company’s technology and business model mature. Earlier this year, I wrote about Microsoft’s investment in the self-driving car startup Cruise, which also made Microsoft Azure the preferred cloud of Cruise and its owner General Motors. I noted at the time that Microsoft’s success is in maintaining a safe distance from developing sectors. Instead of making one big acquisition, Microsoft casts a wide net by making smaller investments in several companies. This gives it a good foothold into many innovative sectors. As these sectors mature, Microsoft is gradually entering partnerships with the more successful startups. And when the time is right, it will acquire the company that gives it the best leverage in the market. We can see this exact cycle with Nuance as Microsoft evolved from being Nuance’s cloud provider to its partner to its owner. And this evolution tells us a lot about Microsoft’s AI strategy, which I think is very smart, given how fast things can change in the AI industry. The enterprise AI sector has come a long way toward creating applications that can solve real-world problems. But we still haven’t figured out many things. And as new technologies and companies continue to develop, Microsoft will be watching and picking winners. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/17/jensen-huang-interview-from-the-grace-cpu-to-engineers-metaverse-of-the-omniverse/,Nvidia CEO Jensen Huang interview: From the Grace CPU to engineer’s metaverse,"Nvidia CEO Jensen Huang delivered a keynote speech this week to 180,000 attendees registered for the GTC 21 online-only conference. And Huang dropped a bunch of news across multiple industries that show just how powerful Nvidia has become. In his talk, Huang described Nvidia’s work on the Omniverse, a version of the metaverse for engineers. The company is starting out with a focus on the enterprise market, and hundreds of enterprises are already supporting and using it. Nvidia has spent hundreds of millions of dollars on the project, which is based on 3D data-sharing standard Universal Scene Description, originally created by Pixar and later open-sourced. The Omniverse is a place where Nvidia can test self-driving cars that use its AI chips and where all sorts of industries will able to test and design products before they’re built in the physical world. Nvidia also unveiled its Grace central processing unit (CPU), an AI processor for datacenters based on the Arm architecture. Huang announced new DGX Station mini-sucomputers and said customers will be free to rent them as needed for smaller computing projects. And Nvidia unveiled its BlueField 3 data processing units (DPUs) for datacenter computing alongside new Atlan chips for self-driving cars. Here’s an edited transcript of Huang’s group interview with the press this week. I asked the first question, and other members of the press asked the rest. Huang talked about everything from what the Omniverse means for the game industry to Nvidia’s plans to acquire Arm for $40 billion. Jensen Huang: We had a great GTC. I hope you enjoyed the keynote and some of the talks. We had more than 180,000 registered attendees, 3 times larger than our largest-ever GTC. We had 1,600 talks from some amazing speakers and researchers and scientists. The talks covered a broad range of important topics, from AI [to] 5G, quantum computing, natural language understanding, recommender systems, the most important AI algorithm of our time, self-driving cars, health care, cybersecurity, robotics, edge IOT — the spectrum of topics was stunning. It was very exciting. Question: I know that the first version of Omniverse is for enterprise, but I’m curious about how you would get game developers to embrace this. Are you hoping or expecting that game developers will build their own versions of a metaverse in Omniverse and eventually try to host consumer metaverses inside Omniverse? Or do you see a different purpose when it’s specifically related to game developers? Huang: Game development is one of the most complex design pipelines in the world today. I predict that more things will be designed in the virtual world, many of them for games, than there will be designed in the physical world. They will be every bit as high quality and high fidelity, every bit as exquisite, but there will be more buildings, more cars, more boats, more coins, and all of them — there will be so much stuff designed in there. And it’s not designed to be a game prop. It’s designed to be a real product. For a lot of people, they’ll feel that it’s as real to them in the digital world as it is in the physical world. Omniverse enables game developers working across this complicated pipeline, first of all, to be able to connect. Someone doing rigging for the animation or someone doing textures or someone designing geometry or someone doing lighting, all of these different parts of the design pipeline are complicated. Now they have Omniverse to connect into. Everyone can see what everyone else is doing, rendering in a fidelity that is at the level of what everyone sees. Once the game is developed, they can run it in the Unreal engine that gets exported out. These worlds get run on all kinds of devices. Or Unity. But if someone wants to stream it right out of the cloud, they could do that with Omniverse, because it needs multiple GPUs, a fair amount of computation. That’s how I see it evolving. But within Omniverse, just the concept of designing virtual worlds for the game developers, it’s going to be a huge benefit to their work flow. Question: You announced that your current processors target high-performance computing with a special focus on AI. Do you see expanding this offering, developing this CPU line into other segments for computing on a larger scale in the market of datacenters? Huang: Grace is designed for applications, software that is data-driven. AI is software that writes software. To write that software, you need a lot of experience. It’s just like human intelligence. We need experience. The best way to get that experience is through a lot of data. You can also get it through simulation. For example, the Omniverse simulation system will run on Grace incredibly well. You could simulate — simulation is a form of imagination. You could learn from data. That’s a form of experience. Studying data to infer, to generalize that understanding and turn it into knowledge. That’s what Grace is designed for, these large systems for very important new forms of software, data-driven software. As a policy, or not a policy, but as a philosophy, we tend not to do anything unless the world needs us to do it and it doesn’t exist. When you look at the Grace architecture, it’s unique. It doesn’t look like anything out there. It solves a problem that didn’t used to exist. It’s an opportunity and a market, a way of doing computing that didn’t exist 20 years ago. It’s sensible to imagine that CPUs that were architected and system architectures that were designed 20 years ago wouldn’t address this new application space. We’ll tend to focus on areas where it didn’t exist before. It’s a new class of problem, and the world needs to do it. We’ll focus on that. Otherwise, we have excellent partnerships with Intel and AMD. We work very closely with them in the PC industry, in the datacenter, in hyperscale, in supercomputing. We work closely with some exciting new partners. Ampere Computing is doing a great ARM CPU. Marvell is incredible at the edge, 5G systems and I/O systems and storage systems. They’re fantastic there, and we’ll partner with them. We partner with Mediatek, the largest SOC company in the world. These are all companies who have brought great products. Our strategy is to support them. Our philosophy is to support them. By connecting our platform, Nvidia AI or Nvidia RTX, our raytracing platform, with Omniverse and all of our platform technologies to their CPUs, we can expand the overall market. That’s our basic approach. We only focus on building things that the world doesn’t have. Question: I wanted to follow up on the last question regarding Grace and its use. Does this signal Nvidia’s perhaps ambitions in the CPU space beyond the datacenter? I know you said you’re looking for things that the world doesn’t have yet. Obviously, working with ARM chips in the datacenter space leads to the question of whether we’ll see a commercial version of an Nvidia CPU in the future. Huang: Our platforms are open. When we build our platforms, we create one version of it. For example, DGX. DGX is fully integrated. It’s bespoke. It has an architecture that’s very specifically Nvidia. It was designed — the first customer was Nvidia researchers. We have a couple billion dollars’ worth of infrastructure our AI researchers are using to develop products and pretrain models and do AI research and self-driving cars. We built DGX primarily to solve a problem we had. Therefore it’s completely bespoke. We take all of the building blocks, and we open it. We open our computing platform in three layers: the hardware layer, chips and systems; the middleware layer, which is Nvidia AI, Nvidia Omniverse, and it’s open; and the top layer, which is pretrained models, AI skills, like driving skills, speaking skills, recommendation skills, pick and play skills, and so on. We create it vertically, but we architect it and think about it and build it in a way that’s intended for the entire industry to be able to use however they see fit. Grace will be commercial in the same way, just like Nvidia GPUs are commercial. With respect to its future, our primary preference is that we don’t build something. Our primary preference is that if somebody else is building it, we’re delighted to use it. That allows us to spare our critical resources in the company and focus on advancing the industry in a way that’s rather unique. Advancing the industry in a way that nobody else does. We try to get a sense of where people are going, and if they’re doing a fantastic job at it, we’d rather work with them to bring Nvidia technology to new markets or expand our combined markets together. The ARM license, as you mentioned — acquiring ARM is a very similar approach to the way we think about all of computing. It’s an open platform. We sell our chips. We license our software. We put everything out there for the ecosystem to be able to build bespoke, their own versions of it, differentiated versions of it. We love the open platform approach. Question: Can you explain what made Nvidia decide that this datacenter chip was needed right now? Everybody else has datacenter chips out there. You’ve never done this before. How is it different from Intel, AMD, and other datacenter CPUs? Could this cause problems for Nvidia partnerships with those companies, because this puts you in direct competition? Huang: The answer to the last part — I’ll work my way to the beginning of your question. But I don’t believe so. Companies have leadership that are a lot more mature than maybe given credit for. We compete with the ARM GPUs. On the other hand, we use their CPUs in DGX. Literally, our own product. We buy their CPUs to integrate into our own product — arguably our most important product. We work with the whole semiconductor industry to design their chips into our reference platforms. We work hand in hand with Intel on RTX gaming notebooks. There are almost 80 notebooks we worked on together this season. We advance industry standards together. A lot of collaboration. Back to why we designed the datacenter CPU, we didn’t think about it that way. The way Nvidia tends to think is we say, “What is a problem that is worthwhile to solve, that nobody in the world is solving and we’re suited to go solve that problem and if we solve that problem it would be a benefit to the industry and the world?” We ask questions literally like that. The philosophy of the company, in leading through that set of questions, finds us solving problems only we will, or only we can, that have never been solved before. The outcome of trying to create a system that can train AI models, language models, that are gigantic, learn from multi-modal data, that would take less than three months — right now, even on a giant supercomputer, it takes months to train 1 trillion parameters. The world would like to train 100 trillion parameters on multi-modal data, looking at video and text at the same time. The journey there is not going to happen by using today’s architecture and making it bigger. It’s just too inefficient. We created something that is designed from the ground up to solve this class of interesting problems. Now this class of interesting problems didn’t exist 20 years ago, as I mentioned, or even 10 or five years ago. And yet this class of problems is important to the future. AI that’s conversational, that understands language, that can be adapted and pretrained to different domains, what could be more important? It could be the ultimate AI. We came to the conclusion that hundreds of companies are going to need giant systems to pretrain these models and adapt them. It could be thousands of companies. But it wasn’t solvable before. When you have to do computing for three years to find a solution, you’ll never have that solution. If you can do that in weeks, that changes everything. That’s how we think about these things. Grace is designed for giant-scale data-driven software development, whether it’s for science or AI or just data processing. Question: You’re proposing a software library for quantum computing. Are you working on hardware components as well? Huang: We’re not building a quantum computer. We’re building an SDK for quantum circuit simulation. We’re doing that because in order to invent, to research the future of computing, you need the fastest computer in the world to do that. Quantum computers, as you know, are able to simulate exponential complexity problems, which means that you’re going to need a really large computer very quickly. The size of the simulations you’re able to do to verify the results of the research you’re doing to do development of algorithms so you can run them on a quantum computer someday, to discover algorithms — at the moment, there aren’t that many algorithms you can run on a quantum computer that prove to be useful. Grover’s is one of them. Shore’s is another. There are some examples in quantum chemistry. We give the industry a platform by which to do quantum computing research in systems, in circuits, in algorithms, and in the meantime, in the next 15-20 years, while all of this research is happening, we have the benefit of taking the same SDKs, the same computers, to help quantum chemists do simulations much more quickly. We could put the algorithms to use even today. And then last, quantum computers, as you know, have incredible exponential complexity computational capability. However, it has extreme I/O limitations. You communicate with it through microwaves, through lasers. The amount of data you can move in and out of that computer is very limited. There needs to be a classical computer that sits next to a quantum computer, the quantum accelerator if you can call it that, that pre-processes the data and does the post-processing of the data in chunks, in such a way that the classical computer sitting next to the quantum computer is going to be super fast. The answer is fairly sensible, that the classical computer will likely be a GPU-accelerated computer. There are lots of reasons we’re doing this. There are 60 research institutes around the world. We can work with every one of them through our approach. We intend to. We can help every one of them advance their research. Question: So many workers have moved to work from home, and we’ve seen a huge increase in cybercrime. Has that changed the way AI is used by companies like yours to provide defenses? Are you worried about these technologies in the hands of bad actors who can commit more sophisticated and damaging crimes? Also, I’d love to hear your thoughts broadly on what it will take to solve the chip shortage problem on a lasting global basis. Huang: The best way is to democratize the technology, in order to enable all of society, which is vastly good, and to put great technology in their hands so that they can use the same technology, and ideally superior technology, to stay safe. You’re right that security is a real concern today. The reason for that is because of virtualization and cloud computing. Security has become a real challenge for companies because every computer inside your datacenter is now exposed to the outside. In the past, the doors to the datacenter were exposed, but once you came into the company, you were an employee, or you could only get in through VPN. Now, with cloud computing, everything is exposed. The other reason why the datacenter is exposed is because the applications are now aggregated. It used to be that the applications would run monolithically in a container, in one computer. Now the applications for scaled out architectures, for good reasons, have been turned into micro-services that scale out across the whole datacenter. The micro-services are communicating with each other through network protocols. Wherever there’s network traffic, there’s an opportunity to intercept. Now the datacenter has billions of ports, billions of virtual active ports. They’re all attack surfaces. The answer is you have to do security at the node. You have to start it at the node. That’s one of the reasons why our work with BlueField is so exciting to us. Because it’s a network chip, it’s already in the computer node, and because we invented a way to put high-speed AI processing in an enterprise datacenter — it’s called EGX — with BlueField on one end and EGX on the other, that’s a framework for security companies to build AI. Whether it’s a Check Point or a Fortinet or Palo Alto Networks, and the list goes on, they can now develop software that runs on the chips we build, the computers we build. As a result, every single packet in the datacenter can be monitored. You would inspect every packet, break it down, turn it into tokens or words, read it using natural language understanding, which we talked about a second ago — the natural language understanding would determine whether there’s a particular action that’s needed, a security action needed, and send the security action request back to BlueField. This is all happening in real time, continuously, and there’s just no way to do this in the cloud because you would have to move way too much data to the cloud. There’s no way to do this on the CPU because it takes too much energy, too much compute load. People don’t do it. I don’t think people are confused about what needs to be done. They just don’t do it because it’s not practical. But now, with BlueField and EGX, it’s practical and doable. The technology exists. The second question has to do with chip supply. The industry is caught by a couple of dynamics. Of course one of the dynamics is COVID exposing, if you will, a weakness in the supply chain of the automotive industry, which has two main components it builds into cars. Those main components go through various supply chains, so their supply chain is super complicated. When it shut down abruptly because of COVID, the recovery process was far more complicated, the restart process, than anybody expected. You could imagine it, because the supply chain is so complicated. It’s very clear that cars could be rearchitected, and instead of thousands of components, it wants to be a few centralized components. You can keep your eyes on four things a lot better than a thousand things in different places. That’s one factor. The other factor is a technology dynamic. It’s been expressed in a lot of different ways, but the technology dynamic is basically that we’re aggregating computing into the cloud, and into datacenters. What used to be a whole bunch of electronic devices — we can now virtualize it, put it in the cloud, and remotely do computing. All the dynamics we were just talking about that have created a security challenge for datacenters, that’s also the reason why these chips are so large. When you can put computing in the datacenter, the chips can be as large as you want. The datacenter is big, a lot bigger than your pocket. Because it can be aggregated and shared with so many people, it’s driving the adoption, driving the pendulum toward very large chips that are very advanced, versus a lot of small chips that are less advanced. All of a sudden, the world’s balance of semiconductor consumption tipped toward the most advanced of computing. The industry now recognizes this, and surely the world’s largest semiconductor companies recognize this. They’ll build out the necessary capacity. I doubt it will be a real issue in two years because smart people now understand what the problems are and how to address them. Question: I’d like to know more about what clients and industries Nvidia expects to reach with Grace, and what you think is the size of the market for high-performance datacenter CPUs for AI and advanced computing. Huang: I’m going to start with I don’t know. But I can give you my intuition. 30 years ago, my investors asked me how big the 3D graphics was going to be. I told them I didn’t know. However, my intuition was that the killer app would be video games, and the PC would become — at the time the PC didn’t even have sound. You didn’t have LCDs. There was no CD-ROM. There was no internet. I said, “The PC is going to become a consumer product. It’s very likely that the new application that will be made possible, that wasn’t possible before, is going to be a consumer product like video games.” They said, “How big is that market going to be?” I said, “I think every human is going to be a gamer.” I said that about 30 years ago. I’m working toward being right. It’s surely happening. Ten years ago someone asked me, “Why are you doing all this stuff in deep learning? Who cares about detecting cats?” But it’s not about detecting cats. At the time I was trying to detect red Ferraris, as well. It did it fairly well. But anyway, it wasn’t about detecting things. This was a fundamentally new way of developing software. By developing software this way, using networks that are deep, which allows you to capture very high dimensionality, it’s the universal function approximator. If you gave me that, I could use it to predict Newton’s law. I could use it to predict anything you wanted to predict, given enough data. We invested tens of billions behind that intuition, and I think that intuition has proven right. I believe that there’s a new scale of computer that needs to be built, that needs to learn from basically Earth-scale amounts of data. You’ll have sensors that will be connected to everywhere on the planet, and we’ll use them to predict climate, to create a digital twin of Earth. It’ll be able to predict weather everywhere, anywhere, down to a square meter, because it’s learned the physics and all the geometry of the Earth. It’s learned all of these algorithms. We could do that for natural language understanding, which is extremely complex and changing all the time. The thing people don’t realize about language is it’s evolving continuously. Therefore, whatever AI model you use to understand language is obsolete tomorrow, because of decay, what people call model drift. You’re continuously learning and drifting, if you will, with society. There’s some very large data-driven science that needs to be done. How many people need language models? Language is thought. Thought is humanity’s ultimate technology. There are so many different versions of it, different cultures and languages and technology domains. How people talk in retail, in fashion, in insurance, in financial services, in law, in the chip industry, in the software industry. They’re all different. We have to train and adapt models for every one of those. How many versions of those? Let’s see. Take 70 languages, multiply by 100 industries that need to use giant systems to train on data forever. That’s maybe an intuition, just to give a sense of my intuition about it. My sense is that it will be a very large new market, just as GPUs were once a zero billion dollar market. That’s Nvidia’s style. We tend to go after zero billion dollar markets, because that’s how we make a contribution to the industry. That’s how we invent the future. Question: Are you still confident that the ARM deal will gain approval by close? With the announcement of Grace and all the other ARM-relevant partnerships you have in development, how important is the ARM acquisition to the company’s goals, and what do you get from owning ARM that you don’t get from licensing? Huang: ARM and Nvidia are independently and separately excellent businesses, as you know well. We will continue to have excellent separate businesses as we go through this process. However, together we can do many things, and I’ll come back to that. To the beginning of your question, I’m very confident that the regulators will see the wisdom of the transaction. It will provide a surge of innovation. It will create new options for the marketplace. It will allow ARM to be expanded into markets that otherwise are difficult for them to reach themselves. Like many of the partnerships I announced, those are all things bringing AI to the ARM ecosystem, bringing Nvidia’s accelerated computing platform to the ARM ecosystem — it’s something only we and a bunch of computing companies working together can do. The regulators will see the wisdom of it, and our discussions with them are as expected and constructive. I’m confident that we’ll still get the deal done in 2022, which is when we expected it in the first place, about 18 months. With respect to what we can do together, I demonstrated one example, an early example, at GTC. We announced partnerships with Amazon to combine the Graviton architecture with Nvidia’s GPU architecture to bring modern AI and modern cloud computing to the cloud for ARM. We did that for Ampere computing, for scientific computing, AI in scientific computing. We announced it for Marvell, for edge and cloud platforms and 5G platforms. And then we announced it for Mediatek. These are things that will take a long time to do, and as one company we’ll be able to do it a lot better. The combination will enhance both of our businesses. On the one hand, it expands ARM into new computing platforms that otherwise would be difficult. On the other hand, it expands Nvidia’s AI platform into the ARM ecosystem, which is underexposed to Nvidia’s AI and accelerated computing platform. Question: I covered Atlan a little more than the other pieces you announced. We don’t really know the node side, but the node side below 10nm is being made in Asia. Will it be something that other countries adopt around the world, in the West? It raises a question for me about the long-term chip supply and the trade issues between China and the United States. Because Atlan seems to be so important to Nvidia, how do you project that down the road, in 2025 and beyond? Are things going to be handled, or not? Huang: I have every confidence that it will not be an issue. The reason for that is because Nvidia qualifies and works with all of the major foundries. Whatever is necessary to do, we’ll do it when the time comes. A company of our scale and our resources, we can surely adapt our supply chain to make our technology available to customers that use it. Question: In reference to BlueField 3, and BlueField 2 for that matter, you presented a strong proposition in terms of offloading workloads, but could you provide some context into what markets you expect this to take off in, both right now and going into the future? On top of that, what barriers to adoption remain in the market? Huang: I’m going to go out on a limb and make a prediction and work backward. Number one, every single datacenter in the world will have an infrastructure computing platform that is isolated from the application platform in five years. Whether it’s five or 10, hard to say, but anyway, it’s going to be complete, and for very logical reasons. The application that’s where the intruder is, you don’t want the intruder to be in a control mode. You want the two to be isolated. By doing this, by creating something like BlueField, we have the ability to isolate. Second, the processing necessary for the infrastructure stack that is software-defined — the networking, as I mentioned, the east-west traffic in the datacenter, is off the charts. You’re going to have to inspect every single packet now. The east-west traffic in the data center, the packet inspection, is going to be off the charts. You can’t put that on the CPU because it’s been isolated onto a BlueField. You want to do that on BlueField. The amount of computation you’ll have to accelerate onto an infrastructure computing platform is quite significant, and it’s going to get done. It’s going to get done because it’s the best way to achieve zero trust. It’s the best way that we know of, that the industry knows of, to move to the future where the attack surface is basically zero, and yet every datacenter is virtualized in the cloud. That journey requires a reinvention of the datacenter, and that’s what BlueField does. Every datacenter will be outfitted with something like BlueField. I believe that every single edge device will be a datacenter. For example, the 5G edge will be a datacenter. Every cell tower will be a datacenter. It’ll run applications, AI applications. These AI applications could be hosting a service for a client or they could be doing AI processing to optimize radio beams and strength as the geometry in the environment changes. When traffic changes and the beam changes, the beam focus changes, all of that optimization, incredibly complex algorithms, wants to be done with AI. Every base station is going to be a cloud native, orchestrated, self-optimizing sensor. Software developers will be programming it all the time. Every single car will be a datacenter. Every car, truck, shuttle will be a datacenter. Every one of those datacenters, the application plane, which is the self-driving car plane, and the control plane, that will be isolated. It’ll be secure. It’ll be functionally safe. You need something like BlueField. I believe that every single edge instance of computing, whether it’s in a warehouse, a factory — how could you have a several-billion-dollar factory with robots moving around and that factory is literally sitting there and not have it be completely tamper-proof? Out of the question, absolutely. That factory will be built like a secure datacenter. Again, BlueField will be there. Everywhere on the edge, including autonomous machines and robotics, every datacenter, enterprise or cloud, the control plane and the application plane will be isolated. I promise you that. Now the question is, “How do you go about doing it? What’s the obstacle?” Software. We have to port the software. There’s two pieces of software, really, that need to get done. It’s a heavy lift, but we’ve been lifting it for years. One piece is for 80% of the world’s enterprise. They all run VMware vSphere software-defined datacenter. You saw our partnership with VMware, where we’re going to take vSphere stack — we have this, and it’s in the process of going into production now, going to market now … taking vSphere and offloading it, accelerating it, isolating it from the application plane. Number two, for everybody else out at the edge, the telco edge, with Red Hat, we announced a partnership with them, and they’re doing the same thing. Third, for all the cloud service providers who have bespoke software, we created an SDK called DOCA 1.0. It’s released to production, announced at GTC. With this SDK, everyone can program the BlueField, and by using DOCA 1.0, everything they do on BlueField runs on BlueField 3 and BlueField 4. I announced the architecture for all three of those will be compatible with DOCA. Now the software developers know the work they do will be leveraged across a very large footprint, and it will be protected for decades to come. We had a great GTC. At the highest level, the way to think about that is the work we’re doing is all focused on driving some of the fundamental dynamics happening in the industry. Your questions centered around that, and that’s fantastic. There are five dynamics highlighted during GTC. One of them is accelerated computing as a path forward. It’s the approach we pioneered three decades ago, the approach we strongly believe in. It’s able to solve some challenges for computing that are now front of mind for everyone. The limits of CPUs and their ability to scale to reach some of the problems we’d like to address are facing us. Accelerated computing is the path forward. Second, to be mindful about the power of AI that we all are excited about. We have to realize that it’s a software that is writing software. The computing method is different. On the other hand, it creates incredible new opportunities. Thinking about the datacenter not just as a big room with computers and network and security appliances, but thinking of the entire datacenter as one computing unit. The datacenter is the new computing unit. 5G is super exciting to me. Commercial 5G, consumer 5G is exciting. However, it’s incredibly exciting to look at private 5G, for all the applications we just looked at. AI on 5G is going to bring the smartphone moment to agriculture, to logistics, to manufacturing. You can see how excited BMW is about the technologies we’ve put together that allow them to revolutionize the way they do manufacturing, to become much more of a technology company going forward. Last, the era of robotics is here. We’re going to see some very rapid advances in robotics. One of the critical needs of developing robotics and training robotics, because they can’t be trained in the physical world while they’re still clumsy — we need to give it a virtual world where it can learn how to be a robot. These virtual worlds will be so realistic that they’ll become the digital twins of where the robot goes into production. We spoke about the digital twin vision. PTC is a great example of a company that also sees the vision of this. This is going to be a realization of a vision that’s been talked about for some time. The digital twin idea will be made possible because of technologies that have emerged out of gaming. Gaming and scientific computing have fused together into what we call Omniverse."
https://venturebeat.com/2021/04/17/the-sase-wave-why-cloud-native-edge-security-is-gathering-huge-momentum/,The SASE wave: Why cloud-native edge security is gathering huge momentum,"Secure Access Service Edge (SASE), commonly pronounced “sassy,” is less than two years old and is already moving the needle when it comes to forging a new market. SASE brings network and security capabilities to the edge, making it possible for distributed workforces to access corporate applications and resources with the same ease and security as they would have at a central office. Of course, the massive shift to working from home — and learning from home — during the pandemic has been a major driver of SASE adoption and deployment. SASE enables network security tools to transition away from private data centers into the public cloud or global cloud network. As a result, all users, regardless of physical location, have the same access and network flow efficiency. This means that remote user traffic no longer has to be backhauled to the corporate LAN, resulting in decreased network traffic. And this reduction in traffic can result in lower costs by allowing companies to downsize corporate Internet broadband and private WAN throughput capacity. SASE platforms are designed to provide exceptionally granular organization-wide defense that considers factors such as user location, user identification, resources used, sensitive data patterns, and any other environmental aspects that impact security integrity. In essence, SASE changes the security spotlight from traditional site-centric models to an agile user-centric approach. We’re seeing a lot of enterprises considering and adopting SASE platforms. Concurrently, we’re seeing cloud providers and communications service providers investing more in delivering SASE capabilities. To understand the SASE market, you have to know a bit about the Software-defined Wide Area Network (SD-WAN) market, since SD-WAN is typically a key SASE component. SD-WAN is a virtual WAN architecture that enables organizations to administer any combination of transport services, including MPLS, 4G/5G, and broadband Internet connectivity to securely connect users to applications. The SD-WAN uses a centralized control function to intelligently and securely direct traffic across the WAN. SD-WAN is well-suited to extending enterprise VPNs to remote sites, especially where MPLS VPN is unavailable or is too costly. So SD-WAN helps organizations connect remote sites, including work-from-home locations, on the VPN wherever MPLS could not work. SASE expands the SD-WAN mission in a couple of ways. First, it brings a very granular level of security to SD-WAN. Second, it is fully cloud-native, whereas many SD-WAN solutions can require a physical presence. As such, SASE forms a logical complement to emerging network-as-a-service (NaaS) offerings that deliver personalized network-slice services to customers on an on-demand basis. We concur with market data that supports SASE’s market momentum in 2021 and beyond. Gartner, for example, projects that by 2024 at least 40% of all enterprises will have explicit strategies to adopt SASE, up from less than 1% at year-end 2018. In addition, the global SASE market is expected to reach a compound annual growth rate of 10.8% by 2026 (according to Market Insight Reports). Such data points suggest SASE is the real deal and will avoid the marketing hype that frequently accompanies the emergence of new technologies, markets, and innovations. Ongoing standards-development initiatives by key industry bodies such as the MEF are also proving crucial in advancing and fortifying SASE acceptance across the ecosystem. Specifically, the MEF SASE security reference architecture includes: The inclusion of SASE services in the MEF standards is vital to fulfilling the open-source priorities of communications service providers, cloud service providers,, and enterprises. Without such standards-backing near its inception, SASE likely would face limited traction or would take longer than anticipated to achieve market impact. SASE creates less complex management and reduces costs of multiple separate services when an organization has various networking security solutions that are integrated into one service. A single suite of security capabilities managed by a single unified solution can also deliver better threat detection and data protection. In addition, an integrated solution helps organizations to unify identity management and authentication policies across all their locations. SASE enables organizations to activate, manage, monitor, and enforce policies across all applications, devices, locations, and users though a single portal, mitigating the need to run around administering disparate policies for separate solutions. SASE relies substantially on the zero-trust network access (ZTNA) approach, which denies users access to data and applications until their identity has been verified, including internal users inside the perimeter of a private network. When establishing access policies, the SASE model takes more than an entity’s identity into account by also factoring in security facets such as enterprise security requirements, user location, time of day, and continuous assessment of trust/risk factors. Organizations are adopting and evaluating SASE to improve the performance of any service where latencies diminish the user experience, such as conference collaboration tools, video monitoring/surveillance, and AR/VR training. SASE mitigates latency by routing traffic across a global edge network designed to assure traffic is processed as close as possible to where it will be used. It uses routing optimization to find the fastest network path based on network traffic conditions and other factors. Since SASE merges single-point security solutions into a single cloud-based service, organizations reduce integration overhead and implementation complexity. This can result in savings related to phasing out manual configuration and maintenance of traditional network and security infrastructure. Communications and cloud service providers are expanding their SASE offerings to increase revenues and boost customer retention as organizations look to impose network-wide security and data-traffic policies across their headquarters, branch offices, and remote workforce. There is growing enterprise interest in combining SD-WAN with private backbones, primarily to avoid the hacker pitfalls and security breaches that pervade the public internet. Enterprises are also increasingly interested in transitioning away from their existing MPLS VPN services, which are typically more restrictive and expensive than SD-WAN. SASE blends the functions of network and security point solutions into a unified, ubiquitous cloud-native service. As such, the SASE market segment is attracting a wide variety of competitors, including players with broad SD-WAN/security/networking portfolios, such as Cisco, HPE/Silver Peak, Oracle, Juniper/128T, and Nokia/Nuage; SD-WAN/cloud portfolios such as VMware, Cloudflare and zScaler; and SD-WAN/security suites such as Fortinet, Versa, and Palo Alto Networks. Additional high-profile players with security portfolio acumen that are targeting the SASE market include Akamai, Forcepoint, McAfee, Mushroom Networks, Netskope, Proofport, and Symantec. Clearly the SASE competitive stakes are sizzling on the supplier side. The competitors that stand out at this early stage of the SASE market all offer solutions aimed at simplifying the SASE adoption process. They include a top-tier networking supplier (Cisco), a content delivery network specialist (Cloudflare), a SASE-oriented startup (Cato Networks), and a broadband network gateway supplier (Benu Networks). Cisco. Among the established network infrastructure suppliers, we see Cisco gaining a competitive edge in the early SASE market. The company already has the SD-WAN, security, and cloud portfolio assets (e.g., the new Cisco Plus hybrid cloud solutions) to directly address the SASE space. What makes Cisco stand out are the SASE-related announcements it made at the 2021 Cisco Live event. Cisco offers its SASE package as a single, unified bundle available on a subscription basis, making it easy to procure, activate, and manage through an intuitive cloud dashboard. The package includes Cisco’s Meraki and Viptella SD-WAN software packages, Duo and Any Connect remote access, Umbrella security, the newly available Duo zero trust security, and additional security components. Cato Networks. Among new players, Cato looks like it will compete successfully long-term in the SASE market segment. Cato Cloud provides a clearly differentiated SASE solution by purpose-designing SD-WAN, network security, and ZTNA into a worldwide, cloud-native offering. Cato connects and secures the full range of enterprise edges, including sites, cloud-resources, and mobile users, with a single worldwide cloud-native platform that is distributed across more than 60 points of presence (PoPs), a clear differentiator. As testament to Cato’s ability to stand out in the nascent SASE market, the company raised $130 million in November 2020, bringing its total funding to $332 million. So it is in a good position to fund strategic business objectives such as building more PoPs and broadening ecosystem partnerships. Cloudflare. Cloudflare also combines SASE with private IP backbones to strengthen its security credentials. Its SASE model extends to both Cloudflare for Infrastructure and Cloudflare for Teams, both of which are backed by one worldwide network that services approximately 25 million internet properties. Cloudflare’s experience as a content delivery network (CDN) network provider gives the company the global network resources key to driving SASE adoption. Already, Cloudflare offers a platform of integrated network and security services across each of its 200+ distributed cities in multiple regions, mitigating the need for organizations to purchase and manage a complex collection of point solutions in the cloud. Cloudflare’s annual 2020 revenues were $431 million and its market cap is registering at $22.3 billion during April 2021, suggesting the company’s SASE proposition is boosting its market momentum. Benu Networks. Benu recently upgraded its BNG portfolio, bringing integration of SASE and 5G Access Gateway Function (AGF) capabilities to its Virtual Broadband Network Gateway (vBNG). vBNG fulfills the burgeoning carrier demand for cloud-native edge solutions. As a result, service providers can swiftly deploy SASE services to their subscribers and provide a unified experience across both mobile and fixed network environments. Benu Networks’ support of SD-defined SASE services, designed to run inside a carrier network, is a clear differentiator by giving operators the comprehensive control and ability to run organization-wide security across business sites, branch offices, and the distributed work-from-home workforce under a unified policy. Through Benu’s implementation of SASE at the service edge, carriers avoid VPN clients, use existing WiFi access points (APs), avoid low performance tunnels, and can support all devices across a distributed network. This approach takes direct aim at SD-WAN/SASE solutions that entail customer on-premise implementations such as Juniper’s 128T session-aware routing technology, which requires the deployment of the 128T Session Smart Router at the customer site. Moreover, Benu offers the combination of SASE with SD-LAN, enabling 5G-like services for fixed connections, device-level network slicing, and streamlined customer premise equipment management. In tandem, Benu’s SD-Edge Platform and vBNG solutions provide the 5G Wireless-Wireline Convergence (WWC) capabilities required to unify the use experience across fixed and mobile implementations, assuring consistent treatment of business traffic through application prioritization and holistic security policies. Overall, the SASE market is showing tangible, long-term momentum in just its second year as a new technology segment. SASE is changing the way enterprises evolve their security implementations, emphasizing cloud-first highly automated solutions that overcome the limitations and costs of traditional security approaches. Service providers of all types are prioritizing SASE as a key capability to expand their influence across the digital ecosystem and win more enterprise business. Competition across the SASE realm is intensifying, with clearly differentiated solutions already available on the market. For enterprises, adopting the SASE approach provides long-term assurances for unified security across the entire organization including the distributed WFH workforce. Through SASE, enterprises gain built-in benefits such as positive return on investment from streamlining complex security and WAN implementations and assimilating user-centric security frameworks while also taking advantage of enduring ecosystem-wide support, including industry-wide standards backing and fast expanding service provider and supplier investments in SASE. It will be exciting to watch this market continue to develop in 2021 and beyond. Daniel Newman is the principal analyst at Futurum Research, which provides research, analysis, advising, and/or consulting to high-tech companies in the tech and digital industries. Ron Westfall is a Senior Analyst at Futurum Research."
https://venturebeat.com/2021/04/16/ai-weekly-data-analytics-keeps-attracting-investment-through-the-pandemic/,AI Weekly: Data analytics keeps attracting investment through the pandemic,
https://venturebeat.com/2021/04/16/harness-coordinates-devops-and-cloud-spending-across-multiple-platforms/,Harness coordinates DevOps and cloud spending across multiple platforms,"Harness.io, a software delivery platform with cloud cost management capabilities, yesterday launched a variety of integrations that expand its services across Amazon GovCloud, Azure, and Google Cloud Platform. The new integrations make it easier for engineers to weave cost considerations into engineering decisions when working across multiple cloud platforms. Harness has long offered basic capabilities across all three of the major cloud services, but it had the most advanced features on Amazon Web Services (AWS). On Azure, Harness has now added support for Azure native deployments and cloud bill analysis. On Google Cloud Platform, it has added critical integrations to bring the features closer to what was available on AWS. Enterprises are starting to develop more scalable and resilient applications using containers and microservices on top of Kubernetes. All the major cloud platforms support Kubernetes, and in theory developers could write an application and deploy it to whichever cloud platform fits their requirements. In practice, however, engineers have to know the differences between the APIs and features of each platform and have the specific deployment for that tool. “Developers shouldn’t need to know the APIs for every container deployment service now or in the future,” Harness senior product manager Rohan Gupta told VentureBeat. Harness allows them to focus on the business functionality, which can be deployed to the appropriate cloud vetted by security and engineering teams and prioritized by cost factors. DevOps and finance teams have traditionally relied on different tools to deploy new apps and analyze cloud spending. Finance receives a bill at the end of the month and has to figure out where money is being spent and whose budget it should be charged back to. With Harness, finance and engineering teams can work together to find ways to optimize costs that address the constraints of business needs and technical infrastructure. In general, billing systems provided by each cloud vendor only go to the granularity of the cloud service, and it’s up to each consumer to define and analyze the relationship between business and cloud service. In an ideal world, these relationships are automatically built, applied, and analyzed. When businesses use a combination of CI/CD and cloud cost management tools together, the business service to cloud service relationships can be automatically added as tags. This requires an impeccable tag hygiene to be sustained for any period, however. “When it comes to cost management, a major challenge is understanding what business applications or services were responsible for costs that are rolled up at the cloud service level,” Gupta said."
https://venturebeat.com/2021/04/16/open-grid-alliance-aims-to-support-cloud-computing-at-the-edge/,Open Grid Alliance aims to support cloud computing at the edge,"VMware and Vapor IO this week kicked off an ambitious effort to rearchitect the internet via a vendor-neutral Open Grid Alliance (OGA) that aims to make network services easier for app developers to consume. Other founding members of the OGA include Dell Technologies, DriveNets, MobiledgeX, and PacketFabric. But the OGA is not seeking to define any specific technology implementations and is open to all interested parties, Vapor IO CEO Cole Crawford told VentureBeat. In fact, the OGA is deliberately staying away from creating any type of formal foundation structure, Crawford added. “A lot of people are suffering foundation fatigue,” he said. Vapor IO today provides its own framework for integrating edge computing platforms and distributed datacenters to enable machine-to-machine communications that are the core of many internet of things (IoT) applications. OGA will embrace technologies that distribute the economics and flexibility of cloud computing platforms out to the network edge. That approach will make it possible for developers and other members of an IT organization to declaratively describe their intent, which will then be used to automatically configure network services. This shift is required because, for example, next-generation wireless technologies will not be able to achieve the level of scale required by billions of intelligent devices that will be connected to the internet, Crawford said. Many of those devices will be running highly distributed applications based on microservices that will not only consume a lot of data but are also latency-sensitive, Crawford noted. Many of the applications will need to run in real time as digital business processes continue to evolve and expand, he added. The internet in its current form is designed around a core-out model that needs to be replaced by an edge-in approach that enables network services to be delivered more efficiently. It could do this by making sure packets travel more directly to where they need to arrive, instead of being widely broadcasted, Crawford added. Some members of OGA are using network virtualization overlays and smart contracts currently associated with distributed ledgers based on blockchain databases to enable that goal. But rather than relying on low-level application programming interfaces (APIs), the idea is to allow platforms to describe their capabilities to an application, VMware VP Kaniz Mahdi said. As the amount of compute horsepower and storage capacity deployed at the network edge increases, it becomes possible to reimagine how the internet could be constructed, Mahdi noted. “More automation and abstraction is required,” she said, adding “More telematics will also be necessary.” The OGA plans to define key principles for the Open Grid and identify open interoperable technologies that adhere to those principles as they emerge. It will also document how these technologies will impact cloud providers, developers, vendors, communication service providers (CSPs), internet service providers (ISPs), and end users. Intent-based networking is not a new idea, of course. Networking vendors have been using that phrase to describe the next era of networking for several years. It will, however, be several years before we see the level of scale the OGA envisions for applying those concepts. In that sense, the OGA is focused on starting a conversation about how the internet needs to change. It’s already apparent the internet in its current incarnation will need to evolve as the world becomes more interconnected. The issue that needs to be determined now is under what type of framework that goal can be achieved in the absence of a single governing body. That body would need to be empowered to define a set of interoperable internet standards in the way the U.S. Defense Advanced Research Projects Agency (DARPA) did some six decades ago."
https://venturebeat.com/2021/04/16/the-deanbeat-a-big-bang-week-for-the-metaverse/,The DeanBeat: A Big Bang week for the metaverse,"The metaverse had a couple of Big Bangs this week that should put it on everyone’s radar. First, Epic Games raised $1 billion at a $28.7 billion valuation. That is $11.4 billion more valuable than Epic Games was just nine months ago, when it raised $1.78 billion at a $17.3 billion value. And it wasn’t raising this money to invest more in Fortnite. Rather, Epic explicitly said it was investing money for its plans for the metaverse, the universe of virtual worlds that are all interconnected, like in novels such as Snow Crash and Ready Player One. Epic Games CEO Tim Sweeney has made no secret of his ambitions for building the metaverse and how it should be open. And while that might sound crazy, he received $200 million from Sony in this round, on top of $250 million received from Sony in the last round. I interpret this to mean that Sony doesn’t think Sweeney is crazy, and that it too believes in his dream of making the metaverse happen. And if Sony believes in the metaverse, then we should expect all of gaming to set the metaverse as its North Star. Epic’s $1 billion in cash is going to be spent on the metaverse, and that amount of money is going to look small in the long run. Epic Games has a foothold to establish the metaverse because it has the users and the cash. It has 350 million-plus registered users for Fortnite. And it has been investing beyond games into things like social networks and virtual concerts, as Sweeney knows that the metaverse — a place where we would live, work, and play — has to be about more than just games. Games are a springboard to the metaverse, but they’re only a part of what must be built. One of the keys to the metaverse will be making realistic animated digital humans, and two of Epic’s leaders — Paul Doyle and Vladimir Mastilović — will speak on that topic at our upcoming GamesBeat Summit 2021 conference on April 28 and April 29. This fits squarely with the notion of building out the experience of the metaverse. We need avatars to engage in games, have social experiences, and listen to live music, according to my friend Jon Radoff (CEO of Beamable) in a recent blog post. Meanwhile, this morning Nvidia announced something called GanVerse, which can take a 2D picture of a car and turn it into a 3D model. It’s one more tool to automate creation for the metaverse. To make the metaverse come to life, we need so many more layers, including discovery tools, a creator economy, spatial computing to deliver us the wow 3D experience, decentralization to make commerce between worlds seamless and permission-less, human interface and new devices that make the metaverse believable, and infrastructure too. And when you think about those things, that is what we got in another Big Bang this week as Nvidia announced its enterprise version of the Omniverse, a metaverse for engineers. By itself, that doesn’t sound too exciting. But drilling deep on it, I learned a lot about how important the Omniverse could be in providing the foundational glue for the metaverse. “The science fiction metaverse is near,” said Nvidia CEO Jensen Huang in a keynote speech this week at the company’s GTC 21 online event. First, Nvidia has been working on the Omniverse — which can simulate real-world physics — for four years, and it has invested hundreds of millions of dollars in it, said Nvidia’s Richard Kerris in a press briefing. Nvidia started this as “Project Holodeck,” using proprietary technology. But it soon discovered the Universal Scene Description language that Pixar invented for describing 3D data in an open, standardized way. Pixar invented this “HTML of 3D” and shared it with its vendors because it didn’t want to keep reinventing 3D tools for its animated movies. “The way to think about USD is the way you would think about HTML for the internet,” Huang said. “This is HTML for 3D worlds. Omniverse is a world that connects all these worlds. The thing that’s unique about Omniverse is its ability to simulate physically and photorealistically.” It open sourced USD about eight years ago, and it has spread to multiple industries. One of the best things about it is that it enable remote collaboration, where multiple artists could work on the same 3D model at once. Nvidia made USD the foundation for the Omniverse, adding real-time capabilities. Now BMW Group, Ericsson, Foster + Partners, and WPP are using it, as are 400 enterprises. It has application support from Bentley Systems, Autodesk, Adobe, Epic Games, ESRI, Graphisoft, Trimble, Robert McNeel & Associates, Blender, Marvelous Designer, Reallusion, and Wrnch. That’s just about the entire 3D pipeline for tools used to make things like games, engineering designs, architectural projects, movies, and advertisements. BMW Group is building a car factory in the Omniverse, replicating exactly what it would build in the real world but doing it first in a “digital twin” before it has to commit any money to physical construction. I saw a demo of the Omniverse, and Nvidia’s engineers told me you could zip through it at 60 frames per second using a computer with a single Nvidia GeForce RTX card (if you can get one). “You could be in Adobe and collaborate with someone using Autodesk or the Unreal Engine and so on. It’s a world that connects all of the designers using different worlds,” Huang said. “As a result, you’re in a shared world to create a theme or a game. With Omniverse you can also connect AI characters. They don’t have to be real characters. Using design tools for these AI characters, they can be robots. They can be performing not design tasks, but animation tasks and robotics tasks, in one world. That one world could be a shared world, like the simulated BMW factory we demonstrated.” Nvidia hopes to test self-driving cars — which use Nvidia’s AI chips — inside the Omniverse, driving them across a virtual U.S., from California to New York. It can’t do that in the real world. Volvo needs the Omniverse to create a city environment around its cars so that it can test them in the right context. And its engineers can virtually sit in the car and walk around it while designing it. The Omniverse is a metaverse that obeys the laws of physics and supports things that are being created by 3D creators around the world. You don’t have to take a Maya file and export it in a laborious process to the Omniverse. It just works in the Omniverse, and you can collaborate across companies — something that the true metaverse will require. Nvidia wants tens of millions of designers, engineers, architects and other creators — including game designers — to work and live in the Omniverse. “Omniverse, when you generalize it, is a shared simulated virtual world. Omniverse is the foundation platform for our AR and VR strategies,” Huang said. “It’s also the platform for our design and collaboration strategies. It’s our metaverse virtual world strategy platform, and it’s our robotics and autonomous machine AI strategy platform. You’ll see a lot more of Omniverse. It’s one of the missing links, the missing piece of technology that’s important for the next generation of autonomous AI.” By building the Omniverse for real-time interaction, Nvidia made it better for game designers. Gamers zip through worlds at speeds ranging from 30 frames per second to 120 frames per second or more. With Nvidia’s RTX cards, they can now do that with highly realistic 3D scenery that takes advantage of real-time ray tracing, or realistic lighting and shadows. And Kerris said that most what you see doesn’t have to be constantly refreshed on every user’s screen, making the real-time updating of the Omniverse more efficient. Tools like Unreal or Unity can plug into the Omniverse, thanks to USD. They can create games, but once the ecosystem becomes mature, they can also absorb assets from other industries. Games commonly include realistic replicas of cities. Rockstar Games built copies of New York and Los Angeles for its games. Ubisoft has built places such as Bolivia, Idaho, and Paris for its games. Imagine if they built highly realistic replicas and then traded them with each other. The process of creating games could be more efficient, and the idea of building a true metaverse, like the entire U.S., wouldn’t seem so crazy. The Omniverse could make it possible. Some game companies are thinking about this. One of the studios playing with Omniverse is Embark Studios. It’s founder is Patrick Soderlund, the former head of studios for Electronic Arts. Embark has backing from Nexon, one of the world’s biggest makers of online games. And since the tools for Omniverse will eventually be simplified, users themselves might one day be able to contribute their designs to the Omniverse. Huang thinks that game designers will eventually feel more comfortable designing their worlds while inside the Omniverse, using VR headsets or other tools. “Game development is one of the most complex design pipelines in the world today,” Huang said. “I predict that more things will be designed in the virtual world, many of them for games, than there will be designed in the physical world. They will be every bit as high quality and high fidelity, every bit as exquisite, but there will be more buildings, more cars, more boats, more coins, and all of them — there will be so much stuff designed in there. And it’s not designed to be a game prop. It’s designed to be a real product. For a lot of people, they’ll feel that it’s as real to them in the digital world as it is in the physical world.” Omniverse enables game developers working across this complicated pipeline, allowing them to be connected, Huang said. “Now they have Omniverse to connect into. Everyone can see what everyone else is doing, rendering in a fidelity that is at the level of what everyone sees,” he said. “Once the game is developed, they can run it in the Unreal engine that gets exported out. These worlds get run on all kinds of devices. Or Unity. But if someone wants to stream it right out of the cloud, they could do that with Omniverse, because it needs multiple GPUs, a fair amount of computation.” He added, “That’s how I see it evolving. But within Omniverse, just the concept of designing virtual worlds for the game developers, it’s going to be a huge benefit to their work flow. The metaverse is coming. Future worlds will be photorealistic, obey the laws of physics or not, and be inhabited by human avatars and AI beings.” On a smaller scale, Roblox also did something important. It cut a deal with Hasbro’s Nerf brand this week, where some new blasters will come to the game. Roblox doesn’t make the blasters itself. Rather, it picks some talented developers to make them, so that it stays true to its user-generated content mantra. That Roblox can partner with a company like Hasbro shows the brands have confidence in Roblox, as it has demonstrated in deals with Warner Bros. Usually, user-generated content and brands don’t mix. The users copy the copyrighted brands, and the brands have to take some legal action. But Roblox invests a lot in digital safety and it doesn’t seem to have as big a problem as other entities. That’s important. We know that Roblox is a leading contender for turning into the metaverse because it has the users — 36 million a day. But the real test is whether the brands will come and make that metaverse as lucrative as other places where the brands show up, like luxury malls. And FYI, we’ve got a panel on Brands and the Metaverse at our GamesBeat Summit 2021 event on April 28 and April 29. Kudos for Steven Augustine of Intel for planting that thought in my brain months ago. I feel like the momentum for the metaverse is only getting stronger, and it is embedding itself in our brains as a kind of Holy Grail — or some other lost treasure in other cultures — that we must find in order to reach our ultimate goals."
https://venturebeat.com/2021/04/16/gamesbeat-summit-2021-growing-the-next-generation-our-leader-packed-agenda/,GamesBeat Summit 2021: Growing the next generation — Our leader-packed agenda,"GamesBeat Summit 2021 is a digital online-only event taking place on April 28 and April 29 as our third virtual conference in a year. We’re looking forward to getting together in the physical world or in the metaverse one day, but in the meantime, we’ve learned how to deliver a good experience with an online event. I’ve included the full agenda below and will update it as we fill in the final names. We’ve got 87 speakers at the moment and 45 of them — about 52% — come from diverse backgrounds. We’ve got top speakers like Bobby Kotick of Activision Blizzard, Phil Spencer of Microsoft, and Laura Miele of Electronic Arts. And we have others you haven’t heard about who are making a difference in the ranks. This is our most diverse event ever, and it will be the first one with a member of the U.S. Congress — Yvette Clarke, Democrat from New York’s 9th District. She will join Stanley Pierre-Louis of the Entertainment Software Association and Laila Shabir, CEO of Girls Make Games for a talk on the importance of STEAM (science, technology, engineering, art, and math) education for gaming. We’ve also confirmed our awesome emcees Kahlief Adams of Spawn on Me and Andrea Rene of What’s Good Games. Rene will moderate our second annual Women in Gaming Breakfast with speakers that include Samantha Ryan of Electronic Arts and Brenda Romero of Romero Games. Since our last post, we’ve added a lot of speakers including Karthik Bala of Velan Studios, Samir Agili of Tilting Point, tech futurist Cathy Hackl, Ken Martin of GreenPark Sports, Kimberly Voll of the Fair Play Alliance, James Zhang of Fifth Era and Concept Art House, Tim O’Brien of Scopely, Amir Rahimi of Scopely, Eric Seufert of Mobile Game Dev Memo, former NBCUniversal Games head Chris Heatherly, Jon Radoff of Beamable, Michael Pachter of Wedbush Securities, Tammy McDonald, and Gabby Dizon of Yield Guild Games. Diversity, inclusion, and mental health challenges are going to be big topics for discussion. And we’ll explore how games can keep their historic growth going and at the same time explore new parts of the business including blockchain and nonfungible tokens, the post-IDFA world, augmented and virtual reality, and the metaverse. Michael Pachter of Wedbush Securities will open with a talk on the explosion of opportunities that come from having an unprecedented amount of money coming into the industry through investments, public offerings, and acquisitions. We want to continue our reputation as the most intimate gaming event where business meets passion. Our event will include fireside chats, panels, and small-group roundtables. We’ll provide Q&A sessions for VIP attendees, and a way for attendees to network with each other and make new connections. We have a wide range of partners including the International Game Developers Association and Women in Gaming International. And our sponsors include Lego Ventures, Anzu, Xsolla, Jam City, Adjust, Accenture, Rogue Games, Epic Games, Scopely, Singtel, the Entertainment Software Association, Wildlife, Perforce, Outfit7, and more. For attendees, you’ll be getting invitations to join using the email you used to register. If you upgrade to VIP, you’ll be able to join things like our GamesBeat Slack (which we’ve already started), one-on-one meetings in Grip, roundtables, and our Zoom Q&A sessions with our speakers. 8 a.m. Tutorial for watching and participating in the event 8:10 a.m. – 8:30 a.m. Introduction Emcee Kahlief Adams, Spawn on Me Dean Takahashi of GamesBeat 8:30 a.m. – 8:40 a.m. Gaming’s time to shine Michael Pachter, managing director at Wedbush Securities and an analyst for the games industry for 20 years, will talk about
what it means to have so much money coming into the game business at all levels. Michael Pachter, Wedbush Securities 8:40 a.m – 9:00 a.m. Scaling creativity through the Scopely Operating System Join Scopely Chief Business Officer Aaron Loeb and moderator Dean Takahashi for a conversation on how game companies can maximize creativity among rapidly growing global teams. Aaron Loeb, Scopely Moderator: Dean Takahashi, GamesBeat 9:00 a.m. – 9:30 a.m. Enabling the next generation of leaders Game director Brenda Romero of Romero Games and technical director Sushama Chakraverty of Prodigy Education examine the role of leadership in the game industry through the lens of their own intertwining careers. Brenda Romero, Romero Games Moderator: Sushama Chakraverty, Prodigy Education 9:30 a.m. – 10:00 a.m. Diving into digital humans with Epic Games Join Vladimir Mastilovic and Paul Doyle of Epic Games and Wanda Meloni of M2 Insights for a discussion on the evolution of digital humans and how MetaHuman Creator simplifies the creation of unique, convincing digital humans. Paul Doyle, Epic Games Vladimir Mastilović, Epic Games Moderator: Wanda Meloni, M2 Insights 10:00 a.m. – 10:30 a.m. How learning through play intersects with gaming Rob Lowe, Lego Ventures Karsten Lund, Director, Light Brick Studio Moderator: Keza MacDonald, Guardian 10:30 a.m. – 11:00 a.m. Magic Meets the Measured: striking the right balance to make great games Chris DeWolfe, CEO of Jam City, talks about striking the right balance in taking care of employees, taking advantage of growth opportunities during the pandemic, and doing the right things for gamers. Chris DeWolfe, Jam City Moderator: Dean Takahashi, GamesBeat 11:00 a.m. – 11:30 a.m. Game VCs who bring the value of operating experience to investing Game VCs who are former operators at companies talk about the value — and the challenges — that they can bring to the table in helping startups grow. Andrew Sheppard, Transcend Fund Jens Hilgers, Bitkraft Ventures Ed Fries, 1Up Ventures Moderator: Eric Goldberg, Playable Worlds 11:30 a.m. – 12:00 p.m. What IDFA changes mean for the game industry Apple is requiring users to opt-in for sharing their private info on iOS, and that’s going to affect the ability for mobile game makers to target ads at gamers. What comes next? Eric Seufert, Mobile Game Dev Memo Moderator: Dean Takahashi, GamesBeat 12:00 pm -12:30 pm The next generation of mobile game development With more and more individuals turning to games amidst a global pandemic, the importance of mobile gaming is at an unprecedented height. But how does mobile gaming keep up with other platforms? Publishers talk about empowering the next generation of developers. Victor Lazarte, Wildlife Studios Ken Martin, GreenPark Sports Samir Agili, Tilting Point Moderator: Joost van Dreunen, author of One Up 12:30 p.m. -1:10 p.m. The post-pandemic world of gaming Bobby Kotick, chairman and CEO of Activision Blizzard, will speak at one of our events for the first time in a fireside chat about the acceleration of games into the top tier of all entertainment. But what comes next? And how will we navigate 2021 and beyond? Bobby Kotick, Activision Blizzard Moderator: Dean Takahashi, GamesBeat 12:30 p.m.-1:30 p.m. Roundtable (VIP attendees eligible) 12:30 p.m.-1:30 p.m. Your Culture isn’t what you think it is Caroline Stokes, Forward Roundtable (VIP attendees eligible) 1230 pm to 1:30 pm Life in a post-IDFA world Faith Price, Double Down Interactive Dan Barnes, N3twork 1:10 p.m.-1:40 p.m. Team Xbox on gaming for everyone Microsoft’s Phil Spencer will lead a discussion about the company’s journey to bring the joy and community of gaming to everyone on the planet. The panelists will explore the challenges of ensuring that gaming is inclusive for all players and that the game industry welcomes all creators. Agnes Kim, Microsoft Esteban Lora, Microsoft Cierra McDonald, Microsoft Moderator: Phil Spencer, head of Xbox at Microsoft 1:40 p.m. -2 p.m. The future of accessibility in the game industry Daniel Melville was born without an arm. But he has managed to become a gamer as well as an ambassador for Open Bionics, which recently produced a Metal Gear-themed bionic arm for him. He’ll discuss this and other themes about accessibility and inclusion in the game industry with Keisha Howard of Sugar Gamers. Daniel Melville, Open Bionics Moderator: Keisha Howard, Sugar Gamers 2 p.m. – 2:30 p.m. Changing the game Robert Antokol, CEO of Playtika, and Michael Metzger, partner at Drake Star Partners, will talk about Playtika’s journey from launch, to expansion into casual games and recent public offering.  Robert will discuss Playtika’s Boost Platform and acquisition of independent gaming studios as well as share his vision for Playtika in the years to come. Robert Antokol, Playtika Moderator: Michael Metzger, Drake Star Partners 2:30 p.m. – 3 p.m. STEAM, why it matters to video games, diversity, the future workforce, and the economy STEAM (science, technology, engineering, art, and math) education is not only critical to the success of video game industry, but also to creating meaningful opportunities for people from different backgrounds and to American competitiveness. This panel will discuss the increasingly important role of STEAM in the overall American economic landscape, its impact on the video game industry, and youth leadership. Laila Shabir, Girls Make Games Yvette Clarke, U.S. Congresswoman from New York’s 9th District Moderator: Stanley Pierre-Louis, Entertainment Software Association 3 p.m. – 3:30 p.m. What to expect in a post-pandemic gaming world Join Frank Azor, AMD’s Chief Architect of Gaming Solutions and Marketing, as he shares thoughts on how the COVID-19 pandemic has and will continue to impact the gaming industry, and how companies and developers are adapting to that change. Frank Azor, AMD Moderator: N’Gai Croal, Hit Detection 3:30 p.m. – 3:50 p.m. Burnout: What it is and what we can do A lot of people are suffering from burnout as the pandemic and quarantine continue into year two. What is burnout (beyond exhaustion and overwork)? This game-focused talk will detangle and explain the aspects of burnout, and provide actionable, practical steps for addressing it inside your studio and workplace. Raffael Boccamazzo, Take This 3:50 p.m. – 4:20 p.m. A strategy for a global gaming business Mike Vorhaus of Vorhaus Advisers will interview Simon Zhu, general manager at NetEase, about the company’s approach to the gaming industry. They will talk about NetEase’s focus on quality as well as its investments in Bungie, Theorycraft, Behaviour Interactive, and Quantic Dream. Simon Zhu, NetEase Moderator: Mike Vorhaus 4:20 p.m. – 4:50 p.m. Designing blockbuster games infused with diversity What happens when you start designing a major video game and try to infuse diversity throughout the characters and environment? Halley Gross — a seasoned Hollywood writer on shows such as Westworld participated in such as project as the co-writer for The Last of Us Part II. The game won more than 215 awards for Game of the Year. She shares some of the lessons she learned about writing blockbuster narratives with an inclusive lens. Halley Gross Moderator: Dean Takahashi 4:50 p.m. to 5:20 p.m. Why gaming M&A and investments have exploded and show no signs of slowing M&A, public offerings, and investments have exploded for games during the pandemic. We’ll have a fireside chat with Lars Wingefors, CEO of Embracer Group, which has been the most active of all game company acquirers. Lars Wingefors, Embracer Group Moderator: Nick Tuosto, Liontree 5:20 p.m.-6:20 p.m. Reception and networking on Clubhouse Hosts Jon Radoff and Dean Takahashi 8:00 a.m. – 9:30 p.m. Women in gaming breakfast Samantha Ryan, EA Brenda Romero, Romero Games Emily Greer, Double Loop Games Moderator: Andrea Rene 9:30 am – 9:40 am Introduction Andrea Rene, What’s Good Games 9:40 am – 9:45 am Opening comments Mike Minotti 9:45 a.m. – 10:05 a.m. It takes a village The game industry hasn’t always been kind to its own. And as a result, it has lost members to suicide. Mark Chandler of the International Games Summit on Mental Health Awareness and Jason Docton of Rise Above the Disorder will talk about what could be done so the game industry and gamers take care of their own better. Mark Chandler, The International Games Summit on Mental Health Awareness Jason Docton, Rise Above the Disorder 10:05 a.m. – 10:25 a.m. How to do remote game development When the world went remote, game development companies knew how to make it work. Many have been coordinating remote contributors for years. But with everyone at home, demands for media and games have dramatically increased. Our panel will discuss how to overcome challenges and build better games. Mark James, Striking Distance Studios Karthik Bala, Velan Studios Moderator: Brad Hart, CTO of Perforce 10:25 a.m. to 10:45 a.m. Pulling the future from the cloud Tim Guhl of Singtel and Lisa Cosmas Hanson of Niko Partners talk about the opportunities in gaming that will depend on 5G networks, including augmented reality, low-latency multiplayer games, and esports. Tim Guhl, Singtel
Moderator: Lisa Cosmas Hanson, Niko Partners 10:45 a.m. – 11:15 a.m. Making great games is only going to get more complicated Laura Miele started her career in a game studio 25 years ago and today she is leading one of the largest collectives of game creators in the world at Electronic Arts. EA’s 20+ global studios. Geoff Keighley, creator of The Game Awards, will talk with Miele about the ever-changing world of game development and how she navigates through the internal and external challenges to deliver the best possible games to players. Laura Miele, Electronic Arts Moderator: Geoff Keighley 11:15 a.m. – 11:45 a.m. Advertising in games: Tapping into the next wave Brands are finally realizing the potential of gaming and are ready to treat it as an advertising medium. This panel will explain the evolution of advertising in games and why brands are now finally looking at games as an advertising category and what is on the horizon. Ronnie Nelis, Lion Castle Itamar Benedy, Anzu Gabrielle Heyman, Zynga Moderator: Steve Peterson, StoryPhorce 11:45 a.m. to 12:15 p.m. Representation in-game matters, and it starts with developers Nearly half of all gamers now identify as female. But in the development community, female representation is still sorely lacking. With such a skewed balance among the people who make our games, how can we make sure we’re meeting the needs of all our players, who are increasingly expressing their needs to see themselves represented in the games they play? Hear from top female development leads across Activision Blizzard about addressing the gender gap in the games, and on the teams who make them. Jennifer Oneal, Blizzard Entertainment Lydia Bottegoni, Blizzard Entertainment Nour Polloni, Studio Head, Beenox Moderator: Eunice Lee, Activision 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) NFTs and games 101 Nonfungible tokens have taken the art and music collectible worlds by storm, and now they’re ready to disrupt games. Are NFTs the next big thing to monetize games, or are they a flash in the pan? Chris Heatherly Jon Radoff, Beamable James Zhang, Fifth Era Gabby Dizon, Yield Guild Games 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) Understanding the future of consolidation, expanding capital pools, and hyperactivity in gaming M&A Hemal Thakur, Goldman Sachs Alina Soltys, Quantum Tech Partners 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) Expanding the esports audience Daniel Evans, Reely Robyn St. Germain, Houston Outlaws Rebecca Longawa, Rokkr, Version1, GameHers Heather Garozzo, VP Talent at Dignitas 12:15 p.m. – 12:35 p.m. How to do M&A right Scopely acquired FoxNext Games in January 2020. The companies set a goal of a seamless merging of the two teams. It turns out that, during the pandemic, M&A has exploded in the game industry. Two Scopely leaders will talk about the lessons learned. Tim O’Brien, Scopely Amir Rahimi, Scopely Moderator: Nick Tuosto, Liontree 12:35 p.m. – 12:55 p.m. Scaling the right way: When a startup is no longer a startup Katie Jansen, chief marketing officer at AppLovin, and Katie Madding, chief product officer at Adjust, share similar stories in their respective careers, with both having joined their current companies early on, and then having witnessed the trajectories of those companies skyrocket in a relatively short amount of time. Learn about their unique journeys in navigating these exciting, yet uncharted periods of hyper-growth. Katie Jansen, AppLovin Moderator: Katie Madding, Adjust 1:15 p.m. – 1:45 p.m. The past, present, and future of XR and mental health Since the onset of COVID-19, 53% of adults in the United States reported that their mental health has been negatively impacted. This is particularly evident for front line workers, where one-third have reported elevated levels of mental distress. This panel will bring together professionals in multiple sectors—Kelli Dunlap (Kentlands Psychotherapy, Take This), Noah Falstein (The Inspiracy), and Brennan Spiegel (Cedars-Sinai Medical Center)—to delve deeper into how this rapidly evolving technology is being used to directly help mental health. Kelli Dunlap, American University Noah Falstein, The Inspiracy Brennan Spiegel, Cedars-Sinai Moderator: Susanna Pollack, Games for Change 1:45 p.m. – 2:15 p.m. How Iron Galaxy Studios avoids crunch One of Iron Galaxy’s Studios’ many core values is people – the strength of their team is the foundation of their success. They focus first and foremost on the well-being of their employees, making sure staff feel cared for and valued. Co-CEO’s Adam Boyes and Chelsea Blasko can share their leadership philosophies and how they continue to offer a healthy work/life balance. Chelsea Blasko, Iron Galaxy Adam Boyes, Iron Galaxy Moderator: Eve Crevoshay, Take This 2:15 p.m. – 2:45 p.m. A game changing experience for everyone This panel will discuss issues of digital civility and the challenges to getting through to the next level as games become even more mainstream. What challenges need to be addressed in order for the gaming industry to seize the exponential opportunity at hand? Christian Kelly, Accenture Laura Higgins, Roblox Kimberly Voll, Fair Play Alliance Moderator: Seth Schuler, Accenture 2:45 p.m. – 3:15 p.m. How global publishers can differentiate themselves? What are the challenges of global publishing? How do you use IP? How can you adapt to new platforms, app stores, business models, and regional markets? Chris Hewish, Xsolla Matt Casamassina, Rogue Games Anthony Crouts, Tencent Moderator: Lisa Cosmas Hanson, Niko Partners 3:15 p.m. – 3:35 p.m. When is it time to launch a new IP? Outfit7 has one of the biggest intellectual properties for games and entertainment with Talking Tom. It has created lots of apps and games based on it. It has endless runner games, virtual life simulations, videos, and others that tap into Tom’s personality. But the company also needs to create new IPs for the future. What’s the right time to do that and how? Ante Odić, Senior VP of Product, at Outfit7 Shawn Layden, former head of Sony Worldwide Studios Marty O’Donnell, Highwire Games Moderator: Dean Takahashi, GamesBeat 3:35 p.m. – 3:55 p.m. Brands x Creators: Collaborating to scale economic opportunity for everyone Celebrity influencers have come out of the woodworks in the past decade, but not everybody can be the next Ninja. What can the industry do to create the next generation of streamers who can make a living at their trade? We’ll explore this in a panel on getting paid as a streamer. Stu Grubbs, Lightstream Natasha Zinda, ZombaeKillz Moderator: Andrea Rene, What’s Good Game 3:55 p.m. – 4:25 p.m. Brands and the metaverse We’ve been stuck in the Zoomverse during the pandemic, and we can’t wait for an actual metaverse to arrive. Gaming will likely be the big draw for consumers. And if the consumers come, the brands will follow. We’ll ask some brands if they believe in the metaverse. Ryan Mullins, Aglet Ian Fitzpatrick, New Balance Perry Nightingale, WPP Moderator: Cathy Hackl 4:25 p.m. – 4:40 p.m. Visionary Awards Tammy McDonald 4:40 p.m. – 4:45 p.m. Closing session 4:45 p.m. – 5:45 p.m. Networking on Clubhouse Hosts Jon Radoff and Dean Takahashi"
https://venturebeat.com/2021/04/16/device42-extends-aiops-reach-of-it-infrastructure-discovery-tool/,Device42 extends AIOps reach of IT infrastructure discovery tool,"Device42 today announced it has added support for additional platforms to an AIOps tool that enables organizations to discover IT resources residing in the cloud and in on-premises IT environments. As enterprise IT environments become more extended, platforms are being added in a way that is not always immediately discernible to a centralized IT team. Developers, for example, now routinely spin up virtual machines on public clouds without any intervention on the part of an internal IT team required. AIOps tools from Device42 employ machine learning algorithms to create device and application topologies and impact charts that surface all the IT infrastructure being employed across a hybrid cloud computing environment. The latest update adds the ability to also discover cloud databases, along with other types of cloud services — in addition to storage resources in on-premises IT environments. Those assets are discovered using the application programming interfaces (APIs) that have been exposed by cloud services providers and IT vendors that provide infrastructure for on-premises IT environments. Once all those assets are mapped, it becomes possible for IT teams to employ a search function to uncover, for example, dependencies between multiple services being employed, Device42 cofounder and CEO Raj Jalan told VentureBeat. Not every IT organization may need an AI platform to keep track of its IT assets. But the larger an organization becomes, the more challenging it is to monitor what infrastructure is being employed where and for what purpose. Cloud services, especially, tend to be dynamically consumed, which makes trying to keep track of usage manually nearly impossible. It’s not uncommon for IT organizations to find themselves being billed for cloud services they had no idea where being employed until the invoice from the cloud service provider arrived. At the same time, more application workloads are now starting to be pushed out to the network edge in places centralized IT teams often have no way to physically reach. With the addition of each new platform to an IT environment, the probability an organization is going to run afoul of one compliance regulation or another increases, Jalan noted. The Device42 platform makes it easier for IT teams to uncover potential issues long before any audit might, he said. This capability can also play a role in helping IT teams discover application workloads they may want to move from on-premises IT environments to the cloud or vice versa. Overall, the rate of change in IT environments is accelerating as organizations add, for example, Kubernetes clusters to run microservices-based applications that are being rolled out by a line-of-business unit. IT organizations now need AIOps capabilities to keep track of the changes and updates being made across an extended enterprise. “IT has become too complex to run without it,” he said. Longer-term, it’s not clear to what degree AIOps will remain a discrete concept or simply become part of the IT service management (ITSM) firmament. At some point, every ITSM tool is going to be infused with machine learning algorithms to some degree. Of course, many IT professionals are dubious about the capabilities of AIOps platforms. But the longer machine learning algorithms are employed, the more accurate they become since they continuously learn from their environment. As the overall size of the IT environment continues to expand, increasing the size of the IT staff needed to manage it become financially impractical. It’s now only a question of to what degree AI will be employed to augment IT teams that have no other viable way of managing modern IT environments."
https://venturebeat.com/2021/04/16/paxafe-which-offers-visibility-into-b2b-supply-chains-raises-2-25m/,"Paxafe, which offers visibility into B2B supply chains, raises $2.25M","Paxafe, a platform using AI and machine learning to classify and contextualize supply chain data, today announced it has raised $2.25 million. The startup says it will leverage the funds to support the rollout of its platform and further develop its AI technology. Historically, a lack of visibility throughout the supply chain has resulted in product losses and operational inefficiencies. The problem is quite the opposite today, with an abundance of data from sensors and aggregator platforms providing shippers, carriers, and insurers with key information. But this data doesn’t always drive smarter decision-making, mainly because it tends to lack context and structure. Paxafe aims to address this with a platform designed for predictive routing, time to arrival estimation, and adverse event prediction for business-to-business shipments. The company delivers the status of goods transported via ocean cargo, air freight, or parcel, relying on a combination of sensors and modeling to provide pricing and insurance coverage for shipments. The software automatically converts sensor data into contextual insights, sending alerts if a shipment experiences potentially harmful deviations. “While there is no shortage of asset tracking and visibility platforms available, cargo losses keep rising year over year, and cargo loss ratios are not improving,” a spokesperson told VentureBeat via email. “Current solutions in the market are the equivalent of fire detectors — once a problem occurs, they wake up and notify stakeholders that something is wrong.  What they cannot do is actually diagnose the ‘how’ and ‘why’ behind that particular event of interest. Without a precise and automated diagnosis, business-to-business shippers find it virtually impossible to build accurate and consistent prediction models that enable supply chain risk mitigation across future shipments.” Paxafe claims its “adaptive insurance” calculations factor in critical supply chain factors, as well as confounding variables like weather and traffic. The platform also considers things like the product being shipped, the number of stops along the route, the time of the shipment, and the mode of transportation. Supply chain challenges have been amplified during the pandemic. For retail, it’s estimated that inventory is accurate just 63% of the time, on average. But stakeholders with superior visibility into their supply lines consistently outperform the competition. Seventy-nine percent of companies with high-performing supply chains achieve revenue growth greater than the mean within their industries, according to Logistics Bureau. Despite this, Paxafe says that over the past six months it has embarked on a series of pilots with enterprises across health care, perishables, oil and gas, logistics, manufacturing, jewelry, and insurance verticals. The company also launched a commercial version of its platform, converting a number of its pilot customers — about 10 so far — to commercial partners. “Paxafe was prelaunch when the pandemic hit, so it didn’t have a material impact on customer operations — more so on planning, fundraising, and go-to-market strategy,” cofounder and CEO Ilya Preston told VentureBeat in an email interview. “The pandemic has only reinforced the demand for technology visibility platforms like ours, given the pandemic’s impact on various supply chains — from product integrity and temperature control requirements for the vaccine itself to all of the lead time increases and delays that [are] impacting customer inventories, product availability, [and] time of arrival.” Ubiquity Ventures led Paxafe’s seed round announced today, bringing the Milwaukee, Wisconsin-based company’s total raised to date to over $3 million."
https://venturebeat.com/2021/04/16/facebook-claims-ai-can-predict-drug-combinations-to-treat-complex-diseases/,Facebook claims AI can predict drug combinations to treat complex diseases,"Facebook today detailed what it claims is the first single AI model capable of predicting the effects of drug combinations, dosages, timing, and other types of interventions like gene deletion. Developed in collaboration with Helmholtz Zentrum München, Facebook says the model could accelerate the process of identifying combinations of medications and other treatments that might lead to better outcomes for diseases. Discovering ways to repurpose existing drugs has proven to be a powerful tool to treat diseases including cancer. In recent years, doctors have seen success with “drug cocktails” to combat malignant conditions and continue to explore personalized treatments for patients. But finding an effective combination of existing drugs at the right dose is extremely challenging, in part because there are nearly infinite possibilities. Researchers would have to try from 5,000 to 19 billion solutions to find the optimal regimen given a pool of 100 drugs. Facebook’s open source model — Compositional Perturbation Autoencoder (CPA) — ostensibly addresses this with a self-supervision technique that observes cells treated with drug combinations and predicts the effect of new combinations. Unlike supervised models that learn from labeled datasets, Facebook’s generates labels from data by exposing the relationships between the data’s parts, a step believed to be critical to achieving human-level intelligence. CPA’s predictions take hours as opposed to the years that might elapse with conventional methods, allowing researchers to select the most promising results for validation and follow-up, according to Facebook. In biology, RNA sequencing is used as a way to measure the gene expressions of cells at the molecular level and study the effects of perturbations including drug combinations. Academia and industry have released RNA sequencing datasets containing up to millions of cells and 20,000 readouts per cell to facilitate biomedical research. Facebook leveraged these datasets to train CPA using an approach called auto-encoding, in which data is compressed and decompressed until summarized into patterns useful for prediction. CPA first separates and learns the key attributes about a cell, such as the effects of a certain drug, combination, dosage, time, gene deletion, or cell type. It then independently recombines the attributes to project their effects on the cell’s gene expressions. For example, if one of the datasets had information on how drugs affect different types of cells A, B, C, and A+B, CPA would learn the impact of each drug in a cell-type specific fashion and then recombine each in order to extrapolate interactions between A+C, B+C, and A+B. To test CPA, Facebook says it applied the model to five publicly available RNA sequence datasets with measurements and outcomes of drugs, doses, and other confounders on cancer cells. Benchmarked in terms of the R2 metric, which represents the accuracy of the gene expression predictions, Facebook claims that CPA “stayed consistent” between training and testing — an indication of robustness. Moreover, CPA’s predictions of the effects of drug combinations and doses on cancer cells matched those found in the testing dataset “reliably.” Facebook believes that CPA can “dramatically” accelerate the process of identifying optimal combinations of treatments, as well as pave the way for new opportunities in the development of medications. Toward this end, the company is making available APIs and a software package designed to let researchers plug in datasets and run through predictions. “Our hope is that pharmaceutical and academic researchers as well as biologists will utilize [CPA] to accelerate the process of identifying optimal combinations of drugs for various diseases,” Facebook program manager Anna Klimovskaia and research scientist David Lopez-Paz wrote in a blog post. “In the future, [CPA] could not only speed up drug repurposing research, but also — one day — make treatments much more personalized and tailored to individual cell responses, one of the most active challenges in the future of medicine to date.” While Facebook claims that CPA is novel in its architecture, it isn’t the first algorithm engineered to predict drug interactions. In July 2018, Stanford researchers detailed an AI system that can anticipate the effects of drug combinations by modeling the more than 19,000 proteins in the body that interact with each other and with medications. Researchers at the MIT-IBM Watson AI Lab, Harvard School of Public Health, Georgia Institute of Technology, and IQVIA more recently created an AI tool called CASTER that estimates potentially harmful and unsafe drug-to-drug interactions. A separate Harvard group has proposed applying AI to identify candidates for drug repurposing in Alzheimer’s disease. And researchers at Aalto University, University of Helsinki, and the University of Turku in Finland created a machine learning model that projects how combinations of drugs might kill various cancer cells."
https://venturebeat.com/2021/04/15/marketplacer-takes-another-step-in-growth-strategy/,Marketplacer Takes Another Step in Growth Strategy," Deepens integration with Salesforce Commerce Cloud  SAN JOSE, Calif.–(BUSINESS WIRE)–April 15, 2021– Marketplacer, a global technology Platform as a Service (PaaS) company that builds successful and scalable online marketplaces, announces the acceleration of its growth strategy through a new funding round including Salesforce Ventures and closer alignment with Salesforce Commerce Cloud. In addition to a capital investment, Marketplacer is certified for the Salesforce partner marketplace, adding capability to Salesforce Commerce Cloud’s enterprise customers to sell third-party products through their Salesforce Commerce Cloud instance. Marketplacer and Salesforce’s go-to-market strategies are tightly aligned, giving both companies the opportunity to innovate and work with customers more closely, and making it easier for Salesforce Commerce Cloud customers to grow through a marketplace strategy. Marketplacer Co-Founder and Executive Chairman, Jason Wyatt, is thrilled to count Salesforce Ventures amongst its partners and investors in the Marketplacer journey; “It is fantastic to be able to include Salesforce, an iconic name in the world of digital innovation, in the evolving Marketplacer story. We have a shared vision to connect business and its customers in new ways to enable them to grow faster, and with this partnership we are both able to deliver on these objectives even further.” With the newly launched Marketplacer cartridge for Salesforce Commerce Cloud, enterprises around the world now have the ability to accelerate their growth with ease. From implementing strategies such as driving sales from dropship sellers, adding new categories or third-party range extension – Marketplacer makes it possible. Marketplacer is used in both B2C and B2B environments and has helped over 90 enterprises build and deploy their own successful marketplace strategies, connecting over 20,000 businesses worldwide. “Building relationships and knowing your customer is key to the success of any commerce business,” said Lidiane Jones, EVP & GM, Salesforce Commerce Cloud. “For many customers, this requires a suite of integrations made possible by our partners. With Marketplacer’s cartridge for Salesforce Commerce Cloud, companies will be able to accelerate growth with a leading marketplace solution that takes businesses beyond physical stores and inventory holdings.” Rob Keith, Head of Australia, Salesforce Ventures adds, “Marketplacer is tapping into one of the most consequential trends for the industry. Marketplacer and Salesforce Commerce Cloud are well positioned to support our joint customers as consumers increase their digital spend and sellers seek new direct routes to reach their customers.” The investment follows a $20 million capital raise in Q4 of 2020 and the recent announcement of establishing U.S. operations and appointment of Jim Stirewalt as North American President. Across its portfolio, Marketplacer clients include some of the largest and most recognised brands today, such as Myer, BikeExchange, Metcash, FishBrain, Nokia, Surfstitch, Petstock, Providoor and Bob Jane T-Marts to name a few. For more information or to enquire about Marketplacer’s services, visit www.marketplacer.com. To view employment opportunities with Marketplacer Australia or US, please visit Marketplacer’s LinkedIn page for further details. About Marketplacer: Established in 2016 in Melbourne, Australia, Marketplacer is a global technology Platform as a Service (PaaS) company equipped with all the tools and functionality needed to build a successful and scalable online marketplace, at speed. To date, Marketplacer has helped over 90 businesses execute their own successful marketplace strategies and connected over 20,000 businesses worldwide. The Marketplacer platform exists to make growth simple, from implementing strategies such as driving sales from dropship sellers, adding new categories or third-party range extension – Marketplacer makes it possible. Marketplacer is responsible for the business transformations of some of Australia’s largest retail, brand distributor and franchise engines as well as communities, including Myer, SurfStitch, Metcash, Bob Jane T-Marts, FishBrain, Providoor and Petstock to name a few. About Salesforce Ventures: Salesforce is the global leader in Customer Relationship Management (CRM), bringing companies closer to their customers in the digital age. Salesforce Ventures, the global investment arm of Salesforce, invests in the next generation of enterprise technology that extends the power of the Salesforce Platform. Salesforce Ventures is building the world’s largest ecosystem of enterprise cloud companies and extending that technology to customers. Portfolio companies receive funding, strategic advisory, and operating support, and can easily join Pledge 1% to make giving back part of their business model. Salesforce Ventures has invested in more than 400 companies, including DocuSign, GoCardless, Guild Education, nCino, Twilio, Zoom, and others across 22 countries since 2009. For more information, please visit www.salesforce.com/ventures.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210415005967/en/ Nicole JordanRadix Collective for Marketplacer U.S.nicole@radixcollective.com"
https://venturebeat.com/2021/04/15/kryon-throws-down-the-gauntlet-for-better-rpa-governance/,Kryon throws down the gauntlet for better RPA governance,"Robotic process automation (RPA), which mimics human activity and automates mundane tasks, is all the rage. But privacy and governance concerns persist. Recognizing these challenges, Kryon recently became the first RPA vendor to earn ISO 27701 certification. “This framework is essential for any RPA company doing business in Europe, due to GDPR, or any other region with similar data privacy regulations,” Kryon CTO Shay Antebi told VentureBeat. He believes ISO 27701 could become the first widely adopted data privacy standard for RPA vendors. The ISO certification applies to real-time process discovery, as well as bot design, deployment, and management. RPA applications, called bots, are often programmed to access sensitive systems and information as part of process automation projects. An attacker can exploit access to these bots to steal data or gain unauthorized access to systems and applications in a cyberattack. RPA and process mining vendors have addressed several standards and best practices to ensure privacy. While ISO 27001 is an older certification for information security management systems (ISMS), ISO 27701 is an extension standard that builds upon and enhances that with a framework for privacy information management systems (PIMS) to secure and manage personally identifiable information. Kryon had already achieved ISMS certification back in 2019, so catching up with the new extension was a matter of building on this earlier work. Organizations looking to get certified to ISO 27701 will either need to have an existing ISO 27001 certification or implement ISO 27001 and ISO 27701 together as a single implementation audit. Enterprises need to maintain vigilance around industry-specific regulation, particularly in health care and finance, two of the largest markets for RPA. Enterprises using ISO-certified tools like Kryon’s will still need to ensure that their existing systems and applications that interact with RPA tools are compliant. RPA platforms often integrate with other applications on the back end to complete a process. For example, Kryon created a software bot for a health care organization in Israel that automates setting up appointments for patients to receive the two-shot COVID vaccine. That front-end bot, which chats with the patient, also interacts with the organization’s patient record system behind the scenes to complete the process. These applications need to be secured, as well. “This is a great example of when an upfront investment is absolutely necessary to protect yourself from potentially huge losses,” Antebi said. Meeting security certifications requires not only an investment of time and resources but also the right technology, processes, and framework. Security sometimes comes as an afterthought in the software development lifecycle. But it needs to be considered first for RPA to scale. “If the goal is widespread adoption of RPA in the enterprise, then the industry needs to deliver solutions with enterprise-grade security,” Antebi said. Kryon has been investing in solutions to push the envelope of privacy and governance further, such as a way to mask sensitive information in documents and on systems screens without losing the necessary context. Antebi said, “We are always looking for more ways to add value for our customers — offering the best security available is one way to do that.”"
https://venturebeat.com/2021/04/15/google-cloud-partners-with-osisoft-to-simplify-industry-cloud-migration/,Google Cloud partners with OSIsoft to simplify industry cloud migration,"Google Cloud announced a partnership with industrial data platform OSIsoft to make it easier for enterprises to migrate industrial workloads to the cloud. “Ultimately, we want to enable customers to harness the power of the cloud without having to depart from their existing systems,” Google Cloud managing director Dominik Wee told VentureBeat. “Our goal is for manufacturers to be able to scale across thousands of pieces of equipment and dozens of sites with ease — and manage this via a single control pane.” OSIsoft’s PI Integrator for Business Analytics makes it easier to pipe data streams into Google services such as Cloud Storage, BigQuery, and Pub/Sub. As part of the partnership, the companies released GCP deployment scripts for PI Core, the on-premises component moving PI runtime data to the cloud. Data from multiple sites is then consolidated and connected to Google Cloud’s Smart Data Platform for analysis. There are multiple ways to architect and deploy a PI system. OSIsoft created best practice topologies that were designed and tested by its own deployment team to optimize PI system performance. The new deployment scripts are based on OSIsoft’s latest topologies. “Customers’ deployments of PI Core in Google Cloud are significantly simplified with these scripts,” Wee said. OSIsoft provides core infrastructure for two-thirds of industrial Fortune 500 companies and supports over 2 billion data streams in industries as varied as energy, mining, oil and gas, utilities, pharmaceutical, facilities, and manufacturing. The company, which recently merged with United Kingdom-based industrial software vendor Aveva, builds out a special purpose control and management service for operational technology (OT), such as the pumps and motors in factories and plants. Wee said industrial enterprises are starting to move beyond the era of AI “pilot purgatory” to deliver real business value. In manufacturing, getting access to machine-level data is critical to enable AI-based decision-making. Google Cloud has been collaborating with manufacturing companies, independent software vendors, and other vendors — including Ingersoll Rand and Johnson Control — to optimize time-to-market for customers. The partnership with OSIsoft spans industries and enables the data ingestion pipeline to Google Cloud, where enterprises can apply Google’s AI solutions to deliver machine uptime and improve quality and overall equipment effectiveness. Wee said more partnership announcements are on the horizon. “Manufacturing is complex, and it requires deep domain expertise, particularly on the OT layer,” Wee said. “Our aim is to provide simplicity and choice.”"
https://venturebeat.com/2021/04/15/autonomous-trucking-company-plus-will-use-ai-and-billions-of-miles-of-data-to-train-self-driving-semis/,Autonomous trucking company Plus will use AI and billions of miles of data to train self-driving semis,"This article is part of a VB Lab Insight series paid for by Plus. The safest drivers are those with the most experience. Studies show it can take years of practice for automobile drivers to become careful and competent road users. Similarly, the more experience a truck driver has the less likely it is that they will cause a serious crash. What holds true for human drivers holds true for autonomous driving systems — up to a point. The safest self-driving vehicle platforms are those that have accumulated the most experience. Since driving experience is so important, how can technologists make sure computerized driving systems get the training they need to operate safely on the nation’s roads and highways? Solving this challenge is the key to unlocking a fully autonomous future. Thanks to advances in sensor technology and artificial intelligence (AI), an automated truck is capable of analyzing many objects on the road and making a decision about how to respond.  This is accomplished in large part by training so-called “deep learning” algorithms. Repeatedly expose a self-driving system to all kinds of obstacles, from a cut-in vehicle to a construction site, and the system will start to understand how to react when an obstruction appears on the highway. Here it is important to note that unlike people, machines lack common sense and don’t do well handling novel situations. Human drivers know to slow down in the face of an unexpected obstacle — a bear, say — because we can make decisions based on similar situations we have already encountered or extrapolate from other incidents. Unlike humans, however, deep neural networks can only learn from data they have been trained on, whether from public roads, closed courses, or computer simulations. So back to the original question: How do you train the machines so they are exposed to the full range of the driving experience? Plus’s goal is to help truck drivers on long-haul routes, where they encounter a variety of road and weather conditions. In addition to closed-road testing and computer simulations, the company’s PlusDrive system is learning on the open road, where the trucks can be exposed to real-world obstacles and situations. Junk flying from a pickup bed. Ice slicks. A wind turbine blade. A zigzagging motorcycle.  Though these so-called “long tail” phenomena comprise less than 1% of the time behind the wheel, knowing how to safely navigate them is critical for machines. Society expects that a computer-operated machine must be at least an order of magnitude safer than a human driver. Starting this summer, Plus will put its supervised automated driving system into factory production. It is also retrofitting existing trucks with the system. By this time next year, hundreds of automated trucks powered by PlusDrive will be on the road, hauling commercial cargo. Human drivers will be behind the wheel. Like an experienced professional training a new recruit, Plus drivers will monitor the autonomous trucks while teaching them how to handle unexpected obstacles. Plus estimates that its fleet will accumulate billions of collective miles before the company deploys fully driverless vehicles. Taking an evolutionary approach to full autonomy enables the company to rack up miles more quickly, with the assistance of on-board professional drivers who are training and validating the system. To support its global deployment in the U.S., China, Europe, and other markets, Plus recently raised $420 million in new funding. The drivers benefit too. The Plus supervised autonomous trucking solution elevates the role of the truck driver, upskilling them in preparation for an autonomous future. At the same time a digital co-pilot will ease driver exhaustion on long-haul routes, and fleets will spend less on the hiring process. The system yields other gains. Fuel comprises about a third of a trucking company’s operating budget, by far the largest cost for heavy trucks. When an automated system understands the road, pulling in GPS and weather data too, they optimize shifting and braking. Plus has run pilot projects showing that  PlusDrive saves 10% of the tank compared to the most efficient drivers, a win for the bottom line and the environment. Commercial space travel, solar-powered cities, autonomous vehicles — the first two visions of the future depend on specific economic inflection points, while the third is wholly dependent on the amount of data a system has accumulated. Plus is building the necessary feedback loop of information today. Its trucks are accumulating the data. Its drivers, who are among the safest and most efficient Class A drivers, are training the system with their responses. Its engineers are fine-tuning PlusDrive’s algorithms and decisions. And eventually PlusDrive will be one of the safest and most experienced drivers on the road. Plus is applying autonomous trucking technology to trucks today. For more information, please visit www.plus.ai. VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/15/misfits-gaming-esports-group-launches-women-of-misfits-speaker-series/,Misfits Gaming esports group launches Women of Misfits speaker series,"Misfits Gaming Group is leaning into female gamers with the launch of its Women of Misfits speaker series, and the esports company will turn this into a wider platform over time. The Boca Raton, Florida-based company will use this to elevate issues for women in gaming and esports, and it’s happening at a time when problems such as sexual harassment and under-representation of women at game studios and at esports organizations have been in the headlines. Women are a prevalent part of the esports and gaming landscape. Nearly 40% of all gamers are women, with 80% of them being 18 or older. The Women of Misfits initiative will provide a space for them to discuss ideas and be inspired by influential women both inside and outside the organization in addition to supporting the growth and development of women within MGG. We’ll have a Women in Gaming Breakfast at our GamesBeat Summit 2021 on April 28 and April 29. It a series of monthly guest speakers. The first are Chris Evert, 18-time Grand Slam singles champion and tennis legend; GloZell Green, a comedian and YouTuber; Bianca Smith, the first Black woman to serve as a professional baseball coach; Angela Ruggiero, the CEO Sports Innovation Lab and four-time Olympian and Gold Medalist for the U.S. Hockey team; and Maya Enista Smith, the executive director of the Born This Way Foundation. The focus of the Women of Misfits platform will be mentorship, development, network, and advocacy. The platform will be led by female executives within MGG such as chief development officer Hillary Matchett; president of media and branding Ella Pravetz; chief revenue officer Lagen Nash; president of Misfits Agency Amy Palmer; vice president of Communications Becca Henry; chief wellness adviser Carolyn Rubenstein; and cofounder Laurie Silvers.  The Women of Misfits platform includes a monthly speaker series with industry leaders and visionaries that will air on MGG’s YouTube channel. The sessions will be moderated by MGG executives and guest speakers will share topics that matter to them and inspire both the gaming community and women to pursue their dreams. “I am truly inspired and amazed with our women at MGG and their many accomplishments and eager to watch this platform ascend,” said Misfits CEO Ben Spoont in a statement. “The determination and dedication to push one another to break the boundaries as women within the esports industry is remarkable, and I am confident this platform will resonate not only within MGG but also within our wider community.”"
https://venturebeat.com/2021/04/15/user-experience-testing-and-monitoring-startup-userzoom-raises-100m/,User experience testing and monitoring startup UserZoom raises $100M,"UserZoom, a company developing software to measure digital experiences including apps and websites, today announced that it raised $100 million in a venture round led by Owl Rock. The proceeds, which the company says it plans to put toward expansion and hiring, come as UserZoom acquires EnjoyHQ, an experience insights startup that centralizes customer feedback and research findings into a single dashboard. Surveys show that design can be as important for the success of a digital experience as content and functionality. It’s estimated that every $1 invested in user experience nets a $100 return, a 9,900% ROI. And according to Forrester Research, frictionless user experience design could potentially raise customer conversion rates up to 400%. UserZoom lets customers establish criteria for user experience performance, track it over time, and then align it with business results. The metrics the platform records can be used to inform product strategy and information architecture, as well as to continuously test and validate throughout the design and development processes. With UserZoom, companies can evaluate and benchmark products with target users by conducting interviews remotely or funneling them through tests of structure, navigation, and ease of use. These customers can also collect and monitor surveys and invite website visitors or app users to a study, or create shareable reports based on notes, clips, and other aggregated test data. This kind of user experience testing and monitoring can be critical to preventing user frustration. PricewaterhouseCoopers found that 32% of customers leave a brand they loved after just one bad experience. Moreover, 13% of customers tell 15 or more people about their bad experiences, while 72% tell 6 or more people about good experiences. “COVID-19 has led to a significant acceleration in consumers’ rate of digital adoption and their expectations for a seamless, convenient and personalized experience,” cofounder and co-CEO Alfonso de la Nuez said in a press release. “This investment … is proof that businesses are waking up to the fact that traditional ways of designing and measuring digital experiences are no longer good enough. Today’s users and customers demand more, and leaders need the data and insights that UserZoom provides to allow them to quickly make confident, customer- and data-driven decisions to stay in front of the competition.” The user experience monitoring market is large and growing, with a value that’s expected to climb from $1.5 billion in 2019 to $3.7 billion by 2023, Markets and Markets anticipates. UserZoom competes with companies including Usabilla and Qualaroo, as well as UserVoice. But UserZoom claims to count tech giants like Google, Oracle, and half of the Fortune 100 among its clientele. And SVP of product Andrew Jensen believes that the integration of EnjoyHQ’s product will set UserZoom apart from the rest. “Our acquisition of EnjoyHQ is yet another example of our goal to simplify and improve the XIM process,” Jensen said, adding that he expects an integrated UserZoom and EnjoyHQ product to be available in the second half of 2021. “Our vision is to allow our customers to connect project insights from UserZoom with a broader customer experience and user experience integrated experience insights hub, where thousands of users across the organization can conduct analysis, share feedback, and discuss proposals needed to create seamless experiences that drive business outcomes.” This latest funding round brings UserZoom’s total raised to over $130 million. The San Jose, California-based company previously nabbed $34 million in October 2015 in a series A round."
https://venturebeat.com/2021/04/15/digital-transformation-stalwart-bizagi-hires-first-cio-to-boost-enterprise-automation/,Digital transformation stalwart Bizagi hires first CIO to boost enterprise automation,"Bizagi, one of the oldest players in the digital transformation market, recently hired Antonio Vázquez as its first chief information officer (CIO). The company hopes to improve its IT processes to take advantage of the rapidly growing RPA, process mining, and enterprise automation markets. In business since 1989, Bizagi provides business process management integration for all of the major RPA platforms. Bizagi’s tools make it easier to model and simulate processes, define business rules, and automate business processes. “I’m going to be that person who looks at all of the capabilities that are occurring in terms of business and all of the capabilities in terms of IT and defines the gap between them,” Vázquez told VentureBeat. He has also assembled a team of experts who will design business processes to address those gaps. The pandemic has demonstrated the importance of business continuity and resiliency in a rapidly shifting economy. But Vázquez is concerned that the shift to remote work and rise of the hybrid workforce have dispersed teams. He believes that moving to the cloud and eliminating as many legacy systems as possible is the best way to improve resiliency. Since Bizagi sells business architecture, Vázquez believes it’s critical to handle it in the same way internally as externally. He plans to consolidate Bizagi’s core capabilities in client and partner-facing technology into a single system and restructure the approach in terms of customer experience management. He will also look at governance and address cloud and security implications for internal processes. “Because we are focusing on both our own movement to the cloud and our customers’ transition, it’s critical to have a robust governance plan in place to ensure a seamless migration in all aspects,” Vázquez said. Finally, the company is also aligning its current internal infrastructure to make it easier to evaluate IT performance metrics against business goals. “The business side can’t expect IT to speak the business language and vice versa, but it’s necessary in order to compete on a grand scale,” Vázquez said."
https://venturebeat.com/2021/04/15/google-researchers-boost-speech-recognition-accuracy-with-more-datasets/,Google researchers boost speech recognition accuracy with more datasets,"What if the key to improving speech recognition accuracy is simply mixing all available speech datasets together to train one large AI model? That’s the hypothesis behind a recent study published by a team of researchers affiliated with Google Research and Google Brain. They claim an AI model named SpeechStew that was trained on a range of speech corpora achieves state-of-the-art or near-state-of-the-art results on a variety of speech recognition benchmarks. Training models on more data tends to be difficult, as collecting and annotating new data is expensive — particularly in the speech domain. Moreover, training large models is expensive and impractical for many members of the AI community. In pursuit of a solution, the Google researchers combined all available labeled and unlabelled speech recognition data curated by the community over the years. They drew on AMI, a dataset containing about 100 hours of meeting recordings, as well as corpora that include Switchboard (approximately 2,000 hours of telephone calls), Broadcast News (50 hours of television news), Librispeech (960 hours of audiobooks), and Mozilla’s crowdsourced Common Voice. Their combined dataset had over 5,000 hours of speech — none of which was adjusted from its original form. With the assembled dataset, the researchers used Google Cloud TPUs to train SpeechStew, yielding a model with more than 100 million parameters. In machine learning, parameters are the properties of the data that the model learned during the training process. The researchers also trained a 1-billion-parameter model, but it suffered from degraded performance. Once the team had a general-purpose SpeechStew model, they tested it on a number of benchmarks and found that it not only outperformed previously developed models but demonstrated an ability to adapt to challenging new tasks. Leveraging Chime-6, a 40-hour dataset of distant conversations in homes recorded by microphones, the researchers fine-tuned SpeechStew to achieve accuracy in line with a much more sophisticated model. Transfer learning entails transferring knowledge from one domain to a different domain with less data, and it has shown promise in many subfields of AI. By taking a model like SpeechStew that’s designed to understand generic speech and refining it at the margins, it’s possible for AI to, for example, understand speech in different accents and environments. When VentureBeat asked via email how speech models like SpeechStew might be used in production — like in consumer devices or cloud APIs — the researchers declined to speculate. But they envision the models serving as general-purpose representations that are transferrable to any number of downstream speech recognition tasks. “This simple technique of fine-tuning a general-purpose model to new downstream speech recognition tasks is simple, practical, yet shockingly effective,” the researchers said. “It is important to realize that the distribution of other sources of data does not perfectly match the dataset of interest. But as long as there is some common representation needed to solve both tasks, we can hope to achieve improved results by combining both datasets.”"
https://venturebeat.com/2021/04/15/rockset-integrates-real-time-analytics-platform-with-relational-databases/,Rockset integrates real-time analytics platform with relational databases,"Rockset today announced it has integrated its analytics database with both MySQL and PostgreSQL relational databases to enable organizations to run queries against structured data in real time. Rather than having to shift data into a cloud data warehouse to run analytics, organizations can now offload analytics processing to a Rockset database running on the same platform, Rockset CEO Venkat Venkatramani told VentureBeat. The Rockset platform is based on Facebook-developed RocksDB, an open source log structured database engine based on a key/value store that has been extended to support SQL queries. The approach enables organizations to offload queries to an indexing engine that can process sub-second queries while transactions continue to be processed using a relational database, Venkatramani added. The issue many organizations face today is that they already have extensive investments in open source relational databases. Neither MySQL nor PostreSQL are designed to process analytics at scale, which is one reason so many organizations have either adopted a NoSQL database or a data lake in the cloud. Replacing those databases with a proprietary relational database that can also process analytics in real time would be cost-prohibitive for many. Rockset is making a case for an alternative approach based on a Converged Index that can be employed to analyze structured relational data, as well as semi-structured, geographical, and time-series data in real time. Complex analytical queries can be scaled to include JOINS with other databases, data lakes, or event streams. All fields are entered into a converged index that includes an inverted index, a columnar index, and a row index. In addition to integrations with open source relational databases, the company also provides connectors to MongoDB, DynamoDB, Kafka, Kinesis, Amazon Web Services (AWS), and Google Cloud Platform, among others. As organizations collect data in real time, they increasingly also need to analyze it in real time, Venkatramani said. “Batch-based workloads are becoming real-time workloads,” he added. Moving data into a data lake using a batch-oriented process only provides a means to process a larger amount of historical data, Venkatramani said. IT organizations may still have a need for a data lake, but real-time analytics are going to be at the heart of most digital business processes, Venkatramani noted. Rockset earlier this year published the results of a Star Schema Benchmark test showing millisecond-latency query performance against the Star Schema Benchmark (SSB). The company claims it’s the only vendor to publish benchmarks showing it can execute queries up to 9.4 times faster than rivals while simultaneously ingesting 1 billion events a day with one second of data latency. The company last fall raised an additional $40 million to grow its workforce and accelerate product development and research while bolstering its go-to-market efforts. It’s not clear to what degree batch-oriented processes that have dominated IT architecture for decades will give way to real-time platforms. Historically, the data organizations have applied analytics to is usually several hours to a day old because the underlying database has typically been updated overnight using a batch-oriented process. Today organizations want to be able to continuously apply analytics to, for example, clickstream data from social media feeds — in real time, as it’s being processed. Other use cases include supply chain logistics and delivery tracking systems, gaming leaderboards, fraud detection systems, health and fitness trackers, and ecommerce applications. Of course, the days when organizations standardized on one database platform are long over. The challenge now is weaving a polyglot set of databases together in a way that allows an organization to take advantage of the capabilities of multiple platforms optimized for varying classes of workloads."
https://venturebeat.com/2021/04/15/only-13-of-organizations-are-delivering-on-their-data-strategy-survey-finds/,"Only 13% of organizations are delivering on their data strategy, survey finds","A new survey of C-suite data, IT, and senior tech executives finds that just 13% of organizations are delivering on their data strategy. The report, which was based on a survey of 351 respondents at organizations earning $1 billion or more in annual revenue, found that machine learning’s business impact is limited largely by challenges in managing its end-to-end lifecycle. MIT Technology Review Insights and Databricks conducted the survey, which canvassed companies including Total, the Estée Lauder companies, McDonald’s, L’Oréal, CVS Health, and Northwestern Mutual. Among the findings was that only a select group of “high achievers” — the aforementioned 13% — delivered measurable business results across the enterprise. This group succeeded by paying attention to the foundations of sound data management and architecture, which enabled them to “democratize” data and derive value from AI and machine learning technologies, according to the report’s authors. “Managing data is highly complex and can be a real challenge for organizations. But creating the right architecture is the first step in a huge business transformation,” report editor Francesca Fanshawe said in a press release. Every chief data officer interviewed for the study ascribed importance to democratizing analytics and machine learning capabilities. This, they said, will help end users make more informed business decisions — the hallmarks of a strong data culture. The respondents also advocated embracing open source standards and data formats. But what remains the most significant challenge is the lack of a central place to store and discover machine learning models, 55% of executives said. That’s perhaps why 50% are currently evaluating or actively implementing new, potentially cloud-based data platforms. As Broadridge VP of innovation and growth Neha Singh noted in a recent piece, many firms try to develop AI solutions without having clean, centralized data pools or a strategy for actively managing them. Without this critical building block for training AI solutions, the reliability, validity, and business value of any AI solution is likely to be limited. Organizations’ top data priorities over the next two years fall into three areas, all supported by wider adoption of cloud platforms, according to the report. These are: improving data management; enhancing data analytics and machine learning; and expanding the use of all types of enterprise data, including streaming and unstructured data. “There are many models an enterprise can adopt, but ultimately the aim should be to create a data architecture that’s simple, flexible, and well-governed,” Fanshawe continued. The MIT and Databricks findings come after Alation’s latest quarterly State of Data Culture Report, which similarly discovered that only a small percentage of professionals believe AI is being used effectively across their organizations. A lack of executive buy-in was a top reason, Alation reported, with 55% of respondents to the company’s survey citing this as more important than a lack of employees with data science skills. The findings agree with other surveys showing that, despite enthusiasm around AI, enterprises struggle to deploy AI-powered services in production. Business use of AI grew a whopping 270% over the past several years, according to Gartner, while Deloitte says 62% of respondents to its corporate October 2018 report adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of companies that have seen half their AI projects fail will tell you."
https://venturebeat.com/2021/04/15/bigeye-raises-17m-to-algorithmically-monitor-data-quality/,Bigeye raises $17M to algorithmically monitor data quality,"Bigeye, a data quality engineering platform, today announced it has raised $17 million in a series A round led by Sequoia Capital. The company says the funds will be used to improve its platform and help make it available to more data teams. Data is increasingly critical to enterprises and is woven into the products and services that directly affect customers. To keep pace, data engineering has increased in scale, complexity, and automation, leading to a number of significant workflow challenges. A clear majority of employees (87%) peg data quality issues as the reason their organizations failed to successfully implement AI and machine learning, according to a recent Alation report. San Francisco, California-based Bigeye, previously called Toro Data Labs, employs machine learning to enable companies to instrument data lakes and warehouses with thousands of data quality metrics. Founded in 2020, the platform automatically instruments datasets and pipelines with metrics, creating alerts driven by anomaly detection techniques. Bigeye uses connectors and read-only accounts to connect to data sources and record health metrics. Available in fully managed software-as-a-service form or as an on-premises app for enterprises, Bigeye samples objects like tables and generates recommended metrics based on data profiling and semantic analysis. By default, all metrics have automatic thresholds enabled — within 5 to 10 days, Bigeye learns the behavior of the metrics and begins to make adjustments. When those thresholds are reached, the platform sends alerts via email, Slack, and other channels and optionally triggers remediation steps. For one company, Bigeye identified that its customer data had a number of rows in which the values had been written into the wrong columns. The percentage of rows affected was small enough that analysts might not have spotted it, but at the scale that the company was working, it could have led to hundreds of customer support tickets that would have needed to be resolved. Bigeye can draw from Snowflake, Redshift, BigQuery, and other popular sources, and its no-code interface allows teams to create, edit, and read configuration and metric histories. The company says that as a part of its efforts to improve the platform, it recently increased support for service-level agreements, which can help engineers build trust through transparency with users. As processes around data remain a hurdle in adopting AI — 34% of respondents to a 2021 Rackspace survey stated poor data quality as the reason for AI R&D failure — observability solutions like Bigeye are attracting investments. There’s Aporia, Monte Carlo, and WhyLabs, a startup developing a solution for model monitoring and troubleshooting. Another competitor is Domino Data Lab, a company that claims to prevent AI models from mistakenly exhibiting bias or degrading. “Right now, modern data teams are held up by the heroics of data engineers, analysts, and data scientists trying to triage data quality incidents after something has already gone wrong. We’ve been the people who have to stay up until 3 a.m. on a Saturday trying to backfill a pipeline — and it doesn’t feel heroic,” cofounder and CEO Kyle Kirwan told VentureBeat via email. “For companies to realize the value of their data, it needs to be effortless for data teams to measure, improve, and communicate data quality for their organizations.” But Bigeye has already successfully courted large customers, including Instacart, Crux Informatics, and Lambda School. In addition to Sequoia, Costanoa Ventures also participated in Bigeye’s latest funding round. The three-year-old company has 11 employees, and the funds bring its total raised to $21 million."
https://venturebeat.com/2021/04/15/apple-facebook-and-two-different-visions-of-the-internet/,"Apple, Facebook, and 2 different visions of the internet","Presented by AdColony When 98.5% of your business is based on advertising and a genuine threat comes from another business that isn’t a direct competitor of yours, you’d probably consider that a crisis. In response to this crisis, Facebook took out a full-page ad in the New York Times. It’s not just Facebook that made noise and tried to out-engineer Apple’s guidelines. A coalition of major Chinese developers attempted to fingerprint devices via the CAID (Chinese Advertising Identifier), but Apple responded with a fast and detailed slap down of the attempt. Facebook’s full-page ad and larger PR campaign was more subtle than the CAID, but it’s a move that has been seen as desperate, particularly because of the primary argument that they were standing up to Apple “for small businesses everywhere,” when, in fact, the biggest impact will be their own bottom line. For them, nearly billions of dollars is at stake. And it’s all because of one company and their decision to give consumers a choice. Apple claims that users should know when their data is being collected and shared across other apps and websites, and they should have the choice to allow that or not. “App Tracking Transparency in iOS 14 does not require Facebook to change its approach to tracking users and creating targeted advertising, it simply requires that they give users a choice,” Apple said in a statement. What Apple is doing here is calling out the fact that, for lack of federal government regulation that protects consumers from what they believe to be a violation of privacy, Apple will go ahead and do it for them using the sheer power and ubiquity of its own platform. Sure, by touting “Privacy. That’s iPhone,” in a massive ad campaign and hoisting it up on a pedestal like they would a new piece of hardware, it can feel like privacy is their product. Apple has been using privacy as a differentiating factor in its market positioning for the past decade. Now, as part of that, they are hawking consumer choice. But ultimately what this comes down to is that Apple has a different vision of the future of the internet. For Apple, their vision is of a clean, curated web where content — at least the content that they are responsible for distributing — is from trusted sources, high-quality and is primarily paid for up-front or through subscriptions, not through advertising. This isn’t new news. Anyone who follows Apple could see this coming. In 2015, Apple Music became a subscription, then we saw streaming video (Apple TV+) and gaming (Arcade) added to the mix. And now, of course, there’s the bundle option of Apple One. So it’s not surprising that analysts believe this is the road they are heading down. The fast pace of technical innovation means consumers want to own the latest and greatest, and subscriptions offer flexibility to upgrade at a lower upfront cost. Additionally, Millennials and Gen Z tend to have a rent versus buy mentality, which applies not just to cars and homes but music and video streaming. It’s not just about philosophy, of course. Apple can say that they believe in privacy, a clean internet where you pay for premium content via subscriptions. But what it comes down to is they sell hardware, not software. Facebook and Google, on the other hand, are software companies. So of course they believe that the internet — and everything that lives on and around it, including content in mobile apps — should be free. For them, advertising is the “ultimate tax” you pay to access content. And, while you previously paid tax solely with your attention, it’s now paid with data. Thanks to documentaries like The Social Dilemma, as well as the massive increase in malware/spyware on the internet and cybersecurity hacks, consumers are becoming more aware of how deep that cost really is. In many ways, we are going back to the early days of the web where context was king and media was valued when it came from a trustworthy source, but in that world consumers need to pay up…with money. So — when Apple asks you if you want to be tracked across apps and websites, what they are really asking you is “How do you want to pay for your content?” We’re already seeing verticalization from ad networks and MMPs in an effort to combine information under the umbrella of “first party data” so as to not qualify their behavior as tracking under Apple’s App Tracking Transparency (ATT) framework. The other question is whether the CAID will see widespread enough adoption. Alasdair Pressney is Director of Product Strategy – Advertiser Products at AdColony. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/15/ibm-acquires-business-process-automation-startup-myinvenio/,IBM acquires business process automation startup MyInvenio,"IBM this morning announced it has acquired MyInvenio, a process mining software company based in Reggio Emilia, Italy. IBM says the buyout, which is subject to regulatory approval and expected to close by Q3 2021, will help enterprises tackle streamlining backend processes across accounting, sales, and production. Intelligent process automation, or technology that automates monotonous, repetitive chores traditionally performed by human workers, is big business. Forrester estimates that automation and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could eventually be automated in about 60% of occupations. And the pandemic is bolstering the uptake of automation, with nearly half of executives telling McKinsey in a report that their automation adoption has accelerated moderately. IBM’s purchase of MyInvenio builds on its existing OEM agreement with the company, announced in November. As a part of this deal, IBM plans to integrate MyInvenio’s capabilities with its automation portfolio, which spans IBM Cloud Pak for Business Automation, IBM’s hybrid cloud software, and over 600 prebuilt workflows on Red Hat OpenShift. IBM says MyInvenivo’s technologies might also eventually become available to IBM business partners to assist customers in generating insights into their workflows. MyInvenio, which was founded in 2013, offers a platform designed to spotlight inefficiencies, bottlenecks, and tasks in industries that can benefit from automation. MyInvenio can run simulations to assess automation opportunities and measure the benefits of applying automation at the outset. Leveraging historical process execution data and desktop behavior enables the platform to determine exactly where to apply RPA bots, automated decisions, and AI models. Companies can use this to examine the most common IT requests, for instance, and automate resolution. Paired with its automation services, IBM envisions MyInvenio’s software allowing organizations to streamline things like accounts receivable by looking at interactions with enterprise resource planning and invoicing systems. The software can help determine if employees spend a lot of time manually examining invoices and auditing purchase requests to find anomalies, two processes where AI-powered automation can potentially boost efficiency while cutting costs. The MyInvenio acquisition is IBM’s second this year after Salesforce consultancy 7Summits in January, and it follows IBM’s purchase of RPA developer WDG Automation last July — signaling the company’s ambitions in the enterprise automation space. According to Grand View Research, RPA alone is a market opportunity anticipated to be worth $3.97 billion by 2025. “Digital transformation is accelerating across industries as companies face increasing challenges with managing critical IT systems and complex business applications that span the hybrid cloud landscape,” IBM general manager Dinesh Nirmal said in a press release. “With IBM’s planned acquisition of MyInvenio, we are continuing to invest in building the industry’s most comprehensive suite of AI-powered automation capabilities for business automation so that our customers can help employees reclaim their time to focus on more strategic work.”"
https://venturebeat.com/2021/04/15/tetrascience-raises-80m-to-help-life-sciences-companies-analyze-data/,TetraScience raises $80M to help life sciences companies analyze data,"TetraScience, an R&D cloud data management company focused on life sciences and drug discovery, today announced that it raised $80 million in a series B round co-led by Insight Partners and Alkeon Capital. The company says that the funding will allow it to ramp up its go-to-market efforts, expand its technical capabilities, and double the size of its team. The pandemic underscores the need to accelerate scientific discovery toward delivering treatments that improve human life. This arguably starts with enabling breakthroughs in biopharma R&D, where some labs remain dependent upon legacy software and data stacks, resulting in bottlenecks. Fewer than 12% of all drugs entering clinical trials end up in pharmacies, and it takes at least 10 years for medicines to complete the journey from discovery to the marketplace. TetraScience, which was founded by former Harvard and MIT researchers, offers cloud storage, an API, and prebuilt integrations with lab instruments, informatics applications, analytics, and data science partners that pull in and reconcile data. TetraScience can automatically harmonize information across vendors and formats and enable users to share data, reports, analyses, and apps with internal teams. TetraScience offers tools for prototyping, building, testing, and scaling distributed data apps. A growing number of bundled apps are available from TetraScience as well as its network partners, and the platform natively supports data science tools, templates, models, pipelines, and custom configurations. As data and tools become more readily available, life sciences organizations will tap the technologies necessary to boost their output, some experts predict. For example, an IDC report anticipates that this year, 50% of pharmaceutical and biotech companies will use analytics or AI with IoT data to optimize the supply chain. The report also projects that 30% of life science organizations will have achieved data excellence by 2022. “COVID-19 has heightened the urgency to replatform to the cloud to take advantage of advanced data science and AI and machine learning as pharmaceutical companies face enormous societal, governmental, and shareholder pressure to radically accelerate discovery,” CEO Patrick Grady told VentureBeat via email. “Inbound interest in the Tetra R&D data cloud from global pharmaceutical companies has increased dramatically, despite very little investment in marketing.” For one customer — the Johnson Research Group at the MIT Department of Chemistry — TetraScience deployed its product with equipment types within a lab, including freezers and chemical fume hoods. Previously, the lab lacked control over nanoparticle synthesis reaction parameters including temperature monitoring, the ability to turn off a hot plate, and remote syringe pump activation. Now, scientists at the lab can see whether a batch is good based on how much the temperature fluctuated during the synthesis, eliminating the need for manual analysis. Sixty-one-employee TetraScience’s funding comes on the heels of the company’s 10-times revenue growth 1,000% uptick in annual recurring revenue (ARR) in 2020, reflecting additions to its customer base like Novo Nordisk, Merck & Co., and SmartLabs. (ARR is forecast to grow more than 300% in 2021, TetraScience says.) It brings the Boston-based company’s total raised to over $88 million following an $8 million series A in October 2019."
https://venturebeat.com/2021/04/15/keyfactor-125m-acquires-primekey-machine-identity-management-platform/,Keyfactor raises $125M and merges with PrimeKey to create a machine identity management platform,"In a world where machines outnumber people on the internet, identifying and authenticating those devices is becoming increasingly critical to security. Keyfactor, which helps automate the management of certificates for connected devices, today announced two big steps toward seizing this market opportunity. The first is a merger with PrimeKey, which creates the certificates that serve as crucial tools for identifying machines. The second is $125 million in new funding to power growth and comes two years after the company raised $77 million. Keyfactor is dubbing the merged company a “machine identity management platform.” “Now more than ever, enterprises must operate in a zero trust world, and machine identity management can no longer be ignored as part of an identity and access management (IAM) strategy,” Keyfactor CEO Jordan Rackie said in a statement. The company said that Keyfactor alone has seen revenues grow 50% year over year. The merger with PrimeKey will now allow Keyfactor to deliver end-to-end certificate service, which should help simplify identity management, even as systems become more complex. In a press release, Keyfactor executives note that Gartner has been bullish on Identity and Access Management Technologies. Many companies still fail to include machines in their authentication strategy, a major weakness that could create greater vulnerabilities. The ability to address that opportunity drew the $125 million investment from Insight Partners."
https://venturebeat.com/2021/04/15/cado-security-raises-10m-for-cloud-cybersecurity-forensics/,Cado Security raises $10M for cloud cybersecurity forensics,"Digital forensics platform Cado Security today announced a $10 million series A investment led by Blossom Capital, with participation from existing backers. The funds bring the company’s total raised to $11.5 million and will be used to support growth in engineering, customer support, and go-to-market operations. Some experts estimate that legacy forensics tools only provide 5% or less of the data needed to investigate a cloud attack. Forensics analysts often determine that an attack is not worth further investigation, due to the level of effort required to dig deeper. But these attacks aren’t slowing. Some 20% of organizations get hit with cyberattacks six or more times a year, and 80% say they’ve experienced at least one incident in the last year so severe it required a board-level meeting, according to a report from IronNet. James Campbell and Chris Doman founded Cado Security in 2020 with the goal of addressing challenges in cloud security forensics. Campbell, who previously led PricewaterhouseCoopers’ cyber response service and Australia’s national Australian Signals Directorate as associate director, teamed up with ThreatCrowd creator Doman to build a forensics platform that speeds up investigations of cloud attacks. “We founded Cado Security right in the midst of the pandemic in April 2020, as enterprises were shifting to the cloud, to enable their remote workforces to successfully work from anywhere,” Campbell told VentureBeat via email. “This uptick in the cloud introduced new complexities and risks enterprises had never seen before. Security teams didn’t have the time to become experts in the cloud amidst the shift, and hackers noticed.”  Cado Security automatically captures and processes data to visualize and investigate attacks, leveraging an analysis engine that detects malicious files, suspicious events, personally identifiable information, and financial data. Employing a combination of full-content inspection, log parsing, event correlation, and machine learning models, Cado Security’s platform indexes files and logs for later inspection, creating a human-readable timeline of events. “[Our] platform has a unique detection engine that uses machine learning in order to identify financial or personally identifiable data across systems that have been impacted by an event,” Campbell explained. “Many of the existing solutions provide an incident overview, which represents a fraction of the actual data related to the event, meaning you’re more likely to miss something big … [Cado] can see data attempting to be exfiltrated by a hacker, even when they are not using any malicious software to evade detection.” According to Gartner, nearly 70% of enterprises plan to accelerate spending on cloud services in 2021. As more data moves to the cloud, attacks on cloud infrastructures are increasing significantly, putting new pressures on security teams to respond quickly. Cado Security claims it has seen “significant demand” despite competition in the over $34.5 billion cloud security market. Netskope recently raised $340 million at a $3 billion valuation, while Valtix nabbed $14 million in June 2019. There’s also Bitglass, which raked in $70 million for its cloud-native platform that helps companies monitor and secure employee devices. “Data is moving to the cloud at an alarming rate. We founded Cado Security to help enterprises quickly and easily conduct deep forensic investigations across modern cloud environments to stay one step ahead of today’s cybercriminals,” Campbell said. “Our platform is [one of the few solutions] that can capture data across short-term environments, such as containers and auto-scaling infrastructures, enabling security teams to effectively investigate threats.” Ten Eleven Ventures also participated in London-based Cado Security’s latest funding round."
https://venturebeat.com/2021/04/14/dell-finally-spins-off-vmware-stake-in-9-7b-deal/,Dell finally spins off VMware stake in $9.7B deal,"(Reuters) — Dell Technologies said on Wednesday it would spin off its 81% stake in cloud computing software maker VMware to create two standalone public companies in a move that will help the PC maker reduce its pile of debt. VMware is currently Dell’s best-performing unit, as it has benefited from companies looking to cut costs and move to the cloud, a shift that has been accelerated by the COVID-19 pandemic. Shares of Dell rose more than 8% in extended trading. VMware will distribute a special cash dividend of between $11.5 billion and $12 billion to all of its shareholders, including Dell, which will receive between $9.3 billion and $9.7 billion. For Dell, the special dividend will help reduce its long-term debt of $41.62 billion, much of which was taken on during its $67 billion acquisition of VMWare’s then-majority owner EMC in 2016. The companies said the deal will simplify their capital structures. Both companies will also enter into a commercial arrangement to continue to align sales activities and for the co-development of solutions. VMware, whose software helps companies squeeze more work out of datacenter servers, has been looking for a CEO after previous boss Pat Gelsinger was tapped to lead Intel. Dell first announced the spinoff plans in July last year. The deal is expected to close in the fourth quarter."
https://venturebeat.com/2021/04/14/building-a-company-culture-that-directly-impacts-your-bottom-line-vb-live/,Building a company culture that directly impacts your bottom-line (VB Live),"Presented by TriNet How do you create a company culture that empowers your employees to keep growing and gives you a competitive edge? Learn about what makes a successful company culture, how to strengthen your culture to align with your organization’s values and goals and more in this VB Live event. Register here for free. “Company culture is unbelievably instrumental in driving your top-line and bottom-line goals,” says Deepa Gandhi, co-founder and COO of the handbag brand Dagne Dover. “Often people get too focused on hitting KPIs and target revenue goals and forget that the people are the ones who actually drive that.” For Gandhi, it’s a simple equation: If you have happy people, workers who are excited to come to work, and excited to work at your company, then they’re going to be that much more motivated to do their best job. Customers feel that energy, sense when a brand’s external image is matched by the energy of its people, she says. “We started our company saying we wanted to make sure we didn’t just sell great handbags, but we also wanted to build a company with great culture, female-forward, and flexible,” she says. “And we hire for culture.” That, she says, is the number-one rule. Of course, table stakes for recruitment is ensuring a candidate has the requisite skills to do the job. But a job interview should focus on establishing the relationship and culture fit from the start. You want to know who this person, is, what they care about, and what they value. You want them to know about your company, who you are, and how you like to work, and ensure there’s strong cultural affinity. Even somebody who doesn’t have as much experience, but seems really promising can be a strong cultural fit. “You could have the most experienced person out there, and if they’re not a cultural fit, they can end up being toxic for the team,” she says. “Hire for culture, because if you don’t bring in people that support what you’re trying to build internally, it’s all going to break down in the end.” To create a positive company culture that attracts and retains that kind of talent, listening to your employees, to your teammates, to your co-founders, is pivotal. You may have a specific perspective on what you believe is a positive culture or a positive workplace experience, but that might not be what your employee finds important or what they’re ultimately looking for in a career. “Being able to listen and then adjust and evolve your team culture accordingly — that makes a massive difference,” she explains. Another piece is incentivizing people based around culture. For performance evaluations, it’s not enough to look at an employee’s technical or job performance – it’s critical for Gandhi and team to look at how they contributed to the culture. “The ones that go above and beyond to build a better, stronger culture for us, we reward them for that, and it becomes a motivating factor,” she says. This kind of environment, in which your people feel welcome, accepted, and valued, directly contributes to innovation. A flat hierarchy should cast a wide circle in the brainstorming and creative process, where everyone’s voices hold equal weight and purpose – as Gandhi says, you never know where a great idea will come from. Contrast that with organizations that have built stiff barriers between cross- functional partners. “I’ve worked at other companies where the design team did not want to hear from anybody else at the company,” says Gandhi. “Especially in retail companies, your creative and design team hardly interface with the more analytical business and financial side of the company. As a result, you end up often having an imbalance where one overpowers the other, and one feels undervalued.” At Dagne Dover, the finance and analytical side of the business don’t dictate to the design side about what to make and how to sell, but share data and analytics with their counterparts, and make decisions together. Creative partners bring new ideas to the business side for input on making them work from a revenue perspective, and launch products in a way that optimizes not just for creative and brand goals, but also revenue goals. “Our thesis is, let’s empower each other through the decision-making process,” she says. “It’s a great conversation, and one you don’t see often enough.” To learn more about why a good company culture is so essential, what makes a culture great, how to encourage employee passion and buy-in, and more, don’t miss this VB Live event. Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/14/domino-accelerates-mlops-with-new-nvidia-integrations/,Domino accelerates MLOps with new Nvidia integrations,"Domino Data Lab announced new integrations with Nvidia this week to make it easier to adopt AI infrastructure, scale GPU clusters, run more virtual workloads on high-end GPUs, and package AI apps into container infrastructure. Domino’s tools streamline the grunt work associated with building out AI and ML applications. Domino automatically spins up workspaces or models on shared infrastructure so many people can share the same infrastructure. When someone is finished with a workload, Domino spins down that workspace to free up the resources for someone else. Domino also tracks usage, letting IT administrators see consumption and make informed decisions about when to increase computing power. Gartner considers AI orchestration tooling that includes MLOps to be a key trend in 2021. Domino currently supports ephemeral clusters built on Apache Spark and Ray, and the company plans to add support for Dask this fall. Domino strategic partnerships VP Thomas Robinson told VentureBeat that Spark has traditionally excelled at large-scale data processing and transformations. Ray has simplified distributed training and hyperparameter optimizations, and Dask has excellent integration with commonly used Pandas and NumPy libraries. Domino also improved the ability to provision GPU clusters required to run AI training jobs that require more than one Nvidia GPU. Traditionally, it could be difficult and time-consuming to set up machines, ensure network connectivity, and install proper libraries. In addition, it is uncommon for enterprises to give data scientists access and permission to manipulate infrastructure directly. As a result, teams often leave clusters idle between larger projects, rather than reallocate the individual machines for smaller projects. To improve utilization rates, Domino makes it possible to spin up and spin down interactive sessions, batch jobs, or models hosted on Nvidia DGX infrastructure to allow multiple concurrent and consecutive sessions. Previously users depended on email and spreadsheets to coordinate workloads, which was inefficient. Domino will add support for Nvidia’s multi-instance GPU technology in September. MIG allows a single GPU to be sliced up into smaller portions (7 slices per GPU for each of the 8 GPUs in a DGX A100 — a total of 56 slices). This will make it possible to divide the capacity of a larger GPU server or cluster into multiple instances or partitions to host many more predictive models on smaller GPU instances. While many deep learning training workloads require a whole machine or multiple machines in a cluster, research, or inference (prediction), workloads are much less GPU-intensive. “By allowing the GPU to be portioned into pieces, you can have more researchers doing discovery work in notebooks on smaller GPU slices,” Robinson said. Domino also announced immediate support for Nvidia’s new NGC container registry service. This makes it easier to package vetted application and configuration settings into container instances that bake in best practices. This means a data scientist doesn’t have to spend time figuring out how to set up and install all the drivers and tools they need. It also allows organizations to standardize on these containers. NGC currently supports RAPIDS, TensorFlow, PyTorch, and CUDA. Domino additionally supports containers for SAS, MATLAB, Amazon SageMaker, and private container repositories. Finally, Domino worked with Nvidia and NetApp to develop a preconfigured hardware/software package called the ONTAP AI Integration Solution. “This is a specced, tested, and verified packaging of everything you need to accelerate your data science work — so there’s no guesswork and no setup needed for an IT department,” Robinson said."
https://venturebeat.com/2021/04/14/linux-foundation-creates-research-division-to-study-open-source-impact/,Linux Foundation creates research division to study open source impact,"In the latest sign of the growing influence of open source software, the Linux Foundation announced it is creating a new research unit to provide greater insight into the technology, as well as the people creating it. Linux Foundation Research will be a new division with a broad mandate to explore the social and technical aspects of open source in the hopes of expanding the types of people who participate and encouraging more enterprises to adopt the technology. Among the group’s priorities are examining diversity and security. “As open source has become a fundamental part of the global technology supply chain, we started to see a need to really provide deep insight so that we could all collectively make better decisions about how to improve the way development communities work, improve incentives to create better software outcomes, and understand the economics of open source,” Linux Foundation executive director Jim Zemlin said. Discussions about creating the research group had percolated as the Linux Foundation conducted various research projects in collaboration with other institutions over the years. As helpful as those projects were, they also began to highlight knowledge gaps. Eventually, the foundation believed its work with thousands of companies and hundreds of thousands of developers would give it an immense amount of data it could better leverage through its own organization. Hilary Carter, who has made a mark in research at the Blockchain Research Institute, has been recruited to oversee the new research unit. She said she still expects much of the research to be conducted in collaboration with other organizations. Her core objectives include creating awareness of what is happening in open source communities, what important products are being developed, and what vulnerabilities lie in open source ecosystems. “In creating greater awareness, we hope to increase participation in those communities,” Carter said. “And I think one of the ways we do that is by addressing some of the issues within communities, particularly around diversity and inclusion.” She’s also hoping more people will be inspired to join those communities by highlighting how open source software can impact society in terms of diversity, social justice, and climate change. As part of the new initiative, the foundation will also create the Linux Foundation Research Advisory Board, which will gather external experts to help develop the research agenda."
https://venturebeat.com/2021/04/14/nvidia-forms-inception-vc-alliance-to-connect-ai-startups-with-venture-capital/,Nvidia forms Inception VC Alliance to connect AI startups with venture capital,"Nvidia has formed its Inception VC Alliance to connect AI startups with venture capital. The move will help connect more than 7,500 startups in the company’s Inception program for AI tech with venture capital firms. Jeff Herbst, vice president of business development and head of Inception at Nvidia, unveiled the alliance today at the AI Day for VCs event during Nvidia’s annual GTC 21 conference. Nvidia CEO Jensen Huang unveiled the company’s latest products on Monday in a keynote speech where he talked about the company’s new Grace central processing unit (CPU). “We always felt a very strong connection to the ecosystem. We give them technology, we introduce them to our 150 different software development kits, we give them joint marketing, we introduce them to investors,” Herbst said in an interview with VentureBeat. “We give them Cloud Credits. We give them discounts for GPUs.” AI adoption is growing across industries, and startup funding has been booming. Investment in AI companies increased 52% last year to $52.1 billion, according to PitchBook. The Inception AI startups are up 9 times from 2016, Herbst said. The alliance aims to help investment firms identify and support leading AI startups early, as part of their effort to realize meaningful returns down the line. The goal is to educate VCs about AI opportunities and nurture startups, Herbst said. “AI is growing like a weed. We’re over 7500 companies, and it’s not going to be long before we’ve doubled that,” he said. “The ecosystem is clearly exploding. And VCs are a super important part of it. Startups need VCs, and VCs need startups. It’s just that simple fuel for startups to grow. We have thousands of VCs that are already part of our ecosystem, but we’ve never formalized the partnership with them until now.” Founding members of the alliance include venture firms NEA, Acrew, Mayfield, Madrona Venture Group, In-Q-Tel, Pitango, Vanedge Capital, and Our Crowd. More VCs can apply here. The Nvidia Inception VC Alliance is part of the Nvidia Inception program, an acceleration platform for startups working in AI, data science, and HPC. These startups represent every major industry and are located in more than 90 countries. Among its benefits, the alliance offers VCs exclusive access to high-profile events, visibility into top startups actively raising funds, and access to growth resources for portfolio companies. “It’s both a corporate goal and a personal goal to extend this ecosystem around the world,” Herbst said. Nvidia currently counts about 40 companies it has invested in directly. Around 300 Inception companies are making presentations at the GTC 21 event, which is expected to have an online audience of about 150,000. And around 35 of the startups are in emerging markets, Herbst said. “Is there parity in the world with AI startups? No,” Lopez Research analyst Maribel Lopez said on the panel. “Do we have a long way to go? Yes. But I’m seeing exciting things like Cuda, a fintech startup in microfinance in Africa.” These startups are using AI for a wide range of tasks, like figuring out what percentage of fisheries in the world are operating illegally. “Now that Jensen has shown the roadmap, people know that Nvidia is a complete platform, with CPUs, GPUs, DPUs, and everything that enables these startups to do their life’s work.” On Monday, Herbst moderated a panel on investing in startups around the globe and the need to create a more diverse ecosystem for entrepreneurs. He estimated there are 12,000 to 15,000 AI startups around the world and said Nvidia is only in touch with about half of them through Inception. “It’s an open invitation to join our ecosystem,” Herbst said. “Nvidia loves startups.” Herbst said about 16% of Inception members are part of the health care industry. Growth areas include robotics, self-driving cars and trucks, and data science."
https://venturebeat.com/2021/04/14/how-wayfair-and-burts-bees-optimize-digital-creative-for-every-social-platform-vb-live/,How Wayfair and Burt’s Bees optimize digital creative for every social platform (VB Live),"Presented by yellowHEAD In digital marketing, there’s no such thing as a one-size-fits-all creative for all platforms. To really perform, you need data. Learn how to use data to optimize your creatives for every channel, align your strategy with user demographics, and get results in this on-demand VB Live event. Access free on demand right here. Digital marketers face a singular challenge: trying to educate and win customers over and unlock loyalty in an increasingly fragmented media and advertising landscape, says Courtney Lawrie, global head of brand and integrated growth marketing at Wayfair. At Wayfair, they’re well positioned to tackle this challenge, however. “We’ve taken a data-driven and performance-oriented approach in our personalization and relevance initiatives since day one,” Lawrie says. “We make that happen at scale by leveraging our in-house software programs, our machine learning algorithms and analytics, to ensure that we’re serving every customer the optimal content at the right time on any touch point across all different types of platforms.” In the home category, she explains, customers are driven by inspiration, and often have a unique vision for their home. With data, they’re working to anticipate their customers’ needs and to create a shopping experience uniquely tailored to their tastes, which directly informs content and creative asset creation efforts. At Burt’s Bees, says Melissa Culbertson, associate manager of brand engagement and social strategy, they look at engagement and reach from an organic social standpoint. However, they’ve started to pay closer attention to the data on deeper levels of engagement, particularly around comments and shares. It’s easy for people to tap to like a video or an image when you’re scrolling through TikTok or Instagram, but comments and shares take a bit more effort, and that’s a better gauge of engagement. But when measuring content performance, they drill down to content pillars — for instance, a values-based piece of content, such as organic social for Earth Day wouldn’t be measured against a product-based piece of content. They’re also able to compare content formats against one another. They know from data that GIFs and video-based content on Instagram, for example, don’t perform as well as static assets. “That doesn’t mean we don’t do that, because sometimes for the story you need to tell you need a longer-form piece of creative than just an image,” she says. “But we would measure video-based content against video-based content to get a better idea of whether it was successful or not.” At Wayfair, they’re exploring the opportunity to increase awareness in other emerging home categories, like home improvement and renovation, large appliances, and housewares. Those emerging categories lend themselves to some of these subcultures. In the home improvement space, for example, they’re leaning in and looking at DIY enthusiast subcultures, particularly during the pandemic. “Subcultures can spread your brand’s message to a whole new audience with already ingrained loyalty, which is kind of amazing,” says Kinney Edwards, global head of Creative Lab at TikTok. “But you have to do the work. You have to learn their language, speak to them in a real and authentic way. It takes more effort to craft your message creatively. But the reward is in the way in which the subculture community will adopt your messages and become brand ambassadors for you in an organic and passionate way.” There’s an opportunity to tap into these communities and tell engaging stories across social platforms. The key is to remember that each social platform serves a purpose in a user’s life, says Noa Miller, marketing creative strategist at YellowHEAD, and that information is as essential to the creative concept as drilling down to a user’s demographic information. On each platform, a user is consuming content very differently. “We believe that we need to come up with a strong creative concept, and then have that concept be translated to fit what a person is doing once they arrive at these different platforms,” Miller says. For example, on TikTok, the first thing a user sees is a video with sound on. On Facebook, they’ll be scrolling through videos with no sound on. “I need to understand, as a marketer, what’s the way that our audience is consuming their media on each platform, and then create creatives that are a perfect fit for that,” she says. YellowHEAD worked with Tinder to create a campaign around their new video chat feature, creating three different ads with the same concept, but each from a different platform’s perspective: one for Facebook, one for Instagram, and one for TikTok. Facebook scrolling time is about three seconds, which requires a catchy opening to make the audience stick and want to see more of the ad’s story. On Instagram, they cropped the video into three story ads, each highlighting a different fact about why video chatting is a great idea, to keep the consumer skipping from one to the other, and see that there’s a storyline. For TikTok, they filmed the ad with real people, and designed it to look as native as possible to the platform. “We try to instill in brands and marketers that there are best practices to getting to that quality creative,” Edwards says. “Understand the ecosystem by engaging with it, playing around, putting that mindset on, because as you’re creating content for these users, you want it to feel like it’s for them and not for you.” That means building a narrative, and approaching the platform with an audience-first mindset, he adds. The community respects when you’re direct and to the point, but delivering that message in a way that demonstrates you get the platform. However, Culbertson says, her best piece of advice is that while the platform’s best practices are an excellent guide, not to rely on them solely. “Use that as a starting point, but then test different creatives, see what works and what doesn’t. It’s very individualized to your brand, your company and so on,” she explains. “Based on that data, you can iterate and build better creative as you go along.” “Don’t try to be perfect and overthink it,” Edwards adds. “Action drives action. Trends, culture that happens so quickly, you don’t want to get caught up in doing it perfectly. You want to be part of what’s happening. Just dive in.” For the performance-oriented marketers and brands, Lawrie adds, “Don’t get too bogged down by short-term metrics. Make sure you also look at your creative over a long-term horizon. Make sure those short-term metrics are a proxy for long-term outcomes.” And Miller notes that creating different creatives for each platform shouldn’t break your budget. “It doesn’t always have to be a big production — things can be done easily,” she says. “Today everyone is a content creator. Try to make it fit to the platform without additional big productions needed.” For more about creating authentic brand stories that really work, across every platform, accessing and leveraging data to ensure your message is on target, and more, don’t miss the rest of this VB Live event Access free on demand here. You’ll learn: Speakers:"
https://venturebeat.com/2021/04/14/paving-the-yellowbrick-road-to-closer-integration-with-cloud-data-stores/,Paving the Yellowbrick road to closer integration with cloud data stores,"Traditionally, data warehouses meant enterprises had to either commit to one platform or suffer the complexity of managing multiple quasi-compatible infrastructures. With that in mind, Yellowbrick Data recently launched Yellowbrick Manager to make it easier for administrators to manage data warehouses across distributed cloud and on-premises deployments with a simplified interface. Users can control Yellowbrick data warehouses in public clouds, on Yellowbrick hardware instances, in private clouds, and even at the network edge, the company said. Yellowbrick Manager provides a unified control system that uses the Kubernetes container orchestration system to enable users to manage and control both cloud and on-premises deployments with enhanced performance capabilities. “It’s this single unified control plane, together with the adoption of Kubernetes in our database software, that sets us apart,” Yellowbrick CTO Mark Cusack told VentureBeat. The company also added agile data movement capabilities to help customers integrate Yellowbrick with data lakes built on cloud object stores like Amazon S3. A technology preview is expected in early May, and general availability is set for the second half of 2021. Also in early May, there will be an update to Yellowbrick data warehouse release 5, with data lake integration enhancements that include native support for cloud object storage (including Amazon S3 and Azure Data Lake Storage Gen 2). One of the criticisms of hybrid clouds has been around the disjointed set of technologies and user experiences across private and public clouds. There are different identity and access management approaches, which makes it harder to govern access to data across these environments, and they are typically provisioned differently. Yellowbrick is positioning itself as the first data warehouse for the distributed cloud by introducing a unified control plane that works across the most common cloud platforms. This control plane increases the simplicity of running multiple data warehouses in different physical and line-of-business locations across an enterprise. The Kubernetes cloud native architecture provides Yellowbrick Manager with a single unified control panel to provision new data warehouse instances in different clouds, manage existing infrastructure, and monitor deployments. Yellowbrick said Yellowbrick Data Warehouse queries run 3 times faster on Andromeda-optimized instances for private clouds than on the company’s first-generation architecture. The performance improvements are also the result of the company switching to new AMD 64-core CPUs, increasing bandwidth between server notes, and adding dual proprietary Kalidah scan accelerator cards that offload workloads such as filtering, compression/decompression, and row/column transposition from CPUs. “The Andromeda instance is the fastest platform to run Yellowbrick on,” Cusack said. The company is optimizing deployments to take advantage of differences across the big cloud providers. For example, if a public cloud instance has high-performance storage, Yellowbrick can adapt to the underlying hardware to take advantage of those benefits. Yellowbrick is focusing on improving the management aspects for distributed clouds rather than joining them together directly. Competitors tend to fall into two major categories: legacy data warehouses such as Teradata, Oracle, and SQL Server or cloud-only data warehouses like Snowflake, Amazon Redshift, and Microsoft Azure Synapse. Yellowbrick seeks to differentiate itself by taking a unified approach to addressing distributed data challenges like data gravity and sovereignty requirements. It was designed from the ground up to optimize price/performance across bare metal and virtualized infrastructure in public clouds. Yellowbrick product marketing VP Justin Kestelyn says Yellowbrick has an advantage over legacy vendors that have been doubling down on older architectures and have a harder time with real-time analytics. Traditional cloud vendors have not been aggressively pursuing hybrid options for analytics at the edge. “We’re winning business from all of these vendors by providing the best price/performance and the lowest deployment risk,” Cusack said."
https://venturebeat.com/2021/04/14/rocket-software-acquires-asg-technologies-to-boost-infrastructure-management-tools/,Rocket Software acquires ASG Technologies to boost infrastructure management tools,"Rocket Software this week announced it intends to acquire ASG Technologies as part of an ongoing effort to expand the reach of its portfolio. Terms of the deal were not disclosed. ASG Technologies is best known for providing a configuration management database (CMDB) that is widely employed in IBM environments. But in recent years, it has expanded its portfolio to include infrastructure management tools, as well as business process management (BPM) and content management software it gained by acquiring Mowbly in 2018 and the acquisition of Mobius Management Systems in 2007. Rocket Software, which is privately held by Bain Capital, has historically focused on middleware and tools that are employed to modernize mainframe environments. With the acquisition of ASG Technologies, the company will expand the scope of its product offerings to include more infrastructure management tools and software further up the application stack, newly appointed Rocket Software president Milan Shetti told VentureBeat. Rocket Software sees the acquisition of ASG Technologies as part of a larger strategy to expand its reach across the enterprise, Shetti noted. The overall size of the mainframe market opportunity, however, continues to grow as the IT platform becomes more tightly integrated with distributed computing platforms running inside and outside of cloud computing environments, Shetti added. In the months ahead, the company will continue looking to expand its portfolio through organic and inorganic acquisitions, Shetti noted. Over the course of its history, Rocket Software has made more than 45 acquisitions, including Zephyr, Shadow, Aldon, and D3. The acquisition of ASG Technologies will present opportunities across a soon-to-be expanded product portfolio, Shetti noted. “We will continue to be an acquisitive company,” he said. Of course, there is no shortage of rival offerings for managing applications and IT infrastructure in what is becoming an extended enterprise computing environment that reaches from public clouds all the way to the network edge. At the same time, the management of data and the applications employed to create that data are becoming more disaggregated. As that trend continues, the need for more sophisticated tools that can manage what is evolving into multiple centers of data gravity will become more pressing. One of the areas Rocket Software will invest in is developing the machine learning models needed to automate the management of a wide range of IT management tasks, Shetti said. In effect, an arms race to build next-generation tools for managing enterprise IT environments — also known as AIOps — is now well underway. There is no consensus on how sustainable AIOps is, given the degree to which all IT management tools will employ machine and deep learning algorithms. But the next generation of IT management platforms will continuously learn about the IT environment as changes and updates are made. It’s not likely these tools will replace the need for IT administrators as IT environments continue to become more complex. However, the amount of time spent trying to discover the root cause of a specific IT issue should be sharply reduced. In addition, those platforms will enable IT organizations to compensate for a current shortage of IT skills that limits the degree to which such environments can scale. In theory, an IT team should be able to leverage AI platforms to manage several orders of magnitude more workloads as IT becomes more automated. It may be a while before the promise of AIOps is fully realized, but the future of IT management can already be seen in large enterprises. It’s just a question of how long it might take for those AI capabilities to be pervasively applied across all enterprises."
https://venturebeat.com/2021/04/14/gains-in-cloud-digital-transformation-boost-sap-2021-q1-revenue-outlook/,"Gains in cloud, digital transformation boost SAP 2021 Q1 revenue outlook","(Reuters) — German software group SAP on Tuesday nudged its outlook for 2021 revenue higher after reporting first-quarter results showing gains in cloud sales, following the launch of a new business transformation initiative. SAP said it now expects cloud and software revenue this year of between 23.4 billion euros and 23.8 billion euros ($28 billion to $28.4 billion) at constant currency, up 100 million euros from prior guidance and a rise of 1-2% year-on-year. Its forecast for adjusted annual operating profit was unchanged at 7.8 billion euros to 8.2 billion euros, representing a decline of 1-6% from last year’s outturn. The company, based in Walldorf, Germany, prereleased what it called “stellar” first-quarter results that showed CEO Christian Klein’s new focus on selling business transformation as a service via its Rise with SAP package gaining traction. New cloud business, measured as current cloud backlog, rose 19% at constant currencies in the first quarter to 7.63 billion euros — the fastest in five years — while adjusted cloud revenue gained 13% at constant currency. Total revenue, which includes SAP’s traditional mainstays of license sales and service revenues, rose by 2% in the quarter at constant currency to 6.35 billion euros. Reported operating profit was depressed by executive share compensation, which SAP accounts for as a cash expense. After stripping out that effect, adjusted operating profit rose by 24% to 1.74 billion euros at constant currency. SAP prereleased the results, as German stock exchange rules require when results diverge from expectations or management adjusts guidance. The company is due to report full quarterly results on April 22. ($1 = 0.8370 euros.) Reporting by Douglas Busvine, editing by Sam Holmes."
https://venturebeat.com/2021/04/14/how-process-mining-and-analytics-complement-each-other/,How process mining and analytics complement each other,"At the recent ABBYY Reimagine conference, executives discussed how process mining and analytics are distinct technologies that can complement each other to help teams better understand business processes. Process mining helps identify inefficiencies or opportunities for improving how companies do things, while analytics help businesses measure performance and identify opportunities. Together, they can deliver the best of both worlds. Better analytics and data prep workflows allow process mining tools to offer a glimpse inside various business processes. And process mining tools help executives understand and improve data science processes used by applications and improve overall reporting. Process mining applications are optimized for processes that live entirely within ERP or CRM applications but require manual work to handle other applications. With more applications and data moving to the cloud, manual steps become an issue. “Everybody’s got too much data, and everybody’s processes are time-consuming and cumbersome,” Capitalize Analytics managing partner Eric Soden said. “Somehow we’ve got to get better and do more and be faster, and the data isn’t getting any smaller.” Soden found that analytics tools like Alteryx help organize, prepare, and reformat data in a form suited for process analytics. This makes it easier to identify more dependencies or bottlenecks within a process. For example, improved visibility into a driver monitoring app may uncover a manual step that is causing bottlenecks for processing shipping manifest logs. This step can be automated using something like robotic process automation (RPA) technology. When many people are involved, processes can become harder to understand, Soden said. An agreement system may coordinate the marketing and sales teams and also interact with other applications for purchase orders, invoices, and shipping. And an analytics tool can reformat the data used by the system into the format required by the process mining tool. With process mining, it becomes easier to visualize a wider variety of processes, generate better data, and improve business results. Alteryx Global Partner marketing VP Steve Wooledge said scaling up analytics can lead to process bottlenecks in enterprises. Process mining tools like ABBYY Timeline make it easier to understand, streamline, and automate analytics workflows, as well as building analytics modules that can be used in downstream applications. For example, an Alteryx customer ran a monthly process to calculate its fixed assets that took 40 hours and required a team of 10 contract workers to manage. Modeling this with process mining and creating a repeatable workflow allowed them to reduce that to 2.5 hours. Process mining automatically documented the process, which was useful for compliance and governance. Process mining comes in once an enterprise has established repeatable analytic workflows — such as analyzing volume discount opportunities, running fraud detection models, and analyzing a constantly evolving mix of assets — that executives monitor and optimize. Many analytics use cases involve blending data from multiple applications, databases, or spreadsheets — or even documents that may all be maintained by different departments."
https://venturebeat.com/2021/04/14/how-the-human-bot-dynamic-inspires-extraordinary-service-delivery/,How the human / bot dynamic inspires extraordinary service delivery,"Presented by NICE RPA It’s no secret that Covid-19 has put businesses under significant operational pressure to improve their service delivery approaches to stay competitive. Along with the enforced shift to new ways of working — which are likely to continue after the pandemic — one of the main drivers has been evolving customer expectations. Today’s customers expect the same high-level service no matter what the situation. They demand quick and accurate answers to complex questions. And if their needs aren’t met, they’ll look elsewhere. What’s more, they now expect to receive personalized and customized experiences. This is a trend that has increased in prominence as we’ve all become accustomed to operating in a digitally driven reality. The need to be agile and adaptable to customer needs in the moment is now abundantly clear. As a result, enterprises are being forced to adapt and embrace new technologies. One technology that can have a transformative impact is Robotic Process Automation (RPA). More specifically, attended automation. The combination of attended bots with employees can amplify the customer experience, while also driving additional operational efficiencies. Unlike unattended bots that run on backend servers, working on highly structured and repetitive tasks that don’t require human intervention, attended bots live on the employee’s desktop. They go beyond basic process automation, with the ability to guide employees in real time with next-best action advice. This makes attended automation ideally suited to support customer service teams during times of disruption and help them meet customers’ evolving and often complex needs. Equipped with RPA technology, businesses can augment human talent by streamlining processes and automating the more mundane, time-consuming admin tasks. This frees service reps so that they can focus on more complex, higher-value work. Such work may include more creative and strategically driven activities that add value to customers in a relevant and personal way. In today’s competitive landscape, this is what will drive customer satisfaction and long-term loyalty. Attended automation can also provide real-time, context-specific guidance during live customer interactions. From proactively gathering and summarizing data from multiple applications, to providing quick links to relevant data depending on the employee’s needs, this real-time intelligence can take the pressure off reps and give them the support needed to delight customers with tailored advice. Most importantly, it empowers reps to deliver more engaging, memorable, and superior customer service. The real-time guidance component of attended automation prompts employees to interact with customers in the most effective and beneficial way, such as reading a compliance script or presenting a promotional offer at the right moment. As a result, employees will feel empowered to deliver a high-quality service, while customers will get the experience they have come to expect. Augmenting human talent isn’t the only way attended automation can help inspire higher levels of service delivery. With humans and attended bots collaborating, several additional value drivers can be achieved. For example, attended automation solves many of the productivity and processing accuracy challenges facing businesses. Bots can complete the same desktop tasks faster, typically reducing average handle time by 20%, and allowing employees to get more done in their working day and direct more time towards value-added work that enriches the customer experience. Bots also ensure accuracy and compliance across all the desktop tasks they complete, minimizing the risk of data capture errors and compliance breaches down to nearly zero. This can all directly contribute to higher levels of employee satisfaction. Research shows that repetitive work is one of the top 3 reasons for agent attrition. With transactional and process-level tasks taken off the hands of customer service employees, they can focus more on work that requires judgement, empathy, and consulting expertise. As well as increasing their engagement, this will make them happier in their roles and more likely to deliver an exceptional and engaging customer service. Finally, employees can benefit from the enhanced training capabilities offered by attended automation. In fact, a large majority of employees complain about the difficulty they experience in learning new systems. Add to that the need to learn new processes, products, and service offerings, and the time spent in classroom and online training can add up. Real-time process guidance delivers effective on-the-job training that results in meaningful and sustainable learning. Employees get the reassurance of knowing they will be guided every step of the way and supplied with all the support materials they need to provide experiences that keep customers coming back. Ultimately, attended automation exists to amplify human potential and empower extraordinary service delivery. Reducing the admin burden by streamlining internal processes is a key part of the puzzle. But the true value comes from the real-time insights attended bots can provide, inspiring human workers to deliver more engaging, empathetic, and impactful customer service experiences — no matter what’s thrown at them. Oded Karev is General Manager of NICE RPA. With extensive experience in corporate strategy and operations, Oded leads NICE’s global Advanced Process Automation line of business, covering the full spectrum of robotics solutions. Prior to his current role, he served as Director of Corporate Strategy at NICE, leading some of the company’s key growth initiatives. Before joining NICE, Oded specialized in delivering multi-channel strategies, operating model designs and digital transformation projects for Accenture. Oded is a respected industry thought leader and key note speaker in the field of Robotic Process Automation. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/14/oracle-integrates-cloud-service-with-servicenow-itsm-platform/,Oracle integrates cloud service with ServiceNow ITSM platform,"Oracle today revealed it has integrated its cloud services with ServiceNow’s IT service management (ITSM) platform. IT organizations that have standardized on ServiceNow will be able to view Oracle Cloud Infrastructure (OCI) services alongside other public cloud services presented via the ServiceNow Service portal and the ServiceNow ITOM Visibility application, Oracle’s Clive D’Souza said. That significantly increases the chance IT teams will opt to employ OCI to deploy application workloads, D’Souza added. “It’s just like having space on a store shelf,” he said. All Oracle Cloud discoverable cloud resources will now also be extracted and stored in the ServiceNow Configuration Management Database (CMDB) repository, which can then be used to monitor the availability of those resources. That integration will also feed data into the ServiceNow AIOps platform IT teams are employing to monitor workloads. The integration with ServiceNow comes as more IT organizations are routinely employing multiple cloud services. Many IT teams have now determined that different cloud services optimally run different classes of workloads. IT teams are not necessarily forcing cloud service providers to bid to run workloads as much as they are simply trying to determine which service will provide the best performance for a specific class of workloads, D’Souza noted. In the longer term, D’Souza said Oracle will continue to evaluate its options for integrating its services with other ITSM platforms that are widely employed in the enterprise. The effort to make its cloud services more accessible follows Oracle’s move last month to encourage IT teams to employ its OCI platform. Today, Oracle is helping some organizations move a relatively small number of on-premises Oracle workloads to its public cloud at no additional cost. The Oracle Cloud Lift Services initiative provides access to Oracle engineers that are specifically trained to migrate both applications and databases to the Oracle Cloud Infrastructure (OCI) service. IT organizations have been employing cloud services for the better part of a decade, and yet the bulk of enterprise applications continue to run in on-premises IT environments. In theory, many more IT organizations will be centralizing the management of public cloud and on-premises IT environments as they embrace hybrid cloud computing. But for the time being, each cloud computing environment tends to be managed in isolation from the others. As a result, IT teams find themselves hiring multiple specialists to manage each environment, increasing labor costs with each addition. Hybrid cloud computing promises to reduce those costs while making it simpler for IT teams to deploy application workloads more dynamically on various platforms. Eventually, IT teams will also employ that capability to force cloud service providers to compete more aggressively to run those workloads. It may be a while before hybrid cloud computing becomes the new IT normal. But thanks in part to advances in automation and AI, it will soon become simpler for IT organizations to tame what has become a highly complex distributed computing environment. In the meantime, IT organizations will continue to be torn between keeping as many of their cloud options as open as possible and signing lower-cost long-term contracts."
https://venturebeat.com/2021/04/14/sales-management-platform-atrium-raises-13-5m-to-surface-insights-with-ai/,Sales management platform Atrium raises $13.5M to surface insights with AI,"Atrium today revealed it has raised $13.5 million in seed funding to drive the adoption of a sales management platform that leverages AI to surface issues and opportunities of specific interest to sales managers. While most organizations already have a customer relationship manager (CRM) application in place, Atrium is making a case for a software-as-a-service (SaaS) platform that combines data from CRM applications with other applications to enable sales managers to closely track actual levels of customer engagement. For example, if a deal is expected to close in the current quarter but there have been no communications with the customer via email or any other medium, analytics software infused with AI algorithms will issue an alert, company cofounder and chief revenue officer Peter Kazanjy told VentureBeat. Armed with that insight, it then becomes possible for sales managers to more proactively intervene when necessary, Kazanjy added. Sales managers can track a library of key performance indicators (KPIs) using a diagnostic tool that identifies the root causes for sales issues. These might range from consistently poor sales pipeline management to a customer’s tendency to not sign a contract until the very last week of the quarter in the hopes of maximizing discounts and other concessions. The challenge sales managers face today is not so much gaining access to data as it is making sense of it all to identify issues impacting the sales pipeline, Kazanjy said. Much of that data, however, resides in indecipherable dashboards that in some cases conflict with one another, Kasanjy added. “Sales managers today are awash in data,” he said. Atrium claims more than 100 organizations are now using its platform as an alternative to dashboards in other sales applications that lack the context sales managers require. Customers include Salesloft, Chorus, and LaunchDarkly. Atrium also hosts a Modern Sales Pros online community made up of more than 20,000 revenue leaders that work for more than 7,000 companies worldwide. The seed round of funding is being provided by Bonfire Ventures, Charles River Ventures, and Bullpen Capital. Sales management has for much of the past decade been slowly evolving to become a more data-driven science. Organizations of all sizes are being required to predict revenue flows more accurately, which Kazanjy noted requires more visibility into the sales pipeline. The level of forecasting accuracy is now viewed as an indicator of the degree to which a sales manager is on top of events occurring within their territory. Sometimes those events are beyond the control of any sales team. In other instances, however, a salesperson may be deliberately under-forecasting sales opportunities to lower their sales quotas, otherwise known as “sandbagging.” Regardless of their motivation, the sales manager is usually held more accountable to sales forecasts than an individual sales representative would be. Sales managers today are being held to a higher standard than ever at a time when the signal-to-noise ratio within sales pipelines has never been greater. It’s simply too difficult for the average sales manager to make sense of all that input without the aid of an application platform created for that specific purpose, Kazanjy said. Ultimately, turnover in a sales team is bad for business. It takes an average of six months or more to bring a new sales representative up to speed on their accounts. The primary goal for investing in sales software should be to minimize that turnover as much as possible by surfacing the insights a sales manager needs to make a course correction while there’s still enough time."
https://venturebeat.com/2021/04/13/princeton-digital-group-secures-230-million-financing-charts-out-1-billion-expansion-in-china/,"PRINCETON DIGITAL GROUP Secures $230 Million Financing, Charts Out $1 Billion Expansion in China","  SINGAPORE & SHANGHAI–(BUSINESS WIRE)–April 14, 2021– Singapore-based Princeton Digital Group (PDG), Asia’s leading data center provider, today announced it has secured USD 230 million debt refinancing from China Merchants Bank, as part of its USD 1 billion expansion plan in China. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413006080/en/ The refinancing validates PDG’s momentum, with confirmation of the completion of its fully contracted 42MW Shanghai data center campus. PDG also kicked off construction of its 43MW campus in Nanjing and design work of its 60MW campus in Nantong, and is actively evaluating acquisition opportunities in the Beijing, Shenzhen, and Shanghai areas as part of its 300MW expansion plan. PDG continues to build on its position as the largest international partner to domestic cloud companies in China, with ongoing partnership as they expand across Asia. For international companies, PDG is the only global operator in China offering the scale and service expected from these companies. “On the heels of our latest round of equity financing from Ontario Teachers’ Pension Plan and Warburg Pincus, this debt from China Merchants Bank is a testament to our ability to successfully execute on our strategy to build and deliver scale across APAC’s fastest-growing markets. Our execution track record has helped us win business from some of the world’s largest cloud companies,” says Rangu Salgame, Chairman and CEO of PDG. “We are delivering on the capacity requirements our hyperscale customers seek, when and where they need it. It is core to our strategy to be the leading data center operator in APAC.” “China is seeing growth come from both enterprise and hyperscale users of data center colocation. China is home to a number of cloud infrastructure platforms, social media, e-commerce, and content providers that all have multi-MW requirements. This has created a healthy stream of demand that guarantees steady and consistent colocation absorption,” stated Jabez Tan, Head of Research at Structure Research. According to Structure Research estimates, the data center colocation market in China generated USD $13.5 billion in revenue in 2020 and is projected to grow at a 13.6% CAGR in the next five years. Visit www.princetondg.com for more information about PDG.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413006080/en/ Princeton Digital GroupGrace ChenPR@princetondg.com +65 6679 6273"
https://venturebeat.com/2021/04/13/latest-edelman-survey-rates-trust-in-tech-at-a-21-year-low/,Latest Edelman survey rates trust in tech at a 21-year low,"The technology sector plummeted from being the most trusted industry sector in 2020 to 9th place in 2021, according to the 21st annual analysis from communications firm Edelman. Lack of accountability and unwillingness to self-govern is eroding the public’s trust in technology. Trust in technology reached all-time lows in 17 of 27 countries over the past year, Edelman said in its recent 2021 Edelman Trust Barometer: Trust In Technology report. The report is based on a survey of more than 33,000 people from 28 countries, including both general population respondents and what the firm calls “informed public respondents” for a well-rounded picture. Trust and fear have a reciprocal relationship: The faster one rises, the faster the other drops. Traditionally, the technology sector was something of an expert at managing the two, but that is no longer the case. Edelman found that fear of technology is growing at a faster rate than trust in technology. It will take years for the technology industry to bounce back and regain the public trust. Edelman’s survey results show respondents feel both betrayed by, and fearful of, technology. Job loss is the single greatest driver of societal fears, followed by the loss of civil liberties. There is a 6% drop in the number of people who are willing to share their personal information online. Social media, traditional media, and search engines are also at record low levels of trust. While the technology industry is full of entrepreneurs who believe in unleashing creativity and innovation and pursuing moonshot ideas, there are also those who monitor customers and invade privacy. The tendency to use technology as an authoritarian tool to monitor dissent is a concern, which explains China’s 16% drop in trust. The sheer drop is ironic, because China is also a global leader in tech R&D, innovation, and tech manufacturing. Edelman recorded one of the steepest declines in trust in the eight months between May 2020 and January 2021, when the public’s trust in technology dropped from 74% to 67%. People were increasingly concerned about AI and robots, and 53% of the respondents in Edelman’s survey worried the pandemic would accelerate the rate at which their employers would replace human workers with AI and robots. Cyberattackers capitalizing on the pandemic didn’t help matters, as 35% of respondents reported being fearful of attackers and breaches. Edelman’s Trust in Technology study presents a paradox between tech employees and their employers. Employer trust is highest among tech sector employees, with 83% saying they trust their employers, and 62% believing they have the power to make corporations change. Yet the public’s trust in those employers is plummeting. The disconnect comes from the public perception that humans are not controlling technology, but that technology is trying to control them. There is a growing perception that technology — especially social media — is more capable at manipulating people than previously believed. One way for the industry sector to regain some trust is to re-evaluate how they handle customer data and to be transparent about what they do with the information. Businesses as a whole are still trusted in most of the countries surveyed, with 61% of all respondents trusting companies above nonprofit organizations, government, and media. The most effective step businesses can take to increase trust is to guard the quality of information. Additional factors include embracing sustainable practices, implementing a robust COVID-19 health and safety response, driving economic prosperity, and emphasizing long-term thinking over short-term profits. However, just saying they will protect information isn’t enough. Businesses need to take a data-centric security approach to achieve greater resiliency and cybersecurity. Businesses should also address the concerns employees have over job loss and automation. They should be transparent and honest with their employees if robotics and automation are part of the business plan. Investing in re-skilling employees for new jobs is a great way to transform a business digitally. In short, senior management teams should remember that lasting transformation starts with employees."
https://venturebeat.com/2021/04/13/video-conferencing-company-touchcast-uses-ai-to-add-context-to-conversations/,Video conferencing company Touchcast uses AI to add context to conversations,"Video conferencing is increasingly becoming a commodity as technology giants like Microsoft and Google incorporate the feature into their free services. Touchcast is staying a step ahead of the giants by innovating on AI-powered services for premium users. Special effects are important, but the key differentiator lies in creating more context to drive the next wave of communication, Touchcast CEO Edo Segal told VentureBeat. Touchcast is doing so by taking advantage of Nvidia Maxine, a software development kit for creating GPU powered applications. The SDK includes various primitives for things like AI powered background removal, simulating eye contact, and measuring body pose in sports. “The fact that a company like Nvidia, the leader in AI powering hardware, has the foresight to invest in the research and development on the conceptual and software side helps companies like Touchcast accelerate time to market and focus on building on the shoulders of giants,” said Segal. Nvidia Maxine sets a new baseline of capabilities from which to innovate. “It allows us to focus on other areas where there is still no work being done as we chart this frontier,” Segal said. One big goal is to reduce the effort involved in creating quality events. Live presenters can be virtually teleported into mixed reality sets without a green screen. Live semantic segmentation uses AI to separate a person from the background in high quality, making it possible to automatically place people in a mixed reality set. “This literally used to take days or weeks of work and rendering and is now done live,” Segal said. Neural upscaling can clean a basic webcam image and scale it to an ultra-HD 4K screen. This works in a similar way to an artist asked to paint a mural from a small picture by intuiting how they might fill in the missing parts. Another new feature called auto framing can keep a speaker centered in the view even when they move. Words can be automatically transcribed, translated, and dubbed into multiple languages. Maxine allows all of this to occur in a fraction of a second so that the audio appears in sync with the speaker. Another new feature is the ability to break up a video and better organize it with summaries, table of contents, and short-form articles. A talk can be broken down by themes and have machine-generated titles and descriptions for each section. “Humanity has long lost its ability to commit to long-form content, and by creating this AI article view, we allow the viewer to skim the content quickly in the same way you might do with a blog post,” Segal said. Segal is also excited about the potential for semantic vector search to help bring new context to content discovery. “We believe that the next generation of search and discovery will evolve to ambient streams of information that are contextualized to the task you are performing,” he said. He has been working on this problem for decades and wrote about it in 2009. Semantic vector search works more like the human associative memory system rather than traditional Boolean keyword searches. It starts by translating content into concepts into a multi-dimensional space such that closely related concepts are represented closer to each other. Video conferencing is a crowded market, but Segal believes it is still growing because the idea of what constitutes a communications platform is also expanding. Previous advances focused on better compression and noise reduction algorithms, but they didn’t do much to help people make sense of the material being communicated. Segal is excited about features that aren’t easy to see but that help make information more accessible, such as how neural networks can instantly add context and curate what we communicate to make information better and more relevant. These innovations will usher in “the age of inferences” that could increase comprehension, accessibility, and insight, Segal said."
https://venturebeat.com/2021/04/13/deeplite-raises-6-million-series-seed-to-enable-ai-for-everyday-life/,Deeplite Raises $6-million Series Seed to Enable AI for Everyday Life,"MONTREAL–(BUSINESS WIRE)–April 13, 2021– Montreal-based AI startup Deeplite Inc. today announced the closing of a $6-million seed financing round led by the Boston-based venture capital firm PJC with participation from leading AI technology venture firms and industry leaders Innospark Ventures, Differential Ventures, Ajay Shah Executive Chairman, Smart Global Holding, and included follow on investment from Somel Investments, BDC Capital and Desjardins Capital. This financing will accelerate Deeplite’s R&D development, expand the team and accelerate market expansion for its optimization software that delivers faster, smaller and more energy-efficient AI models. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005850/en/ “We are incredibly impressed with the team, technology and market adoption of Deeplite’s software stack for Edge AI” said Rob May, General Partner at PJC and writer of the world’s most popular newsletter on Artificial Intelligence – InsideAI. “Deploying AI, particularly deep learning, on resource-constrained devices, is a broad challenge in the industry with scarce AI talent and know-how available. Deeplite’s automated software solution will create significant economic benefit as Edge AI continues to grow as a major computing paradigm.” The significant challenge when deploying deep learning models commercially is how large, processor-heavy and power-intensive they can be to operate. Deeplite solves this problem by providing an automated software engine to optimize DNN (Deep Neural Network) models and enable AI for edge computing on any device. Incubated at TandemLaunch in Montreal, Deeplite launched in 2019 when co-founders Dr. Ehsan Saboori, Davis Sawyer and Nick Romano teamed up to bring AI to Everyday Life. Since the initial release of Deeplite’s NeutrinoTM software in mid-2020, there has been tremendous demand from major OEM brands, semiconductor and application companies for Neutrino’s automated optimization engine. AI engineers can use the software within existing MLOps frameworks like PyTorch, ONNX or TensorFlow to create highly compact, energy-efficient AI models that save on cloud costs and allow new applications to run on small, battery-powered edge devices. “AI for Everyday Life is at the heart of what we are building. Deep learning is poised to bring massive benefits by way of automation, to unlock the potential to run AI on billions of microcontrollers (MCUs) in billions of devices at the point of data capture” said Deeplite CEO, Nick Romano. “We are excited to team up with PJC and this blue-chip syndicate as we enable AI to become untethered, decentralized and everywhere.” Deeplite has been named to the CB Insights AI 100 list of Top 100 privately held AI companies in the world and is also collaborating with Turing-Award winner and deep learning pioneer Yoshua Bengio at Mila/UdeM, a renowned AI research institute based in Montreal. “Accelerating time-to-market for accurate computer vision and perception AI models is fundamental to realizing the value of many diverse applications that will have a positive impact on our everyday life,” said Professor Yoshua Bengio, Scientific Director of Mila. “Addressing the challenge of running complex and sizeable deep neural networks on limited compute power is crucial, and we’re excited to support Deeplite’s unique technology strategy and the innovation resulting from this partnership.” This funding will be invested in research, development, sales and marketing to accelerate our product roadmap and go-to-market. Deeplite has key roles to fill and continues to hire in our Montreal and Toronto hubs and recruits globally as an accredited partner of Canada’s Global Talent Stream program. Major milestones ahead include a free community version coming out this spring and the release of Deeplite’s proprietary, ultra-efficient inference engine targeted for commodity CPUs and MCUs later this year. About Deeplite Based in Montreal, Canada, Deeplite is an AI software company dedicated to enabling AI for everyday life. Deeplite uses AI to automatically make other AI models faster, smaller and more energy-efficient creating highly compact, high-performance deep neural networks for deployment on edge devices such as cameras, sensors, drones, phones and vehicles. Deeplite was named to the 2020 CB Insights AI100 list of top 100 privately-held AI companies and has been featured by Gartner, Forbes, Inside AI and ARM AI as a premier Edge AI innovator. To learn more about Deeplite, visit www.deeplite.ai. About PJC PJC is an early-stage venture capital firm investing in innovative, high-growth companies that are building technology to disrupt the status quo. The firm seeks to be the first institutional capital deployed into a company and is focused on being a true partner to entrepreneurs as they drive their companies toward becoming industry leaders in their respective categories. The firm has invested in over 100 companies across North America including notable companies like Expensify, Nest, Blockfi, GetWellNetwork, Nexamp, and Yandex. PJC was founded in 2001 and is based in Boston, MA. For more information, visit www.pjc.vc.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005850/en/ Media:Anastasia Hamel – Marketing Manager, Deepliteanastasia@deeplite.ai"
https://venturebeat.com/2021/04/13/google-makes-business-process-tool-appsheet-automation-generally-available/,Google makes business process tool AppSheet Automation generally available,"Last year, Google launched AppSheet Automation, an “intent-driven” experience in Google Cloud powered by AI that enabled enterprises to connect to a number of data sources to model automated business processes. After several months in early access, Google today announced that AppSheet Automation is generally available with new capabilities, including document processing, a monitoring app, and expanded eventing support. According to Forrester, while automation has been a major force reshaping work since before the pandemic, it’s taking on a new urgency in the context of business risk and resilience. A McKinsey survey found that at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce reported that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than 4 hours saved each week per employee. AppSheet Automation, which arose from Google’s acquisition of AppSheet in January 2020, is an AI-enabled, no-code development platform designed to help automate existing business processes. The service offers an environment for building custom apps and pipelines, delivering governance capabilities and leveraging AI to understand goals and construct process artifacts.  One new feature in AppSheet Automation, Intelligent Document Processing, automatically extracts text from unstructured files like invoices and W-9s to eliminate the need for manual entry. Another, a monitoring app, allows customers to build AppSheet apps that can then monitor their automations. Google also extended AppSheet Automation’s data source eventing, which previously supported Salesforce, to include Google Workspace Sheets and Drive in the general release. Looking ahead, the company says it’s building the ability to embed rich AppSheet views in Gmail to enable users to perform approvals on the go.  “Digital transformation has been an enterprise priority for years, but recent Google Cloud research reinforces that the mandate is more pressing today than ever, with most companies increasing their technology investments over the last year,” Prithpal Bhogill, product manager on AppSheet’s business application platform, wrote in a blog post. “While there are many dependencies shaping the future of work, the challenge is to leverage technology to support shifting work cultures. Automation is the rallying point for this goal.” The launch of AppSheet Automation follows news that Google will collaborate with robotic process automation (RPA) startup Automation Anywhere to accelerate the adoption of RPA with enterprises “on a global scale.” As a part of its agreement with Automation Anywhere, Google plans to integrate the former company’s RPA technologies, including low- and no-code development tools, AI workflow builders, and API management, with Google Cloud services like Apigee, AppSheet, and AI Platform. Automation Anywhere and Google said they’ll also jointly develop solutions geared toward industry-specific use cases, with a focus on financial services, supply chains, health care and life sciences, telecommunications, retail, and the public sector."
https://venturebeat.com/2021/04/13/start-prepping-for-open-finance-if-you-want-to-gain-more-market-sharevb-live/,Start prepping for open finance — if you want to gain more market share(VB Live),"Presented by Envestnet | Yodlee Open banking is only the beginning — the open finance trend is bigger. Join this VB Live event to learn about open finance, what it means for your business, how it is revolutionizing customer experiences in insurance, utilities, telecommunications, and more. Register here for free. Historically, consumers have been at the mercy of screen-scraping, where companies can try and gain access to data, often without their knowledge. Open banking gives back control to the consumer. It’s the sanctioned, secure, reliable, and user-controlled sharing of basic financial data. A user can permission third-party financial service applications, such as accounting apps, tax apps, mortgage applications and so on, to access their data in a reliable and secure manner. The bank is the steward of the consumer’s information, but doesn’t own it. Open finance goes a step further — or the next step, says David Nohe, CEO of FinGoal, which builds analytics and infrastructure for financial brands. Open finance gives the user control of all of their financial information, including insurance data, telecom data, and utilities like gas, electric, even trash and recycling expenses. By giving access to your Geico or Allstate account through an open finance platform, a company like FinGoal gains visibility not just into how much you spend on insurance, but exactly what you get from insurance, he explains. That allows them to shop around, constantly looking to ensure the consumer has the best deal in the market to fit their needs, and isn’t over-paying for coverage or features they don’t use, Nohe says. The same is true in telecom or utilities. You’re paying a certain amount to Verizon every month, but unless you can securely share the details of the plan, you lose the opportunity to leverage an analytics platform that can tell you what kind of deal you could land at another company. “In an open finance world, companies like ours can better serve end users in comparison shopping and ensuring they have the best value, or catching when they’ve been overcharged, or when there was a fee that they might not realize happened,” Nohe says. Credit unions are embracing new ways of delivering information to their members, says Dawn Sirras, SVP fintech partnerships at Constellation Digital Partners, an open development platform for credit unions. Digital banking doesn’t need to be limited to viewing your balance, checking transactions, and making transfers. It can be a much more comprehensive, engaging, and interactive experience. “With open finance, we’re creating a hyper-personalized experience, where the member can look at a 360-view of their financial well-being, performance, and opportunities for improvement,” says Sirras. “By expanding the data set, we give the consumer the opportunity for a much more holistic view, which in turn just leads to better decision-making, and being more informed about their financial position and how they’re performing in the marketplace and in life.” There’s still work to be done, both in terms of regulation and the tech surrounding the initiative, but the groundwork is being laid right now for open finance. Large institutions have seen the writing on the wall for the future of data privacy regulation, particularly with the passing of recent laws in California and the EU confirming that user data belongs to the user. Industry pundits are betting that these laws are likely to be replicated at the federal level within the next handful of years. Smart companies need to be prepared — and be mindful of what happened to the banks in the last decade, Nohe says. Banks that were reluctant to share data, and made it harder for their customers to access their data through platforms like Mint.com, lost customers. So regardless of the size or age of your company, this open finance world is inevitable — data sharing is only going to become more powerful, and the data is only going to become richer, with higher fidelity. That means organizations need to be proactive, and start positioning themselves to take advantage of the opportunities it will bring, Nohe says. Now is the time to think about your road map, your customers today and in the future, and how access to more third-party data will enable you to serve them better, both when they’re on your platform and when they’re in third party platforms. “If you’re thinking about it in that holistic way, where you’re serving your users wherever they are, whether they’re in your interface, your applications, or someone else’s applications, you’re going to win market share and make users happy,” he says. “A lot of the industry will not go that route — they’ll be slow and stubborn to react. But it’s a ton of opportunity for folks that are thinking about the future.” Sirras agrees, urging companies to not be constrained by the past, or current processes. “We all need to be willing to look at new ways to deliver value to our customers or our members, to be more efficient, to be more effective with what we’re doing,” she says. “If that means a period of discomfort as we adjust to change, we need to be okay with that if it brings about a greater end product.” Don’t miss out! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/13/gartner-pc-sales-q1-hit-highest-growth-rate-20-years/,Gartner: PC sales in Q1 hit highest growth rate in 20 years,"(Reuters) — Global shipments of personal computers rose at their fastest pace in two decades in the first quarter as people bought computers to help them work and study remotely during the COVID-19 crisis, according to research firm Gartner. PC shipments, which include both laptops and desktop computers, grew 32% in the quarter to 69.9 million units, Gartner said. China’s Lenovo Group grabbed the lead with a 25.1% market share, followed by HP, Dell, Apple, and Acer, according to the report. “This growth should be viewed in the context of two unique factors: comparisons against a pandemic-constrained market and the current global semiconductor shortage,” said Mikako Kitagawa, research director at Gartner. “Without the shipment chaos in early 2020, this quarter’s growth may have been lower.” The coronavirus-driven surge in demand, along with an unprecedented shortage in semiconductor microchips, has strained the supply chain of personal computers. While the chip shortage was originally concentrated in the auto industry, it has now spread to a range of other consumer electronics, including smartphones, refrigerators, and microwaves. Gartner said demand for Google Chromebooks, which it does not account for in its analysis, tripled in the first quarter, thanks to strong demand from educational institutions in North America. “It is still reasonable to conclude that PC demand could remain strong even after stay-home restrictions ease,” Gartner said."
https://venturebeat.com/2021/04/13/social-yield-farming-platform-don-key-locks-2-million-from-cryptos-top-players/,Social Yield Farming Platform Don-Key Locks $2 Million From Crypto’s Top Players,"TORTOLA, British Virgin Islands–(BUSINESS WIRE)–April 13, 2021– Don-Key has completed a private funding round to bootstrap it’s Defi social yield farming platform. The $2.2 million was raised in the company’s first round from leading blockchain funds, including, Black Edge Capital, AU21. Genesis Block Ventures, Spark Digital, Solidity Ventures, MoonWhale, and Morningstar Ventures. Don-Key aims to reduce the barrier for entry for both yield farmers and liquidity providers, opening the DeFi world for those two distinct groups of people that either don’t master the skills to create strategies or hold a low volume of funds in order to participate within the DEFI world. Outsmarting Defi With Social Farming With the yield farming craze that’s taken the DeFi world by storm over the past 12 months, the number and complexity of yield farms has soared, as have the number of blockchains that now support liquidity mining. Farming is not for the faint-hearted or tech-averse however: it’s a hazardous pursuit that calls for negotiating constantly fluctuating APYs, mitigating impermanent loss, enduring smart contract risk, and frequently entering and exiting positions. On paper, yield farming sounds like a simple way to earn a passive income from your crypto holdings. In practice, executed in a perilous environment where a single mistake can prove costly. That is where Don.Key comes in: By utilizing the proven success of social trading, the platform maximizes the upside by making it easier for humble investors and farmers to generate consistent yield, whatever their ability level. This will be achieved by bringing liquidity providers and yield farmers together on the Don-Key. finance platform, where gamified social trading strategies will incentivize participants to work together and reap the spoils. Don-Key will do for yield farming what social (or “copy”) trading has done for cryptocurrency trading. Gil Shpirman, Co-Founder and CEO of Don-Key said: “We are very excited to see our vision come to life, I think that what is so special about our project is that everyone that is working on it, is also going to be a future user once we launch. That’s not something you see on every project, and I think it says a lot of what we are trying to build here.” “We are excited to be partnering with Don-Key to bring social trading to DeFi. The growth of the DeFI industry has been breathtaking, but the experience is still difficult for many new users. Don-Key addresses this with customizable strategies and ‘copy farming’ make the decision making simpler for the common user. We look forward to developing the product and growing the feature set with them. Said Leslie T, Cofounder and Partner, GBV” Key features will include: About: Don-key.finance started in 2020, at the beginning of the yield farming craze, with a very clear vision of bringing the simplicity of ‘copy trading’ to the complex world of DeFi Yield Farming. An initiative that came from real necessity, Don-key’s founders are DeFi enthusiasts from Israel, Cyprus, Ukraine and India with a true passion to democratize yield farming and helping crypto users around the world stay up to date with the constant evolution of the yield farming space.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005830/en/ Gil Shpirman, CEOGil@don-key.finance"
https://venturebeat.com/2021/04/13/informatica-modernizes-ipaas-platform-using-microservices-and-ai/,Informatica modernizes iPaaS platform using microservices and AI,"At its online Informatica World event today, Informatica announced a cloud platform that employs microservices and an AI engine to combine data management capabilities and enable data and application programming interface (API) integration. The Informatica Intelligent Data Management Cloud (IDMC) is a revamped implementation of an Informatica platform that makes more extensive use of an existing AI engine dubbed CLAIRE. This is used to analyze the metadata generated by each integration, chief product officer Jitesh Ghai said. In effect, IDMC creates a graph that makes it possible to track the relationship between various integrations for optimization and compliance purposes. “It’s a system of record for metadata,” Ghai said. That’s critical because it enables the Informatica integration platform-as-a-service (iPaaS) environment to function as a single source of truth for all integrations on an end-to-end basis, Ghai added. In all, IDMC now provides more than 200 discrete services that have been augmented using the CLAIRE AI engine, he noted. At the same time, Informatica is launching a series of services it makes available on various public clouds and rolling out updates. In addition to making IDMC available on the Microsoft Azure cloud, Informatica is adding an Informatica Cloud Data Governance & Catalog offering for the Amazon Web Services (AWS) cloud. Available in preview, it promises to make it easier to apply data governance policies to the massive amounts of data accumulating in AWS environments, Ghai noted. Finally, Informatica announced that its Cloud Data Integration Elastic (CDI-E) service for processing massive amounts of data at scale is now generally available on Google Cloud, along with a tool for managing APIs and an enhanced version of a visual tool dubbed Cloud Mass Ingestion that makes it easier to ingest data. Collectively, these capabilities are intended to address the need for a more flexible approach to integration at a time when organizations are increasingly launching complex digital business process initiatives spanning multiple applications and data sources, Ghai said. Integrations today routinely involve gigabytes of data that need to be programmatically moved between platforms, Ghai added. That level of automation requires a platform on a microservices-based architecture that can easily scale up and down to meet those requirements, he noted. The CLAIRE AI engine, meanwhile, provides the algorithms required to ensure data quality is maintained across all those integrations, Ghai added. Most legacy iPaaS platforms — like the applications they integrated — are based on monolithic architectures. Today, most new applications are being constructed using a more modular approach that’s based on microservices with their own APIs. The Informatica IDMC makes it simpler for IT teams to construct and deploy microservices that can programmatically invoke a set of integration services to access data strewn across an extended enterprise. Theoretically, a legacy iPaaS environment could service those requests, but not at the level of scale that might be required by thousands of microservices. The shift to microservices-based applications in the enterprise is just beginning, and the bulk of enterprise applications are still based on monolithic applications. But with each new application deployed, the number of microservices running in production environments steadily increases, along with the amount of data being consumed by those microservices. In addition, many monolithic applications will over time be refactored to run as a set of more modular microservices to make applications both easier to upgrade and more resilient. Rather than failing outright, a microservices-based application is designed to degrade gracefully by rerouting requests when a specific service is unavailable. It may be a while before microservices proliferate across the enterprise, but it’s now more a question of if than when. That will inevitably make IT environments more complex, which Informatica is betting will result in a lot more reliance on AI-infused iPaaS services."
https://venturebeat.com/2021/04/13/1password-expands-into-secrets-management-to-help-enterprises-secure-their-infrastructure/,1Password expands into secrets management to help enterprises secure their infrastructure,"Password-management platform 1Password is expanding into the “secrets management” space, helping developer teams across the enterprise safeguard private credentials, such as API tokens, keys, certificates, passwords, and anything used to protect access to companies’ internal applications and infrastructure. Alongside the launch, 1Password has also announced its first acquisition with the purchase of SecretHub, a Dutch startup founded in 2018 that claims to protect “nearly 5 million enterprise secrets” each month. Following the acquisition, SecretHub will be shuttered entirely, with its whole team — including CEO Marc Mackenbach — joining 1Password. Recent data from GitGuardian, a cybersecurity platform that helps companies find sensitive data hidden in public codebases, revealed a 20% rise in secrets inadvertently making their way into GitHub repositories. If this data falls into the wrong hands, it can be used to gain access to private internal systems. By way of example, Uber revealed a major breach back in 2017 that exposed millions of users’ personal data. The root cause was an AWS access key hackers discovered in a personal GitHub repository belonging to an Uber developer. There has been a flurry of activity across the secrets management space of late. Israeli startup Spectral recently exited stealth with $6.2 million in funding to serve developer operations (DevOps) teams with an automated scanner that finds potentially costly security mistakes buried in code. San Francisco-based Doppler, meanwhile, last month raised $6.5 million in a round of funding led by Alphabet’s venture capital arm GV and launched a bunch of new enterprise-focused features. 1Password has built a solid reputation over its 16-year history, thanks to a platform that can store passwords securely and simplify log-in. It allows consumers and businesses to log into all their online services with a single click (rather than having to manually input passwords) and can also be used to store other private digital data, such as credit cards and software licenses. The Toronto-based company raised its first (and only) round of funding back in 2019, securing $200 million to help it push further beyond the consumer sphere and cement itself as an integral security tool for the enterprise. Today, 1Password claims some 80,000 business customers, including enterprise heavyweights such as IBM, Slack, Dropbox, PagerDuty, and GitLab. With its latest “secrets automation” product, the company is striving to make its platform stickier for existing and potential clients searching for an all-in-one platform that protects all their credentials — from employees’ email passwords to core backend business systems. While 1Password’s existing password-management toolset helps people securely access accounts without having to remember dozens of passwords, the “automation” facet of its new product name refers to machine-based system workflows that, for example, enable an application to securely “talk” to a database. “This means being able to roll secrets into your infrastructure directly from within 1Password,” chief product officer Akshay Bhargava told VentureBeat. “We are the first company encompassing human and machine secrets.” Typically, infrastructure secrets can be splayed across countless cloud providers and services, but according to 1Password, it’s not uncommon for companies to cut corners or use a dubious combination of hacks and homegrown tools to manage the security around this issue. According to Bhargava, 1Password was working on a secrets management solution before it acquired SecretHub. In fact, many of 1Password’s customers were already storing their infrastructure secrets in its vaults. “Our customers have raised this workflow as something they’d like 1Password to solve,” Bhargava said. “It’s fair to say our first version is homegrown, and we’ve been focused on solving this problem for a while.” Secrets automation allows admins to define which people and services have access to secrets, as well as what level of access is granted. At launch, it integrates with HashiCorp Vault, Terraform, Kubernetes, and Ansible, with “more on the way.” However, 1Password is also announcing a deeper partnership with GitHub, which will see the duo collaborate to “solve problems for our shared customers and users,” according to Bhargava. “We plan to build a workflow to support customers in delivering secrets and configuration into their CI/CD pipelines on GitHub,” he said. As for costs, all companies will receive three credits for free. The cost then rises to $29 per month for 25 credits, $99 for 100 credits, and $299 for 500 credits. “We prorate based on usage,” Bhargava added. “We will work with companies needing more than 500 credits a month on an individual basis.” In terms of how credit is consumed, companies configure the 1Password vaults they want secrets automation to access and then stipulate the permissions for a development environment with tokens. “If an API client needs read and write access to data stored in a 1Password vault, that access is defined using a token,” Bhargava explained. “One token, accessing one vault, is what defines a credit. If that same API client needs to access two vaults, that then becomes two credits. And similarly, if a single token is created for read access to vault A and another for write access to vault B, that becomes two credits.”"
https://venturebeat.com/2021/04/13/robin-io-brings-pay-as-you-go-pricing-to-red-hat-private-clouds/,Robin.io brings pay-as-you-go pricing to Red Hat private clouds,"Robin.io, a leader in Kubernetes storage and data management, is bringing pay-as-you-go pricing to the Red Hat Marketplace for Robin Cloud Native Storage (CNS). Dynamic pricing models are among the most alluring features of public cloud services but not commonly seen in private clouds. But that is changing as more vendors take advantage of new metering capabilities available in private cloud services like Red Hat OpenShift. Kubernetes storage solutions have traditionally been priced using an annual licensing model, especially for on-premises deployments. The flexibility of paying for hourly consumption promises greater flexibility for businesses deploying ephemeral workloads, such as extract, transform, and load (ETL) processing; AI/ML workloads, such as data preprocessing, feature extraction, and model training; and ad-hoc data analysis on Red Hat OpenShift. “This helps customers with flexible licensing terms that can reduce costs and encourage more experimentation,” Robin.io director of product Ankur Desai told VentureBeat in an email. His team had to develop a metering integration with Red Hat to make this work securely and privately to mitigate concerns about external monitoring. The Red Hat Marketplace makes it easier for enterprises to provision cloud services for hybrid workloads that may span multiple private and public clouds. Robin CNS takes advantage of the “metering definition” as provided by the Red Hat Marketplace operator. There is no information sharing or network connection required with Robin because the Red Hat Marketplace operator collects consumption metrics for Robin CNS and passes the aggregated consumption metrics to the billing service. Robin.io installs natively on any Kubernetes distribution within minutes and creates a block and file storage solution by pooling available storage resources such as HDDs, SSDs, and cloud disks. It also automates complex storage management and data management operations on Kubernetes and provides a simple API to developers. It is also application-aware, in that it understands the scope of a stateful application on Kubernetes and wraps all relevant components — including data, metadata, and config data — into a single entity. All data management operations, such as snapshots, backups, and migration, are performed on the entire application, not just the data. This is important because many microservice applications are designed to be stateless in order to improve scalability and performance. Dynamic pricing could play an important role in the growth of storage capabilities deployed alongside more dynamic applications common with microservice and container architectures."
https://venturebeat.com/2021/04/13/aclima-adds-climate-and-environmental-justice-leaders-to-advisory-board/,Aclima adds climate and environmental justice leaders to advisory board,"Most companies add business advisors to their rosters. But Aclima’s mission is split between business and social good, and today the company announced it’s adding climate and environmental justice leaders to its advisory board. Aclima uses sensors on its own fleet of cars to measure global pollution on a block-by-block level and apply that data to combating climate change. Companies and governments can subscribe to Aclima in order to evaluate the environment and take actions to alleviate pollution. But Aclima has also declared that it’s a Public Benefit Corporation, meaning its charter is to serve the public good through business — maximizing impact, rather than just profit — CEO Davida Herzl told VentureBeat in an interview. “As a technology community, we’ve barely scratched the surface of technology’s potential to really serve to solve big problems that matter and to serve people and the planet,” Herzl said. “If you look back at 2020, it showed us the problems that we need to be solving. It’s about public health, it’s about climate change and the things that are really existential for society. And technology has a really big role to play.” She added, “One of the things that came to a head in 2020 was definitely the issue of racial and social justice. When we think of our technology, we want to be sure that we’re building it and delivering it in a way that actually advances social good for society. We are bringing those voices to the table and bringing them into our process. Our board is a brain trust that ensures that the science and technology that we’re developing is really calibrated to the needs of society.” I talked to the company last summer when California was on fire and smoke was turning our skies orange. Herzl believes the air we breathe is a critical infrastructure. Access to clean air is a human right, yet 90% of us don’t have it, she said. Systemic racial injustices have led to disproportionate environmental burdens for communities of color. And the same emissions that harm our health are changing the climate, she said. These converging crises call on each of us to pool our collective resources, energy, and ingenuity to innovate for environmental justice. Herzl said places like West Oakland are among the most polluted in the Bay Area, and it’s no accident that the area has such a high concentration of communities of color. In the U.S., Black people are 3 times more likely to die from exposure to air pollution, she said. “The data we generate highlights where technologies need to be deployed, not only to reduce emissions, but also to advance these really critical questions of environmental justice,” she said. Herzl wants the company to be a beacon for talent that cares about causes. Aclima hires interns, and Herzl noted that this crew of graduates coming out of college has not only survived a pandemic but is “looking down the barrel of a climate crisis.” And there are a growing number of companies in the “social capital” market that are doing good while making money. Herzl said her team includes a lot of talented engineers and scientists and programmers who built the network and sensors for measuring pollution. But the common thread is that they’re building technology for a good cause. The company also hires drivers for its fleet of cars in the communities that are affected by pollution, and it creates a path for those drivers to rise in the company. Aclima is dedicated to catalyzing bold climate action that protects public health, reduces emissions, and advances equity. The company works in collaboration with environmental advocates across sectors to diagnose air pollution hotspots, target emissions reductions and interventions, and measure progress. “We are bridging very different worlds with our advisory board,” Herzl said. “Our advisers are really excited to see technological innovation being applied to the problems that they have been advocating about for years.” The company’s advisory board reflects those values and includes the following new members: Aclima methodologies have been developed over a decade by leading scientists and validated by research institutions. This newly expanded advisory board will build and strengthen connections with community-engaged researchers. Herzl believes there is a sea change in climate action underway in the U.S., with the requirement for investment in environmental justice across all sectors. These new members of the Aclima Advisory Board, which was originally formed in 2015, will join existing advisors Bill Reilly, Martin Goebel, Luc Vincent, Nick Parker, Greg Niemeyer, and David Sherman."
https://venturebeat.com/2021/04/13/density-launches-occupancy-tracking-software-for-offices-raises-25m/,"Density launches occupancy-tracking software for offices, raises $25M","Density, a startup developing a range of people-counting, AI-powered sensors, today announced it has raised $25 million. Density says the capital will be used to accelerate growth and scale initiatives as it releases an addition to its software suite called Portfolio. In many respects, Density’s products were tailored for a health crisis. Cities around the world have imposed limits on businesses — particularly restaurants — regarding the number of customers they allow in. Moreover, the shift to working from home and pandemic-driven financial headwinds have companies questioning the need for physical office space. Even before the pandemic, U.S. Commercial Real Estate Services estimated unused commercial property in the U.S. to be worth about $1 trillion. In August, Capital Markets reported that direct commercial sales of real estate fell 29% globally to $321 billion in the first six months of 2020 (year-over-year). To this end, Portfolio draws on Density sensor data to show real-time, day-over-day “return to office”  insights. The company says the new benchmarking feature was designed to help real estate teams view office usage information across a collection of properties. Portfolio automatically records weekly occupancy and usage changes, enabling users to set safe maximum capacities based on local requirements. Beyond this, the software surfaces data over time to enable companies to “right-size” their workspaces. Density leverages depth-measuring hardware and an AI backend to perform crowd analytics that overcome the challenges posed by corners, hallways, doorways, and conference rooms. Clients like Pepsi, Delta, Verizon, Uber, Marriot, and ExxonMobil use its stack to figure out which parts of their offices get the most and least use and deliver people-counting metrics to hundreds and even thousands of employees. Density cofounder and CEO Andrew Farah conceived of Density’s technology while he was in graduate school at Syracuse and working at a mobile software development firm. His initial goal — to measure how busy a popular coffee shop was — led him to explore a couple of solutions before settling on the one that formed the foundation for Density’s people-counting sensors. According to Farah, Density’s tracking offers an advantage over other approaches: peace of mind. Unlike a security camera, its sensors can’t determine the gender or ethnicity of the people it tracks, nor perform invasive facial recognition. This approach also allows Density’s sensors to do occupancy detection inside of rooms where cameras can’t go for compliance reasons. Density recently launched Open Area, a sensor that uses AI and radar to track physical workspace usage. More capable than Density’s previous sensors, it detects key points on people’s bodies that the device translates in software to point clouds on a 3D graph. For instance, when positioned above a desk where people are seated, Open Area can show the rough outline of those people as they come and go. The pandemic initially hurt Density’s sales because many customers temporarily shut down. But since the close of its series B funding in June 2018 and its $51 million round in July, the company says its sensors have counted more than 150 million people in dozens of countries across hundreds of millions of square feet. Existing and previous Density backers include Kleiner Perkins, 01 Advisors, Upfront Ventures, Founders Fund, Ludlow Ventures, Launch, Disruptive, and LPC Ventures, with participation from Alex Rodriguez, Julia and Kevin Hartz, Cyan and Scott Banister, and others. This latest round brings the New York-based company’s total raised to over $100 million."
https://venturebeat.com/2021/04/13/sambanova-raises-over-600m-to-mass-produce-ai-chips-for-training-and-inference/,SambaNova raises $676M to mass-produce AI training and inference chips,"SambaNova Systems, a startup developing chips for AI workloads, today announced it has raised $676 million, valuing the company at more than $5 billion post-money. SambaNova says it plans to expand its customer base — particularly in the datacenter market — as it becomes one of the most capitalized AI companies in the world with over $1 billion raised. AI accelerators are a type of specialized hardware designed to speed up AI applications such as neural networks, deep learning, and various forms of machine learning. They focus on low-precision arithmetic or in-memory computing, which can boost the performance of large AI algorithms and lead to state-of-the-art results in natural language processing, computer vision, and other domains. That’s perhaps why they’re forecast to have a growing share of edge computing processing power, making up a projected 70% of it by 2025, according to a recent survey by Statista. SambaNova occupies a cottage industry of startups whose focus is developing infrastructure to handle AI workloads. The Palo Alto, California-based firm, which was founded in 2017 by Oracle and Sun Microsystems veteran Rodrigo Liang and Stanford professors Kunle Olukotun and Chris Ré, provides systems that run AI and data-intensive apps from the datacenter to the edge. Olukotun, who recently received the IEEE Computer Society’s Harry H. Goode Memorial Award, is leader of the Stanford Hydra Chip Multiprocessor research project, which produced a chip design that pairs four specialized processors and their caches with a shared secondary cache. Ré, an associate professor in the Department of Computer Science at Stanford University’s InfoLab, is a MacArthur genius award recipient who’s also affiliated with the Statistical Machine Learning Group, Pervasive Parallelism Lab, and Stanford AI Lab. SambaNova’s AI chips — and its customers, for that matter — remain largely under lock and key. But the company previously revealed it is developing “software-defined” devices inspired by DARPA-funded research in efficient AI processing. Leveraging a combination of algorithmic optimizations and custom board-based hardware, SambaNova claims it’s able to dramatically improve the performance and capability of most AI-imbued apps.  SambaNova’s 40-billion-transistor Cardinal SN10 RDU (Reconfigurable Dataflow Unit), which is built on TSMC’s N7 process, consists of an array of reconfigurable nodes for data, storage, and switching. It’s designed to perform in-the-loop training and allow for model reclassification and optimization on the fly during inference-with-training workloads. Each Cardinal chip has six controllers for memory, enabling 153 GB/s bandwidth, and the eight chips are connected in an all-to-all configuration. This last bit is made possible by a switching network that allows the chips to scale. SambaNova isn’t selling Cardinal on its own, but rather as a solution to be installed in a datacenter. The basic unit of SambaNova’s offering is called the DataScale SN10-8R, featuring an AMD processor paired with eight Cardinal chips and 12 terabytes of DDR4 memory, or 1.5 TB per Cardinal. SambaNova says it will customize its products based on customers’ needs, with a default set of networking and management features that SambaNova can remotely manage. The large memory capacity ostensibly gives the SN10-8R a leg up on rival hardware like Nvidia’s V100. As SambaNova VP of product Marshall Choy told the Next Platform, the Cardinal’s reconfigurable architecture can eliminate the need for things like downsampling high-resolution images to low resolution for training and inference, preserving information in the original image. The result is the ability to train models with arguably higher overall quality while eliminating the need for additional labeling. On the software side of the equation, SambaNova has its own graph optimizer and compiler, letting customers using machine learning frameworks like PyTorch and TensorFlow have their workloads recompiled for Cardinal. The company aims to support natural language, computer vision, and recommender models containing over 100 billion parameters — the parts of the model learned from historical training data — as well as a larger memory footprint allowing for hardware utilization and greater accuracy. SambaNova has competition in a market that’s anticipated to reach $91.18 billion by 2025. Hailo, a startup developing hardware to speed up AI inferencing at the edge, in March 2020 nabbed $60 million in venture capital. California-based Mythic has raised $85.2 million to develop custom in-memory compute architecture. Graphcore, a Bristol, U.K.-based startup creating chips and systems to accelerate AI workloads, has a war chest in the hundreds of millions of dollars. And Baidu’s growing AI chip unit was recently valued at $2 billion after funding.  But SambaNova says the first generation of Cardinal taped out in spring 2019, with the first samples of silicon already in customers’ servers. In fact, SambaNova had been selling to customers for over a year before this point — the only public versions are from the Department of Energy at Lawrence Livermore and Los Alamos. Lawrence Livermore integrated one of SambaNova’s systems with its Corona supercomputing cluster, primarily used for simulations of various physics phenomena. SambaNova is also the beneficiary of a market that’s seeing unprecedented — and sustained — customer demand. Surges in car and electronics purchasing at the start of the pandemic have exacerbated a growing microchip shortage. In response, U.S. President Joe Biden recently committed $180 billion to R&D for advanced computing, as well as specialized semiconductor manufacturing for AI and quantum computing, all of which have become central to the country’s national tech strategy. “We began shipping product during the pandemic and saw an acceleration of business and adoption relative to expectations,” a spokesperson told VentureBeat via email. “COVID-19 also brought a silver lining in that it has generated new use cases for us. Our tech is being used by customers for COVID-19 therapeutic and anti-viral compound research and discovery.” According to Bronis de Supinski, chief technology officer at Lawrence Livermore, SambaNova’s platform is being used to explore a technique called cognitive simulation, where AI is used to accelerate processing of portions of simulations. He claims a roughly 5 times improvement compared with GPUs running the same models. Along with the new SN10-8R product, SambaNova is set to offer two cloud-like service options: The first — SambaNova AI Platform — is a free-to-use developer cloud for research institutions with compute access to the hardware. The second — DataFlow as a Service — is for business customers that want the flexibility of the cloud without paying for the hardware. In both cases, SambaNova will handle management and updates. Softbank led SambaNova’s latest funding round, a series D. The company, which has over 300 employees, previously closed a $250 million series C round led by BlackRock and preceded by a $150 million series B spearheaded by Intel Capital."
https://venturebeat.com/2021/04/13/pegasystems-adds-enterprise-ai-tools-to-simplify-analysis-and-forecasting/,Pegasystems adds enterprise AI tools to simplify analysis and forecasting,"Pegasystems today announced it’s adding the ability to apply AI to business processes midstream to enable companies to determine whether an anticipated outcome will occur as expected. Pegasystems is also adding a feature that lets companies analyze streaming event data on platforms such as the open source Apache Kafka software that is now being widely employed to enable organizations to transfer data in near real time. Pega Process AI combines machine learning algorithms, event processing, business rules, natural language processing, and predictive analytics with low-code tools to analyze processes in real time, Pegasystems CTO Don Schuerman said. That approach makes it possible for organizations to make adjustments to processes to, for example, ensure a service level agreement (SLA) is met. As organizations invest in myriad digital business transformation initiatives, many are discovering the batch-oriented legacy applications that typically process data overnight are not well-suited to driving interactions with customers in near real time, Schuerman noted. As a result, organizations are modernizing applications using platforms such as Kafka that enable them to stream data between applications and platforms. The way legacy applications handle data winds up constraining digital processes that need to process data in real time, he added. “The shift from batch to reactive real-time processes has become table stakes,” he said. Support for event streaming will play a critical role in enabling organizations to achieve that goal. Rather than having to wait to analyze the data at rest on a cloud platform, for example, it is possible to analyze streaming event data in transit, Schuerman noted. As organizations look to infuse AI capabilities into business processes, the tensions that always exist between building a capability versus acquiring it will naturally surface. Pegasystems is making a case for an extensible platform based on AI models it creates and curates within the context of the Pega Platform. That capability makes it simpler for organizations to experiment with AI without having to hire data scientists to construct AI models using various open source toolkits. In contrast, it often takes a data science team several months to construct an AI model that may never make it into a production environment. Schuerman said that rather than simply experimenting with AI capabilities, organizations should generally work backward from a desired business outcome. That approach reduces the chances an organization will wind up investing time and resources into an AI project that never makes it into a production environment. There’s no doubt organizations of all sizes are investing in various forms of AI as part of larger digital business transformation initiatives. The challenge these organizations face is that most data scientists are not especially well-versed in how any given business process should be optimized, which can lead to a lot of trial and error. Providers of platforms such as Pegasystems make it possible for the average business analyst who knows how to work with low-code tools to apply AI to processes they know intimately. It also makes it easier for them to alter those processes, should the AI models need to be updated or start drifting toward a sub-optimal outcome. As AI becomes more democratized, the processes it can be applied to far exceed the number of data science experts that will be available anytime soon. Out of both necessity and fear, organizations are going to enable business users to at least experiment with AI before rivals apply those same capabilities. Hopefully, guardrails will ensure AI models are properly vetted so they do more good than harm."
https://venturebeat.com/2021/04/13/enterprise-security-platform-intrigue-expands-attack-surface-management-with-2m-round/,Enterprise security platform Intrigue expands attack surface management with $2M round,"Security startup Intrigue.io has raised $2 million from LiveOak Venture partners to accelerate product development for its attack surface management (ASM) platform. Intrigue developed an open source approach for discovering and investigating vulnerabilities across mobile, work-from-home, and cloud infrastructures. With even the best-equipped organizations struggling to consistently and automatically identify assets, find exposures, assess security risks, and address problems quickly, attack surface management has become an important area of concern for security teams. The massive adoption of cloud, SaaS, and mobile across a distributed workforce means organizations have an expanding, evolving, and changing attack surface. “We see attack surface management as a continuous process that security teams perform to discover intelligence about assets and exposures, direct that intelligence to the right owner in the business, and enable the business owner to mitigate the risk,” Intrigue founder and CEO Jonathan Cran told VentureBeat in an email. From a technical perspective, attack surface management involves scanning, exploring, and inventorying assets across the enterprise, partner, and third-party infrastructure to map the attack surface and then monitoring the assets to detect any changes to the configuration and exposure to known threats. The final component involves mitigating risks by addressing the vulnerability or fixing the configuration error. Traditional security tools were designed for fixed assets and have trouble tracking software-defined assets that are constantly changing. The Intrigue platform uses various open and customer-configured data sources to find assets across hosts, apps, cloud services, and user accounts. Intrigue relies on a graph database to capture assets and their security properties with more precision than would be possible with a traditional configuration management database. Intrigue Enterprise relies on non-linear mapping technology for asset discovery, workflows for automatic scoping and vulnerability control, and integrations with other network tools. Intrigue Core, an open source asset discovery project that serves as the backbone for the company’s enterprise offering, relies on discoveries by community members. For example, this week one community member introduced a capability to find a device’s internal IP address from F5 load balancers. “This kind of pivot, in combination with the multitude of vulnerability checks driven by threat [intelligence], helps our customers gain an edge on attackers,” Cran said. If the enterprise has a tool that’s good at finding open network ports, Intrigue can incorporate that technology as a data source to enrich the asset inventory. The Intrigue team is also building out integrations into various threat management tools, including CMDB, SOAR, incident management, and ticketing solutions to speed incident resolution in response to the discovery of new security threats. The new investment would help Intrigue expand beyond existing integrations such as BGP routing tables, on-demand DNS lookups, cloud providers, cloud asset repositories, historical DNS, historical Passive DNS, internet-wide scanning, reverse Whois, historical reverse Whois, social media account lookups, GitHub and GitLab repositories, and threat indicator of compromise (IoC) repositories. The latest round of funding will also help support the security and developer communities contributing to Intrigue Core. “You can’t secure what you can’t see. Intrigue goes far beyond current offerings to give enterprises visibility into their entire public-facing footprint so they can both monitor it and secure it,” said LiveOak Venture Partners principal Creighton Hicks."
https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/,Synopsys: 84% of codebases contain an open source vulnerability,"The number of codebases containing at least one open source vulnerability increased by nine percentage points in 2020, according to a new report from Synopsys, the silicon design company behind open source security management platform Black Duck. In the sixth Open Source Security and Risk Analysis (OSSRA) report, Synopsys said it has provided an “in-depth snapshot of open source security, compliance, licensing, and code quality risk in commercial software,” observing that of the 1,546 commercial codebases scanned by Black Duck in 2020, 84% contained at least one open source vulnerability — up from 75% in last year’s report. Most modern software relies to some degree on open source software, as it saves companies the time and resources needed to develop and maintain every component internally. Black Duck, which Synopsys bought in 2017 for $547 million, is one of several software composition analysis (SCA) platforms, with others including Sonatype, which was acquired by Vista Equity Partners in 2019; Snyk, which recently closed a $300 million round of funding; and WhiteSource, which last week raised $75 million. Companies use these platforms to identify every open source component in their stack to surface vulnerabilities and license compliance risks. And it’s these open source “audits” Synopsys and Black Duck primarily use as the basis for their annual OSSRA report. The 1,546 codebases that constituted this year’s report spanned 17 industries, including aerospace, fintech, IoT, and telecommunications, with Synopsys concluding that 98% of codebases contain open source code. This is marginally down from the 99% it reported last year, but incremental deviations are to be expected — the bottom line is that most applications continue to rely on open source components. So why would vulnerabilities be spreading at this rate? Tim Mackey, principal security strategist at the Synopsys Cybersecurity Research Center (CyRC), thinks that while there are some complexities behind the growth of vulnerabilities, for most companies the problem is essentially one of scale. “If you look at the average number of components in an application over the last three years, it’s gone from 298 to 445 and now to 528,” he told VentureBeat. “If someone designed their update and patching processes to manage 300 components per app in 2018, they probably didn’t expect usage to grow that much in two years. Then if you overlay that US-CERT (U.S. cybersecurity and infrastructure agency) reported an average of slightly more than 48 new CVEs (common vulnerabilities and exposures) each day in 2020, keeping up with patching is a huge problem.” At the heart of the problem is the vast array of open source software packages out there. A slew of tools have emerged to help developers and companies make sense of the open source world. Openbase, for example, wants to be the Yelp for open source software packages. OpenLogic’s Stack Builder, meanwhile, helps enterprises choose the right combination of open source software for their needs. And Two Sigma Ventures’ Open Source Index highlights GitHub’s most popular projects right now. But while selecting the right package is important, keeping abreast of updates is equally essential. In short, developers often struggle to keep on top of their open source stack and remember where they got their open source components from when it’s time to download patches. This is an area where companies such as Synopsys are carving their niche. The broad industry consensus is that vulnerabilities are rife within open source code, and bad actors are hell-bent on exploiting them. In its State of Software Security: Open Source Edition report last year, app security company Veracode noted that 70% of applications contained a security flaw in an open source library, while Sonatype recently reported a 430% surge in attacks targeting open source software supply chains. But not all vulnerabilities are created equal, and many offer limited scope for hackers to exploit. In an interview with VentureBeat this week, WhiteSource CEO and cofounder Rami Sass said the company’s research showed that only “15% to 30% of vulnerabilities are effective — the majority of open source vulnerabilities are not called by the proprietary code.” This means it’s important to distinguish between imminently dangerous vulnerabilities and minor flaws. With that in mind, Synopsys’s latest report found that the percentage of codebases containing high-risk open source vulnerabilities grew 11 percentage points to 60% in 2020, with “high-risk” defined as a vulnerability that has been actively exploited, has “documented proof-of-concept exploits,” or has been “classified as a remote code execution vulnerability.” Moreover, several of the top 10 open source vulnerabilities identified in the 2019 report not only reared their heads again in 2020 but showed sizable percentage increases — this, according to Mackey, was the biggest surprise the company saw in its audit. “Normally, we’d expect to see exposure to any given CVE decline over time,” he said. “After all, once a vulnerability is reported, most teams will want to apply the patch.” The top two vulnerabilities were related to jQuery, and both demonstrated double-digit year-on-year growth.  Away from the vulnerability sphere, the latest OSSRA report found that the number of codebases containing open source license conflicts fell marginally year-on-year from 67% to 65%, with nearly three-quarters of these related to a GNU General Public License. Meanwhile, 26% of the codebases used open source with either no license or a customized license. This is important because customized open source licenses often need to be evaluated for potential IP issues or legal uncertainties. Elsewhere, the report showed that 91% of codebases contained open source dependencies with zero development activity in the past two years, up from 88% the previous year. This might not be a problem, but it means the vast majority of codebases, according to Synopsys audits, contain an open source dependency with no recent new features, enhancements, or — more importantly — security fixes. What does this all mean? For one thing, software — open source or otherwise — can become vulnerable if nobody is at the wheel. This is why the Linux Foundation set up the The Core Infrastructure Initiative (CII), with backing from tech heavyweights such as Amazon, Google, Microsoft, Cisco, IBM, and Intel, to support open source projects that are critical to the internet and related devices and systems. But it also means enterprise-focused commercial companies can monetize open source projects with the promise of added features and (enhanced) security. And companies such as Synopsys, WhiteSource, Snyk, and Sonatype can build billion-dollar businesses by helping developer teams keep on top of their open source stack and ensuring major flaws are addressed quickly."
https://venturebeat.com/2021/04/13/waylay-announces-waylay-io-the-first-low-code-developer-friendly-data-automation-and-orchestration-platform/,"Waylay Announces Waylay IO, The First Low-code Developer-friendly Data Automation and Orchestration Platform"," Waylay IO holds the promise of coding at a fast pace, with minimal setup effort and guaranteeing quick deployment  GHENT, Belgium–(BUSINESS WIRE)–April 13, 2021– Waylay, a leading enterprise automation software company at the forefront of digital transformation has today announced Waylay IO, its new product offering for developers. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005106/en/ Waylay IO is a low-code development platform that reduces the complexity of the application development process. Its automation technology uses small snippets of code, reusable across different use cases, and orchestrated by the powerful rules engine. The automation platform offers necessary tools in one place and helps developers to experiment with data and create insightful new applications and business models quicker than ever before. “The basic building blocks of our low-code platform are small snippets of code that can be reused, just like lego bricks,” said Veselin Pizurica, Founder & CTO of Waylay. “Serverless is the perfect environment for this low-code lego brick approach and Waylay IO guarantees an extremely powerful rules engine to orchestrate these code snippets. Waylay is on the mission to ease the complexity of serverless and liberate developers from getting bogged down into actions that have nothing to do with the problems they ought to solve. The Waylay IO automation concept is so powerful that it is revolutionary. Our first user feedback is unanimously positive and I was impressed to see how fluent and quick developers get up and running and can be more experimental and creative with data without losing focus or valuable time.” Waylay IO is built on top of open-source alternatives, without dependencies on any specific cloud provider. It allows easy integration of API-enabled services and provides excellent debugging and observability capabilities. The platform has built-in state-of-the-art security models similar to Auth0. In short, the Waylay IO platform is a premade automation stack where API gateways, multitenancy, lambdas, databases, and all other services are embedded and pre-installed. Coders only need to code – no hassle, fast, efficient, simple.Waylay provides the Waylay Academy with training and demo material, use cases and templates to kickstart ideas. The support forum and engaged community create the perfect setting to interact with peers and Waylay experts. “Industry analysts (*) point out that there are two impediments hampering the serverless adoption,” said Leonard Donnelly, CEO of Waylay. “The first one is related to architecture complexity: tracing, observability, debugging, deployment, and the second one is related to fear of losing control, fear of vendor lock-in, fear of weak security, unpredictable cost, etc. Waylay IO is designed to solve these problems.” The Waylay IO pricing model is based on consumption, users only pay what they use.During the launch period, new users will receive a $200 coupon at registration: www.waylay.io/io (*) O’Reilly serverless survey: Concerns, what works, and what to expect ABOUT WAYLAY Waylay is a leading enterprise IT-OT digital unification software company delivering low-code orchestration, automation and analytics software solutions. Waylay has a passion for supporting citizen developer communities and ensuring it puts all valuable data to work for developers, data scientists and domain experts. Most importantly, Waylay guarantees its customers a significant reduction in cost and time to market new digital transformation projects to eventually make their enterprises become one.Waylay offers 3 products: Waylay IO for developers and SMEs, Waylay Enterprise, and Waylay Digital Twin for Salesforce. Find out more at www.waylay.io  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005106/en/ PRESS CONTACT Elly SchietseCMO, WaylayElly.schietse@waylay.io Tel +32 479 761825"
https://venturebeat.com/2021/04/13/how-it-leaders-stay-on-the-right-side-of-innovation-with-open-cloud-vb-live/,How IT leaders stay on the right side of innovation with open cloud (VB Live),"Presented by Supermicro With cloud computing accelerating worldwide, companies need to determine what best fits their needs in compute, storage, networking, and the software stack. Join this VB Live event to learn how open clouds reduce latencies, improve innovation, and more. Register here for free. “Thinking about openness is the key to being an IT leader in the cloud computing space,” says Michael McNerney, vice president of marketing and network security at Supermicro. “And that means building your cloud to take advantage of performance and innovation not just today, but in 18 months, 36 months, and on and on.” Cloud infrastructure has become a catch-all for modern datacenter architecture, and the open cloud has been leveraging the innovation in open-source software and industry-standard hardware over the last decade, McNerney says. And that innovation continues at a rapid clip. Moore’s law, that every 18 months hardware will double in performance at the same cost, is not slowing, and is perhaps even accelerating, when you include CPU, memory, storage, and I/O. “The key for IT leaders right now is to make sure they stay on the right side of innovation, and live on that lower cost, better performance curve that the cloud has been driving over time,” he says, “And stay able to leverage rapid improvements to deliver new and more products and services.” But there are a lot of solutions out there, and determining what works for your technology, your industry, and for your company overall means there are a huge number of variables to take into consideration. The key, McNerneyy says, is nailing down your basic business strategy – everything from what your decision points are and how you compete in the market to what’s your value add, and what your competitive landscape will look like in the future “You want to optimize both your software and your hardware for optimal performance and efficiency – the days of the same general-purpose server being used for every application are gone,” he says. “Every workload requires an optimized platform.” For AI, you need high-throughput systems that can manage the high-power consumption of the CPU/GPU solutions. Storage requires a spectrum of offerings from all-flash NVMe boxes for highly responsive applications to scale-out 60/90 bay storage systems for archival. And the same is true in CPUs now. Intel’s newest processor has better instructions per clock and more cores, but the major performance enhancements came from accelerators for specific workloads, like crypto, networking, speed select for CSPs, and more. And from a cost and innovation perspective, the traditional lock-in model that IT departments have embraced in the past is prohibitive. “That’s why we come back to the open cloud,” he says. “When you keep your environment open, you can control costs, leverage multiple vendors, and optimize by taking advantage of the new and best technology without worrying about being locked in to a specific vendor.” The industry has shown again and again that both open standards and open source drives innovation, McNerney says. With open source, technology shared with a community of people who are able to contribute to its development drives quality and innovation. When multiple parties can contribute to a specific implementation without locking you into their implementation you can draw from a rich pool of tools and applications. Open standard interfaces mean that there is a standard layer to build below or build on top of.  That shared standard interface layer is the key to innovation. He points to the evolution of storage: all-flash NVMe drives delivered an order of magnitude better performance than traditional media. But the storage interface didn’t change immediately; NVMe drives supported traditional protocols and applications would get the benefit simply by changing the drive. The open standard interface meant over time the software could be adjusted to maximize the NVMe protocol and deliver even better application performance. Open standards across the features of the cloud allows innovation at every level and allows companies to take advantage of an ecosystem of vendors and solutions for every problem or project. In the end, the most important decision to make is to build openness and Moore’s Law into your cloud strategy. “For openness, make decisions that keep your options open in a way that allows you to control costs, allow substitutions, and drive competitive rivalry in your suppliers,” he says. “For Moores’ Law, ask yourself, what would I do with twice the compute performance or half the costs? Which projects are stalled today that could provide competitive advantage in 18 months when the performance is ready?” To learn more about the benefits of open cloud, and how to ensure you nail the three main components for success – the right hardware, software stack, and network choice – don’t miss this VB Live event! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/13/grid-ai-launches-platform-for-training-ai-models-on-the-cloud-at-scale/,Grid.ai launches platform for training AI models on the cloud at scale,"Grid.ai today announced the general availability of Grid, a new platform that enables researchers and data scientists to train AI models in the cloud. The company says that Grid enables development and training “at scale” without requiring advanced skills in machine learning engineering. Machine learning practitioners can run into challenges when scaling AI workloads because of the infrastructure needed to train and deploy into production. Moreover, this infrastructure can be expensive to maintain. A Synced study estimated that the University of Washington’s Grover fake news detection model cost $25,000 to train in about two weeks and OpenAI reportedly racked up a whopping $12 million to train its GPT-3 language model. At a high level, Grid offers an interface for training models on GPUs, processors, and more. It’s a web app with a command line interface that optimizes datasets to work at the level needed for production and “cutting-edge” research. Grid computes in real time to make it easier to quantify the R&D efforts of AI projects. Additionally, the platform provides access to Jupyter notebooks, a way for data scientists to bundle their answers with the Python code that produced it. To use Grid, users simply clone a project from their GitHub repositories and make minor code changes. Data and artifacts stay on their infrastructure — Grid only orchestrates. And experiment artifacts, like weights and logs, are automatically saved to the cloud so that they can be accessed during or after a training run. Grid will be available starting April 13 in three plans: individual, team, and enterprise. The individual plan is the cost of the cloud processing plus 20%, while the team plan is $1,000 a month in addition to the cloud costs and 15%. The launch of Grid comes after its namesake company, Grid.ai, emerged from stealth in October 2018 with $18.6 million in venture capital backing. Cofounded by William Falcon, the creator of PyTorch Lightning, one of the fastest-growing machine learning frameworks in the world, Grid.ai’s mission is to reduce the distance between deep learning research and its practice in real-life businesses. Grid leverages PyTorch Lightning to decouple the code required to define a full deep learning model from the code required to run it on hardware. Falcon says the main goal is to help companies focus on delivering value instead of worrying about the machines they’re running or clusters. “Think back to when electricity came about. Only a few had access to it until the power grid made it available to everyone,” Falcon told VentureBeat via email. “That’s the goal of Grid.ai, to democratize access by removing the need to be an expert engineer. ML practitioners should be focused on delivering value through models and data, not on becoming expert engineers.”"
https://venturebeat.com/2021/04/13/nvidia-now-expects-q1-sales-to-pass-5-3b-forecast/,Nvidia now expects Q1 sales to pass $5.3B forecast,"(Reuters) — Nvidia’s first-quarter revenue will be above its earlier forecast of $5.3 billion, the chipmaker said on Monday, as it benefits from strong demand for chips that power datacenters and cryptocurrency mining. The company’s shares, which rose on the back of a slew of product announcements during the session, were trading nearly 6% higher. Demand for Nvidia’s gaming graphic chips has soared during the COVID-19 pandemic, but the bigger boost to its sales has come from an aggressive push into artificial intelligence chips, which handle tasks such as speech and image recognition in datacenters. “While our fiscal 2022 first quarter is not yet complete, Q1 total revenue is tracking above the $5.30 billion outlook,” chief financial officer Colette Kress said in a statement. Nvidia’s unit that supports cryptocurrency mining is now expected to report sales of $150 million in the quarter, up from the previous forecast of $50 million, the company said. Nvidia’s upbeat forecast comes as the chip industry continues to reel from a global shortage of components, disrupting the wide-scale manufacturing of products, including vehicles and smartphones. “We expect demand to continue to exceed supply for much of this year,” Kress said. Earlier on Monday, Nvidia said it was planning to make a server processor chip based on technology from Arm, putting it in the most direct competition yet with rival Intel. Reporting by Munsif Vengattil in Bengaluru, editing by Ramakrishnan M. and Anil D’Silva."
https://venturebeat.com/2021/04/13/lexon-breaks-company-record-with-6-wins-at-the-red-dot-design-award-2021/,Lexon Breaks Company Record with 6 Wins at the Red Dot Design Award 2021," The French brand turns 30 and proves its continuous leadership in innovative product design  PARIS–(BUSINESS WIRE)–April 13, 2021– Lexon today announced that it has received 6 Red Dot Awards for its outstanding product design quality in multiple categories, setting a new company record for the number of awards won in a single competition. Sister company MyKronoz also takes home a Red Dot Award for its MyScale, a smart and design body scale with a large and intuitive color display. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005520/en/ This announcement comes only a few weeks after Lexon’s best-selling Oblio was honored with two prestigious international prizes – a TIME Best Invention Award and a CES Innovation Award – confirming the French brand’s continued leadership in innovative product design, with a total of more than 200 awards received since its creation in 1991. “2021 marks the 30th anniversary of Lexon, so this exceptional recognition comes at a perfect timing. We couldn’t be happier to celebrate this milestone with such paramount achievement which proves our consistent ability to create objects that combine aesthetics and function, but most importantly, that are part of our daily lives. 2021 is also synonymous with the global crisis, one that inevitably led us to adapt our business to new realities, to rethink our products portfolio and to focus even more on designing solutions that truly meet our changing environments, whether it’s for the home, the work or the new mobility. Today, these distinctions not only celebrate our resilience, they also validate the quality of the experience we strive to bring to our customers in all circumstances,” says Boris Brault, Lexon’s CEO. Since its acquisition by BOW Group in 2018, Lexon has experienced significant online growth and staggering expansion in the US where the brand is today listed among the most prominent retailers including Target, Nordstrom, Neiman Marcus, Best Buy and MoMA Design Store. In the coming weeks, Lexon will pursue its international development by completing the takeover of its long-term Chinese Master License who has built the brand domestically for the past 10 years, opening more than 80 retail stores, developing online sales on JD.com and Taobao, as well as growing profitable B2B activities. The successful integration of Lexon among BOW Group has created highly valuable synergies with sister company MyKronoz, in terms of product design, R&D capabilities, sales channels and digitalization. BOW Group has thereby proven its ability to acquire external companies within its manufacturing, selling and distribution structures to scale its business operations. Today, the Group is looking at new opportunities to further enlarge its portfolio with product offering and services, in order to become a leading 360° platform of lifestyle consumer tech. The Lexon award-winning products include: Oblio: A must-have innovation for today’s world, Oblio is a sleek UV-C sanitizer that prevents the spread of harmful bacteria found on our smartphones. Using its built-in UV-C LED technology located on its front interior, Oblio can deliver a 360° disinfection by simply flipping the phone to expose each surface for a 20-minute sanitizing cycle. Oblio’s efficiency has been proven through lab testing to kill 99.9% of viruses present on the surface of smartphones, including H1N1. Also acting as a 10W wireless charger, Oblio can fully charge a smartphone in 3 hours. Its discrete vase-like shape makes it a beautiful design item that can be proudly displayed in a home or office, encouraging a break from your screen while your phone is being fast-charged and deeply sanitized. Compatible with all mobile phones for sanitizing function, and all Qi-enabled smartphones such as the latest iPhone & Android devices for the wireless charging function. MSRP: 79.90 $/€ Mino T: Made for exploring, Mino T is an outdoor speaker designed with a sleek carabiner to clip to your bag or bike. Boasting an IPX4 water resistance combined with an ultra-robust finish, it is ready to follow you in all your adventures. Carefully engineered with a passive bass system, Mino T allows you to enjoy a more balanced, fuller sound on the go with up to 5h of continuous playtime. Built with a considered design in mind, this speaker is also a stand-alone modern piece that you can proudly display indoors, on its matching stand. MSRP: 49.90 $/€ Mina M: With its compact size, statement-making aluminum base and water-resistant casing, the Mina M is the perfect way to inspire any space, indoor and out. Mina M brings an impressive 9 LED light color range to your bedside, workspace, or patio. The color change and dimming function are all controlled by a press down feature on the top of the light. Mina M is rechargeable using any Qi-enabled wireless charging station. You can also charge Mina M by using a USB-C charging cable. The Mina M has up to 24h of battery life from a single charge, making it the perfect companion all day and night. MSRP: 49.90 $/€ Mino+: This mini portable Bluetooth® speaker fits in the palm of your hand to deliver an impressive 3W sound quality for up to 3 hours! Mino+ offers two charging methods: wirelessly with any Qi charging pad or via its USB-C port. When connected to your phone, you can use Mino+ to take selfies, start and stop recording videos, and answer calls using the bottom button. Thanks to its built-in TWS technology, you can connect two Mino+ together and double the pleasure with an immersive stereo sound. Mino+ comes in 3 finishes (aluminum, chrome and glossy) and 24 colors to bring a stylish touch to your decor. MSRP: 29.90 $/€ These four products have been designed in collaboration with Manuela Simonelli & Andrea Quaglio. Powersound: No connector, no cable: PowerSound is a revolutionary 2-in-1 device that combines a 5000 mAh wireless power bank with a 360° Bluetooth speaker, while charging and recharging itself entirely wirelessly. Capable of charging all Qi-enabled smartphones or earbuds, it can be recharged by setting it on top of any wireless station. Thanks to its 5W built-in Bluetooth® speaker, PowerSound allows you to enjoy an immersive sound for up to 20 hours. Easy to carry and ultra-stylish, PowerSound has a smooth design crafted from durable synthetic leather and fabric. MSRP: 79.90 $/€ C-Pen: The C-Pen offers a unique 2-in-1 writing and storage experience. A ballpoint pen with a 32 GB USB-C memory drive thoughtfully engineered into the pencap. 32 GB of memory means you can transfer and store up to 4000 pictures, 7000 songs, or 40 hours of video. Compatible with computers, tablets, and smartphones with a USB-C port, the C-Pen is a handy way to transfer memories or notes between all your devices. The must have accessory for the modern on-the-go person, available in 6 colors in a glossy or matt finish and working with a universal refill. MSRP:39.90 $/€ These two products have been designed in collaboration with Alain Berteau. About the Red Dot Award: The Red Dot Award – Product Design, whose origins date back to 1955, appraises the best products created every year. In roughly 50 categories, manufacturers and designers can enter their innovations in the competition. According to the motto “In search of good design and innovation”, the jury evaluates the entries and only awards a Red Dot to products that win them over with their high design quality. About Lexon: Since its creation in 1991, Lexon has relentlessly pushed the limits and created a difference in the world of design while remaining true to its commitment to make small objects useful, beautiful, innovative and affordable. Whether in electronics, audio, travel accessories, office or leisure, Lexon has established a special relationship with creativity and partnered with the best designers around the world to create timeless collections of lifestyle products. Following its recent acquisition by BOW Group, a global player in the lifestyle and wearable consumer markets, Lexon is writing a new chapter in its history, experiencing a staggering international growth and digital expansion. Today, with 30 years of existence, more than 200 awards, collaboration with some of the most renowned designers, a retail presence in more than 90 countries across the Globe, Lexon has truly established itself as a worldly-known French design brand.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005520/en/ MediaAnnabel Corlaypress@lexon-design.com"
https://venturebeat.com/2021/04/12/cloudera-partners-with-nvidia-expand-gpu-usage-across-ai-applications/,Cloudera partners with Nvidia to expand GPU usage across AI applications,"Cloudera and Nvidia announced a collaboration that will allow organizations to use GPUs in more areas across the AI development lifecycle. Cloudera will integrate its Cloudera Data Platform with Nvidia’s accelerated Apache Spark 3.0 libraries. The integration will make it easier to add machine learning workflows to processes and create architectures without requiring GPU customization. Enterprises will be able to make changes to their data science workflows without having to also update the Nvidia integration manually. GPUs have shown tremendous promise in enhancing the data science side of AI development, enabling enterprises to run some types of workloads on top of GPUs. However, analytics often involve processes that span multiple teams, forcing enterprises to invest in customizing GPU integrations for those use cases. Gartner has predicted that creating new architecture patterns that help operationalize data science and ML pipelines will be one of the major trends in 2021. The partnership will allow enterprises to use GPUs across modern data workflows that span data preparation, data science, and analytics tasks. The typical workflow includes many steps including data ingestion, data curation, data pipeline automation, data science exploration, model development, testing, deployment, model monitoring and retraining, and delivery into the business. Cloudera has been busy in making these processes and the handoffs between them much easier over the last year. The Apache Spark 3.0 libraries are accelerated using Nvidia’s RAPIDS platform, which will dramatically accelerate much of the boring prep work required to bring new machine learning models into production. For example, the US Internal Revenue Service is already seeing a threefold improvement in data science workflows for fraud detection, said Joe Ansaldi, IRS technical branch chief for the Research Applied Analytics & Statistics Division, in a statement. Speeding up data preparation tasks and training models faster will save on infrastructure costs as well. GPU-accelerated Apache Spark 3 runs natively on CDP and can plug into high performance compute tools, Cloudera said. Cloudera was a trailblazer in the development of data lakes built on top of the Hadoop platform. Cloudera merged with Hortonworks, another Hadoop vendor, in 2018 and combined the technologies into a modern architecture called the Cloudera Data Platform (CDP). At the time, many speculated this spelled the end of Hadoop data warehouses, but Cloudera has continued to innovate and extend Hadoop into a more nimble workflow. Cloudera added Applied ML Prototypes (AMPs), a framework for packaging AI and ML models for data scientists, to CDP earlier this year. AMPs allow teams to take the guesswork out of ML projects with prebuilt business application templates for specific use cases, and they often run on Nvidia GPU hardware. Cloudera Data Engineering (CDE) streamlines the data engineering and prep work at the start of a project. This solved common problems data engineers face, such as scheduling and orchestration of complex data, troubleshooting and performance tuning tools for data flows, and improving collaboration with analytic and data science teams. The RAPIDS Accelerator for Apache Spark will be available in CDP Private Cloud this summer. Nvidia and Cloudera will roll out additional accelerated offerings in CDP over time, starting with Accelerated Deep Learning and Machine Learning in CDP Public Cloud in May. “This means that no matter where customers require these GPUs (from on-prem to public cloud, to hybrid cloud and beyond), they’ll be able to leverage best-in-class GPUs out of the box,” said Santiago Giraldo, Cloudera director of product marketing for data engineering and machine learning."
https://venturebeat.com/2021/04/12/why-any-developer-can-and-should-join-the-esports-boom-vb-live/,Why any developer can — and should — join the esports boom (VB Live),"Presented by Xsolla The global esports audience is 495 million strong and growing. And it’s surprisingly easy for developers and publishers to add tournaments, event streaming, and VIP content to their own platform to engage that fanbase. Learn how to tap into the opportunity in this VB Live event. Register here for free. For the past six years, esports global viewership has been booming, with no signs of stopping. At this rate, the worldwide audience is expected to grow to nearly 650 million by 2023, with revenue expected to surpass $1.5 billion. “Esports is one of the best ways for game developers to create a fandom and a following for their game, which in turn produces monetization opportunities,” says Berkley Egenes, vice president of marketing – game commerce at Xsolla. The esports opportunity isn’t just game sales. The audience for competitive, skill-based games is growing, and they’re becoming increasingly mainstream: It’s not just gaming, it’s media, pop culture, and commerce too. And developers benefit not just from the size of their player base, but from the esports athletes who are the force of personality at the center of these games. They act as ambassadors, often streaming content on their own wildly popular channels, and encouraging their fans to follow along with the in-game action, which generates more fans and more players of the game itself. Egenes points to the incredible success of Fortnite, a free-to-play game turned esport. It gained major traction on the streaming channels early on, and then developers embraced the idea of moving into esports . The 2019 Fortnite World Cup had the biggest payout at the time, $30 million. The numbers that Fortnite was seeing just continued to grow and grow, along with many of the top esports franchises like Call of Duty, League of Legends, DOTA2, Overwatch, and more. To tap into the global boom, Egenes recommends partnering with platforms like FACEIT, ESL, and PGL which enable developers to host tournaments and offer competitive gameplay opportunities. These events attract everyone from pros to casual gamers, and generate massive viewership channels for fans to follow along — very similar to the way fans can follow the real-world sports of the NFL or the Premier League, and so on. While a large enterprise game can develop its own platform, if you’re new to esports, platforms like FACEIT help you with everything from hosting and management to marketing the event. FACEIT has 20 million+ players around the world. Developers that work with the platform are getting their competitive gameplay tournaments in front of those millions of fans. And by partnering with an established platform with a secure payment provider, a developer can create a subscription program and greatly expand their payment methods on a global scale, Egenes says. “With the global scale of our payment platform, we’re able to help partners reach more fans and create more player engagement than was possible before,” he says. “Now players can compete in CS:GO, Hearthstone, PUBG, DOTA, Valorant, and other games through matchmaking tournaments, providing skill-based tracking, and anti-cheat detection – benefits that all of these subscribers are looking for.” Players want to participate, entering tournaments in which they can cover themselves in glory and potential prize money, but to do so, they need to hand out personal information. Safety and security is a priority for these players and for the fans tuning in. “Pro esports athletes just want to focus on the game – they want to avoid external perils when they’re going up against other players from across the world,” Egenes says. “If they can do it in a safe, secure environment on a platform like FACEIT, it encourages much higher probability of players getting involved.” But the biggest question is, when is the right time to launch an esports strategy? “It’s sexy and cool right now, but to jump in as your game is initially coming out, you need to do a lot of testing,” he says. “You need to talk to your community and see: one, is it something they’re interested in, and two, is there viability for your game to be an esport on a long-term basis. You don’t want to be a flash in the pan.” The opportunity to get your game on the big screen, across Twitch, and talked about online is something that needs to be considered in any competitive game launch, he adds. And your strategy should be considered from a long-term perspective, even as the boom continues beyond the pandemic. “It’s going to be exciting as we move from 100% online to hybrid tournaments and ultimately back to 100% live and in person again,” he says. “The point when we know we’re back in live esports is when we can put fans back in an arena. I’m optimistic and thinking we’re getting close.” Don’t miss out! Register here for free. Attendees will learn about: Speakers:"
https://venturebeat.com/2021/04/12/some-fda-approved-ai-medical-devices-are-not-adequately-evaluated-stanford-study-says/,"Some FDA-approved AI medical devices are not ‘adequately’ evaluated, Stanford study says","Some AI-powered medical devices approved by the U.S. Food and Drug Administration (FDA) are vulnerable to data shifts and bias against underrepresented patients. That’s according to a Stanford study published in Nature Medicine last week, which found that even as AI becomes embedded in more medical devices — the FDA approved over 65 AI devices last year — the accuracy of these algorithms isn’t necessarily being rigorously studied. Although the academic community has begun developing guidelines for AI clinical trials, there aren’t established practices for evaluating commercial algorithms. In the U.S., the FDA is responsible for approving AI-powered medical devices, and the agency regularly releases information on these devices including performance data. The coauthors of the Stanford research created a database of FDA-approved medical AI devices and analyzed how each was tested before it gained approval. Almost all of the AI-powered devices — 126 out of 130 — approved by the FDA between January 2015 and December 2020 underwent only retrospective studies at their submission, according to the researchers. And none of the 54 approved high-risk devices were evaluated by prospective studies, meaning test data was collected before the devices were approved rather than concurrent with their deployment. The coauthors argue that prospective studies are necessary, particularly for AI medical devices, because in-the-field usage can deviate from the intended use. For example, most computer-aided diagnostic devices are designed to be decision-support tools rather than primary diagnostic tools. A prospective study might reveal that clinicians are misusing a device for diagnosis, leading to outcomes that differ from what would be expected. There’s evidence to suggest that these deviations can lead to errors. Tracking by the Pennsylvania Patient Safety Authority in Harrisburg found that from January 2016 to December 2017, EHR systems were responsible for 775 problems during laboratory testing in the state, with human-computer interactions responsible for 54.7% of events and the remaining 45.3% caused by a computer. Furthermore, a draft U.S. government report issued in 2018 found that clinicians not uncommonly miss alerts — some AI-informed — ranging from minor issues about drug interactions to those that pose considerable risks. The Stanford researchers also found a lack of patient diversity in the tests conducted on FDA-approved devices. Among the 130 devices, 93 didn’t undergo a multisite assessment, while 4 were tested at only one site and 8 devices in only two sites. And the reports for 59 devices didn’t mention the sample size of the studies. Of the 71 device studies that had this information, the median size was 300, and just 17 device studies considered how the algorithm might perform on different patient groups. Partly due to a reticence to release code, datasets, and techniques, much of the data used today to train AI algorithms for diagnosing diseases might perpetuate inequalities, previous studies have shown. A team of U.K. scientists found that almost all eye disease datasets come from patients in North America, Europe, and China, meaning eye disease-diagnosing algorithms are less certain to work well for racial groups from underrepresented countries. In another study, researchers from the University of Toronto, the Vector Institute, and MIT showed that widely used chest X-ray datasets encode racial, gender, and socioeconomic bias. Beyond basic dataset challenges, models lacking sufficient peer review can encounter unforeseen roadblocks when deployed in the real world. Scientists at Harvard found that algorithms trained to recognize and classify CT scans could become biased toward scan formats from certain CT machine manufacturers. Meanwhile, a Google-published whitepaper revealed challenges in implementing an eye disease-predicting system in Thailand hospitals, including issues with scan accuracy. And studies conducted by companies like Babylon Health, a well-funded telemedicine startup that claims to be able to triage a range of diseases from text messages, have been repeatedly called into question. The coauthors of the Stanford study argue that information about the number of sites in an evaluation must be “consistently reported” in order for clinicians, researchers, and patients to make informed judgments about the reliability of a given AI medical device. Multisite evaluations are important for understanding algorithmic bias and reliability, they say, and can help in accounting for variations in equipment, technician standards, image storage formats, demographic makeup, and disease prevalence. “Evaluating the performance of AI devices in multiple clinical sites is important for ensuring that the algorithms perform well across representative populations,” the coauthors wrote. “Encouraging prospective studies with comparison to standard of care reduces the risk of harmful overfitting and more accurately captures true clinical outcomes. Postmarket surveillance of AI devices is also needed for understanding and measurement of unintended outcomes and biases that are not detected in prospective, multicenter trial.”"
https://venturebeat.com/2021/04/12/nvidia-launches-tao-an-enterprise-workflow-for-ai-development/,"Nvidia launches TAO, an enterprise workflow for AI development","During its GTC 2021 virtual keynote, Nvidia introduced a new product designed to help enterprises choose, adapt, and deploy machine learning models. Called TAO and available starting today in early access, it enables transfer learning as well as other machine learning techniques from a single, enterprise-focused pane of glass. Transfer learning’s ability to store knowledge gained while solving a problem and apply it to a related problem has attracted considerable attention in the enterprise. Using it, a data scientist can take an open source model like BERT, for example, which is designed to understand generic language, and refine it at the margins to comprehend the jargon employees use to describe IT issues. TAO integrates Nvidia’s Transfer Learning Toolkit to leverage small datasets, giving models a custom fit without the cost, time, and massive corpora required to build and train models from scratch. TAO also incorporates federated learning, which lets different machines securely collaborate to refine a model for the highest accuracy. Users can share components of models while ensuring datasets remain inside each company’s datacenter. In machine learning, federated learning entails training algorithms across client devices that hold data samples without exchanging those samples. A centralized server might be used to orchestrate rounds of training for the algorithm and act as a reference clock, or the arrangement might be peer-to-peer. Regardless, local algorithms are trained on local data samples and the weights — the learnable parameters of the algorithms — are exchanged between the algorithms at some frequency to generate a global model. TAO also incorporates Nvidia TensorRT, which dials a model’s mathematical coordinates to a balance of the smallest model size with the highest accuracy for the system it’ll run on. Nvidia claims that TensorRT-based apps perform up to 40 times faster than CPU-only platforms during inference. Elements of TAO are already in use in warehouses, in retail, in hospitals, and on the factory floor, Nvidia claims. Users include companies like Accenture, BMW and Siemens Industrial. “AI is the most powerful new technology of our time, but it’s been a force that’s hard to harness for many enterprises — until now. Many companies lack the specialized skills, access to large datasets or accelerated computing that deep learning requires. Others are realizing the benefits of AI and want to spread them quickly across more products and services,” Adel El Hallak, director of product management for NGC at Nvidia, wrote in a blog post. “TAO … can quickly tailor and deploy an application using multiple AI models.” The benefits of AI and machine learning can feel intangible at times, but surveys show this hasn’t deterred enterprises from adopting the technology in droves. Business use of AI grew a whopping 270% from 2015 to 2019, according to Gartner, while Deloitte says 62% of respondents for its corporate October 2018 report deployed some form of AI, up from 53% a year ago. Bolstered by this growth, Grand View Research predicts that the global AI market size will reach $733.7 billion by 2027."
https://venturebeat.com/2021/04/12/nvidia-unveils-rental-model-for-dgx-superpod-mini-supercomputers/,Nvidia unveils rental model for DGX Station A100 mini supercomputers,"The day has come when you can rent your own mini supercomputer. When you’re done with it, you can return it to Nvidia. That’s the plan for the company’s new cloud-native supercomputer, the Nvidia DGX Station A100. Nvidia made the announcement during the keynote speech at its GTC 2021 event. You can use the supercomputer for a short period of time when you need it and then return it after you’re done, said Manuvir Das, head of enterprise computing at Nvidia, in a press briefing. The DGX Station is a multi-tenant supercomputer that can be shared by as many as 28 data scientists. Also announced at GTC is a new Nvidia DGX SuperPod that will be available with Nvidia’s Bluefield-2 DPUs, enabling a cloud-native supercomputer. A DGX SuperPod consists of a bunch of individual DGX Station computers. “You can think of us progressing in two directions, one with constant innovation to raise the bar. But the other is to really democratize AI to put it in the hands of as many companies and scientists as we possibly can,” Das said. “We are also announcing for the first time a rental model. Instead of procuring a DGX Station, customers will be able to rent a station directly from Nvidia at a low monthly price point, and they can use it for as long as they choose and then return it to Nvidia. So that’s an important direction that we are taking with the Station.” This opens the world of AI to more enterprise customers as they investigate areas such as AI, drug discovery, autonomous vehicles, and more. Those DPUs can offload, accelerate, and isolate users’ data — providing customers with secure connections to their AI infrastructure. Nvidia DGX SuperPod will be available with Nvidia’s Bluefield-2 DPUs, enabling a cloud-native supercomputer. It’s a full bare-metal supercomputer that is also sharable. In a keynote speech at GTC 21, Nvidia CEO Jensen Huang said the company will focus on three kinds of chips: DPUs, central processing units (CPUs) like Grace, and graphics processing units (GPUs). It will alternate between Arm-based and x86-based products, he said. “We have to make AI easier to use,” Huang said. This is the first time Nvidia is turning to a rental model for DGX Stations. The idea is to broaden AI adoption for enterprise IT departments that need to support the work of teams in multiple locations or to help academic and research institutions, which often have to grant outside organizations access to their computers. DGX Stations start at $149,000, while the DGX SuperPod starts at $7 million and scales to $60 million. The company also announced NVIDIA Base Command, which enables multiple users and IT teams to securely access, share, and operate their DGX SuperPod infrastructure. Base Command coordinates AI training and operations on DGX SuperPod infrastructure to enable the work of teams of data scientists and developers located around the globe. The DGX SuperPods are AI supercomputers featuring 20 or more Nvidia DGX A100 systems and Nvidia InfiniBand HDR networking. Among the latest to deploy DGX SuperPODs to power new AI solutions and services is Sony, which uses the DGX SuperPod for its corporate research team to infuse AI across the company. Other customers are Naver, Recursion, MTS, and VinAI. Additionally, Nvidia and Schrödinger today separately announced a strategic partnership designed to harness DGX SuperPods to further accelerate drug discovery at a supercomputing scale. Nvidia also introduced a subscription offering for the DGX Station A100. The new subscription program makes it easier for companies at every stage of growth to accelerate AI development outside the datacenter for teams working in corporate offices, research facilities, labs, and home offices. The DGX A100 is a collection of DGXs working together as one cluster of infrastructure to produce a supercomputer, Das said. Each box is capable of 2.5 teraflops of computing power. Cloud-native, multi-tenant Nvidia DGX SuperPods will be available in Q2 through Nvidia’s global partners."
https://venturebeat.com/2021/04/12/mozilla-winds-down-deepspeech-development-announces-grant-program/,"Mozilla winds down DeepSpeech development, announces grant program","In 2017, Mozilla launched DeepSpeech, an initiative incubated within the machine learning team at Mozilla Research focused on open sourcing an automatic speech recognition model. Over the next four years, the DeepSpeech team released newer versions of the model capable of transcribing lectures, phone conversations, television programs, radio shows, and other live streams with “human accuracy.” But in the coming months, Mozilla plans to cease development and maintenance of DeepSpeech as the company transitions into an advisory role, which will include the launch of a grant program to fund a number of initiatives demonstrating applications for DeepSpeech. DeepSpeech isn’t the only open source project of its kind, but it’s among the most mature. Modeled after research papers published by Baidu, the model is an end-to-end trainable, character-level architecture that can transcribe audio in a range of languages. One of Mozilla’s major aims was to achieve a transcription word error rate of lower than 10%, and the newest versions of the pretrained English-language model achieve that aim, averaging around a 7.5% word error rate. It’s Mozilla’s belief that DeepSpeech has reached the point where the next step is to work on building applications. To this end, the company plans to transition the project to “people and organizations” interested in furthering “use-case-based explorations.” Mozilla says it’s streamlined the continuous integration processes for getting DeepSpeech up and running with minimal dependencies. And as the company cleans up the documentation and prepares to stop Mozilla staff upkeep of the codebase, Mozilla says it’ll publish a toolkit to help people, researchers, companies, and any other interested parties use DeepSpeech to build voice-based solutions. Mozilla’s work on DeepSpeech began in late 2017, with the goal of developing a model that gets audio features — speech — as input and outputs characters directly. The team hoped to design a system that could be trained using Google’s TensorFlow framework via supervised learning, where the model learns to infer patterns from datasets of labeled speech. The latest DeepSpeech model contains tens of millions parameters, or the parts of the model that are learned from historical training data. The Mozilla Research team started training it with a single computer running four Titan X Pascal GPUs but eventually migrated it to two servers with 8 Titan XPs each. In the project’s early days, training a high-performing model took about a week. In the years that followed, Mozilla worked to shrink the DeepSpeech model while boosting its performance and remaining below the 10% error rate target. The English-language model shrank from 188MB to 47MB and memory consumption dropped by 22 times. In December 2019, the team managed to get DeepSpeech running “faster than real time” on a single core of a Raspberry Pi 4.  Mozilla initially trained DeepSpeech using freely available datasets like TED-LIUM and LibriSpeech as well as paid corpora like Fisher and Switchboard, but these proved to be insufficient. So the team reached out to public TV and radio stations, language study departments in universities, and others they thought might have labeled speech data to share. Through this effort, they were able to more than double the amount of training data for the English-language DeepSpeech model. Inspired by these data collection efforts, the Mozilla Research team collaborated with Mozilla’s Open Innovation team to launch the Common Voice project, which seeks to collect and validate speech contributions from volunteers. Common Voice consists not only of voice snippets but of voluntarily contributed metadata useful for training speech engines, like speakers’ ages, sex, and accents. It’s also grown to include dataset target segments for specific purposes and use cases, like the digits “zero” through “nine” and the words “yes,” ” no,” ” hey,” and ” Firefox.” Today, Common Voice is one of the largest multi-language public domain voice corpora in the world, with more than 9,000 hours of voice data in 60 different languages including widely spoken languages and less-used ones,  like Welsh and Kinyarwanda. Over 164,000 people have contributed to the dataset to date. To support the project’s growth, Nvidia today announced that it would invest $1.5 million in Common Voice to engage more communities and volunteers and support the hiring of new staff. Common Voice will now operate under the umbrella of the Mozilla Foundation as part of its initiatives focused on making AI more trustworthy. As it winds down the development of DeepSpeech, Mozilla says its forthcoming grant program will prioritize projects that contribute to the core technology while also showcasing its potential to “empower and enrich” areas that may not otherwise have a viable route toward speech-based interaction. More details will be announced in May, when Mozilla publishes a playbook to guide people on how to use DeepSpeech’s codebase as a starting point for voice-powered applications. “We’re seeing mature open source speech engines emerge. However, there is still an important gap in the ecosystem: speech engines — open and closed — don’t work for vast swaths of the world’s languages, accents, and speech patterns,” Mark Surman, executive director of the Mozilla Foundation, told VentureBeat via email. “For billions of internet users, voice-enabled technologies simply aren’t usable. Mozilla has decided to focus its efforts this side of the equation, making voice technology inclusive and accessible. That means investing in voice data sets rather than our own speech engine. We’re doubling down on Common Voice, an open source dataset that focuses on languages and accents not currently represented in the voice tech ecosystem. Common Voice data can be used to feed [open speech] frameworks … and in turn to allow more people in more places to access voice technology. We’re [also] working closely with Nvidia to match up these two sides of the inclusive voice tech equation.”"
https://venturebeat.com/2021/04/12/nvidia-debuts-drive-atlan-system-on-chip-for-autonomous-vehicles/,Nvidia debuts Drive Atlan system-on-chip for autonomous vehicles,"At GTC 2021, Nividia this morning unveiled Drive Atlan, an AI-enabled system-on-a-chip for autonomous vehicles that will target automakers’ 2025 models. Capable of delivering just over 1,000 trillion operations per second (TOPs), Atlan includes Nvidia’s next-generation GPU architecture, ARM CPU cores, and deep learning and computer vision accelerators. An MIT report estimates truly autonomous vehicles might not hit the streets for a decade. Still, some experts predict the pandemic will hasten the adoption of autonomous transportation technologies. Despite needing disinfection, driverless cars can potentially minimize the risk of spreading disease. Allied Market Research predicts that the the global autonomous vehicle market will be valued at $556.67 billion by 2026. Atlan, which integrates an Nvidia BlueField data processing unit for networking, storage, and security functions, is 4 to 33 times as powerful in terms of compute as Nvidia’s other autonomous driving-focused chips, Drive Xavier and Drive Orin. Xavier can reach up to 30 TOPs, while Orin tops out at 254. Nvidia CEO Jensen Huang says Atlan is designed to handle the large number of AI applications that run simultaneously in driverless vehicles.  “Our new Atlan is truly a technical marvel, fusing all of Nvidia’s strengths in AI, auto, robotics, safety and BlueField-secure datacenters to deliver safe, autonomous-driving fleets,” Huang said in a press release. “The transportation industry needs a computing platform that it can rely on for decades. The software investment is too immense to repeat for each car.” Nvidia also announced it would expand its partnership with Volvo to use Orin in upcoming Volvo models. The first car featuring the chip will be the XC90, scheduled to be revealed next year, which will leverage software developed by Volvo’s Zenseact subsidiary, as well as backup systems for steering and braking. A Xavier-powered server will manage core functionalities inside the car, like energy management and driver assistance, while Orin will handle vision and lidar sensor data processing. Nvidia and Volvo began collaborating in 2018, when Volvo agreed to use Xavier for the computers in cars built on its scalable product architecture (SPA2) platform. SPA2 ships with an autonomous driving feature, Highway Pilot, that can be activated in supported locations and conditions. Nvidia also launched its newest Hyperion platform, which includes sensors, high-performance compute, and software for level 4 autonomous vehicle development. The 8th-gen Hyperion employs two Orin chips to process data from 12 exterior cameras, three interior cameras, nine radars, and one lidar sensor. Calibrated for 3D data collection and compatibility with Nvidia’s existing Drive AV and Drive IX software stacks, the platform supports real-time record and capture for driving data processing. The 8th-gen Hyperion will be available later in 2021."
https://venturebeat.com/2021/04/12/nvidia-launches-jarvis-conversational-ai-framework-in-general-availability/,Nvidia launches Jarvis conversational AI framework in general availability,"At its GTC 2021, Nvidia this morning announced the general availability of its Jarvis framework, which provides developers with pretrained AI models and software tools to create interactive conversational experiences. Nvidia says that Jarvis models, which first became available in May 2020 in preview, offer automatic speech recognition, as well as language understanding, real-time language translations, and text-to-speech capabilities for conversational agents. The ubiquity of smartphones and messaging apps — spurred by the pandemic — have contributed to the increased adoption of conversational technologies. Fifty-six percent of companies told Accenture in a survey that conversational bots and other experiences are driving disruption in their industry. And a Twilio study showed 9 out of 10 consumers would like the option to use messaging to contact a business. Leveraging GPU acceleration, Jarvis’ pipeline can be run in under 100 milliseconds and deploy in the cloud, in a datacenter, or at the edge. The framework includes models trained on over 1 billion pages of text and over 60,000 hours of speech that can be adjusted, optimized, fine-tuned with custom data, and tailored to different tasks, industries, and systems.  T-Mobile is among Jarvis’ early users, and Jarvis — which supports five languages including English, Chinese, and Japanese — has racked up more than 45,000 downloads since becoming available early last year. According to Nvidia, the telecom giant is using the framework to help resolve customer service issues in real time. Even before the pandemic, autonomous agents were on the way to becoming the rule rather than the exception, partly because consumers prefer it that way. According to research published last year by Vonage subsidiary NewVoiceMedia, 25% of people prefer to have their queries handled by a chatbot or other self-service alternative. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Nvidia also announced that it’s partnering with Mozilla Common Voice, an open source collection of voice data for startups, researchers, and developers to train voice-enabled apps, services, and devices. The world’s largest multi-language public domain voice dataset, Common Voice contains over 9,000 total hours of contributed voice data in 60 different languages. Nvidia says it’s using Jarvis to develop pretrained models with the dataset that it will then offer to the community for free. “We launched Common Voice to teach machines how real people speak in their unique languages, accents, and speech patterns,” Mozilla executive director Mark Surman said in a press release. “Nvidia and Mozilla have a common vision of democratizing voice technology — and ensuring that it reflects the rich diversity of people and voices that make up the internet.” Newly revealed features in Jarvis will be released in the second quarter of 2021 as part of Nvidia’s ongoing open beta program. Developers can download the framework today from Nvidia’s NGC catalog."
https://venturebeat.com/2021/04/12/nvidia-announces-morpheus-an-ai-powered-app-framework-for-cybersecurity/,"Nvidia announces Morpheus, an AI-powered app framework for cybersecurity","During its GTC 2021 virtual keynote this morning, Nvidia announced Morpheus, a “cloud-native” app framework aimed at providing cybersecurity partners with AI skills that can be used to detect and mitigate cybersecurity attacks. Using machine learning, Morpheus identifies, captures, and acts on threats and anomalies, including leaks of sensitive data, phishing attempts, and malware. Morpheus is available in preview from today, and developers can apply for early access on Nvidia’s landing page. Reflecting the pace of adoption, the AI in cybersecurity market will reach $38.2 billion in value by 2026, Markets and Markets projects. That’s up from $8.8 billion in 2019, representing a compound annual growth rate of around 23.3%. Just last week, a study from MIT Technology Review Insights and Darktrace found that 96% of execs at large enterprises are considering adopting “defensive AI” against cyberattacks. Morpheus essentially enables compute nodes in networks to serve as cyberdefense sensors — Nvidia says its newly announced BlueField-3 data processing units can be specifically configured for this purpose. With Morpheus, organizations can analyze packets without information replication, leveraging real-time telemetry and policy enforcement, as well as data processing at the edge. Thanks to AI, Morpheus can ostensibly analyze more security data than conventional cybersecurity app frameworks without sacrificing cost or performance.  Developers can create their own Morpheus skills using deep learning models, and Nvidia says “leading” hardware, software, and cybersecurity solutions providers are working to optimize and integrate datacenter security offerings with Morpheus, including Aria Cybersecurity Solutions, Cloudflare, F5, Fortinet, Guardicore Canonical, Red Hat, and VMware. Morpheus is also optimized to run on a number of Nvidia-certified systems from Atos, Dell, Gigabyte, H3C, HPE, Inspur, Lenovo, QCT, and Supermicro. Businesses are increasingly placing their faith in defensive AI like Morpheus to combat the growing number of cyberthreats. Known as an autonomous response, defensive AI can interrupt in-progress attacks without affecting day-to-day business. For example, given a strain of ransomware an enterprise hasn’t encountered in the past, defensive AI can identify the novel and abnormal patterns of behavior and stop the ransomware even if it isn’t associated with publicly known compromise indicators (e.g., blacklisted command-and-control domains or malware file hashes). According to the above-mentioned MIT and Darktrace survey, 44% of executives are assessing AI-enabled security systems and 38% are deploying autonomous response technology. This agrees with findings from Statista. In a 2019 analysis, the firm reported that around 80% of executives in the telecommunications industry believe their organization wouldn’t be able to respond to cyberattacks without AI."
https://venturebeat.com/2021/04/12/nvidia-unveils-grace-arm-based-cpu-for-giant-scale-ai-and-hpc-apps/,Nvidia unveils Grace ARM-based CPU for giant-scale AI and HPC apps,"Nvidia unveiled its Grace processor today. It’s an ARM-based central processing unit (CPU) for giant-scale artificial intelligence and high-performance computing applications. It’s Nvidia’s first datacenter CPU, purpose-built for applications that are operating on a giant scale, Nvidia CEO Jensen Huang said in a keynote speech at Nvidia’s GTC 2021 event. Grace delivers 10 times the performance leap for systems training giant AI models, using energy-efficient ARM cores. And Nvidia said the Swiss Supercomputing Center and the U.S. Department of Energy’s Los Alamos National Laboratory will be the first to use Grace, which is named for Grace Hopper, who pioneered computer programming in the 1950s. The CPU is expected to be available in early 2023. “Grace is a breakthrough CPU. It’s purpose-built for accelerated computing applications of giant scale for AI and HPC,” said Paresh Kharya, senior director of product management and marketing at Nvidia, in a press briefing. Huang said, “It’s the world’s first CPU designed for terabyte scale computing.” The CPU is the result of more than 10,000 engineering years of work. Nvidia said the chip will address the computing requirements for the world’s most advanced applications — including natural language processing, recommender systems, and AI supercomputing — that analyze
enormous datasets requiring both ultra-fast compute performance and massive memory. Grace combines energy-efficient ARM CPU cores with an innovative low-power memory subsystem to deliver high performance with great efficiency. The chip will use a future ARM core dubbed Neoverse. “Leading-edge AI and data science are pushing today’s computer architecture beyond its limits — processing unthinkable amounts of data,” Huang said in his speech. “Using licensed ARM IP, Nvidia has designed Grace as a CPU specifically for giant-scale AI and HPC. Coupled with the GPU and DPU, Grace gives us the third foundational technology for computing and the ability to re-architect the datacenter to advance AI. Nvidia is now a three-chip company.” Grace is a highly specialized processor targeting workloads such as training next-generation NLP models that have more than 1 trillion parameters. When tightly coupled with Nvidia GPUs, a Grace-based system will deliver 10 times faster performance than today’s Nvidia DGX-based systems, which run on x86 CPUs. In a press briefing, someone asked if Nvidia will compete with x86 chips from Intel and AMD. Kharya said, “We are not competing with x86 … we continue to work very well with x86 CPUs.” Grace is designed for AI and HPC applications, but Nvidia isn’t disclosing additional information about where Grace will be used today. Nvidia also declined to disclose the number of transistors in the Grace chip. Nvidia is introducing Grace as the volume of data and size of AI models grow exponentially. Today’s largest AI models include billions of parameters and are doubling every two and a half months. Training them requires a new CPU that can be tightly coupled with a GPU to eliminate system bottlenecks. “The biggest announcement of GTC 21 was Grace, a tightly integrated CPU for over a trillion parameter AI models,” said Patrick Moorhead, an analyst at Moor Insights & Strategies. “It’s hard to address those with classic x86 CPUs and GPUs connected over PCIe. Grace is focused on IO and memory bandwidth, shares main memory with the GPU and shouldn’t be confused with general purpose datacenter CPUs from AMD or Intel.” Underlying Grace’s performance is 4th-gen Nvidia NVLink interconnect technology, which provides 900 gigabyte-per-second connections between Grace and Nvidia graphics processing units (GPUs) to enable 30 times higher aggregate bandwidth compared to today’s leading servers. Grace will also utilize an innovative LPDDR5x memory subsystem that will deliver twice the bandwidth and 10 times better energy efficiency compared with DDR4 memory. In addition, the new architecture provides unified cache coherence with a single memory address space, combining system and HBM GPU memory to simplify programmability. “The Grace platform and its Arm CPU is a big new step for Nvidia,” said Kevin Krewell, an analyst at Tirias Research, in an email. “The new design of one custom CPU attached to the GPU with coherent NVlinks is Nvidia’s new design to scale to ultra-large AI models that now take days to run. The key to Grace is that using the custom Arm CPU, it will be possible to scale to large LPDDR5 DRAM arrays far larger than possible with high-bandwidth memory directly attached to the GPUs.” Grace will power the world’s fastest supercomputer for the Swiss organization. Dubbed Alps, the machine will feature 20 exaflops of AI processing. (This refers to the amount of computing available for AI applications.) That’s about 7 times more computation than is available with the 2.8-exaflop Nvidia Seline supercomputer, the leading AI supercomputer today. HP Enterprise will be building the Alps system. Alps will work on problems in areas ranging from climate and weather to materials sciences, astrophysics, computational fluid dynamics, life sciences, molecular dynamics, quantum chemistry, and particle physics, as well as domains like economics and social sciences, and will come online in 2023. Alps will do quantum chemistry and physics calculations for the Hadron collider, as well as weather models. “This is a very balanced architecture with Grace and a future Nvidia GPU, which we have not announced yet, to enable breakthrough research on a wide range of fields,” Kharya said. Meanwhile, Nvidia also said that it would make its graphics chips available with Amazon Web Services’ Graviton2 ARM-based CPU for datacenters for cloud computing. With Grace, Nvidia will embark on a mult-year pattern of creating graphics processing units, CPUs, and data processing units (CPUs), and it will alternate between Arm and x86 architecture designs, Huang said."
https://venturebeat.com/2021/04/12/nvidia-announces-bluefield-3-dpus-for-ai-and-analytics-workloads/,Nvidia announces BlueField-3 DPUs for AI and analytics workloads,"At GTC 2021, Nvidia this morning took the wraps off of the BlueField-3 data processing unit (DPU), the latest in its lineup of datacenter machines built for AI and analytics workloads. BlueField-3 packs software-defined networking, storage, and cybersecurity acceleration capabilities, offering what Nvidia claims is the equivalent of up to 300 CPU cores of horsepower — or 1.5 TOPs. As of 2019, the adoption rate of big data analytics stood at 52.5% among organizations, with a further 38% intending to use the technology in the future, according to Statista. The advantages are obvious. A 2019 survey by Enterprenuer.com found that enterprises implementing big data analytics have seen a profit increase of 8% to 10%. Nvidia’s BlueField-3 DPUs features 300GbE/NDR interconnects and can deliver up to 10 times the compute of the previous-generation BlueField-2 DPUs, with 22 billion transistors, while isolating apps from the control and management plane. The 16 ARM A78 cores inside can manage 4 times the cryptography performance, and BlueField-3 is the first DPU to support fifth-generation PCIe and time-synchronized datacenter acceleration.  BlueField-3 can additionally act as a monitoring agent for Morpheus, Nvidia’s AI-enabled cloud cybersecurity platform that was also announced today. Moreover, it takes advantage of DOCA, the company’s datacenter-on-a-chip architecture for building software-defined, hardware-accelerated networking, storage, security, and management apps running on BlueField DPUs. BlueField-3 is expected to sample in the first quarter of 2022. It’s fully backward-compatible with BlueField-2, Nvidia says. “Modern hyperscale clouds are driving a fundamental new architecture for data centers,” Nvidia founder and CEO Jensen Huang said in a press release. “A new type of processor, designed to process data center infrastructure software, is needed to offload and accelerate the tremendous compute load of virtualization, networking, storage, security and other cloud-native AI services. The time for BlueField DPU has come.” Nvidia’s datacenter business, which includes its DPU segment, is fast becoming a major revenue driver for the company. In February, it posted record quarterly revenue of $1.9 billion, up 97% from a year ago. Full-year datacenter revenue jumped 124%, to $6.7 billion."
https://venturebeat.com/2021/04/12/nvidia-reveals-omniverse-enterprise-for-simulating-products-and-worlds/,Nvidia reveals Omniverse Enterprise for simulating products and worlds,"Nvidia has announced its Omniverse, a virtual environment the company describes as a “metaverse” for engineers, will be available as an enterprise service later this year. CEO Jensen Huang showed a demo of the Omniverse, where engineers can work on designs in a virtual environment, as part of the keynote talk at Nvidia’s GPU Technology Conference, a virtual event being held online this week. I also moderated a panel on the plumbing for the metaverse with a number of enterprise participants. Huang said that the Omniverse is built on Nvidia’s entire body of work, letting people simulated shared virtual 3D worlds that obey the worlds of physics. “The science fiction metaverse is near,” he said in a keynote speech. “One of the most important parts of Omniverse is that it obeys the laws of physics.” The Omniverse is a virtual tool that allows engineers to collaborate. It was inspired by the science fiction concept of the metaverse, the universe of virtual worlds that are all interconnected, like in novels such as Snow Crash and Ready Player One. The project started years ago as a proprietary Nvidia project called Holodeck, named after the virtual reality simulation in Star Trek. But it morphed into a more ambitious industry-wide effort based on the plumbing made possible by the Universal Scene Description (USD) technology Pixar developed for making its movies. Nvidia has spent years and hundreds of millions of dollars on the project, said Richard Kerris, Nvidia media and entertainment general manager, in a press briefing. Omniverse debuted in beta form in December. More than 17,000 users have tested it since then, and now the company is making the Omniverse available as a subscription service for enterprises. It’s just the kind of thing that engineers need during the pandemic to work on complex projects remotely. BMW Group, Ericsson, Foster + Partners, and  WPP are using Omniverse. It has application support from Bentley Systems, Adobe, Autodesk, Epic Games, ESRI, Graphisoft, Trimble, Robert McNeel & Associates, Blender, Marvelous Designer, Reallusion, and Wrnch. And support comes from the likes of Asus, Boxx Technologies, Cisco, Dell Technologies, HP, Lenovo, and Supermicro. More than 400 enterprises are going to use the new version for enterprises starting this summer. It comes with enterprise support for fully established enterprises, Kerris said. The Omniverse, which was previously available only in early access mode, enables photorealistic 3D simulation and collaboration. It’s a metaverse that obeys the laws of physics, and so it enables companies and individuals to simulate things from the real world that can’t be tested easily in the real world, like self-driving cars, which can be dangerous to pedestrians if they aren’t perfected. Mattias Wikenmalm, technical specialist at Volvo, said on the panel that it’s necessary to simulate not just the car but the context around the car, like a city environment. “The foundation is still the data, and this is the first time we can be data native, where we don’t have to focus on moving data between different systems. In this case, data is a first-class citizen,” Wikenmalm said. “It’s so nice we can just focus on the data and borrow our data for different applications and transform that data. Exchanging data between systems has been complex. If we can get that out of the way, we can start building a proper metaverse.” BMW is using Omniverse to simulate a full car factory before it builds it. And there’s no limit to the testing. If someone wanted to create an entire city, or even build a simulation of the entire United States, for a self-driving car testing ground, it would be possible. It is intended for tens of millions of designers, engineers, architects, and other creators to use at the same time. The designers can work on the same parts of their designs at the same times without overwriting each other, with changes offered as options for others to accept. That makes it ideal for large teams to work together. Susanna Holt, vice president of engineering for Autodesk, said on the panel that being able to understand someone else’s data is important, and it means you don’t have to be locked into a single tool or workflow. “We need the bits to talk to one another, and that’s been so hard until now,” she said. “It is still hard, as you have to import and export data. With USD, it’s the beginning of a new future.” The Omniverse uses Nvidia’s RTX 3D simulation tech to enable engineers to do things like work on a car’s design inside a simulation while virtually walking around it or sitting inside it and interacting with it in real time. Martha Tsigkari, partner at architectural firm Foster + Partners, said on the panel that the architecture and construction industries really need the ability to transfer data easily from one site to the next. “Being able to do that in an easy way without having to think about how we change that information is really important,” Tsigkari said. “In order to run really difficult simulations, or understand how buildings perform, we need to use all kinds of software to do this. Working in these processes right now can be painful, and we need to create all of these bespoke tools to do this. A future where this becomes a seamless process and opens to all kinds of industries is a fantastic opportunity that we need to grasp and go for.” Engineers on remote teams will be able to work alongside architects, 3D animators, and other people working on 3D buildings simultaneously, as if they were jointly editing a Google Doc, Kerris said. He added, “The Omniverse was built for our own needs in development.” Pixar’s Universal Scene Description (USD) is the HTML of 3D, and it’s the foundation for sharing different kinds of images from multiple parties in Omniverse, said Kerris. “We felt that with the entire community starting to move towards this open platform for exchanging 3D information including the objects, scenes, materials and everything, it was the best place for us to start with the foundation for what this platform would become,” Kerris said. Pixar’s USD standard came from over a decade of film production. Guido Quaroni is director of engineering and 3D immersive at Adobe, and before that he was at Pixar, where he was responsible for open sourcing USD. In a panel at GTC, he said the idea emerged at Pixar in 2010 as the company was dealing with multiple libraries that dealt with large scenes in its movies. “Some of the ideas in USD go back 20 years to Toy Story 2, but the idea was to formalize it and write it in a way that we could eventually open source it,” Quaroni said. He worked with Sebastian “Spiff” Grassia, head of the team that built USD at Pixar. “We knew that every studio kind of had something like it,” Quaroni said. “And we wanted to see if we could offer something that became the standard, because for us, the biggest problem was the plugins and integrations with third parties. Why not give it to the world?” The problem that they had was that they needed to be able, at any point in the film pipeline, to extract an asset, to massage it with a third-party tool, and to stick it back into the production process without losing information, said Michael Kass, distinguished engineer at Nvidia and software architect of the Omniverse, in an interview. Grassia said USD is an interchange format for data. “It represents decades of Pixar’s experience in building software that supports collaborative filmmaking,” Grassia said. “It’s for collaborative authoring and viewing for a very large 3D scene. It handles combining, assembling, overriding, and animating the assets that you have created in a non-destructive way. That allows for multiple artists to work on the same scene concurrently.” Before USD, artists had to check out a piece of digital art, work on it, and check it back in. With USD, Nvidia has enabled sharing across all applications and different ways of viewing the art. The changes are transmitted back and forth. A large number of people can view and work on the same thing, Kass said. A feature dubbed Nucleus serves as a traffic cop that communicates what is changing in a 3D scene. Early on, Pixar tried to create tools itself, but it found there were tools like Maya, 3D Studio Max, Unreal Engine, or Blender that were more advanced at doing particular tasks. And rather than have to train those vendors to continuously update their tools, Pixar made USD available as an open standard. The platform also uses Nvidia technology, such as real-time photorealistic rendering, physics, materials, and interactive workflows between industry-leading 3D software products. Pixar built a renderer, a data visualization engine dubbed Hydra. It was designed in a way to hook up other data sources, like a Maya image. So the artists can work with large datasets without having the vendor translate everything into their own native representation. Kass and his colleagues at Nvidia found that USD was a “golden nugget” that let them represent data in a way that could be used for all sorts of different purposes. “We decided to put USD at the center of our virtual worlds, but at Pixar, most of the collaboration was not real time. So we added on top of USD the ability to synchronize with different users,” Kass said. The real test has been making sure that USD can be useful beyond the media and entertainment applications. Omniverse enables collaboration and simulation that could become essential for Nvidia customers working in robotics, automotive, architecture, engineering, construction, and manufacturing. “There really isn’t anything else like it,” Kerris said. “Pixar built the standard, and we saw the potential in it. This is a demand and a need that everybody has. Can you imagine the internet without a standard way of describing a web page? It used to be that way. With 3D, no two applications use the same language today. That needs to change, or else we really can’t build the metaverse.” Nvidia extended USD, which was built for Pixar’s needs, and added what is necessary for the metaverse, Kass said. “We got to stand on top of giants, but we are pushing it forward in a direction they weren’t envisioning when they started,” he added. Nvidia built a tool called Omniverse Create, which accelerates scene composition and allows users in real time to interactively assemble, light, simulate, and render scenes. It also built Omniverse View, which powers seamless collaborative design and visualization of architectural and engineering projects with photorealistic rendering. Nvidia RTX Virtual Workstation software gives collaborators the freedom to run their graphics-intensive 3D applications from anywhere. Omniverse Enterprise is a new platform that includes the Nvidia Omniverse Nucleus server, which manages the database shared among clients, and Nvidia Omniverse Connectors, which are plug-ins to industry-leading design applications. With all of the applications working live, artists don’t have to go through a laborious exporting or importing process. “Omniverse is an important tool for industrial design — especially with human-robot interactions,” said Kevin Krewell, an analyst at Tirias Research, in an email. “Simulation is a big new market for GPU cloud services.” The Omniverse and USD aren’t going to lead to the metaverse overnight. Tsigkari said that getting so many creative industries to work together has been a huge challenge, particularly for architecture firms that have to pull so many different disciplines to get work done from conception to completion. “You need a way to allow for the creative people to quickly pass things directly from engineers to consultants so they can do their analysis and pass it on to the manufacturers,” she said. “In the simplest way, this doesn’t exist.” At the same time, different industries work on different timetables, from long cycles to real time. “For us, this has been really crucial to be able to do this in a seamless way where you don’t have to think about the in-between space,” she said. Holt at Autodesk said she would like to see USD progress forward in dealing with huge datasets, on the level of modeling cities for construction purposes. “It’s not up to that yet,” she said. “Some changes would be needed as we take it into other areas like construction.” Grassia said there are features that allow of “lazy loading,” or different levels of detail becoming visible as a huge dataset loads. Lori Hufford, vice president of applications integration at Bentley Systems, said on a panel her team has had good results so far working on large models. “I’m really excited about the open nature of USD,” she said. “We’ve been very impressed with the scale we have been able to achieve with USD.” The enterprise version will support Windows and Linux machines, and it is coming later this year. What can you do in this engineer’s metaverse? You can simulate the creation of robots through a tool dubbed Isaac. That lets engineers create variations of robots and see how they would work with realistic physics, so they can simulate what a robot would do in the real world by first making the robot in a virtual world. There are also Omniverse Connectors, which are plugins that connect third-party tools to the platform. That allows the Omniverse to be customized for different vertical markets. BMW is using Omniverse to simulate the exact details of a car factory, simulating a complete physical space. The company calls the factory a “digital twin.” The factory has enough detail to include 300 cars in it at a given time, and each car has about 10 gigabytes of data. Thousands of planners, product engineers, facility managers, and lean experts within the global production network are able to collaborate in a single virtual environment to design, plan, engineer, simulate, and optimize extremely complex manufacturing systems before a factory is actually built or a new product is integrated. Milan Nedeljkovic, member of the board of management of BMW AG, said in a statement that the innovations will lead to a planning process that is 30% more efficient than before. Eventually, Omniverse will enable BMW to simulate all 31 of its factories. Volvo is designing cars inside Omniverse before committing to physical designs, while Ericsson is simulating future 5G wireless networks. Industrial Light & Magic has been evaluating Omniverse for a broad range of possible workflows, but particularly for bringing together content created across multiple traditional applications and facilitating simultaneous collaboration across teams that are distributed all over the world. Foster + Partners, the United Kingdom architectural design and engineering firm, is implementing Omniverse to enable seamless collaborative design to visualization capabilities to teams spread across 14 countries. Activision Publishing is exploring Omniverse’s AI-search capabilities for its games to allow artists, game developers and designers to search intuitively through massive databases of untagged 3D assets using text or images. WPP, the world’s largest marketing services organization, is using the Omniverse to reinvent the way advertising content is made by replacing traditional on-location production methods with entirely virtual production. Perry Nightingale, senior vice president at WPP, said on a panel that he is seeing collaboration on an enormous scale with multiple companies working together. “I’m excited how far that could go, with governments doing it for city planning and other sorts of grand scale collaboration around USD,” Nightingale said. Nvidia will use Omniverse to enable Drive Sim 2.0, which lets carmakers test their self-driving cars inside Omniverse. It uses USD as Nvidia transitions from game engines to a true simulation engine for Omniverse, said Danny Shapiro, senior director for automobiles at Nvidia. Nvidia’s own developers will now be able to support new hardware technologies earlier than they could in the past. “We initially built it for our own needs, so that when technologies were being developed in different groups that they could share immediately, rather than have to wait for the development of it into their particular area,” Kerris said. “The same holds true with our developers. It used to be if we brought a technology out, we would then work with our developers, and it would take a period of time for them to support it. However, by building this platform that crosses over these, we have the ability now to bring out new technologies that they can take advantage of day one.” One question is how well Omniverse will be able to deal with latency, or interaction delays across the cloud. That would be important for game developers, who have to create games that operate in real time. Scenes built with Omniverse can be rendered at 30, 60, or 120 frames per second as needed for a real-time application like a game. Kerris said in an earlier chat that most of what you’re looking at doesn’t have to be constantly refreshed on everybody’s screen, making the real-time updating of the Omniverse more efficient. Nvidia’s Nucleus tech is a kind of traffic cop that communicates what is changing in a scene as multiple parties work on it at once. As for viewing the Omniverse, gamers could access it using a high-end PC with a single Nvidia RTX graphics card. Huang said in his speech, “The metaverse is coming. Future worlds will be photorealistic, obey the laws of physics or not, and inhabited by human avatars and AI beings.” He said that games like Fortnite or Minecraft or Roblox are like the early versions of the metaverse. But he said the metaverse is not only a place to play games. It’s a place to simulate the future. “We are building cities because we need to simulate these virtual worlds for our autonomous vehicles,” Kerris said. “We need a world in which we can train them and test them. Our goal is to scale it so so you could drive continuously drive a virtual car continuously from Los Angeles to New York, in real time, using the actual hardware that’s going to be inside the car and give it a virtual reality experience plugged into its sensory inputs, the output of our simulator, and fool it into thinking it’s in the real world. And for that, it has to be an extremely large world. We’re not quite there yet. But that is what we are moving towards.” For game companies, I can foresee game publishers eventually trading around their cities, as one might build a replica of Paris while another might build New York. After all, if everyone works with USD technology, there might not be a need to rebuild every city from scratch for simulations like games. Ivar Dahlberg, technical artist at Embark Studios, a game studio in Stockholm, said it is tantalizing to think about trading cities back and forth between game developers who are working on city-level games. “Traditionally, developers have focused on a world for someone else to experience,” he said. “But now it seems there are lots more opportunities for developers to create something together with the inhabitants of that world. You can share the tools with everybody who is playing. That ties in quite nicely to the idea of a metaverse. USD is definitely a step in that direction.” Tsigkari said, “That is an experience that may not be very far out. It won’t matter if one company builds Paris, London, or New York. It will be more about what you are doing with those assets. What is the experience that you offer to the user with those assets?” As I saw recently in the film A Glitch in the Matrix, it will be easier to believe in the future that we’re all living in a simulation. I expect that Nvidia will be able to fake a moon landing for us next."
https://venturebeat.com/2021/04/12/nlpcloud-io-helps-app-developers-add-language-processing/,NLP Cloud helps app developers add language processing,"NLP tools and services are taking off, but developers often struggle with the hurdle of getting NLP models into production. NLP Cloud is a new AI startup focused on lowering the barriers for developers trying to create apps for sorting support tickets, extracting leads, analyzing social networks, and developing tools for economic intelligence. NLP has been around for decades, but interest has seen a dramatic uptick with the recent introduction of transformers, a new type of neural network. Google researchers demonstrated in 2017 how transformers dramatically improved the speed, performance, and precision of NLP tools. Transformers made possible the much larger models Google’s BERT and OpenAI’s GPT-3. The capabilities are available through innovative open source libraries Hugging Face and spaCy. Developing accurate models and pushing models into production are two different processes. NLP Cloud intends to close this gap by reducing the barriers to production — providing NLP capabilities via an API, rather than a raw AI model that must be pushed into production. Developers only need to worry about integrating the API into their application. “Today, the main challenge remaining in NLP projects is clearly the production side,” NLP Cloud CTO and founder Julien Salinas told VentureBeat in an email. New NLP models make it easier for more types of developers to experiment with weaving language capabilities into their projects. Possible use cases include scanning web pages and other unstructured text and extracting name entities as part of lead generation before conducting sentiment analysis on support tickets and sorting them based on urgency. Content marketers can use the platform to summarize text and generate headlines. Properly deploying and running AI models in production requires strong DevOps, programming, and AI capabilities. Few developers have mastered all three disciplines, especially within smaller companies. The team may have data science knowledge but not the DevOps capabilities, or software engineers who need to deploy NLP without hiring a data science team. The company is focusing on making the best available open source models easier to deploy rather than developing its own models. This allows it to focus on improving the developer experience rather than tweaking the underlying models. Salinas said the company selected Hugging Face and spaCy for their respective strengths. Hugging Face’s transformers are more advanced and accurate than spaCy, Salinas said. Hugging Face is also building a huge open source repository for NLP models, which makes selecting the best model for a given use case more convenient. SpaCy is faster and less resource-intensive than other NLP libraries. The library has been around longer and recently added the capability to natively support transformer-based models. In the future, Salinas plans to add conversational models for chatbots, new summarization models that can handle bigger pieces of text, and text generation models. He also hopes to eventually support more languages but believes non-English models still need more work. Since its launch three months ago, NLP Cloud has been growing rapidly. It currently has around 500 users, 30 of them paid users. While most of the users are startups the company has begun to see some larger customers."
https://venturebeat.com/2021/04/12/consensys-project-virtue-poker-completes-strategic-investment-round-of-5m/,Consensys Project Virtue Poker Completes Strategic Investment Round of $5m,"TA’ XBIEX, Malta–(BUSINESS WIRE)–April 12, 2021– Virtue Poker, a multi-chain, Ethereum-based decentralized poker platform has now completed a strategic investment round of $5 million. Virtue Poker was part of Coinlist’s Seed Winter 2021 Batch and is funded by notable investors, including Pantera Capital, Consensys, DFG Group, Jez San from FunFair. Poker Hall of Famer Phil Ivey is a stakeholder as well as a public spokesperson for the company. “I’ve been working with the Virtue Poker team for nearly 3 years, watching them build a next generation poker platform” said Phil Ivey. “Using a blockchain based system creates a more secure and globally accessible payment system. I’m excited to continue my partnership with the Virtue Poker team and work to bring the platform to poker communities worldwide”. Virtue Poker is the first and only blockchain-based company to be issued a license by the Malta Gaming Authority. The Virtue Poker team and the Malta Gaming Authority worked together over nearly two years in establishing a regulatory framework suitable for blockchain based gambling applications. The company can now legally operate and compete in most global markets worldwide to deliver the promise of fair playing and transparency to mainstream adoption. “After years of consultation, in person meetings, and effort – Virtue Poker can proudly say we are the only licensed blockchain based poker application in the market” said CEO Ryan Gittleson. “Blockchain technology provides modern and secure payment infrastructure that provides global accessibility to consumers, unlike our competitors. By working with regulators to become a licensed online gambling company, Virtue Poker now has legitimacy to crossover and compete for customers from legacy providers to bring blockchain based wagering mainstream.” The funds raised will be used to bootstrap Virtue Poker’s mainnet launch, which is scheduled for May 2021. Virtue Poker will be hosting an exhibition tournament with players like Phil Ivey, Joe Lubin and others as the official launch. One of the oldest projects in the blockchain space and one of the first to have been incubated by Consensys, Virtue Poker was founded shortly after the Ethereum network debuted. Consensys founder Joe Lubin identified online gambling as an industry that was ripe for blockchain disruption, prompting him to take Virtue Poker into his Ethereum-centered family of projects. “I’m excited to see the Virtue Poker team realize its mission in bringing transparency and trust to the online poker industry” said Joe Lubin, Founder of Consensys. “By working with regulators in becoming the first licensed blockchain based platform, Virtue Poker legitimizes the use of this technology in the industry long term going forward.” Through a combination of P2P networking and the verifiable nature of blockchain, Virtue Poker brings an unprecedented degree of trust and transparency to the online poker industry, which is thought to be plagued by shady and dishonest algorithms and costly third-party payment processor middlemen, especially for withdrawals. Virtue Poker uses both Ethereum smart contracts and sidechain infrastructure to provide next-generation security and transparency for players. About Virtue Poker Virtue Poker is a decentralized poker platform that uses the Ethereum blockchain and peer-to-peer networking to provide an online poker site that’s safe, honest and fun. It was founded in 2016 within Consensys, the leading full stack Ethereum software engineering company and incubator founded by Ethereum co-founder Joe Lubin in 2014. Backed by Consensys and stakeholder Phil Ivey, Virtue Poker is on a mission to make blockchain-based betting mainstream. Learn more: https://virtue.poker/  View source version on businesswire.com: https://www.businesswire.com/news/home/20210412005615/en/ Ryan Gittlesonryan@virtue.poker"
https://venturebeat.com/2021/04/12/intel-advances-in-silicon-photonics-can-break-the-i-o-power-wall-with-less-energy-higher-throughput/,"Intel: Advances in silicon photonics can break the I/O “power wall” with less energy, higher throughput","This article is part of the Technology Insight series, made possible with funding from Intel. As we create more content, deploy more sensors at the network’s edge, and replicate more data for AI to contextualize, the demand for compute bandwidth roughly doubles every three years. Keeping up is becoming increasingly difficult as modern computing architectures get closer and closer to the theoretical performance limits of electrical connections linking their processors, storage, and networking components. Silicon photonics technology—a combination of silicon integrated circuits and semiconductor lasers—may help  overcome the bottlenecks imposed by electrical I/O, replacing copper connections with optical ones at the board and package level. According to James Jaussi, senior principal engineer and director of Intel’s PHY research lab, miniaturized silicon photonics components open the door to architectures that are more disaggregated. That could look like pools of compute, memory, and peripheral functionality distributed throughout the system; connected over long distances with optical links, software-defined infrastructure, and high-speed networking. For now, integrated photonics is still the stuff of lab experiments. But a number of breakthroughs introduced during Intel’s recent Labs Day show that the technology is capable of lower power, higher performance, and greater reach than today’s server interconnects. KEY POINTS: Today, silicon photonics technology is used in datacenters for connecting switches that might be miles apart. On one end, transceivers (devices able to transmit and receive) convert electrical signals to light, which is then sent across optical fiber. At the other end, those optical signals are changed back into electrical. What makes the conversion from electrical to optical a worthwhile endeavor? In short, higher bandwidth, coverage over greater distances, and an immunity to electromagnetic interference. But traditional optical transceivers are expensive. Their transmitter and receiver sub-assemblies must be carefully constructed and hermetically sealed for protection, which makes it difficult for manufacturers to keep up with demand. And the myriad of components that go into a transceiver take up significant space. Silicon photonics packs many of the optical and electronic pieces used to build a transceiver into highly integrated chips. These chips are manufactured in advanced fabs by the same machines that produce the latest CPUs, GPUs, and FPGAs. They enjoy the benefits of cutting-edge lithography, automation, and economies of scale, making them much smaller and less expensive than the technology they replace. Intel introduced its own family of 100 Gb/s transceivers based on semiconductor lasers back in 2016, ten years after demonstrating the technology alongside researchers from UC Santa Barbara. It quickly scored wins with performance-sensitive customers like Microsoft’s Azure cloud computing service. Since then, it has shipped more than four million 100 Gb/s modules, according to Labs Day presentations. Intel has its sights set on scaling optical I/O volumes several orders of magnitude higher though—into the billions of devices. That would take optical beyond rack-to-rack communications in the datacenter and down to the board level, right onto the compute engines where electrical I/O currently dominates. Intel calls this research integrated photonics. If electrical I/O works so well between the server boards and processing packages, why look to silicon photonics as a replacement? Unfortunately, electrical interconnects are struggling to keep those resources fed, and every bit of speed-up comes at the cost of disproportionately more power consumption. There’s a wall in sight, and that’s making optical I/O an appealing alternative. Although silicon photonics transceivers offer notable advantages over traditional optical designs, their components are still too large, too expensive, and too power-hungry to displace electrical I/O within servers. The breakthroughs announced at Labs Day 2020 change this. Jaussi says there are six ingredients in the company’s recipe for integrated photonics: light generation, amplification, detection, modulation, CMOS interface circuits, and package integration. Intel already has a hybrid silicon laser in its portfolio, which is used on its silicon photonics transceivers for converting electrical signals into light. So, it’s focusing on the other five building blocks. In a basic transmitter, the laser creates light onto which data is encoded by a modulator. Existing silicon modulators are large, and therefore expensive in the context of integrated photonics. New micro-ring modulators announced during Labs Day shrink this component’s footprint by more than 1000x. Voltage supplied by a circuit above the modulator either traps light in the ring or allows it to travel down its waveguide. A detector at the other end interprets the absence or presence of light as zeroes and ones. The photodiodes in existing silicon photonics optical transceivers rely on materials like Germanium or Indium Phosphide to “see” light in the wavelengths used to move data. Silicon, it was thought, had no light detection capability in that range. Intel showed otherwise by using its all-silicon micro-ring structure as a photodetector operating at 112 Gb/s. “A major advantage of this development is processing and material cost reduction,” says Jaussi. Intel multiplies the bandwidth through each fiber by capturing multiple wavelengths (or colors) of light from one laser. This technology is called wavelength division multiplexing. In his Labs Day demo, Jaussi showed four micro-rings trapping four separate wavelengths from a single optical channel to convey four bits of data. In the early days of silicon photonics research, this would have taken four different lasers, plus a multiplexer. Doing it with one is key to moving data fast enough in  a space-constrained application like on-package I/O, where there isn’t room for lots of laser firing next to each other. The addition of a semiconductor optical amplifier helps optimize integrated photonics systems for power consumption, since an amplifier provides light power more efficiently than the laser. These amplifiers are made from the same materials as the multi-wavelength laser—an important consideration for manufacturing at volume. As part of Intel’s Labs Day demonstration, Haisheng Rong, principal engineer at Intel Labs, showed off a photonic IC with the hybrid silicon laser, micro-ring modulators, an optical amplifier, and micro-ring photodetectors integrated together and manufactured in a high-volume CMOS fab. He was joined by fellow principal engineer Ganesh Balamurugan, who described the electrical IC responsible for driving and controlling Intel’s micro-ring modulators. The two ICs are stacked, one on top of the other, and connected with copper pillars. “This is an example of how we can tightly integrate energy-efficient CMOS circuits with silicon photonics using 3D packaging,” says Balamurugan. “Such cointegration is key to delivering performance and cost-optimized optical transceivers.” By integrating silicon photonics building blocks with compute resources, Intel believes it can break the current trend of larger processors with more I/O pins, which are needed to satisfy growing bandwidth requirements. Silicon photonics makes it possible to achieve lower power consumption, greater throughput between compute elements, and reduced pin counts, all in a smaller footprint. The company is already showing off high-performance Ethernet switch silicon co-packaged with silicon photonics engines, designed to address the power and cost/complexity issues posed by electrical I/O scaling limitations within two switch generations. It’ll be longer before we see integrated photonics inside of servers—Intel acknowledges that the technology isn’t on the product implementation path yet. However, over time, the company hopes to scale its silicon photonics platform up to 1 Tb/s per fiber at 1pJ of energy consumed per bit, reaching distances of up to 1 km. With electrical I/O facing an impending power wall and silicon photonics already a successful component of Intel’s networking catalog, this is a technology you’ll want to keep an eye on. "
https://venturebeat.com/2021/04/12/safeguard-nabs-45m-to-combat-cybersecurity-risks-using-ai/,SafeGuard Cyber nabs $45M to combat cybersecurity risks using AI,"SafeGuard Cyber, a cloud platform designed to protect assets from cybersecurity threats and risk factors, today announced it has raised $45 million in a mix of equity and debt. This brings SafeGuard Cyber’s total raised to over $69 million and will be used to expand its business and technology capabilities, the company says. In a 2017 Deloitte survey, only 42% of respondents considered their institutions to be extremely or very effective at managing cybersecurity risk. The pandemic has certainly done nothing to alleviate these concerns. Despite increased IT security investments companies made in 2020 to deal with distributed IT and work-from-home challenges, nearly 80% of senior IT workers and IT security leaders believe their organizations lack sufficient defenses against cyberattacks, according to IDG. SafeGuard Cyber, which was founded in 2014, develops products that identify risks in communication channels such as social media, chat apps, and collaboration platforms — like Slack, LinkedIn, and WhatsApp. SafeGuard Cyber also helps companies take action and claims it can shield high-profile or targeted individuals from account takeovers, spearphishing, malicious content, threats of violence, and misinformation, as well as bad actor connections. Moreover, SafeGuard Cyber says it can protect enterprise and employee accounts from inbound threats, including brand impersonation, while providing visibility into potential threat vectors. “Social engineering has traditionally been carried out within emails but has since evolved to incorporate a more targeted, soft-attack approach across social media, collaboration, and mobile chat channels where traditional security defenses simply do not detect and stop such attacks,” cofounder and CEO Jim Zuffoletti told VentureBeat via email. “Attackers also now realize that mass attacks have a low conversion rate and that targeted spearfishing, while more time-intensive, has a much higher success rate. From a cybercriminal’s point of view, social engineering is the perfect means to deliver a broad array of damaging exploits, such as ransomware that encrypts company data, or attacks that are focused on cyberespionage.” According to a 2017 MicroFocus report, up to 463 billion gigabytes of data will be generated every day by 2026. To accommodate the coming influx, organizations are beginning to transform operations models through the adoption of AI to optimize, enhance, and automate threat and risk visibility, detection, and response capabilities, Zuffoletti says. To this end, SafeGuard Cyber leverages an AI-powered engine called Threat Cortex that detects and spotlights risks across different attack surfaces. Threat Cortex searches the dark and deep web to surface attackers and risk events, automatically notifying IT team members when an anomaly crops up. Using SafeGuard Cyber, admins can quarantine unauthorized data from leaving an organization or specific account. It allows them to lock down and revert compromised accounts back to an earlier, uncompromised state. “ThreatCortex allows for transparency in [machine learning] models​, ​offers data analytics​, and manages features and natural language programming building blocks as configurable rules that are used for scoring by the machine learning,” a SafeGuard Cyber spokesperson explained. “Since we support different types of models in the platform that range from out of the box to highly configurable, it is possible to avoid the need to ‘create’ models for each customer.” On the compliance side of the equation, SafeGuard Cyber offers a tool that taps AI to alert employees, customers, and partners if their digital communications are at risk of violating regulations like the Financial Industry Regulatory Authority and Financial Conduct Authority. The platform flags stakeholders about risk assessments in real time, prioritized by machine learning algorithms. It also captures records autonomously with audit trails and archives content and metadata, including legal holds. The global risk management market was valued at $6.25 billion in 2018 and is projected to reach $18.5 billion by 2026, according to Allied Market Research. SafeGuard Cyber has rivals in RiskLens, RiskIQ, Privacera, and Aclaimant, a data-driven safety and risk management platform for the workplace. There’s also LogicGate, which raised $24.75 million in December 2019 to help automate corporate governance, risk, and compliance processes. Despite the competition, Charlottesville, Virginia-based SafeGuard Cyber, which has 78 employees, says its products saw “tremendous” growth in 2020 as enterprises increasingly turned to digital channels during the pandemic. The company counts some of the biggest names in financial services, pharmaceuticals and health care, education, technology, government, sports, media, and entertainment among its customers, Zuffoletti claims. At present, the company manages over 500,000 user and company accounts. For one pharmaceutical client, SafeGuard Cyber’s platform is recording text call notes from customer service interactions. SafeGuard Cyber claims this has enabled the client to gain productivity and analytical insights, satisfying its digital transformation goals. “SafeGuard Cyber achieved 100% supervision coverage of free text call note reporting, powered by language-agonistic machine learning. Initial results indicated that these controls were warranted, as the system flagged over 200 significant company policy violations for investigation within just the first few months of operation,” the spokesperson said. “Today, SafeGuard Cyber is monitoring up to 100,000 call records per day, in multiple languages, with a less than 1% false positive rate. As a result, the company has been able to cut manual review costs by $3 million per month.” NightDragon led SafeGuard Cyber’s strategic growth round announced today, with participation from Cisco Investments and previous investor AllegisCyber."
https://venturebeat.com/2021/04/12/games-and-transmedia-how-we-got-here-and-where-were-going/,Games and transmedia — how we got here and where we’re going,"Presented by Genvid When you create new worlds, particularly worlds which have their own rules and concepts, fans are going to have questions about every aspect of those worlds which they’re not already familiar with. How does a lightsaber work? What the hell is an Ewok, actually? Or, more recently, who is Boba Fett and what do we know about the Mandalorians? Satisfy this curiosity in any format other than the medium of the original and you’re making transmedia. Your existing fans are more engaged and buying more of your products, and you’re adding new fans along the way. Transmedia turns an IP into a brand. Like many successful concepts, transmedia has become so commonplace that we take it for granted. After all, when we look at the way in which a global IP such as Star Wars has used transmedia to add depth, breadth, and reach to its stories, the process seems like such a natural nexus between marketing, merchandising and community engagement that it’s almost impossible to imagine any property achieving that ubiquity without transmedia. Of course, there’s debate on the exact definition — transmedia is a slippery concept and there’s plenty of academic discussion around it. But what I want to cover here is the rough path which transmedia has taken to arrive at one of the most interesting and cutting-edge examples of the concept in use today — what I’ll call ‘concurrent transmedia’: the use of different media formats to tell a story simultaneously, rather than each medium telling stories separately.  This is something we’ve been exploring to great effect in our most recent Massive Interactive Live Event (MILE) Rival Peak: an interactive livestream of a unity-powered CGI “reality show” with a live-action weekly wrap-up show filmed in a TV studio. Outcomes in the interactive stream (such as who is being voted off the show) have huge ramifications for the filmed segments, and we don’t find out what those outcomes are until just a day or so before filming. It’s ambitious and exciting, and it’s working in some very intriguing ways. We think it’s the next step in the evolution of transmedia and storytelling in interactive media. Transmedia licensing has been a standard in the world of digital gaming for decades. Often, these games are simple adaptations of the plot of a film or book, essentially retelling the same story. But more complex examples have seen games extending to more advanced transmedia techniques — telling entirely new “in-universe” stories from the perspectives of characters created specifically for the game itself, adding new themes to the canon of the original. Again, Star Wars, in many ways the ‘gold standard’ of transmedia, has also pushed forward here, with games like The Force Unleashed, Fallen Order, and the KoTOR series exploring different fragments and perspectives of that galaxy far, far away. It makes perfect sense that games, with their uniquely interactive aspect, would thrive best when freed from the chains of rote narrative repetition. Although there’s a clear appeal to playing your favourite hero in a story which you’ve seen played out on the big screen, games shine when they’re allowed to craft experiences which allow agency for the player, rather than aping a story to which we already know the ending. So games can be a good tool in the transmedia playbooks of IP from other sources — but we’re well beyond the days of games always having to follow in the footsteps of other media: some of the world’s biggest transmedia properties started as gaming IP. As gaming has grown to become the world’s most profitable medium, we’ve seen games’ IP blossoming into some truly massive transmedia brands. Pokémon — reckoned to be the highest-grossing media franchise in existence — began as a game before expanding to trading cards, animated series, books, comics, and high-budget Hollywood movies. Whilst Pokémon has made the vast majority ($61B from a total of $92B) of its revenue from straight merchandising, it is the presence of its characters across transmedia which has built it into a brand able to command those merchandising profits.  There are many reasons for Pokémon’s enormous success, and at its core it’s an almost perfect transmedia property. Because of its easily marketed characters, which are almost infinitely re-imaginable, the game’s format lends itself perfectly to the collectible card game and animated series which were the two mainstays of the non-game canon. These two branches of the strategy also neatly demarcated different routes into the fandom — the easy-to-grasp simplicity of the cartoon and the complex, competitive nature of the card game meant that children, and adults, were brought into the IP from opposing ends of the scale of engagement. In addition, both focused their audiences on the Pokémon themselves — which became the products which were the centre of the incredible merchandising empire. Pokémon also expanded beyond these two initial pillars — making guest spot appearances on Nintendo’s cross-franchise showcase Smash Bros. as well as shifting into the quasi-ironic movie market with Detective Pikachu, to say nothing of the massive success of the augmented reality Pokémon Go! mobile game from Niantic. By capturing the minds of a generation of children, and staying with them as they grew into adults, Pokémon has established itself as a global mega-brand which has accrued revenue three times larger than the entirety of the Marvel cinematic universe. As well as being the pre-eminent example of game-led transmedia, one non-official Pokémon transmedia experiment was also a great source of inspiration for our MILE concept: Twitch Plays Pokémon. It was a simple but brilliant idea — a game of Pokémon Red Vs. Blue controlled by the inputs of the viewers of the Twitch channel it was being broadcast on. When it premiered in 2014, TPP became an overnight success, with thousands of participants crowding the channel to deliver often contradictory commands. As chaotic as it was, TPP was an incredible success — over a million people interacted with the stream in the 16 days it took to finish the game. Five years later, Twitch’s head of creator development Marcus Graham hailed it as a pivotal moment in Twitch’s history, acknowledging that it changed the service forever. “TPP not only inspired an entire generation of Pokémon fans, but it directly inspired Twitch. TPP proved that the medium of Twitch was (and still is) ripe for innovation, and that there are new and exciting ways to create interactive content that has never been done before.” There is a direct line from Twitch Plays Pokémon to the development of our concurrent transmedia project Rival Peak. The idea of a stream with a single origin, using only a small amount of computing power at source, but magnified to reach a huge audience through the power of the cloud, was the founding principle behind the technology which enables us to create our own Massive Interactive Live Events. MILEs are only possible with the sort of mass-audience streaming embodied by Twitch, combined with the interactive capabilities of the majority of devices used to view it.  This new transmedia format, combining an existing text (Red Vs. Blue) and an existing distribution technique (Twitch), is not only an excellent example of the evolutionary nature of transmedia, it’s also a brilliant way of bringing together a massive audience in a collectively experienced interactive event with extremely low barriers to entry. Our technology empowers creators to build their own MILEs, with all of the reach and jump-in appeal of TPP, but on any video streaming platform, with any source material, broadcasting to any internet and video capable device. Rival Peak is the biggest example of this concept so far, but how does it come back around to transmedia. The core Rival Peak experience is built in Unity and runs on Facebook Games, broadcast as a stream but able to be interacted with in meaningful ways by participants thanks to Genvid’s unique technology. In addition, we have a weekly “wrap-up show,” filmed in a studio and hosted by Wil Wheaton, which both summarizes the events of the week in-game, and pushes the meta-narrative for the game forward as a whole. On top of that, Rival Peak has an elimination system which is decided by the activity of the participants, so the direction of the wrap-up show and the game itself changes drastically due to factors outside of the control of the developers and writers. The audience is absolutely directing the outcome of both livestream and studio portions of the whole — an interactive, concurrent transmedia project. The results have been incredible. This is a brand new IP, in a totally new format, with an untested transmedia element, subject to potentially massive narrative shifts depending on the actions of participants. In total, we have had 100 million minutes of participants watching and interacting with the livestream. We’ve also had an average of over 10 million viewers for each episode of Rival Speak. These are numbers both game makers and TV studios would be justifiably proud of. Not only has this transmedia approach allowed us to expand our audience beyond those who would be attracted to just one of the consumption methods available, it’s also provided us with the ability to summarize events for new participants, who can go back and watch the existing episodes of Rival Speak — these episodes are also accessible now the season has ended, providing legacy content.  The fact that we were able to film these segments, with a high-profile star and high-production values, in response to audience-decided factors with major ramifications for plotlines, during a pandemic which has all but suspended the production of many filmed experiences, has been incredible. Add in the facts that Rival Peak was built from the ground up in around six months, that we’ve been able to broadcast the whole experience to Facebook’s audience of over 2B people worldwide, translated into eight languages, and Rival Peak looks like a very special project indeed. But as proud as we are, we actually think it’s just the first step in a whole new format of interactive entertainment. MILEs are uniquely positioned to take advantage of this new style of concurrent transmedia project. Because they are infinitely scalable and platform- and engine-agnostic, they can be broadcast on any video stream-capable platform and ported to a new or additional platform with almost no extra work at all — meaning they can live on the same platform as any additional livestreamed or broadcast transmedia video content. This keeps your audience all in one place, allowing for maximum interactivity and retention, and gives you the perfect opportunity to advertise any other transmedia implementations to your existing audience on platform. Plus, this makes them perfect for the ambitious concurrent transmedia projects outlined above. Because they’re available on any device which can handle streamed video, MILEs also have almost no barriers to entry, meaning adoption rates are high — offering an easy in for your brand. Finally, our running costs are low and directly linked to the number of viewers, meaning that it’s very hard to run up unsustainable overheads. Currently, Genvid is the only company in the world to offer the software which makes this possible (our free-to-download SDK is available via our website). We also have a full suite of services and support options available, with a dedicated team of engineers on hand to guide you through every stage of the project from co-development to deployment. Get in touch with us today and find out how we can help you to run your first MILE. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/12/data-access-management-startup-docontrol-emerges-from-stealth-with-13-35m/,Data access management startup DoControl emerges from stealth with $13.35M,"Today marks the launch of DoControl, a platform for automated data access controls designed to improve cloud security and cost efficiency. Emerging from stealth with $13.35 million in funding, including a $10 million series A, DoControl is introducing products that provide monitoring, orchestration, and remediation across software-as-a-service (SaaS) apps like Google Drive and Box. Enterprise SaaS adoption has never been higher. Companies use 16 SaaS apps on average, driving the global industry to an estimated $157 billion. But coinciding with this climb is a decline in app usage transparency. A recent survey of IT leaders conducted by Numerify found that 45% don’t have a complete picture of key apps and business health services, with 57% saying they lacked an overview of IT performance across projects and employees. DoControl provides self-service tools for app monitoring, creating connections through a secure flow that allows DoControl access to the metadata and change logs of each system. The platform automatically creates an inventory of users, external collaborators, assets, third-party domains, and more, enabling visibility and analytics to be used for security investigations, third-party vendor offboarding, compliance evidence, and incident response. DoControl was founded by Adam Gavish, Omri Weinberg, and Liel Ran. Gavis formerly led teams at Google and Amazon and is also an ex-software engineer at data management platform eXelate, which was acquired by Neilsen in 2015 for $200 million, and cloud app analysis startup Skyfence, which Forcepoint bought in 2017. Through no-code workflows, DoControl allows for consistent enforcement across apps, some of which can’t be done natively within each app. Slack and Microsoft Teams chatbots proactively engage with users on behalf of security and IT teams, helping to identify and mitigate outdated or irrelevant information in order to mitigate data breach risk. The pandemic spurred some enterprises to fast-track their adoption of remote work technologies, leveraging SaaS apps to enable the transition. With an uptick in the adoption of SaaS, the focus on managing and securing these apps became increasingly critical. A recent AppOmni study found that two-thirds of IT leaders have less time to oversee SaaS apps, with 68% reporting they rely solely on manual efforts to detect data exposures. New York-based DoControl has 14 employees and competes to a degree with Productiv, which aggregates real-time engagement data and insights for apps and management. Like DoControl, Productiv’s cloud-based dashboard integrates with single sign-on tools to track login activity and extract data from various systems. But in the short time prior to its unveiling, DoControl claims that its over 40 early customers have used the platform to share millions of files, folders, repositories, and other assets with over 10,000 external companies. “In the next five years, digital transformation, remote work, and freelancing will not only accelerate SaaS adoption but also expose more organizations to third-party entities and generate a huge amount of unmanageable data access. As such, SaaS security will become a mandatory, standard checklist in most security programs,” Gavish told VentureBeat via email. “To win this emerging category, SaaS security vendors will have to provide an autonomous solution that not only detects issues but also remediates them automatically on behalf of the customer. This is exactly how DoControl has differentiated itself within this category, by providing fully automated policy enforcement and [a] self-service remediation path for end users that works on behalf of the security team to solve cross-functional and externally facing SaaS data access challenges.” RTP Global led DoControl’s series A, with participation from StageOne Ventures, Cardumen Capital, and CrowdStrike."
https://venturebeat.com/2021/04/12/microsoft-shopping-for-speech-tech-talks-to-buy-nuance-for-16b/,"Microsoft shopping for speech tech, in talks to buy Nuance for $16B [Confirmed]","(Reuters) — Microsoft is in advanced talks to buy artificial intelligence and speech technology company Nuance Communications for about $16 billion, according to a source familiar with the matter. The price being discussed could value Nuance at about $56 a share, the source said, adding that an agreement could be announced as soon as Monday. Bloomberg News, which first reported the potential deal between Nuance and Microsoft, said talks are ongoing and the sale could still fall apart. Burlington, Massachusetts-based Nuance, whose voice recognition technology helped launch Apple assistant Siri, makes software for sectors ranging from health care to the automotive industry. A deal with Nuance would be Microsoft’s second-biggest after its $26.2 billion acquisition of LinkedIn in 2016. Microsoft and Nuance did not immediately respond to Reuters’ request for comment. Update 5:38 a.m. PT: Microsoft confirmed it will buy Nuance Communications in a $19.7 billion deal as the tech giant looks to bolster its suite of enterprise applications with the artificial intelligence firm’s advanced speech technology. Microsoft’s offer price of $56 per share is at a premium of 22.86% over Nuance’s last close at $45.58."
https://venturebeat.com/2021/04/11/what-is-a-log-management-database/,What is a log management database?,"When Socrates reportedly said the “unexamined life is not worth living,” the Greek philosopher didn’t imagine the modern internet with its seemingly unlimited ability to absorb data. Every mouse click, page view, and event seems destined to end up in a log file somewhere. The sheer volume makes juggling all of this information a challenge, which is where a log management database really shines. Collecting information is one thing; analyzing it is much harder. But many business models depend on finding patterns and making sense of the clickstream to gain an edge and justify their margins. The log database must gather the data and compute important statistics. Modern systems are usually tightly coupled with presentation software that distills the data into a visual infographic. Log management databases are special cases of time-series databases. The information arrives in a steady stream of ordered events, and the log files record them. While many web applications are generally focused on web events, like page views or mouse clicks, there’s no reason the databases need to be limited to just this domain. Any sequence of events can be analyzed, such as events from assembly lines, industrial plants, and manufacturing. For instance, a set of log files may track an assembly line, tracking an item as it reaches various stages in the pipeline. The result may be as simple as noting when a stage finished, or it could include extra data about the customization that happened at that stage, like the paint color or the size. If the line is running smoothly, many of the events will be routine and forgettable. But if something goes wrong, the logs can help diagnose which stage was failing. If products need to be thrown away or examined for fault, the logs can narrow that work. Specialized log processing tools began appearing decades ago, and many were focused on simply creating reports that aggregate data to offer a statistical overview. They counted events per day, week, or month and then generated statistics about averages, maxima, and minima. The newer tools offer the ability to quickly search and report on individual fields, like the IP address or account name. They can pinpoint particular words or phrases in fields and search for numerical values. Log data is often said to be “high cardinality,” which means the fields can hold many different values. Indeed, the value in any timestamp is constantly changing. Log databases use algorithms to build indices for locating particular values and optimize these indices for a wide variety of values. Good log databases can manage archives to keep some data while eliminating other data. They can also enforce a retention policy designed by the compliance offices to answer all legal questions and then destroy data to save money when it’s no longer needed. Some log analysis systems may retain statistical summaries or aggregated metrics for older data. The traditional database companies have generally not been focused on delivering a tool for log storage because traditional relational databases have not been a good match for the kind of high cardinality data that’s written much more often than it’s searched. The cost of creating the index that’s the core offering of a relational database is often not worth it for large collections of logs, as there just are’t enough JOINs in the future. Time-series and log databases tend to avoid using regular relational databases to store raw information, but they can store some of the statistical summaries generated along the way. IBM’s QRadar, for instance, is a product designed to help identify suspicious behavior in the log files. The database inside is focused on searching for statistical anomalies. The User Behavior Analytics (UBA) creates behavior models and watches for departures. Oracle is offering a service called Oracle Cloud Infrastructure Logging Analytics that can absorb log files from multiple cloud sources, index them, and apply some machine learning algorithms. It will find issues ranging from poor performance to security breaches. When the log files are analyzed, the data can also be classified according to compliance rules and stored for the future if necessary. Microsoft’s Monitor will also collect log files and telemetry from throughout the Azure cloud, and the company offers a wide range of analytics. An SQL API is one example of a service tuned to the needs of database administrators watching log files of Microsoft’s SQL Server. Several log databases are built upon Lucene, a popular open source project for building full-text search engines. While it was originally built to search for particular words or phrases in large blocks of text, it can also break up values into different fields, allowing it to work much like a database. Elastic is one company offering a tool that starts multiple versions of Lucene on different engines so it will scale automatically as the load increases. The company bundles it together with two other open source projects, LogStash and Kibana, to create what it calls the “ELK stack.” LogStash ingests the data from raw log files into the Elastic database, while Kibana analyzes the results. Amazon’s log analytics feature is also built upon the open source Elasticsearch, Kibana, and LogStash tools and specializes in deploying and supporting the tools on AWS cloud machines. AWS and Elastic recently parted ways, so differences may appear in future versions. Loggly and  LogDNA are two other tools built on top of Lucene. They integrate with most log file formats and track usage over time to identify performance issues and potential security flaws. Not all companies rely on Lucene, in part because the tool includes many features for full-text searching, which is not as important for log processing, and these features add overhead. Sumo Logic, another performance tracking company, ingests logs with its own version of SQL for querying the database. Splunk built its own database to store log information. Customers who work directly with the applications designed to automate monitoring tasks — like looking for overburdened servers or unusual access patterns that might indicate a breach — generally don’t use the database. Splunk’s database is designed to curate the indexes and slowly archive them as time passes. EraDB offers another database with a different core but the same API as Elastic. It promises faster ingestion and analysis because its engine was purpose-built for high cardinality log files without any of the overhead that might be useful for text searching. Log databases are ideal for endless streams of events filled with different values. But not all data sources are filled with high cardinality fields. Those with frequently repeating values may find some reduction in storage by a more traditional tabular structure that can save space. The log systems built upon text search engines like Lucene may also offer extra features that are not necessary for many applications. In a hypothetical assembly line, for instance, there’s little need to search for arbitrary strings or words. Supporting the ability for arbitrary text search requires more elaborate indexes that take time to compute and disk space to store. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/11/automation-is-expanding-how-worried-should-we-be-about-jobs/,Automation is expanding. How worried should we be about jobs?,"A few days ago I was having network problems with the WiFi in my home office — my connection was very slow and video conferences were freezing. After fussing with the mesh network extenders with no result, I called the cable provider. Normally, this involves having to navigate several automated voice response menus before a call center representative comes on the line to help. I explain the problem and they then run a remote diagnostic and usually suggest a cable modem reset. Often, that solves the problem. But in my latest attempt to get back online, the menus had changed and speaking to a representative was no longer an option. Just, “press 1 to reset the modem.” And just like that, it worked. Something is lost in this process, but something is gained. The human was removed, and the problem was resolved — probably in less time. Explaining the problem to a human had often been a source of frustration, due to language challenges or possibly my poor descriptive abilities. In fact, some of my worst customer service experiences have been with this cable company. But with this change it was hard to miss the advance in automation and I had to acknowledge the role of AI as a key enabler. This story is, in fact,  an example of what is now often called intelligent automation, the combination of artificial intelligence and automation that synthesizes vast amounts of information to automate entire processes or workflows I also had to wonder what happened to the call center representative. Did they go on to one of those positions we hear about that entail higher strategy tasks? Or perhaps they received a layoff notice. I tried not to think of their personal circumstances, about whether they could readily find other work or would face real hardships. Then again, maybe this will free this worker to pursue a more interesting opportunity. This dual nature of automation — the increase in efficiency and productivity along with the potential human impacts — is the stuff of anxious dreams. Because we hear the same two stories — many jobs will disappear, while new professions will emerge to replace them. The anxiety lives in the gap between, wondering and worrying about what this new reality will bring. Even if these new professions do not materialize, not to worry because there will be so much wealth generated by AI and automation that every adult will receive a monthly stipend, much as Alaska residents receive from oil royalties. At least that is the point of view of OpenAI Co-founder and CEO Sam Altman, as expressed in a recent blog, where he writes that we are witnessing a “recursive loop of innovation” that is both accelerating and unstoppable. Altman goes on to argue that the AI revolution will generate enough wealth for everyone to have what they need, spinning off dividends of $13,500 a year. It could be that his view is inspired and filled with a generosity of spirit, or it could be disingenuous. The Universal Basic Income Altman envisions would be great as a bonus but a poor Faustian bargain if in the process many join the ranks of the long-term unemployed. Several other people have pointed to the flaws in his proposal. For example, Matt Prewitt, president of non-profit RadicalxChange, commented: “The [Altman] piece sells a vision of the future that lets our future overlords off way too easy, and would likely create a sort of peasant class encompassing most of society.” The prospect of a permanent underclass brought about by AI and automation is increasingly portrayed in fiction looking out on the next 20 to 50 years. In The Resisters, a novel by Gish Jen, unemployed people are deemed “Surplus,” meaning there is no work for them. Instead, they are issued a Universal Basic Income at levels just above subsistence. In the new novel Klara and the Sun from Nobel prize winning author Kazuo Ishiguro, large swaths of the population have been “substituted” by automation. The novel describes how a growing income disparity between those with jobs and those without leads to a fracturing of society with increasing tribalism and fascist ideology. Burn-In, a novel from P. W. Singer and August Cole, describes growing automation that has taken millions of jobs and left many people fearful that the future is leaving them behind. In their extensively documented novel, referencing technology that already exists or is far along in development, AI has advanced so far that once-safe fields such as law or finance have been taken over by algorithms, leading to political backlash, with large numbers of people becoming radicalized in extreme virtual communities. Taken together, these portrayals of the not-too-distant future are a long way from Altman’s utopia. It remains to be seen where automation will lead us. Perhaps what will determine the true tipping point is how our institutions respond to this new reality as it accelerates and evolves. Altman warns that if in response to these changes “public policy doesn’t adapt accordingly, most people will end up worse off than they are today.” Not everyone is concerned about AI and automation. On the one hand, it is broadly granted that the COVID-19 pandemic accelerated automation and reduced employment, what the World Economic Forum describes as a “double-disruption” scenario for workers leading to growing inequality. On the other hand, some argue there will be changes in the types of available work — and some people will be displaced (like my call center representative) — but overall employment will not be greatly impacted. Afterall, as these arguments often go, this is what has happened in prior technology revolutions. According to Richard Cooper, the Maurits C. Boas Professor of International Economics at Harvard University, “new technology often destroys existing jobs, but it also creates many new possibilities through several different channels.” Cooper says those new opportunities can take decades to emerge, though, which doesn’t sync with the pace of post-COVID job losses. Others argue that dystopian predictions about automation are fraught with exaggerated timelines and that the feared robot apocalypse is still far away. Most likely, the full impact of automation will not likely be seen until some years into the future. That is the conclusion of a PwC study from a couple of years ago that described several waves of automation. During the first wave, they expect relatively low displacement, “perhaps only around 3% by the early 2020s.” This could explain why the debate about the impact still seems more theoretical than pressing, with far more substantial impacts over the next 10 to 15 years. During the first and second waves, women could be at greater risk of automation due to their higher representation in clerical and other administrative functions, but later automation will put more men at risk.​ It’s worth underscoring that PwC did this analysis pre-COVID and so its conclusions don’t account for the rapid uptake of automation over the past year and how this could further accelerate the waves of automation going forward.  Source: PwC estimates by gender based on OECD PIAAC data (median values for 29 countries) Nevertheless, we can already see and feel it, and this is permeating throughout society. It is not only those doing routine work who are at risk, but increasingly those in white collar professions. A recent PwC survey of employees worldwide revealed that “60% are worried that automation is putting many jobs at risk; 48% believe ‘traditional employment won’t be around in the future,’ and 39% think it is likely that their job will be obsolete within five years.” The longer-term impact of AI and automation on work is not really in doubt. Many positions will be disrupted and people replaced, even as other employment opportunities may be created. The net effect is likely to be positive for the economy. This could be good for economists and corporate shareholders. Though whether it is positive for a large percentage of the population or produces a sizable permanent underclass is very much to be determined. Automation will not likely bring about either utopia or dystopia. Instead, it will lead to both, with different groups experiencing these very different realities. Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/04/10/black-women-ai-and-historical-patterns-of-abuse/,"Black women, AI, and overcoming historical patterns of abuse","After a 2019 research paper demonstrated that commercially available facial analysis tools fail to work for women with dark skin, AWS executives went on the attack. Instead of offering up more equitable performance results or allowing the federal government to assess their algorithm like other companies with facial recognition tech have done, AWS executives attempted to discredit study coauthors Joy Buolamwini and Deb Raji in multiple blog posts. More than 70 respected AI researchers rebuked this attack, defended the study, and called on Amazon to stop selling the technology to police, a position the company temporarily adopted last year after the death of George Floyd. But according to the Abuse and Misogynoir Playbook, published earlier this year by a trio of MIT researchers, Amazon’s attempt to smear two Black women AI researchers and discredit their work follows a set of tactics that have been used against Black women for centuries. Moya Bailey coined the term “misogynoir” in 2010 as a portmanteau of “misogyny” and “noir.” Playbook coauthors Katlyn Turner, Danielle Wood, and Catherine D’Ignazio say these tactics were also used to disparage former Ethical AI team co-lead Timnit Gebru after Google fired her in late 2020 and stress that it’s a pattern engineers and data scientists need to recognize. The Abuse and Misogynoir Playbook is part of the State of AI Ethics report from the Montreal AI Ethics Institute and was compiled by MIT professors in response to Google’s treatment of Gebru, a story VentureBeat has covered in depth. The coauthors hope that recognition of the phenomena will prove a first step in ensuring these tactics are no longer used against Black women. Last May, VentureBeat wrote about a fight for the soul of machine learning, highlighting ties between white supremacy and companies like Banjo and Clearview AI, as well as calls for reform from many in the industry, including prominent Black women. MIT assistant professor Danielle Wood, whose work focuses on justice and space research, told VentureBeat it’s important to recognize that the tactics outlined in the Abuse and Misogynoir Playbook can be used in almost any arena. She noted that while some cling to a belief in the impartiality of data-driven results, the AI field is in no way exempt from this problem. “This is a process, a series of related things, and the process has to be described step by step or else people won’t get the point,” Wood said. “I can be part of a system that’s actually practicing misogynoir, and I’m a Black woman. Because it’s a habit that is so prolific, it’s something I might participate in without even thinking about it. All of us can.” The playbook outlines the intersectional and unique abuse aimed at Black women in five steps: Step 1: A Black woman scholar makes a contribution that speaks truth to power or upsets the status quo. Step 2: Disbelief in her contribution from people who say the results can’t be true and either think a Black woman couldn’t have done the research or find another way to call her contribution into question. Step 3: Dismissal, discrediting, and gaslighting ensues. AI chief Jeff Dean’s public attempt to discredit Gebru alongside colleagues is a textbook example. Similarly, after current and former Dropbox employees alleged gender discrimination at the company, Dropbox CEO Drew Houston attempted to discredit the report’s findings, according to documents obtained by VentureBeat. Gaslighting is a term taken from the 1944 movie Gaslight, in which a character goes to extreme lengths to make a woman deny her senses, ignore the truth, and feel like she’s going crazy. It’s not uncommon at this stage for people to consider the targeted Black woman’s contribution an attempt to weaponize pity or sympathy. Another instance that sparked gaslighting allegations involved algorithmic bias, Facebook chief AI scientist Yann LeCun, and Gebru. Step 4: Erasure. Over time, counter-narratives, deplatforming, and exclusion are used to prevent that person from carrying out their work as part of attempts to erase their contributions. Step 5: Revisionism seeks to paper over the contributions of Black women and can lead to whitewashed versions of events and slow progress toward justice. There’s been a steady stream of stories about gender and racial bias in AI in recent years, a point highlighted by news headlines this week. The Wall Street Journal reported Friday that researchers found Facebook’s algorithm shows different job ads to men and women and is discriminatory under U.S. law, while Vice reported on research that found facial recognition used by Proctorio remote proctoring software does not work well for people with dark skin over half of the time. This follows VentureBeat’s coverage of racial bias in ExamSoft’s facial recognition-based remote proctoring software, which was used in state bar exams in 2020. Investigations by The Markup this week found advertising bans hidden behind an algorithm for a number of terms on YouTube, including “Black in tech,” “antiracism,” and “Black excellence,” but it’s still possible to advertise to white supremacists on the video platform. Google’s treatment of Gebru illustrates each step of the playbook. Her status quo-disrupting contribution, Turner told VentureBeat, was an AI research paper about the dangers of using large language models that perpetuate racism or stereotypes and carry an environmental impact that may unduly burden marginalized communities. Other perceived disruptions, Turner said, included Gebru building one of the most diverse teams within Google Research and sending a critical email to the Google Brain Women and Allies internal listserv that was leaked to Platformer. Shortly after she was fired, Gebru said she was asked to retract the paper or remove the names of Google employees. That was step two from the Misogynoir Playbook. In academia, Turner said, retraction is taken very seriously. It’s generally reserved for scientific falsehood and can end careers, so asking Gebru to remove her name from a valid piece of research was unreasonable and part of efforts to make Gebru herself seem unreasonable. Evidence of step three, disbelief or discredit, can be found in an email AI chief Jeff Dean sent that calls into question the validity of the paper’s findings. Days later, CEO Sundar Pichai sent a memo to Google employees in which he said the firing of Gebru had prompted the company to explore improvements to its employee de-escalation policy. In an interview with VentureBeat, Gebru characterized that memo as “dehumanizing” and an attempt to fit her into an “angry Black woman” trope. Despite Dean’s critique, a point that seems lost amid allegations of abuse, racism, and corporate efforts to interfere with academic publication is that the team of researchers behind the stochastic parrots research paper in question was exceptionally well-qualified to deliver critical analysis of large language models. A version of the paper VentureBeat obtained lists Google research scientists Ben Hutchinson, Mark Diaz, and Vinodkumar Prabhakaran as coauthors, as well as then-Ethical AI team co-leads Gebru and Margaret Mitchell. While Mitchell is well known for her work in AI ethics, she is most heavily cited for research involving language models. Diaz, Hutchinson, and Prabhakaran have backgrounds in assessing language or NLP for ageism, discrimination against people with disabilities, and racism, respectively. Linguist Emily Bender, a lead coauthor of the paper alongside Gebru, received an award from organizers of a major NLP conference in mid-2020 for work critical of large language models, which VentureBeat also reported. Gebru is coauthor of the Gender Shades research paper that found commercially available facial analysis models perform particularly poorly for women with dark skin. That project, spearheaded by Buolamwini in 2018 and continued with Raji in a subsequent paper published in early 2019, has helped shape legislative policy in the U.S and is also a central part of Coded Bias, a documentary now streaming on Netflix. And Gebru has been a major supporter of AI documentation standards like datasheets for datasets and model cards, an approach Google has adopted. Finally, Turner said, steps four and five of the playbook, erasure and revisionism, can be seen in the departmental reorganization and diversity policy changes Google made in February. As a result of those changes, Google VP Marian Croak was appointed to head up 10 of the Google teams that consider how technology impacts people. She reports directly to AI chief Jeff Dean. On Tuesday, Google research manager Samy Bengio resigned from his role at the company, according to news first reported by Bloomberg. Prior to the restructuring, Bengio was the direct report manager for the Ethical AI team. VentureBeat obtained a copy of a letter Ethical AI team members sent to Google leadership in the weeks following Gebru’s dismissal that specifically requested Bengio remain the direct report for the team and that the company not implement any reorganization. A person familiar with ethics and policy matters at Google told VentureBeat that reorganization had been discussed previously, but this source described an environment of fear after Gebru’s dismissal that prevented people from speaking out. Before being named to her new position, Croak appeared alongside the AI chief in a meeting with Black Google employees in the days following Gebru’s dismissal. Google declined to make Croak available for comment, but the company released a video in which she called for more “diplomatic” conversations about definitions of fairness or safety. Turner pointed out that the reorganization fits neatly into the playbook. “I think that revisionism and erasure is important. It serves a function of allowing both people and the news cycle to believe that the narrative arc has happened, like there was some bad thing that was taken care of — ‘Don’t worry about this anymore.’ [It’s] like, ‘Here’s this new thing,’ and that’s really effective,” Turner said. The playbook’s coauthors said it was constructed following conversations with Gebru. Earlier in the year, Gebru spoke at MIT at Turner and Wood’s invitation as part of an antiracism tech design research seminar series. When the news broke that Gebru had been fired, D’Ignazio described feelings of anger, shock, and outrage. Wood said she experienced a sense of grieving and loss. She also felt frustrated by the fact that Gebru was targeted despite having attempted to address harm through channels that are considered legitimate. “It’s a really discouraging feeling of being stuck,” Wood said. “If you follow the rules, you’re supposed to see the outcome, so I think part of the reality here is just thinking, ‘Well, if Black women try to follow all the rules and the result is we’re still not able to communicate our urgent concerns, what other options do we have?'” Wood said she and Turner found connections between historical figures and Gebru in their work in the Space Enabled Lab at MIT examining complex sociotechnical systems through the lens of critical race studies and queer Black feminist groups like the Combahee River Collective. In addition to instances of misogynoir and abuse at Amazon and Google, coauthors say the playbook represents a historical pattern that has been used to exclude Black women authors and scholars dating back to the 1700s. These include Phillis Wheatley, the first published African American poet, journalist Ida B. Wells, and author Zora Neale Hurston. Generally, the coauthors found that the playbook tactics visit great acts of violence on Black women that can be distinguished from the harms encountered by other groups that challenge the status quo. The coauthors said women outside of tech who have been targeted by the same playbook include New York Times journalist and 1619 Project creator Nikole Hannah-Jones and politicians like Stacey Abrams and Rep. Ayanna Pressley (D-MA). The researchers also said they took a historical view to demonstrate that the ideas behind the Abuse and Misogynoir Playbook are centuries old. Failure to confront forces of racism and sexism at work, Turner said, can lead to the same problems in new and different tech scenarios. She went on to say that it’s important to understand that historical forces of oppression, categorization, and hierarchy are still with us and warned that “we will never actually get to an ethical AI if we don’t understand that.” The AI field claims to excel at pattern recognition, so the industry should be able to identify tactics from the playbook, D’Ignazio said. “I feel like that’s one of the most enormous ignorances, the places where technical fields do not go, and yet history is what would inform all of our ethical decisions today,” she said. “History helps us see structural, macro patterns in the world. In that sense, I see it as deeply related to computation and data science because it helps us scale up our vision and see how things today, like Dr. Gebru’s case, are connected to these patterns and cycles that we still haven’t been able to break out of today.” The coauthors recognize that power plays a major role in determining what kind of behavior is considered ethical. This corresponds to the idea of privilege hazard, a term coined in the book Data Feminism, which D’Ignazio coauthored last year, to articulate people in privileged positions failing to fully comprehend the experience of those with less power. A long-term view seems to run counter to the traditional Silicon Valley dogma surrounding scale and growth, a point emphasized by Google Ethical AI team research scientist and sociologist Dr. Alex Hanna weeks before Gebru was fired. A paper Hanna coauthored with independent researcher Tina Park in October 2020 called scale thinking incompatible with addressing social inequality. The Abuse and Misogynoir Playbook is the latest AI work to turn to history for inspiration. Your Computer Is On Fire, a collection of essays from MIT Press, and Kate Crawford’s Atlas of AI, released in March and April, respectively, examine the toll datacenter infrastructure and AI take on the environment and civil rights and reinforce colonial habits about the extraction of value from people and natural resources. Both books also investigate patterns and trends found in the history of computing. Race After Technology author Ruha Benjamin, who coined the term “new Jim Code,” argues that an understanding of historical and social context is also necessary to safeguard engineers from being party to human rights abuses, like the IBM workers who assisted Nazis during World War II. The coauthors end by calling for the creation of a new playbook and pose a challenge to the makers of artificial intelligence. “We call on the AI ethics community to take responsibility for rooting out white supremacy and sexism in our community, as well as to eradicate their downstream effects in data products. Without this baseline in place, all other calls for AI ethics ring hollow and smack of DEI-tokenism. This work begins by recognizing and interrupting the tactics outlined in the playbook — along with the institutional apparatus — that works to disbelieve, dismiss, gaslight, discredit, silence, and erase the leadership of Black women.” The second half of a panel discussion about the playbook in late March focused on hope and ways to build something better, because, as the coauthors say, it’s not enough to host events with the term “diversity” or “equity” in them. Once abusive patterns are recognized, old processes that led to mistreatment on the basis of gender or race must be replaced with new, liberatory practices. The coauthors note that making technology with liberation in mind is part of the work D’Ignazio does as director of the Data + Feminism Lab at MIT, and what Turner and Wood do with the Space Enabled research group at MIT Media Lab. That group looks for ways to design complex systems that support justice and the United Nations Sustainable Development Goals. “Our assumption is we have to show prototypes of liberatory ways of working so that people can understand those are real and then try to adopt those in place of the current processes that are in place,” Wood said. “We hope that our research labs are actually mini prototypes of the future in which we try to behave in a way that’s anticolonial and feminist and queer and colored and has lots of views from people from different backgrounds.” D’Ignazio said change in tech — and specifically for the hyped, well-funded, and trendy field of AI — will require people considering a number of factors, including who they take money from and choose to work with. AI ethics researcher Luke Stark turned down $60,000 in funding from Google last month, and Rediet Abebe, who cofounded Black in AI with Gebru, has also pledged to reject funding from Google. In other work at the intersection of AI and gender, the Alan Turing Institute’s Women in Data Science and AI project released a report last month that documents problems women in AI face in the United Kingdom. The report finds that women only hold about 1 in 5 jobs in data science and AI in the U.K. and calls for government officials to better track and verify the growth of women in those fields. “Our research findings reveal extensive disparities in skills, status, pay, seniority, industry, job attrition, and education background, which call for effective policy responses if society is to reap the benefits of technological advances,” the report reads. Members of Congress interested in algorithmic regulation are considering more stringent employee demographic data collection, among other legislative initiatives. Google and Facebook do not currently share diversity data specific to employees working within artificial intelligence. The Abuse and Misogynoir Playbook is also the latest AI research from people of African descent to advocate taking a historical perspective and adopting anticolonial and antiracist practices. In an open letter shortly after the death of George Floyd last year, a group of more than 150 Black machine learning and computing professionals outlined a set of actions to bring an end to the systemic racism that has led Black people to leave jobs in the computing field. A few weeks later, researchers from Google’s DeepMind called for reform of the AI industry based on anticolonial practices. More recently, a team of African AI researchers and data scientists have recommended implementing anticolonial data sharing practices as the datacenter industry in Africa continues growing at a rapid pace."
https://venturebeat.com/2021/04/09/microsoft-open-sources-tool-to-use-ai-in-simulated-attacks/,Microsoft open-sources tool to use AI in simulated attacks,"As part of Microsoft’s research into ways to use machine learning and AI to improve security defenses, the company has released an open source attack toolkit to let researchers create simulated network environments and see how they fare against attacks. Microsoft 365 Defender Research released CyberBattleSim, which creates a network simulation and models how threat actors can move laterally through the network looking for weak points. When building the attack simulation, enterprise defenders and researchers create various nodes on the network and indicate which services are running, which vulnerabilities are present, and what type of security controls are in place. Automated agents, representing threat actors, are deployed in the attack simulation to randomly execute actions as they try to take over the nodes. “The simulated attacker’s goal is to take ownership of some portion of the network by exploiting these planted vulnerabilities. While the simulated attacker moves through the network, a defender agent watches the network activity to detect the presence of the attacker and contain the attack,” the Microsoft 365 Defender Research Team wrote in a post discussing the project. Microsoft has been exploring how machine learning algorithms such as reinforcement learning can be used to improve information security. Reinforcement learning is a type of machine learning in which autonomous agents learn how to make decisions based on what happens while interacting with the environment. The agent’s goal is to optimize the reward, and agents gradually make better decisions (to get a bigger reward) through repeated attempts. The most common example is playing a video game. The agent (player) gets better at playing the game after repeated tries by remembering the actions that worked in previous rounds. In a security scenario, there are two types of autonomous agents: the attackers trying to steal information out of the network and defenders trying to block the attack or mitigate its effects. The agents’ actions are the commands that attackers can execute on the computers and the steps defenders can perform in the network. Using the language of reinforcement learning, the attacking agent’s goal is to maximize the reward of a successful attack by discovering and taking over more systems on the network and finding more things to steal. The agent has to execute a series of actions to gradually explore the networks but do so without setting off any of the security defenses that may be in place. Much like the human mind, AI learns better by playing games, so Microsoft turned CyberBattleSim into a game. Capture the flag competitions and phishing simulations help strengthen security by creating scenarios in which defenders can learn from attacker methods. By using reinforcement learning to get the reward of “winning” a game, the CyberBattleSim agents can make better decisions on how they interact with the simulated network. The CyberBattleSim focuses on threat modeling how an attacker can move laterally through the network after the initial breach. In the attack simulation, each node represents a machine with an operating system, software applications, specific properties (security controls), and set of vulnerabilities. The toolkit uses the Open AI Gym interface to train automated agents using reinforcement learning algorithms. The open source Python source code is available on GitHub. Erratic behavior should quickly trigger alarms, and security tools would respond and evict the malicious actor. But if the actor has learned how to compromise systems more quickly by shortening the number of steps it needs to succeed, that gives defenders insight into the places that need security controls and helps with detecting the activity sooner. The CyberBattleSim is part of Microsoft’s broader research into using machine learning and AI to automate many of the tasks security defenders are currently handling manually. In a recent Microsoft study, almost three-quarters of organizations said their IT teams spent too much time on tasks that should be automated. Autonomous systems and reinforcement learning “can be harnessed to build resilient real-world threat detection technologies and robust cyber-defense strategies,” Microsoft wrote. “With CyberBattleSim, we are just scratching the surface of what we believe is a huge potential for applying reinforcement learning to security,” the company added."
https://venturebeat.com/2021/04/09/ai-weekly-continual-learning-offers-a-path-toward-more-humanlike-ai/,AI Weekly: Continual learning offers a path toward more humanlike AI,"State-of-the-art AI systems are remarkably capable, but they suffer from a key limitation: statisticity. Algorithms are trained once on a dataset and rarely again, making them incapable of learning new information without retraining. This is as opposed to the human brain, which learns constantly, using knowledge gained over time and building on it as it encounters new information. While there’s been progress toward bridging the gap, solving the problem of “continual learning” remains a grand challenge in AI. This challenge motivated a team of AI and neuroscience researchers to found ContinualAI, a nonprofit organization and open community of continual and lifelong learning enthusiasts. ContinualAI recently announced Avalanche, a library of tools compiled over the course of a year from over 40 contributors to make continual learning research easier and more reproducible. The group also hosts conference-style presentations, sponsors workshops and AI competitions, and maintains a repository of tutorial, code, and guides. As Vincenzo Lomonaco, cofounding president and assistant professor at the University of Pisa, explains, ContinualAI is one of the largest organizations on a topic its members consider fundamental for the future of AI. “Even before the COVID-19 pandemic began, ContinualAI was funded with the idea of pushing the boundaries of science through distributed, open collaboration,” he told VentureBeat via email. “We provide a comprehensive platform to produce, discuss and share original research in AI. And we do this completely for free, for anyone.” Even highly sophisticated deep learning algorithms can experience catastrophic learning or catastrophic interference, a phenomenon where deep networks fail to recall what they’ve learned from a training dataset. The result is that the networks have to be constantly reminded of the knowledge they’ve gained or risk becoming “stuck” with their most recent “memories.” OpenAI research scientist Jeff Clune, who helped to cofound Uber AI Labs in 2017, has called catastrophic forgetting the “Achilles’ heel” of machine learning and believes that solving it is the fastest path to artificial general intelligence (AGI). Last February, Clune coauthored a paper detailing ANML, an algorithm that managed to learn 600 sequential tasks with minimal catastrophic forgetting by “meta-learning” solutions to problems instead of manually engineering solutions. Separately, Alphabet’s DeepMind has published research suggesting that catastrophic forgetting isn’t an insurmountable challenge for neural networks. And Facebook is advancing a number of techniques and benchmarks for continual learning, including a model that it claims is effective in preventing the forgetting of task-specific skills. But while the past several years have seen a resurgence of research into the issue, catastrophic forgetting largely remains unsolved, according to Keiland Cooper, a cofounding member of ContinualAI and a neuroscience research associate at the University of California, Irvine. “The potential of continual learning exceeds catastrophic forgetting and begins to touch on more interesting questions of implementing other cognitive learning properties in AI,” Cooper told VentureBeat. “Transfer learning is one example, where when humans or animals learn something previously, sometimes this learning can be applied to a new context or aid learning in other domains … Even more alluring is that continual learning is an attempt to push AI from narrow, savant-like systems to broader, more general ones.” Even if continual learning doesn’t yield the sort of AGI depicted science fiction, Cooper notes that there are immediate advantages to it across a range of domains. Cutting-edge models are being trained on increasingly larger datasets in search of better performance, but this training comes at a cost — whether waiting weeks for training to finish or the impact of the electricity usage on the environment. “Say you run a certain AI organization that built a natural language model that was trained over weeks on 45 terabytes of data for a few million dollars,” Cooper explained. “If you want to teach that model something new, well, you’d very likely have to start from scratch or risk overwriting what it had already learned, unless you added continual learning additions to the model. Moreover, at some point, the cost to store that data will be exceedingly high for an organization, or even impossible. Beyond this, there are many cases where you can only see the data once and so retraining isn’t even an option.” While the blueprint for a continual learning AI system remains elusive, ContinualAI aims to connect researchers and stakeholders interested in the area and support and provide a platform for projects and research. It’s grown to over 1,000 members in the three years since its founding. “For me personally, while there has been a renewed interest in continual learning in AI research, the neuroscience of how humans and animals can accomplish these feats is still largely unknown,” Cooper said. “I’d love to see more of an interaction with AI researchers, cognitive scientists, and neuroscientists to communicate and build upon each of their fields ides towards a common goal of understanding one of the most vital aspects of learning and intelligence. I think an organization like ContinualAI is best positioned to do just that, which allows for the sharing of ideas without the boundaries of the academic or industry walls, siloed fields, or distant geolocation.” Beyond the mission of dissemination information about continual learning, Lomonaco believes that ContinualAI has the potential to become a reference points for a more inclusive and collaborative way of doing research in AI. “Elite university and private company labs still work mostly behind close doors, [but] we truly believe in inclusion and diversity rather than selective elitiarity. We favor transparency and open-source rather than protective IP licenses. We make sure anyone has access to the learning resources she needs to achieve her potential.” For AI coverage, send news tips to Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark our AI channel, The Machine. Thanks for reading, Kyle Wiggers AI Staff Writer"
https://venturebeat.com/2021/04/09/ibm-releases-qiskit-modules-that-use-quantum-computers-to-improve-machine-learning/,IBM releases Qiskit modules that use quantum computers to improve machine learning,"IBM is releasing Qiskit Machine Learning, a set of new application modules that’s part of its open source quantum software. The new feature is the latest expansion of the company’s broader effort to get more developers to begin experimenting with quantum computers. According to a blog post by the Qiskit Applications Team, the machine learning modules promise to help optimize machine learning by using quantum computers for some parts of the process. “Quantum computation offers another potential avenue to increase the power of machine learning models, and the corresponding literature is growing at an incredible pace,” the team wrote. “Quantum machine learning (QML) proposes new types of models that leverage quantum computers’ unique capabilities to, for example, work in exponentially higher-dimensional feature spaces to improve the accuracy of models.” Rather than replacing current computer architectures, IBM is betting that quantum computers will gain traction in the coming years by taking on very specific tasks that are offloaded from a classic computing system to a quantum platform. AI and machine learning are among the areas where IBM has said it’s hopeful that quantum can make an impact. To make quantum more accessible, last year IBM introduced an open source quantum programming framework called Qiskit. The company has said it has the potential to speed up some applications by 100 times. In the case of machine learning, the hope is that a system that offloads tasks to a quantum system could accelerate the training time. However, challenges remain, such as how to get large data sets in and out of the quantum machine without adding time that would cancel out any gains by the quantum calculations. Developers who use Qiskit to improve their algorithms will have access to test them on IBM’s cloud-based quantum computing platform."
https://venturebeat.com/2021/04/09/researchers-detail-systemic-issues-and-risk-to-society-in-language-models/,Researchers detail systemic issues and risk to society in language models,"Researchers at Google’s DeepMind have discovered major flaws in the output of large language models like GPT-3 and warn these could have serious consequences for society, like enabling deception and reinforcing bias. Notably, coauthors of a paper on the study say harms can be proliferated by large language models without malicious intent on the creators’ part. In other words, these harms can be spread accidentally, due to incorrect specifications around what an agent should be learning from or the model training process. “We believe that language agents carry a high risk of harm, as discrimination is easily perpetuated through language. In particular, they may influence society in a way that produces value lock-in, making it harder to challenge problematic existing norms,” the paper reads. “We currently don’t have many approaches for fixing these forms of misspecification and the resulting behavioral issues.” The paper supposes that language agents could also enable “incitement to violence” and other forms of societal harm, particularly by people with political motives. The agents could also be used to spread dangerous information, like how to make weapons or avoid paying taxes. In a prime example from work published last fall, GPT-3 tells a person to commit suicide. The DeepMind paper is the most recent study to raise concerns about the consequences of deploying large language models made with datasets scraped from the web. The best known paper on this subject is titled “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” and was published last month at the Fairness, Accountability, and Transparency conference by authors that include former Google Ethical AI team co-leads Margaret Mitchell and Timnit Gebru. This paper asserts language models that seem to be growing in size perpetuate stereotypes and carry environmental costs most likely to be born by marginalized groups. While Google fired both of its researchers who chose to keep their names on the paper and required other Google research scientists to remove their names from a paper that reached a similar conclusion, the DeepMind research cites the stochastic parrots paper among related works. Earlier this year, a paper from OpenAI and Stanford University researchers detailed a meeting between experts from fields like computer science, political science, and philosophy. The group concluded that companies like Google and OpenAI, which control the largest known language models in the world, have only a matter of months to set standards around the ethical use of the technology before it’s too late. The DeepMind paper joins a series of works that highlight NLP shortcomings. In late March, nearly 30 businesses and universities from around the world found major issues in an audit of five popular multilingual datasets used for machine translation. A paper written about that audit found that in a significant fraction of the major dataset portions evaluated, less than 50% of the sentences were of acceptable quality, according to more than 50 volunteers from the NLP community. Businesses and organizations listed as coauthors of that paper include Google and Intel Labs and come from China, Europe, the United States, and multiple nations in Africa. Coauthors include the Sorbonne University (France), the University of Waterloo (Canada), and the University of Zambia. Major open source advocates also participated, like EleutherAI, which is working to replicate GPT-3; Hugging Face; and the Masakhane project to produce machine translation for African languages. Consistent issues with mislabeled data arose during the audit, and volunteers found that a scan of 100 sentences in many languages could reveal serious quality issues, even to people who aren’t proficient in the language. “We rated samples of 205 languages and found that 87 of them had under 50% usable data,” the paper reads. “As the scale of ML research grows, it becomes increasingly difficult to validate automatically collected and curated datasets.” The paper also finds that building NLP models with datasets automatically drawn from the internet holds promise, especially in resolving issues encountered by low-resource languages, but there’s very little research today about data collected automatically for low-resource languages. The authors suggest a number of solutions, like the kind of documentation recommended in Google’s stochastic parrots paper or standard forms of review, like the datasheets and model cards Gebru prescribed or the dataset nutrition label framework. In other news, researchers from Amazon, ChipBrain, and MIT found that test sets of the 10 most frequently cited datasets used by AI researchers have an average label error rate of 3.4%, impacting benchmark results. This week, organizers of NeurIPS, the world’s largest machine learning conference, announced plans to create a new track devoted to benchmarks and datasets. A blog post announcing the news begins with the simple declaration that “There are no good models without good data.” Last month, the 2021 AI Index, an annual report that attempts to define trends in academia, business, policy, and system performance, found that AI is industrializing rapidly. But it named a lack of benchmarks and testing methods as major impediments to progress for the artificial intelligence community."
https://venturebeat.com/2021/04/09/consilient-ucsf-health-intel-deploy-confidential-computing-to-safeguard-data-in-use/,"Consilient, UCSF Health, Intel deploy confidential computing  to safeguard data in use","Sponsored by Intel Confidential computing and privacy-preserving analytics are relatively new terms in the tech world. Yet they’ve continued to gain wider interest as a way to solve the complex challenges of protecting and securing data in use and algorithm code bases on secure public clouds, at the edge, and in data centers — especially for large compute requirements around artificial intelligence. This emerging approach provides a secure, hardware-based platform (confidential computing) for multiple parties to collaborate, allowing an algorithm to compute on sensitive data (via privacy preserving analytics) without exposing either to the other party. Together, the technologies support secure enclaves, virtual machines, databases, and more. Interest has been strong, especially in health care, finance, and government. Two early-adopter examples show the possibilities: In health care, the University of California San Francisco (UCSF) is leveraging its BeeKeeperAI Zero Trust solution to accelerate the validation of clinical-grade AI algorithms that can safely and consistently perform at the point of care. This markedly reduces time and cost, while addressing data security concerns using the latest confidential computing solutions. Consilient’s next-generation, AI-based system fights money laundering and the financing of terrorism, increasing fraud detection by more than 75%. Both cases illustrate how confidential computing can enable collaboration while preserving privacy and regulatory compliance. Unique challenges face developers of health care AI aimed at improving clinical outcomes and workflow efficiencies. UCSF’s Center for Digital Health Innovation (CDHI) experienced these first-hand with partner GE Healthcare in their co-development of the industry’s first regulatory-cleared, on-device health care AI solution. To ensure equitable care for all patients, health care AI applications are required by regulators to be ethnically, clinically, and geographically agnostic. Validating AI models that perform consistently across multiple variables, including race, workflows, and treatment settings requires timely access to highly diverse, non-biased data. UCSF’s BeeKeeperAI Zero Trust platform helps accelerate health care AI by applying privacy-preserving computing to protected, harmonized clinical and health data from multiple institutions. In its proof of concept, BeeKeeperAI leveraged Intel Software Guard Extensions (SGX) and Fortanix’s confidential computing platform running in Microsoft Azure CCE cloud to enable several key protections: Taken together, these security measures help health care organizations protect their data while accelerating the research and development of clinical AI to improve patient care and operational efficiencies. BeeKeeperAI accelerates the development of health care AI by supporting single-source contracting and standardization —  across multiple academic medical institutions — of arms-length harmonization and transformation of data, which remains in the secure control of the data owner at all times. BeeKeeperAI technology is being employed to create a diagnostic screening tool for diabetic retinopathy, the primary cause of blindness in diabetic patients. AiVision’s Olyatis solution uses artificial intelligence algorithms to read images of the eye fundus (the part opposite the pupil). Here’s how it will work: During Phase One collaboration, the BeeKeeperAI confidential computing enclave, powered by Fortanix, will be deployed into the UCSF HIPAA-compliant Azure environment. There, secured by Intel’s SGX technology, UCSF will place an encrypted fundus photo data set into a secured container. AiVision will place the encrypted algorithm into a secure container. Upon mutual agreement, the Olyatis model will move into UCSF’s secure container, where it will be run on the encrypted photo data set.The resulting output will be a validation report documenting the sensitivity and specificity of the algorithm’s performance on the encrypted data. Throughout this process, neither data or algorithm will be visible to one another. Every year, financial institutions and governments around the globe spend billions on anti-money laundering and countering the financing of terrorism (efforts known in the industry as AML/CFT). Unfortunately, experts say the bad actors are winning. The UN estimates that trillions of dollars are laundered every year, roughly 2-5% of worldwide GDP. One study found compliance costs are a hundred times greater than recovered criminal funds. “The current global AML/CFT system is an outdated model that requires a new 21st-century design,” says Juan Zarate, co-founder and chairman of Consilient, a fintech company launched in October. The first-ever Assistant Secretary of the U.S. Treasury for terrorist financing and financial crimes, and global co-managing partner and chief strategy officer at K2 Integrity, Zarate says Consilient is reinventing today’s costly and ineffectual system with “revolutionary” next-generation architecture, governance, and analytics. Consilient’s Dozer technology uses transfer-learning, so that models can be trained across multiple sets of training data, allowing financial institutions to collaborate without putting private data at risk. This secure computing uses Intel SGX technology, which uses a hardware-based Trusted Execution Environment (TEE) to help isolate and protect specific application code and data in memory. The company says its “leapfrog” approach lets financial institutions, authorities, and other regulated actors discover and manage evolving and complicated illicit risks more proactively, effectively, and efficiently. Consilient helps overcome one of the biggest challenges hamstringing financial crime fighters: sharing information among siloed institutions. Today’s reactive system relies on individual institutions to detect and report suspicious activity on their own, explains Zarate. Policy and operational concerns — especially data privacy and security — inhibit information sharing, dynamic feedback, or collective learning, both within and outside the institution. The resulting blind spots make it difficult to identify illicit activity in “island” systems and economies. This paved the way for a behavioral-based, ML-driven governance model that allows its algorithm to access and interrogate data sets in different institutions, databases, and even jurisdictions — without ever moving the data. To make this work, Consilient employs federated learning, a model for distributed machine learning across large and/or diverse datasets. The algorithm, not data, is exchanged between servers at different institutions, enabling it to detect illicit activity more accurately within their networks. Secure, private processing and analysis take place within a protected memory enclave created by Intel SGX built into Xeon processors. This “black box” approach eliminates the need to create complex webs of trust, where data or code could still be exposed to an untrusted party. It facilitates cross-industry machine learning, while still helping to maintain the privacy of individual data and the confidentiality of proprietary algorithms. Consilient can analyze, identify, and find “normal” and “abnormal” patterns in datasets that human eyes and most current technologies cannot see. Traditional rule-based screening and AML/CFT systems typically have a false-positive rate exceeding 95%. Self-learning Dozer technology has proven to reduce the false-positive rate to 12%. With Consilient, government and financial institutions can detect enormous amounts of illicit activity quickly, with far greater accuracy. The company says this capability will help organizations save costs, redeploy personnel, and more effectively prioritize efforts to counter illicit finance. Industry has taken notice; several banks are evaluating the Consilient solution. “Identifying and disrupting the financial networks of criminal enterprises is a top priority for our member banks,” says Greg Baer, president and CEO of the Banking Policy Institute. “This promising technology presents new opportunities to more effectively identify illicit financing at the source.” Go deeper: Confidential Computing with Intel Software Guard Extensions (video)Federated Learning through Revolutionary Technology (white paper)Why Intel believes confidential computing will boost AI and machine learning (article)  Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/09/tableau-adds-support-to-einstein-discovery-for-user-control-over-ai-models/,Tableau adds support to Einstein Discovery for user control over AI models,"Salesforce’s Tableau arm is making a case for employing AI to drive a new era of business science using analytics applications infused with machine learning algorithms the average end user can easily employ. Tableau has added support for Einstein Discovery to the 2021.1 update of its namesake analytics application. Originally developed by Salesforce, the Einstein Discovery module uses machine learning algorithms to surface patterns in data. Including that capability within Tableau will make it possible for business users to employ data science techniques to analyze data without having to write code and without any intervention on the part of a data scientist team, Tableau CTO Andrew Beers said. Prior to being acquired by Salesforce in 2019, Tableau had begun to embed machine learning algorithms within its application, along with natural language processing (NLP) capabilities. The merger with Salesforce enables Tableau to leverage Salesforce’s ongoing Einstein research and development efforts. As analytics applications continue to incorporate capabilities such as Einstein Discovery, organizations will need to determine when the AI model that needs to be constructed is simple enough for end users to create using a graphical tool versus adding yet another project for a data scientist team to complete. “That line today is kind of blurry,” Beers said. Data scientists are, of course, in short supply. And most data science teams have a backlog of projects they are not likely to complete anytime soon. Most are fortunate to be able to successfully complete more than a few projects in a year. However, the backlog of projects a data science team is being asked to complete might decline as more AI capabilities are added to analytics applications. The bulk of AI models end users might ask a data scientist to build would never be built in the first place, as there simply isn’t enough time. The only way to address those requirements is to democratize AI within the context of an application such as Tableau. Those efforts will lead to a set of best practices for business science that will be distinct from the more complex AI projects a typical data science team will take on, said Beers. It’s not quite clear what level of expertise will be required to enable end users to build their own AI models. Einstein Discovery comes with several built-in capabilities for surfacing bias and promoting AI model transparency. But most organizations would be well-advised to review AI models created by end users before making any business decisions that cannot be reversed. In the meantime, organizations should expect AI models to become a lot easier to create using natural language processing (NLP) engines that are increasingly providing support for various speech recognition engines. Business users will be able to invoke those engines to create AI models that would, for example, make it much simpler to engage in what-if analysis involving multiple scenarios. That kind of capability is more valued than ever in the wake of a pandemic that proved conclusively that all business assumptions are subject to rapid change. In fact, as the pace of change continues to accelerate in the age of digital business, it’s doubtful the average user will be able to keep pace without some AI help. That doesn’t mean AI models will replace the need for business analysts, but it does mean the days when data science required a small army of specialists are coming to an end as the AI playing field becomes increasingly level."
https://venturebeat.com/2021/04/09/the-deanbeat-welcoming-xbox-head-phil-spencer-and-diverse-speakers-to-gamesbeat-summit/,The DeanBeat: Microsoft’s Phil Spencer joins our most diverse GamesBeat Summit event,"Black Lives Matter. Border trouble. Anti-Asian hate. Trans athlete sports bans. Sexual harassment. Suicides and the pandemic. These racial, cultural, and social issues are making headlines, and they have spilled into the game industry. It is an important time to take stock of where we are as an industry when it comes to dealing with racism, sexism, and other kinds of toxicity. Just how diverse and inclusive are we? Diversity, inclusion, and mental health challenges are going to be big topics for discussion at our GamesBeat Summit 2021: Growing the Next Generation event coming on April 28 and April 29. Our latest slate of speakers includes Phil Spencer, the executive vice president of gaming at Microsoft, and a panel of Microsoft leaders including Agnes Kim, a senior strategic partner manager for content partnerships; Esteban Lora, the director of external technology and suppliers; and Cierra McDonald, a principal program manager. They’re going to talk on a panel entitled “Team Xbox on Gaming for Everyone.” And they will discuss Microsoft’s journey to bring the joy and community of gaming to everyone on the planet. They will explore the challenges of ensuring that gaming is inclusive for all players and that the game industry welcomes all creators. I’m delighted that important leaders like Spencer care deeply about these issues and that they’ve chosen our conference as a destination to highlight them. But I’m also proud that we’ll have speakers who are off the beaten path and bring new perspectives to our GamesBeat community. About 37 of our 79 speakers (so far) come from diverse backgrounds. That’s about 47% of our speakers who are women, Black, LatinX, or Asian American. We can of course do better, but I’m proud of that ratio, as it’s not easy to accomplish in an industry where women and minorities aren’t well represented. This is our most diverse conference ever. Diversity is a long game, and I’ve waited a long time to hold an event like this — ever since the Los Angeles riots of 1992. Since we last talked about the event, we’ve added speakers including Alina Soltys of Quantum Tech Partners, Katie Madding of Adjust, Katie Jansen of AppLovin, Victor Lazarte of Wildlife, Jason Docton of Rise Above the Disorder, Mark Chandler of TIGS, Ian Fitzpatrick of New Balance, Glen Schofield of Striking Distance Studios, N’Gai Croal of Hit Detection, Stanley Pierre-Louis of the Entertainment Software Association, Kahlief Adams of Spawn On Me, ZombaeKillz, Gabrielle Heyman of Zynga, Brad Hart of Perforce, Christian Kelly and Seth Shuller of Accenture, Ryan Mullins of Aglet, Laura Higgins of Roblox, and Anthony Crouts of Tencent. We’re going to talk about why representation matters at companies (Activision Blizzard gave us a whole panel of women leaders to talk about this), and we’re going to dive into how it makes a difference in the kind of games that get created. We’ve also tapped mental health experts and leaders to talk about issues like burnout, crunch, suicide, and working in the pandemic. Halley Gross, the co-writer of The Last of Us Part II, will talk about why Naughty Dog’s masterpiece, which won 215 Game of the Year Awards in 2020, was so infused with diversity from the non-player characters to strong female leads. I believe it’s important to celebrate the lessons of important games like this one, and to give people a chance to talk about issues that reflect the fact that we just went through one of the most difficult years in modern history. We’re going to celebrate the industry and have a little fun too, now that optimism is coming back. Phil Spencer is the executive vice president of gaming at Microsoft. In this role, Spencer is accountable for leading Microsoft’s gaming business across all devices and services. With his team and game development partners, Spencer continues to push the boundaries of creativity, technical innovation and fun across gaming genres, audiences and devices. Spencer is both a passionate gamer and seasoned gaming executive serving more than 15 years in the gaming industry leading global business, creative and engineering teams. Spencer has held various roles across Microsoft including the head of Xbox; corporate vice president, Microsoft Studios; and GM, Microsoft Game Studios EMEA. In these roles, Spencer led the Xbox organization with the launches of Xbox One S and Xbox One X as well as Microsoft Publishing and the acquisition of Minecraft, and he influenced blockbuster game franchises including Halo, Gears of War, and Forza Motorsport. He also led the expansion of cross-platform gaming with Xbox Live, which now counts more than 50 million monthly active users. Before beginning his career as an intern with Microsoft in 1988, Spencer earned his bachelor’s degree from the University of Washington. He currently serves on the board of Entertainment Software Association and of The Paley Center for Media. Here’s more info about Spencer’s crew: Agnes Kim is leading gaming content partnerships in South Korea, China, and Southeast Asia. Her goal is to engage developers in these markets and bring more content into Xbox’s ecosystem to ensure that our gamers can experience all the amazing content from the world. She was an avid gamer from very young, spending a lot of time in PC Cafes in South Korea and always dreaming of working in a cross section of Entertainment and Technology. She worked at Sony Pictures Entertainment prior to joining Xbox focusing on partnerships and strategic alliances. Before that, she worked at Deloitte consulting concentrating most of her time in M&A projects. Esteban Lora manages the External Technology and Suppliers team at Xbox Game Studios, which is responsible for business relationships with gaming middleware companies, external development studios and music supervision talent supporting all Microsoft franchises and first party Studios including Forza, Halo and Gears of War. He studied computer engineering and was influenced by his interest in 3D graphics and passion for video games, then complemented his technical background by specializing on business management and strategic partnerships in the software industry, where he has been active for more than 20 years working for blue chip companies worldwide. Originally from the south side of Chicago, Cierra McDonald is a principal program manager at Xbox. She has enjoyed 12 years working on Team Xbox and a total of 17 years at Microsoft. Cierra founded the Blacks at Xbox employee community in 2015, through which she organized Xbox’s annual Blacks in Gaming networking event at the Game Developers Conference (GDC) and started the Jerry Lawson Grant for Career Development. In 2019, the International Game Developers Association (IGDA) Foundation presented Cierra with their inaugural Jerry A. Lawson Award for Achievement in Game Development in recognition of her accomplishments as an engineer, advocate, and community leader within the gaming industry. Cierra is passionate about encouraging the youth to embrace gaming as both a hobby and a career where they can express themselves creatively and can develop proficiency across many disciplines, ranging from computer science and research to music and storytelling. She volunteers with youth-oriented nonprofit organizations such as Gameheads and IGNITE and is an advisory board member for the University of Washington Certificate in Game Design. The question is what will keep the growth going. We’ll explore new parts of the business including blockchain and nonfungible tokens, the post-IDFA world, augmented and virtual reality, esports, the metaverse, and the explosion of opportunities that come from having an unprecedented amount of money coming into the industry through investments, public offerings, and acquisitions. Not long ago, I announced the lineup includes Laura Miele of Electronic Arts, Robert Antokol of Playtika, Aaron Loeb of Scopely, Jen Oneal of Blizzard Entertainment, Geoff Keighley of The Game Awards, Lydia Bottegoni of Blizzard, Eunice Lee of Activision, Nour Polloni of Beenox, Ronnie Nelis of Lion Castle, Sushama Chakraverty of Prodigy Education, Wanda Meloni of M2 Insights, Keza MacDonald of the Guardian, Kelli Dunlap of American University, Noah Falstein of The Inspiracy, Brennan Spiegel of Cedars-Sinai, Susanna Pollack of Games for Change, Paul Doyle of Epic Games, Vladimir Mastilović of Epic Games, Caroline Stokes of Forward, and Eve Crevoshay of Take This, Chris Hewish of Xsolla, and Matt Casamassina of Rogue Games. Our other announced speakers so far include Bobby Kotick of Activision Blizzard, Brenda Romero of Romero Games, Jens Hilgers of Bitkraft Ventures, Chris DeWolfe of Jam City, Keisha Howard of Sugar Gamers, Daniel Melville (who has a bionic arm based on Konami’s Metal Gear series), chief gaming architect Frank Azor of Advanced Micro Devices, Iron Galaxy leaders Adam Boyes and Chelsea Blasko, Eric Goldberg of Playable Worlds, Andrew Sheppard of Transcend Funds, Simon Zhu of NetEase, Mike Vorhaus of Vorhaus Advisors, Michael Metzger of Drake Star, Ed Fries of 1Up Ventures, Raffael “Doctor B” Boccamazzo of Take This, Rob Lowe of Lego Ventures, Itamar Benedy of Anzu, Mike Minotti of GamesBeat, Steve Peterson of Storyphorce, Tim Guhl of Singtel, Lisa Cosmas Hanson of Niko Partners, Karsten Lund from Lightbrick Studios, and Liontree’s Nick Tuosto. We want to continue our reputation as the most intimate gaming event where business meets passion. Our event will include fireside chats, panels, and small-group roundtables. We’ll provide Q&A sessions for VIP attendees, and a way for attendees to network with each other and make new connections. We have a wide range of partners including the International Game Developers Association and Women in Gaming International. And our sponsors include Lego Ventures, Anzu, Xsolla, Jam City, Adjust, Accenture, Rogue Games, Accenture, Epic Games, Scopely, Lego, Singtel, the Entertainment Software Association, Wildlife, Perforce, Outfit7, and more. And one more thing: I’m also doing a regular Game Industry event on Clubhouse about the week’s news in gaming on Fridays at 2:30 p.m. Pacific time."
https://venturebeat.com/2021/04/09/verizon-study-companies-relaxed-mobile-security-policies-get-employees-online/,Verizon finds companies relaxed mobile security policies to get employees online,"Security was not a priority for many organizations as they rushed to get employees up and running a year ago, Verizon said, adding that this was especially true for mobile security. According to the fourth annual Verizon Mobile Security Index, 45% of organizations sacrificed mobile security for expediency over the past year. Interestingly, the number of organizations experiencing a compromise or breach dropped to 23% over the past year, a slight decrease from 27% in 2018. The Verizon Mobile Security Index is based on interviews with 865 business professionals located in the U.S., U.K., and Australia who are responsible for buying and managing mobile and IoT devices for their companies. With VPN and Wi-Fi connectivity inconsistent and unreliable for many remote employees during the pandemic, mobile devices and cloud applications quickly became the go-to platforms for getting work done. IT teams are still under pressure to grant greater access privileges to less secure mobile devices, often operating on networks the company doesn’t own. IT supports a wider variety of remote workers than ever before, from commuters to road warriors in sales and service, putting more time pressures on them. Add all these factors together, and it’s easy to understand why mobile devices are the most vulnerable across the threat landscape today. Approximately three-quarters of IT teams (76%) are being asked to relax security policies for mobile devices so employees can meet deadlines and achieve business goals. Verizon’s index reflects the conflicts IT teams face between protecting mobile assets and helping employees do their jobs. IT teams recognize that mobile devices pose significant risks to the organization: 40% named mobile devices as the biggest security risk and 50% said risks posed by mobile devices are growing faster than other risks. As part of its Mobile Security Index, Verizon tracks how many companies have four basic protections: changing all default/vendor-supplied passwords; encrypting sensitive data when sent across open, public networks; restricting access to data on a need-to-know basis; and regularly testing security systems and processes. Even though these four items are considered security fundamentals, the Index found just 9% of organizations had all four in place this year, while the average from previous years was 12%. Almost half (49%) said they regularly tested security systems and processes, but just 39% said they regularly changed passwords or restricted data on a need-to-know basis. Even more worrying, 15% did not have any of these four protections in place. The past year has been especially challenging for organizations relying on legacy trusted and untrusted domains to protect the rapidly growing number of mobile devices that needed to be mapped into domains. And this situation isn’t changing anytime soon. Organizations consequently need to treat identity as the new security perimeter and consider a data-centric security model to scale more effectively. The pandemic caught many organizations without enough laptops and tablet devices to outfit their workforce. There was also an acute laptop shortage, with lead times of 16 weeks or more for many models as manufacturers shut down and supply chains faced disruptions. The pressure points over the past year exposed some of the glaring security weaknesses organizations currently have. Too few are getting basic protections right, but the Verizon Mobile Security Index offers ways to improve. Over the past year, 36% of organizations opened access to corporate resources and systems for employees using their own devices, according to Verizon. The Mobile Security Index tracks both BYOD (bring your own device) and BYOPC (bring your own PC), and it shows many organizations began considering BYOD and BYOPC as viable strategies this year. Over 25% of organizations reported already allowing BYOD, but fewer than 25% of organizations supported BYOPC. A key element of BYOD and BYOPC strategies, or any kind of virtual work, is providing secure access to company applications, databases, and internal systems. If not done correctly, there is the risk of large-scale data breaches. The Cybersecurity Framework from the National Institute of Standards and Technology (NIST) makes a strong case for zero trust frameworks for data-centric security in organizations relying on BYOD and BYOPC. Implementing a zero trust framework requires mobile device management (MDM) and unified endpoint management (UEM) to secure endpoints at scale. Having a UEM platform supporting BYOD and BYOPC devices helps ensure every endpoint can be self-diagnosed and self-remediating. Leading providers of MDM and UEM solutions include Ivanti, Hexnode, ManageEngine, and Sophos. Identities are the new security perimeter and why zero trust in a mobile-first, cloud-first IT environment is a clear path forward."
https://venturebeat.com/2021/04/09/accenture-ai-expert-on-how-first-principles-prevent-problems/,Accenture AI expert on how first principles prevent problems,"As more organizations begin employing AI in production environments, it’s clear not everyone has completely thought through how AI will fundamentally change their business. Most of the focus today tends to be on AI to reduce operational costs in the wake of the economic downturn brought on by the COVID-19 pandemic. VentureBeat talked with Fernando Lucini, global data science and machine learning engineering lead for Accenture, about why organizations shouldn’t focus on initial success. Lucini stressed how important it is for organizations adopting AI to keep first principles uppermost in mind. This interview has been edited for clarity and brevity. VentureBeat: Prior to the COVID-19 pandemic, most organizations were struggling when it came to AI. Now we’re seeing more AI than ever. How has the pandemic impacted those projects? Fernando Lucini: It’s been a confluence of events. CEOs are starting to ask “Where has all the money gone?” People started to ask some really deep questions about those investments. We’re thinking more about the value of AI. From a human perspective, companies that were affected needed to get smart because they were squeezed a bit because of COVID. VentureBeat: Is there a danger organizations are now moving too quickly without really understanding AI? Lucini: We all get very excited about AI, but it needs to run with the right kind of controls and ethics. Three years from now, you’re going to be in a land where there’s a model that connects to a model that connects to a model. It will all be intertwined in a complex way. I think there’s a ways to go. VentureBeat: Will different models conflict with one another? Lucini: There are no models interacting yet, but synthetic data is quite exciting. We have customers who literally can’t get ahold of their own data because it’s so protected, so there’s going to be in the modeling world the concept of synthetic data that is a true synthesis. It’s not a copy anymore. It reflects the original pattern but never has any of the original data. I think there’s going to be a lot of synthetic data out in the world. That’s when you’ll see a model created by a bank interacting with a model from an insurance company. As we move along and we get into more complex models, the winners are going to be those that actually have a great handle on things. They understand how things are happening, why they’re happening, and have strong controls and strong governance around how they do things. VentureBeat: Right now it takes a fair amount of time to train an AI model. Will that process become faster? Lucini: I always joke that if you put five software engineers in a room and you give them five hours, no code will be written but they will know how to compile everything and what standards to use. If you put five data scientists in the next room for the same five hours, you’ll get five models based on five different mechanisms that are badly coded but very brilliant. We need to bring those two things together if you want to get the kind of speed of innovation we need. If you just have a few patterns, it’s very clear that you can go from data to model to production in an industrialized way. Where people fall down at the moment is because there have been loads of pilots in the last six months, but none of them can go to production. VentureBeat: Machine learning operations (MLOps) has emerged as an IT discipline for implementing AI. Does this need to be folded into traditional IT operations? Lucini: In time. Data science and ML engineering are in the same group at Accenture. These folks need to have quite a deep understanding of the mechanisms to make these things. They need to have knowledge that is a little bit more specific to the model. I suspect there’ll be specialization for a while. I don’t think that’s going to go away anytime soon. VentureBeat: There’s a lot of talk about the democratization of AI these days using AutoML frameworks. Is that really possible to achieve? Lucini: It’s inevitable that some of these platforms are doing more and more AutoML. I was speaking to a professor at Stanford a couple of weeks ago, and he was telling me that 90% of the people that go to his course on neural nets are not computer science students. The average education of people understanding statistical mathematics is going up. You also need industry expertise. Having somebody who understands how to use a model but doesn’t understand the problem at hand quite as deeply doesn’t work. My view is you’re going to have more AutoML that people can use, but we’re also going to need more guardrails to make sure that whatever it is they’re using is within the scope of safety. Education takes them to a point where they do understand whether they created a monster or not. We’re going to have to add more of these industry people that know more of the science. There are already generalists and citizen data scientists. I joke with CIOs and CEOs that these people can also be dangerous amateurs. Then you have this debate about how people don’t really understand how cars work and they still drive them. We still test people so they can drive cars. There’s a good reason for that, so let’s do the same. It’s important to have enough of an education. VentureBeat: What’s your best advice to organizations then? Lucini. Think about the first principles. If you think about AI as being important to you, then you should think about what is your business strategy for AI? Not how AI is part of your business strategy. Educate yourself sufficiently so you can apply principles to understand how AI might actually make a difference to what you’re doing. The truth is AI has a hidden cost of learning how to do it at scale. “Think 10 times” is the first principle of education."
https://venturebeat.com/2021/04/09/u-s-blacklists-7-more-chinese-supercomputing-entities/,U.S. blacklists 7 more Chinese supercomputing entities,"(Reuters) — The U.S. Commerce Department said Thursday it was adding seven Chinese supercomputing entities to a U.S. economic blacklist for assisting Chinese military efforts. The Commerce Department said the seven were “involved with building supercomputers used by China’s military actors, its destabilizing military modernization efforts, and/or weapons of mass destruction programs.” The department is adding Tianjin Phytium Information Technology, Shanghai High-Performance Integrated Circuit Design Center, Sunway Microelectronics, the National Supercomputing Center Jinan, the National Supercomputing Center Shenzhen, the National Supercomputing Center Wuxi, and the National Supercomputing Center Zhengzhou to its blacklist. China’s foreign ministry spokesperson Zhao Lijian said Beijing will take “necessary measures” to protect its companies’ rights and interests. “U.S. containment and suppression cannot hold back the march of China’s scientific and technological development,” he said at a daily news conference in Beijing on Friday. Companies or others named on the U.S. Entity List are required to apply for licenses from the Commerce Department and face tough scrutiny when they seek permission to receive items from U.S. suppliers. “Supercomputing capabilities are vital for the development of many — perhaps almost all — modern weapons and national security systems, such as nuclear weapons and hypersonic weapons,” Commerce Secretary Gina Raimondo said in a statement. The new rules take effect immediately but do not apply to goods from U.S. suppliers already en route. During the administration of former U.S. President Donald Trump, the U.S. added dozens of Chinese companies to its economic blacklist, including the country’s top smartphone maker Huawei; top chipmaker SMIC, and the largest drone manufacturer, SZ DJI."
https://venturebeat.com/2021/04/09/study-finds-that-facebook-ad-targeting-algorithms-exhibit-gender-skew/,"Facebook failed to fix ad-targeting gender discrimination, study finds","Two years ago, researchers at the University of Southern California published a study showing that Facebook’s algorithms could deliver job and housing ads to audiences skewed by race and gender. The methodology they used didn’t account for differences in the job qualifications of the targeted audiences. But in a new paper, the coauthors of the original research claim to have found evidence of a skew by gender for job ads on Facebook even when controlling for qualifications. “Our results show Facebook needs to re-evaluate how their algorithms that optimize for user relevance or their business goals in a non-transparent way may result in discriminatory job ad delivery,” Aleksandra Korolova, assistant professor of computer science at the University of Southern California and a lead author on the study, told VentureBeat via email. “Our study also shows that, from an external auditing point of view, Facebook has not made visible progress in improving the fairness of its ad delivery algorithms despite prior studies and a civil rights audit that raised concerns about the role its algorithms may play.” In response, a Facebook spokesperson told VentureBeat via email: “Our system takes into account many signals to try and serve people ads they will be most interested in, but we understand the concerns raised in the report. We’ve taken meaningful steps to address issues of discrimination in ads and have teams working on ads fairness today. We’re continuing to work closely with the civil rights community, regulators, and academics on these important matters.” Many previous studies have established that Facebook’s ad practices are at best problematic. This came to a head in March 2019, when the U.S. Department of Housing and Urban Development filed suit against Facebook for allegedly “discriminating against people based upon who they are and where they live,” in violation of the Fair Housing Act. When questioned about the allegations during a Capital Hill hearing in October 2019, CEO Mark Zuckerberg said that “people shouldn’t be discriminated against on any of our services,” pointing to newly implemented restrictions on age, ZIP code, and gender ad targeting. Facebook claims its written policies ban discrimination and that it uses automated controls — introduced as part of the 2019 settlement — to limit when and how advertisers target ads based on age, gender, and other attributes. Platforms like Facebook leverage algorithms to deliver ads to a subset of a targeted audience. Every time a user visits the company’s website or app, Facebook runs an auction among advertisers who are targeting that user. In addition to the advertiser’s chosen parameters, such as a bid or budget, the auction takes into account an ad relevance score, which is based on the ad’s predicted engagement level and value to the user. To determine what skew might be present in these algorithms, the researchers developed an auditing methodology for benchmarking the delivery of job ads, an area where U.S. law prohibits discrimination based on certain attributes. Title VII of the U.S. Civil Rights Act of 1964 allows organizations who advertise job opportunities to only show preference based on bona fide occupational qualifications, which are the requirements necessary to carry out a job function. In the course of experiments with a nearly $5,000 ad campaign budget, the researchers ran ads on Facebook with gender-neutral text and images across three categories: Since their methodology compared two ads for each category, the researchers selected two jobs at companies for which they had evidence of gender distribution differences. They also ran the ads on LinkedIn to compare the initial findings with algorithms on a different platform. According to the researchers, the results show evidence of a statistically significant gender skew on Facebook compared with no gender skew on LinkedIn. Across three campaign runs on Facebook, the Domino’s ad delivered to a higher fraction of men than the Instacart ad — despite the fact that 98% of delivery drivers for Domino’s are male and over 50% of Instacart drivers are female. Moreover, a higher fraction of women on Facebook saw software engineering ads that the researchers created featuring Netflix (where 35% of employees in tech-related positions are female) versus Nvidia (where 19% of all employees are female). LinkedIn had no such skew. “Facebook’s job ad delivery is skewed by gender, even when the advertiser is targeting a gender-balanced audience,” Korolova and coauthors wrote in the paper. “Our findings suggests that Facebook’s algorithms may be responsible for unlawful discriminatory outcomes.” The researchers recommend several steps that might address this skew, including more targeting and delivery statistics, replacing ad-hoc privacy techniques with rigorous approaches, and reducing the cost of auditing. They emphasize that privacy-preserving techniques such as differentially private data publishing, which aims to output aggregate information without disclosing any person’s record, might be able to strike a balance between auditability and privacy. “We recommend ad platforms use approaches with rigorous privacy guarantees, and whose impact on statistical validity can be precisely analyzed, such as differentially private algorithms, where possible,” the researchers wrote. “Overall, making auditing ad delivery systems more feasible to a broader range of interested parties can help ensure that the systems that shape job opportunities people see operate in a fair manner that does not violate anti-discrimination laws. The platforms may not currently have the incentives to make the changes proposed and, in some cases, may actively block transparency efforts initiated by researchers and journalists; thus, they may need to be mandated by law.”"
https://venturebeat.com/2021/04/08/amazon-launches-ml-powered-maintenance-tool-lookout-for-equipment-in-general-availability/,Amazon launches ML-powered maintenance tool Lookout for Equipment in general availability,"Amazon today announced the general availability of Lookout for Equipment, a service that uses machine learning to help customers perform maintenance on equipment in their facilities. Launched in preview last year during Amazon Web Services (AWS) re:Invent 2020, Lookout for Equipment ingests sensor data from a customer’s industrial equipment and then trains a model to predict early warning signs of machine failure or suboptimal performance. Predictive maintenance technologies have been used for decades in jet engines and gas turbines, and companies like GE Digital’s Predix and Petasense offer Wi-Fi-enabled, cloud- and AI-driven sensors. According to a recent report by analysts at Markets and Markets, predictive factory maintenance could be worth $12.3 billion by 2025. Startups like Augury are vying for a slice of the segment, beyond Amazon. With Lookout for Equipment, industrial customers can build a predictive maintenance solution for a single facility or multiple facilities. To get started, companies upload their sensor data — like pressure, flow rate, RPMs, temperature, and power — to Amazon Simple Storage Service (S3) and provide the relevant S3 bucket location to Lookout for Equipment. The service will automatically sift through the data, look for patterns, and build a model that’s tailored to the customer’s operating environment. Lookout for Equipment will then use the model to analyze incoming sensor data and identify early warning signs of machine failure or malfunction. For each alert, Lookout for Equipment will specify which sensors are indicating an issue and measure the magnitude of its impact on the detected event. For example, if Lookout for Equipment spotted an problem on a pump with 50 sensors, the service could show which five sensors indicate an issue on a specific motor and relate that issue to the motor power current and temperature. “Many industrial and manufacturing companies have heavily invested in physical sensors and other technology with the aim of improving the maintenance of their equipment. But even with this gear in place, companies are not in a position to deploy machine learning models on top of the reams of data due to a lack of resources and the scarcity of data scientists,” VP of machine learning at AWS Swami Sivasubramanian said in a press release. “Today, we’re excited to announce the general availability of Amazon Lookout for Equipment, a new service that enables customers to benefit from custom machine learning models that are built for their specific environment to quickly and easily identify abnormal machine behavior — so that they can take action to avoid the impact and expense of equipment downtime.” Lookout for Equipment is available via the AWS console as well through supporting partners in the AWS Partner Network. It launches today in US East (N. Virginia), EU (Ireland), and Asia Pacific (Seoul) server regions, with availability in additional regions in the coming months. The launch of Lookout for Equipment follows the general availability of Lookout for Metrics, a fully managed service that uses machine learning to monitor key factors impacting the health of enterprises. Both products are complemented by Amazon Monitron, an end-to-end equipment monitoring system to enable predictive maintenance with sensors, a gateway, an AWS cloud instance, and a mobile app."
https://venturebeat.com/2021/04/08/mlops-startup-comet-raises-13m-to-launch-model-monitoring-products/,MLOps startup Comet raises $13M to launch model monitoring products,"MLOps startup Comet today announced that it raised $13 million in a series A funding round led by Scale Venture Partners. The capital, which Comet plans to put toward product, sales, marketing, and engineering growth, comes as the company acquires U.K.-based Stakion to bolster the launch of Comet Model Production Monitoring (MPM), a product that enables organizations to track and monitor AI model quality. MLOps, or machine learning operations, encompasses the ways organizations build and deploy models. In the wake of the COVID-19 pandemic, enterprises have accelerated their investments in AI as part of an effort to drive digital business transformations. MLOps platforms could generate annual revenues in excess of $4 billion by 2025, according to Deloitte. That’s not surprising in light of a McKinsey report suggesting that AI, if successfully implemented, could drive about 20% of a company’s earnings. Comet provides self-hosted and cloud-based MLOps solution that allows data scientists and engineers to track, compare, and optimize experiments and models. The ostensible aim is to deliver insights and data to build better, more accurate AI models while improving productivity, collaboration, and explainability across teams.  Comet supports code panels, an ecosystem of plugins, extensions, and visualizations built by the community and industry teams. It also offers tools like the aforementioned MPM, which is designed to provide visibility into model performance throughout a model’s lifecycle, from creation to production. Comet has a rival in Weights and Biases, a provider of a platform for enabling collaboration and governance across teams building machine learning models. Among others, Domino Data Lab, a startup developing a platform focused on enterprises with large data science teams, is vying for a slice of the growing MLOps segment. But Comet appears to have carved out a chunk of the expanding market. It claims to serve “thousands” of users and “multiple” Fortune 100 companies, with 500% revenue growth over the past year. And this latest funding round, which saw the participation of  Trilogy Equity Partners and Two Sigma Ventures, brings the New York-based company’s total raised to over $19 million."
https://venturebeat.com/2021/04/08/klevu-raises-12m-for-ai-that-personalizes-ecommerce-search/,Klevu raises $12M for AI that personalizes ecommerce search,"Klevu, a company developing AI products that personalize ecommerce search and discovery experiences, today announced it has closed a $12 million series A round. This marks the launch of Klevu’s Discovery Suite, an end-to-end solution that captures online shoppers’ intent to boost conversions, average order value, and loyalty. And it will help fund Klevu’s Global Semantic Lab initiative, which aims to build “next-generation” technologies at the intersection of machine learning and natural language processing for ecommerce. While year-over-year consumer spending in the U.S. dipped last month, the pandemic has supercharged ecommerce. According to data from IBM’s U.S. Retail Index, business closures and shelter-in-place orders accelerated the shift to digital shopping by five years, with online shopping projected to grow nearly 20% in 2020. Based on the survey data from BMC and Mercatus, ecommerce grocery orders alone totaled $5.9 billion, up 3.6% from $5.7 billion in August. Cofounded in Finland in 2013 by Nilay Oza, Niraj Aswani, and Jyrki Kontio, Klevu aims to connect people to products they wish to buy. Through AI and natural language processing-powered technologies, the company’s products help merchants deliver relevant experiences powered by behavior.  Klevu says its search product, which plugs into ecommerce platforms like Shopify, continuously learns from shoppers’ interactions to optimize results. It automatically handles typos while identifying and understanding stop words (e.g., “iPhone” in the phrase “iPhone with case”) and adding contextually relevant synonyms to product catalogs. Klevu also enables merchants to fine-tune search results using rules across certain categories and keywords. Klevu’s complementary category merchandising product reacts to trends in real-time data, enhancing the results that customers see on product category pages. Featuring filtering and custom layouts, Klevu claims it improves website usability by delivering a consistent experience across mobile and desktop. “Text mining is all about analyzing a piece of text and identifying hidden meaningful information from within it. At Klevu, several AI techniques are used to make this happen in the ecommerce domain,” a spokesperson told VentureBeat via email. “Our objective is to enrich the catalog with the information, originally missing in the catalog, that the shoppers may search for … In our training data, we have millions of products from 45 different industries and in 30 different languages. The product data is systematically organized with various product attributes and images associated with each product. This data is then further enhanced, automatically, with an average of 10 million shopping signals collected daily directly from the high-intent shoppers to understand how they search, locate products, and use them.” Conversion remains a challenge in the ecommerce space, despite the pandemic-bolstered growth. As of Q1 2019, just 2.72% of ecommerce website visits converted into purchases, according to Statista. In a recent report, KPMG stressed the need to promote relevant products right at the time of checkout, which the firm said can have an outsized impact on buying decisions.  Klevu customer Puma says it saw an over 50% increase in search-led conversions on one of its websites after adopting the platform. Besides Puma, Klevu says more than 3,000 brands, including Yamaha, Callaway, and Jack Daniel’s, have made over 5 billion APIs to its product. “Amazon is well known for product discovery, and Google is known for its content,” Oza told VentureBeat via email. “Klevu marries these two together to provide a seamless shopping journey, ensuring the consumer is served the most relevant products and stays highly engaged. With this investment, we are well-positioned toward our mission of democratizing discovery in online retail. We will further invest into strengthening our leadership in machine learning- and natural language processing-led innovations for online retail that bring data-driven business success for our customers.” Alfvén & Didrikson led Klevu’s latest funding round, with participation from existing investors EVLI Growth Partners, Jerry Pruttz Holding, and Jonas Dromberg. Klevu has over 90 employees across offices in the U.K., U.S., India, Australia, and Sweden, in addition to Finland. And the company has raised over $17.5 million to date."
https://venturebeat.com/2021/04/08/onetrust-raises-210m-to-expand-its-enterprise-compliance-solutions/,OneTrust raises $210M to expand its enterprise compliance solutions,"OneTrust, a privacy, marketing, security, and data governance firm based in Atlanta, Georgia, today announced it has raised $210 million in a series C extension led by SoftBank’s Vision Fund 2, with participation from Franklin Templeton. OneTrust says the round adds a strategic geographical position in Japan — SoftBank is Tokyo-based — as market demands accelerate in the Asia Pacific region and across the globe. According to a Thales report, about 64% of respondents around the world feel that adhering to compliance requirements is a “very” or “extremely” effective way to keep data secure. But compliance is expensive. In a 2017 PricewaterhouseCoopers survey of execs at U.S., U.K., and Japanese tech companies, 88% said their company planned to spend over $1 million preparing for the EU’s General Data Protection Regulation (GDPR) in the run-up to its full May 2018 implementation. A smaller percentage of respondents (40%) said they expected to spend $10 million or more. Kabir Barday, a former developer at BlackRock and former director of product management at Dell-owned VMWare, anticipated the nearly $51.5 billion compliance management market’s growth in 2016 when he founded OneTrust with co-chair Alan Dabbiere, a cofounder of Manhattan Associates and AirWatch. Barday was an early employee at AirWatch, which raised $200 million in 2013 before VMware acquired it for $1.5 billion. OneTrust went on to raise $210 million in a series B round last February at a whopping $2.7 billion valuation — a valuation the firm more than doubled to $5.1 billion in December 2020. This latest cash infusion comes after roughly a year during which OneTrust grew its customer base to more than 8,000 organizations across 100 countries. According to Barday, nearly half the Fortune 500 companies now use the company’s product suite, including brands like Aetna, Oracle, 21st Century Fox, and Salesforce. OneTrust offers a privacy management program that helps companies comply with the GDPR, the California Consumer Privacy Act (CCPA), and hundreds of other global privacy laws by using research portals and automation tools. The company streamlines the intake and fulfillment of consumer and subject rights requests and allows customers to benchmark against peers, map and inventory records of processing, and generate custom reports as data flows through their organization. OneTrust’s tools enable companies to drive opt-in demand while demonstrating full compliance. Businesses can deploy interfaces and experiences from marketing and sales activities that collect consent and preferences and sync them across channels while automating the fulfillment of consumer rights requests and the maintenance of historical consent records from a single portal. On the third-party risk side of the equation, OneTrust assesses IT and non-IT vendors, direct suppliers, services, legal organizations, franchisees and retailers, agents, and contractors with risk mitigation workflows and ongoing monitoring. It prepopulates security and privacy data on thousands of global vendors in total, each with information at the service and product level, and it lets managers create automated rules to trigger reassessment or receive alerts when enforcement actions occur. It’s safe to say that compliance management is a red-hot sector. In 2019, San Francisco-based TrustArc raised a $70 million round of funding to help companies implement privacy and compliance programs; Privitar nabbed $40 million to better enable businesses to engineer privacy protection into their data projects; and InCountry exited stealth with $7 million in seed funding to help multinationals comply with local data residency regulations. Back in 2018, BigID nabbed $30 million to expand its data privacy management platform for enterprises. And at the end of 2019, LogicGate, which provides a platform that automates processes and compliance tracking, raised $24.75 million to invest in content, frameworks, data partnerships, and integration. To stay ahead of the competition, in 2020 OneTrust launched Athena, an AI and robotic automation engine built into the OneTrust platform. After acquiring Seattle-based Integris, OneTrust also rolled out new data governance and guidance, ethics, and privacy products; DataDiscovery, a data discovery and classification solution; and free tools to automate GDPR and CCPA compliance programs. OneTrust Environmental, Social & Governance is the company’s newest software and allows clients to manage initiatives in those areas, like carbon output or diversity, equity, and inclusion programs. More recently, OneTrust acquired Convercent, a Denver, Colorado-based firm developing “whistleblowing” software that lets employees report problems they see within their company — anonymously or otherwise. The purchase came weeks after OneTrust bought Docuvision for its ability to use AI to redact specific information, and it marks the company’s sixth acquisition overall. OneTrust, which is co-headquartered in London, with additional offices in Bangalore, Melbourne, Seattle, San Francisco, New York, São Paulo, Munich, Paris, Hong Kong, and Bangkok, has over 1,500 employees globally. It recently expanded to France with a dedicated team of local privacy and marketing experts and a datacenter, shortly after announcing new operations in Brazil with hosting options and support for Brazilian Portuguese and “dozens” of other languages. OneTrust has raised $920 million in funding to date from investors Insight Partners, Coatue, and TCV, in addition to Softbank and Templeton. The extension brings the company’s series C round to $510 million."
https://venturebeat.com/2021/04/08/computer-vision-development-platform-crowdai-raises-6-25m/,Computer vision development platform CrowdAI raises $6.25M,"CrowdAI, a computer vision development platform, today announced that it closed a $6.25 million series A financing round led by Threshold Ventures. The fundraising coincides with the launch of the startup’s new solution that allows customers to create AI that analyzes images and videos. The AI skills gap remains a significant impediment to adoption in most enterprises, a 2020 O’Reilly survey found. Slightly more than one-sixth of respondents cited difficulty in hiring experts as a barrier to AI deployment in their organizations. In a separate report, this by Deloitte, 23% of “seasoned” AI adopters characterized the gap between their AI needs and current abilities as “extreme.” Devaki Raj founded CrowdAI in an effort to eliminate adoption blockers specifically in the area of computer vision. Prior to starting CrowdAI,  Raj studied statistics and machine learning at Oxford, where she earned a master’s degree before working on Maps and Android at Google. She left Google 5 years ago to launch CrowdAI alongside Nicolas Borensztein and Pablo Garcia. Borensztein previously founded and sold ad platform Ember to Adaptive Media, while Garcia worked at Google on the AdWords API team building out data pipelines. CrowdAI helps develop computer vision models for clients primarily in the property insurance, finance, and technology markets. The first iteration of the platform focused on cutting down time-to-value for developers and data scientists familiar with AI integrations and workflows, but CrowdAI’s newest release — which is available in free and enterprise versions — requires no coding. “Our business model isn’t about selling already-trained models — quite the opposite. We’re focused on providing a platform for subject matter experts who aren’t data scientists to build their own models that solve their specific problem,” Devaki Raj told VentureBeat via email. “For example, natural gas utilities are under enormous public pressure to identify and mitigate leaks and microleaks from thousands of miles of pipeline and related infrastructure. These leaks are easy to see on thermal video, but reviewing all that footage in real-time just isn’t scalable. Using computer vision, a machine learning model could review terabytes of imagery each day looking for methane leaks so humans don’t have to.”  Controversially, CrowdAI has had contracts with the U.S. military, including one with the U.S. Air Force to turn public satellite data into combat insights as part of a prototype. (CrowdAI’s history with the Air Force began in 2018, when it participated in the inaugural cohort of the Air Force Research Lab’s Hyperspace Challenge.) The company has proposed highlighting patterns, movements, and changes in maps used by airmen that might otherwise go unnoticed, as well as compiling, analyzing, and mapping out regions in which the U.S. military operates. In pitches in front of academics, contractors, investors, and Air Force acquisition officers, CrowdAI has demonstrated its technology identifying flooding in the aftermath of Hurricane Harvey, the extent of damage after wildfires, and buildings after bombings in Aleppo. In one presentation, Raj showed the platform mapping all major roads in Syria within six hours. In a previous interview with Wired, Raj declined to say which applications of CrowdAI’s platform she considered off limits. But a CrowdAI spokesperson clarified that the company doesn’t currently have an Air Force contract for combat insights. “Our goal is to equip the next generation of employees to become educated in how computer vision can empower them,” Raj said. “It’s not about teaching computer science or AI per se, but arming them with those skills so they can recognize where in their workflow AI can be their partner.” It’s proven to be a winning strategy. CrowdAI says it saw a 200% increase in its customer base over the past year. “In some ways, the pandemic has accelerated the need for AI across the board. Companies seem acutely aware of the fragility of the supply chain, and are looking for tools and tech that can help make it more efficient and predictable,” Raj continued. “When it comes to computer vision, our goal is to empower — not replace — humans at the companies and organizations we work with. We help them create AI and machine learning to meet their own specific needs, which frees these people up to focus on higher-order tasks, such as decision making and planning. This means the same workforce can now work smarter and faster because they don’t have to spend as much time on analyzing and managing visual data.” CrowdAI has raised over $10 million to date. Other investors that participated in its series A included Susa Ventures, Ron Conway’s SV Angel, Jerry Yang at AME Cloud, and Y Combinator."
https://venturebeat.com/2021/04/08/facebook-dataset-combats-ai-bias-by-having-people-self-identify-age-and-gender/,Facebook dataset combats AI bias by having people self-identify age and gender,"Facebook today open-sourced a dataset designed to surface age, gender, and skin tone biases in computer vision and audio machine learning models. The company claims that the corpus — Casual Conversations — is the first of its kind featuring paid people who explicitly provided their age and gender as opposed to labeling this information by third parties or estimating it using models. Biases can make their way into the data used to train AI systems, amplifying stereotypes and leading to harmful consequences. Research has shown that state-of-the-art image-classifying AI models trained on ImageNet, a popular dataset containing photos scraped from the internet, automatically learn humanlike biases about race, gender, weight, and more. Countless studies have demonstrated that facial recognition is susceptible to bias. It’s even been shown that prejudices can creep into the AI tools used to create art, potentially contributing to false perceptions about social, cultural, and political aspects of the past and hindering awareness about important historical events. Casual Conversations, which contains over 4,100 videos of 3,000 participants, some from the Deepfake Detection Challenge, aims to combat this bias by including labels of “apparent” skin tone. Facebook says that the tones are estimated using the Fitzpatrick scale, a classification schema for skin color developed in 1975 by American dermatologist Thomas B. Fitzpatrick. The Fitzpatrick scale is a way to ballpark the response of types of skin to ultraviolet light, from Type I (pale skin that always burns and never tans) to Type VI (deeply pigmented skin that never burns).  Facebook says that it recruited trained annotators for Casual Conversations to determine which skin type each participant had. The annotators also labeled videos with ambient lighting conditions, which helped to measure how models treat people with different skin tones under low-light conditions. A Facebook spokesperson told VentureBeat via email that a U.S. vendor was hired to select annotators for the project from “a range of backgrounds, ethnicity, and genders.” The participants — who hailed from Atlanta, Houston, Miami, New Orleans, and Richmond — were paid. “As a field, industry and academic experts alike are still in the early days of understanding fairness and bias when it comes to AI … The AI research community can use Casual Conversations as one important stepping stone toward normalizing subgroup measurement and fairness research,” Facebook wrote in a blog post. “With Casual Conversations, we hope to spur further research in this important, emerging field.” In support of Facebook’s point, there’s a body of evidence that computer vision models in particular are susceptible to harmful, pervasive prejudice. A paper last fall by University of Colorado, Boulder researchers demonstrated that AI from Amazon, Clarifai, Microsoft, and others maintained accuracy rates above 95% for cisgender men and women but misidentified trans men as women 38% of the time. Independent benchmarks of major vendors’ systems by the Gender Shades project and the National Institute of Standards and Technology (NIST) have demonstrated that facial recognition technology exhibits racial and gender bias and have suggested that current facial recognition programs can be wildly inaccurate, misclassifying people upwards of 96% of the time. Beyond facial recognition, features like Zoom’s virtual backgrounds and Twitter’s automatic photo-cropping tool have historically disfavored people with darker skin. Back in 2015, a software engineer pointed out that the image recognition algorithms in Google Photos were labeling his Black friends as “gorillas.” And nonprofit AlgorithmWatch showed that Google’s Cloud Vision API at once time automatically labeled a thermometer held by a dark-skinned person as a “gun” while labeling a thermometer held by a light-skinned person as an “electronic device.” Experts attribute many of these errors to flaws in the datasets used to train the models. One recent MIT-led audit of popular machine learning datasets found an average of 3.4% annotation errors, including one where a picture of a Chihuahua was labeled “feather boa.” An earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. Another computer vision corpus, 80 Million Tiny Images, was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.”  But Casual Conversations is far from a perfect benchmark. Facebook says it didn’t collect information about where the participants are originally from. And in asking their gender, the company only provided the choices “male,” “female,” and “other” — leaving out genders like those who identify as nonbinary. The spokesperson also clarified that Casual Conversations is available to Facebook teams only as of today and that employees won’t be required — but will be encouraged — to use it for evaluation purposes. Exposés about Facebook’s approaches to fairness haven’t done much to engender trust within the AI community. A New York University study published in July 2020 estimated that Facebook’s machine learning systems make about 300,000 content moderation mistakes per day, and problematic posts continue to slip through Facebook’s filters. In one Facebook group that was created last November and rapidly grew to nearly 400,000 people, members calling for a nationwide recount of the 2020 U.S. presidential election swapped unfounded accusations about alleged election fraud and state vote counts every few seconds. For Facebook’s part, the company says that while it considers Casual Conversations a “good, bold” first step, it’ll continue pushing toward developing techniques that capture greater diversity over the next year or so. “In the next year or so, we hope to explore pathways to expand this data set to be even more inclusive with representations that include more geographical locations, activities, and a wider range of gender identities and ages, the spokesperson said. “It’s too soon to comment on future stakeholder participation, but we’re certainly open to speaking with stakeholders in the tech industry, academia, researchers, and others.”"
https://venturebeat.com/2021/04/08/gitpod-nabs-13m-for-cloud-based-open-source-software-development-platform/,Gitpod nabs $13M for cloud-based open source software development platform,"Gitpod, a cloud-based open source development environment, today revealed it has raised $13 million in a round of funding. The German startup also introduced a handful of new features, including native support for Microsoft’s Visual Studio Code editor. The raise comes amid a boom in activity in the browser-based coding sphere, as developers move away from local development environments to the collaboration-friendly cloud — particularly important in a world that has rapidly transitioned to remote work. Moreover, local development can cause problems when it comes to testing performance and security because not everyone has the same technological setup as the developer. Moving to the cloud helps circumvent many of these problems. “Developers are automating the world, yet they waste a lot of precious energy manually setting up and maintaining development environments,” Gitpod CEO Sven Efftinge told VentureBeat. “Millions of developers are slowed down on a daily basis with tedious tasks to get into a productive state while also facing annoying ‘works-on-my-machine’ problems. Our purpose is to remove all friction from the developer experience. This makes everyone always ready to code and software engineering more collaborative, joyful, and secure.” Gitpod broadly adheres to a similar ethos as continuous integration (CI), a popular software engineering practice that involves automatically merging code changes from multiple developers working on the same project. CI is all about ensuring that developers are committing smaller changes more frequently and shipping new code and fixes more quickly. From Gitpod’s perspective, the ethos basically means that it “listens” to changes within a git repository and prebuilds the source code whenever someone pushes a change to it — these prebuilds, according to Efftinge, are “key to preparing dev environments that are truly ready to code.” “We invented prebuilds so application code, configuration, and infrastructure can all be stored as machine-executable code in your git repositories and applied to dev environments automatically and continuously,” he said. “We are preparing your whole dev environment even before you start. Only then, you are always ready to code with a single click.” This also highlights a licensing limitation of Gitpod’s open-core open source model, as its free self-hosted offering only includes limited prebuild times. Gitpod works with all the main git platforms, including GitHub, GitLab, and Bitbucket, allowing developers to spin up a server-side (i.e. not local) development environment from any repository in just a few seconds. This includes the IDE (integrated development environment) and all the related tools and dependencies needed to run the project, including compilers, interpreters, runtimes, build tools, databases, and application servers. In short, Gitpod enables developers to start coding immediately, bypassing the local setup and maintenance process entirely. Cloud-based coding environments aren’t exactly new, with the likes of Codenvy — which was acquired by Red Hat four years ago — built on the Eclipse Che open source cloud IDE. More recently, we’ve seen a slew of cloud-based developer tools, including GitHub’s Codespaces, which launched in early access last year and is similar to Gitpod in many ways. Then there’s CodeSandbox, which raised $12.7 million in October to help developers create a web app development sandbox in the browser; Replit, a browser-based IDE built for cross-platform collaborative coding that raised $20 million in February; and CoScreen, which exited stealth last month with $4.6 million in funding to bring multi-user screen sharing and editing to remote engineering teams. Not all of these are exactly the same proposition as Gitpod, but they demonstrate that development environments are shifting away from “local.” Gitpod’s decision last August to release its platform under an open source AGPL license was a big move for the company, one that afforded developers more freedom to deploy Gitpod however they want, whether through a SaaS subscription managed and hosted by Gitpod or self-hosted on Kubernetes, Amazon’s AWS, or Google Cloud Platform. And a native integration with GitLab announced late last year will only serve to deepen its appeal. But Gitpod’s big pitch is that it’s not purely focused on IDE — it’s about automating developer environments in the cloud. The company is currently piloting a feature that allows users to benefit from Gitpod while working with third-party IDEs such as GoLand or IntelliJ and connect to Gitpod containers from their local environment. “We built Gitpod in a way that its architecture is scalable and it can work with other IDEs as well,” Efftinge said. “The feature is currently still in beta, but [it’s] important to understand our future direction.” It’s worth noting that Gitpod’s target market is developers, so embracing open source makes a great deal of sense. Developers, after all, play a big part in driving companies’ software buying decisions. “Making it open source builds trust and allows users to become contributors as well, or at least take part in the development process,” Efftinge said. “From a business perspective, buying power in companies shifts toward the individual engineer — for Gitpod to be successful, we have to win the hearts and minds of developers.” Founded out of Germany in 2019, Gitpod had previously raised $3 million in funding. Its latest $13 million cash injection was spearheaded by General Catalyst, with participation from Speedinvest, Crane Venture Partners, and Vertex Ventures. Gitpod claims some 350,000 users, including developers from major businesses such as Google, Amazon, Facebook, Uber, Intel, and GitLab, though Gitpod didn’t confirm whether the companies are paying customers or not. “What we can say is that all of those companies have projects where they use Gitpod to streamline their development workflows for either their own developers or for external contributors,” Efftinge said. Alongside its funding, Gitpod also announced today that it now supports Docker and sudo privileges (a Linux program to give temporary root privileges to specific users), which means developers can now run Docker in their workspaces. And Microsoft’s Visual Studio Code will now also work in Gitpod natively. “You get exactly the same editing experience that you would get if you have VS Code installed locally,” Efftinge said."
https://venturebeat.com/2021/04/08/immutable-x/,Immutable X debuts as marketplace platform for NFT games,"Immutable X has launched today as a platform for games with nonfungible tokens (NFTs). That means that those games will have uniquely identifiable digital items that players can earn or buy or sell, allowing the players to own the items permanently. And Immutable X has created a marketplace for players in games such as Gods Unchained to buy and sell the items they have collected. Immutable X, still in its alpha stage, can be a blueprint for other blockchain games, creator marketplaces, traders, and digital economies. The platform’s mainnet will be open to Gods Unchained players, so they can use a marketplace for buying and selling. Immutable X is the brainchild of Immutable, an Australian game team that runs the NFT trading card game Gods Unchained. Gods Unchained is an important NFT game, as it is built by a 40-person development team headed by Chris Clay, the former director of Magic the Gathering: Arena. Gods Unchained is a “play to earn” game, where players can earn collectibles over time, said Immutable founder Robbie Ferguson in a recent interview with GamesBeat. And they can make money by trading those collectibles, including the unique NFTs which can be proven by the blockchain, the secure digital ledger technology, to not be copies. Ferguson said that the move will help accelerate the mass adoption of NFTs in games. In the past few months, NFTs have exploded in other applications such as art, sports collectibles, and music. NBA Top Shot (a digital take on collectible basketball cards) is one example. Published by Animoca Brands and built by Dapper Labs, NBA Top Shot has surpassed $100 million in sales, five months after going public to a worldwide audience. And an NFT digital collage by the artist Beeple sold at Christie’s for $69.3 million. Investors are pouring money into NFTs, and some of those investors are game fans. “With today’s Immutable X Alpha release, we’ve made this vision reality. Any NFT can now be traded, earned, shared, gamed, and collected completely gas-free (gas fees are associated with creating the NFT) on Ethereum,” said Ferguson. In 2020, Immutable found the solution on the crypto frontier with StarkWare, which tapped the benefit of using the Ethereum cryptocurrency and its security without incurring huge fees. Immutable X is built on top of Starkware’s “Layer 2 scaling” technology. The bottom line is that users don’t have to trust in Immutable lasting permanently in order to keep owning their NFTs. They can just trust in Ethereum. Immutable X’s mainnet is now available as the first Layer 2 solution for NFTs on Ethereum, the company said. “At the end of the day, this security is the whole point,” Ferguson said. “Otherwise, you might as well just make a new centralized database. “What our company has become focused on is to scale these games and these applications in a way that is best for users, and ultimately, still decentralized, while still being super easy for mainstream applications to use. That’s why we decided to build Immutable X. We spent a very long time searching for a scaling solution. We’ve got to make the proposition of NFT ownership available to everyone.” Other solutions to Ethereum are creating alternative, faster cryptocurrencies with different methods of reaching a consensus. But these alternatives aren’t as popular as Ethereum. Another solution is to create a side chain, with a different kind of processing for transactions. But Ferguson said those solutions can fail because their security isn’t still as strong as Ethereum’s. If the security fails, then so does the authenticity of the NFT, and that would be disastrous, Ferguson said. “Ethereum is the leading public blockchain in terms of network effects,” Ferguson said. “We decided to go with a true Layer 2 solution. We use Ethereum for everything we do. We’re just compressing the data on it by zero-knowledge proofs [a verification technique], which allows us to reach really high levels of scale.” Last week, Immutable added some Japanese blockchain games to help build momentum for Immutable X, Ferguson said. Gumi, the publisher of Brave Frontier, invested in Double Jump.Tokyo, and MCH spun out of Double Jump.Tokyo. Gumi is the largest shareholder of Double Jump.Tokyo. MCH+ is the name of the partnership between Double Jump.Tokyo and MCH. Together, Double Jump.Tokyo and MCH+ are taking four games to Immutable X: My Crypto Heroes, My Crypto Saga, Crypto Spells, and Brave Frontier Heroes. The transition will be live as Immutable X comes online soon, and once it is done, gamers will be able to truly own the one-of-a-kind items that they buy in games. My Crypto Heroes has lifetime sales of 26,000 Ethereum, or $41 million. NFTs use blockchain technology, the secure and transparent digital ledger behind cryptocurrencies such as Bitcoin and Ethereum, to uniquely identify digital objects. Blockchain can verify the authenticity of something by spreading the verification out among a bunch of computers, a validation process known as reaching consensus, so that you can verify the authenticity and rarity of a particular NFT. But in the past, developers and creators creating NFT games faced a lot of friction to unlock the network effects of a scaled marketplace and economic potential of secondary NFT markets. Those parties faced high “gas fees” per mint or trade. Ferguson said the integration will allow Immutable X users to save on gas fees (the fees for doing transactions on the blockchain) and enjoy more attractive game mechanics. Under the partnership, Immutable will be powering the minting and trading of all games under Double Jump.Tokyo and MCH+ through its scaling protocol for NFTs on Ethereum, Immutable X. Ethereum itself enables NFTs, but it is slow because it handles only about 15 transactions per second and the transaction fees (known as gas fees) can be enormous compared to the price of the digital item. To deal with those problems, Immutable created Immutable X, a Layer 2 scaling solution that sits on top of Ethereum but can handle 9,000 transactions per second and zero gas fees. It also has instant trade confirmation and is designed to make the world of NFTs less opaque, Ferguson said. Ferguson said Immutable X is secure as a scaling solution because it relies on the Ethereum security system itself to guarantee transactions. That’s important because someone could attack the consensus system of a blockchain to try to wrest control of an item. Attacking Ethereum is a lot harder than attacking other solutions such as side chains, Ferguson said. Ethereum has the strongest community around self-custody and decentralization. But it does have its downsides. Ethereum transactions use a lot of energy for verification, and people have pointed out that could prove costly for the environment over time. Ferguson acknowledged that is a drawback, but he also said Ethereum and Bitcoin have a lot of mainstream support and are the strongest available cryptocurrencies. They could thus draw a mass market. Over time, Ferguson believes Immutable X will have a solution for energy usage. Immutable X is ensuring any NFT activity on the protocol will be completely carbon neutral by purchasing carbon credits to offset gas consumed on Ethereum. Ferguson said other projects on Immutable X include Double Jump.Tokyo Inc / MCH+ (My Crypto Heroes, Crypto Saga, Crypto Spells, Brave Frontier Heroes, Chojo), Mintable (NFT marketplace), SuperfarmDAO (decentralized finance NFT farm), Epics GG (collectibles), Illuvium (auto battler role-playing game), Guild of Guardians (mobile RPG), and Crypto Assault (strategy MMO). Rivals include Enjin and Dapper Labs’ Flow. Immutable was started three years ago as Fuel Games, and it saw success with its second game, Gods Unchained, which is a kind of digital Hearthstone. The company now has 90 employees, and it raised $15 million about 18 months ago. Gods Unchained has tens of thousands of players playing every week — with more than 2.5 million matches played — and it has grown tenfold over the past quarter, Ferguson said. That has been enough to make Immutable profitable. But Gods Unchained hasn’t been able to scale up so far, and that’s why Immutable built Immutable X, Ferguson said. As to how Immutable can get millions of players, Ferguson said that the Japanese games have large numbers of players — as many as a million — by comparison. That’s why their move onto Immutable X is so important. “The games are going to get there,” Ferguson said. “The fact that the revenues are here means that this is the correct value proposition for game developers. With the NFT craze with art, that is going to be tremendously good.”"
https://venturebeat.com/2021/04/08/securiti-releases-ai-powered-data-privacy-and-security-platform-to-provide-unified-controls/,Securiti releases AI-powered data privacy and security platform to provide unified controls,"The proliferation of network configurations has added new layers of complexity to digital security. As enterprises embrace approaches like multi-cloud deployments and APIs, it can be more difficult than ever to maintain a broad view of the most sensitive data and detect attacks. But for Securiti CEO Rehan Jalil, the evolution in network architecture represents a big opportunity for companies to embrace a more unified approach to data management and security. To address this emerging challenge, Securiti today announced the release of its new privacy and control platform. The goal is to unify data security, privacy, governance, and compliance across all types of networks. This is particularly critical as more sensitive data moves online, creating even more enticing targets for hackers. “Data brings two big O’s,” Jalil said. “One big O is the opportunity, and we all know about it. But the other big O is the obligations. And those obligations are tied to making sure that you can keep it secure.” The company also announced that Cisco had invested an undisclosed amount as part of a security partnership. In a blog post, Cisco Investments director Prasad Parthasarathi wrote that Securiti had made big strides toward overcoming the fragmented approach to security. The deal is also part of Cisco’s growing investment in security. “The issues of security, privacy, governance, and compliance for sensitive assets and data have been addressed in separate silos,” he wrote. “Legacy architectures have proven to be ill-equipped to handle all these needs, particularly at today’s hyperscale environments with data across hundreds of different types of data systems.” Rather than taking a piecemeal approach to different aspects of data security, the company set out two years ago to build a broad platform, Jalil said. He described the company’s approach as creating a “cyber mesh of a perimeter of security around the data wherever it exists.” With the expanding number of environments, data is becoming increasingly distributed, making it a bigger challenge to manage security, privacy, and compliance. This has given rise to a market of solutions known as Data SPaC. “Often these things are done in silos,” Jalil said. “But our company provides all three capabilities.” The key to enabling that is the company’s sensitive data intelligence (SDI) technology, which helps automate the creation of the security perimeter while building a framework around how data is being used internally. Using artificial intelligence, the SDI discovers, classifies, tags, and catalogs sensitive data that can be found scattered across all cloud environments and on-premises. Once the company has a unified view of that data, Securiti’s tools can help do everything from monitoring to protection and remediation. In addition to catching Cisco’s eye, Securiti has landed on CB Insights’ list of the 100 most innovative AI companies. The company has also raised $81 million in venture capital to date."
https://venturebeat.com/2021/04/08/the-b2b-ecommerce-boom-will-continue-beyond-the-pandemic/,The B2B ecommerce boom will continue beyond the pandemic,"Presented by Amazon Business The last few years have seen a rapid rise in B2B ecommerce adoption across industries. Buyers are shifting purchasing to digital channels to streamline operations and gain access to millions of sellers, while sellers seek new customers and greater efficiency. While buyers’ and sellers’ digital transformations during the pandemic spurred eprocurement adoption, the rise in digitization preceded COVID-19 — and will outlast it. B2B ecommerce has empowered businesses by boosting efficiency and providing a more diverse supplier base, and it will continue to do so as more and more businesses shift online. Today’s purchasing leaders play a critical role in their organizations’ success. They’re being asked to essentially reinvent procurement to free up more time and resources to go directly to support their organizations’ core goals and missions. The disruptions of 2020 shone a light on the importance of efficient, streamlined procurement processes that can be adapted to meet unexpected challenges. In this context, it’s become clear that digital purchasing offers a level of agility, resiliency, and efficiency that simply isn’t possible with traditional manual processes. For example, prior to the pandemic, Exxon Mobil consolidated thousands of transactions into a new procure-to-pay system that allowed employees to purchase supplies from a B2B online store. When the pandemic hit and new supply needs surfaced, the fact that Exxon Mobil had already automated routine purchases enabled their teams to get what they needed quickly and maintain operations globally, while keeping costs in check. Shifting to eprocurement yields efficiency gains for buyers of all sizes, especially when online stores integrate with their internal accounting systems. For large enterprise and government buyers, purchasing through a multi-seller online store that integrates with an enterprise resource planning (ERP) solution enables real-time expense management and can help reduce the overall cost of operations. Small buyers can see efficiency gains from similar integrations that automatically import and categorize purchases from online stores into bookkeeping software, eliminating the need for tedious manual reconciliation. Once organizations understand the efficiencies possible with eprocurement, few if any return to old offline processes. While buyers of all sizes are going digital, the shift has been particularly notable for large enterprise and government buyers, which represent some of the fastest-growing segments in B2B ecommerce today. Digital purchasing is attractive to these large buyers in part because it helps them meet goals around diversifying their supplier bases. For government buyers in particular, it can be important to purchase from local sellers or sellers that possess certain nationally recognized certifications, such as those for small, woman-owned, minority-owned and LGBT-owned businesses. However, sellers with these qualifications sometimes struggle to connect with large buyers through traditional means. For example, a 2020 study by Censeo Consulting Group found that 93% of small businesses experienced “significant barriers” to reaching government buyers. Buyers can use a multi-seller online store with in-depth seller profiles to easily find sellers with desired characteristics, whether that’s a diversity certification or location in a local zip code. Reporting tools help buyers track their spending with sellers in different categories. By enabling buyers to manage seller relationships at scale, features like these also help large enterprises and government entities work with a wide array of small businesses more easily. We’ve already seen these trends accelerate growth for many small, diverse and local sellers. For example, certified Black- and veteran-owned small business Aldevra raised its sales on Amazon Business 315% in part by leveraging its diversity certifications online. Less than five years after joining an online store, the medical and food service equipment supplier is now a contractor for multiple state and local governments and government agencies including the U.S. Department of Defense. The shift to eprocurement benefits both large buyers and small, diverse sellers — and that’s likely to fuel continued growth for a long time. At Amazon Business, we’ve witnessed the accelerating growth of B2B ecommerce firsthand. Within a year of our 2015 launch, we reached 1 million customers and $1 billion in sales. Continuing that momentum, today we are serving more than 5 million customers and have reached $25 billion in worldwide annualized sales. The rise in B2B e-commerce adoption is not just a pandemic trend. Buyers and sellers alike are embracing digitization because it serves their long-term goals for savings, efficiency and supplier diversification. In the future, online stores will continue to innovate and evolve, unlocking new benefits for users — and powering continued growth in 2021 and beyond. Alexandre Gagnon is VP of Amazon Business Worldwide. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/08/tines-which-helps-enterprise-security-teams-automate-repetitive-workflows-raises-26m/,"Tines, which helps enterprise security teams automate repetitive workflows, raises $26M","As businesses seek ways to address the heavily reported technical skills gap, automation is playing an increasingly big role in their efforts. This is perhaps no more evident than it is in the cybersecurity fray, which has a longstanding skills shortage that is only widening, and which has been embracing AI and other automated tools. It’s against that backdrop that Tines is setting out to grab a slice of the $153 billion cybersecurity market, with a rules-based no-code platform that helps enterprise security teams automate repetitive workflows. To help, the company today announced a fresh $26 million investment led by Addition, with participation form Accel and Blossom Capital. Tines can perhaps be best summarized as something like IFTTT for cybersecurity teams, insofar as it enables personnel to pre-build what the company calls “automation stories” that trigger specific steps whenever a particular event occurs — for example, this could be to carry out a log and threat intelligence search to figure out whether a security alert requires further action. For context, most enterprise-level companies employ security professionals that are dedicated to spotting and responding to cyberattacks. They will typically use various automated tools to help them, though these tools can create a lot of noise and false alarms. And this is where Tines shines, as it helps teams automate many of the manual steps that follow after an alert is triggered, freeing security personnel to focus on more mission-critical work. A drag-and-drop interface also opens things up to non-coding team members such as security analysts. Tines is built upon a visual storyboard where users can access seven agents, or “action types,” to construct their automations. They can be combined in any number of ways to create complex automation flows — for example, the webhook agent and email agent can be configured so that the relevant on-call personnel is automatically notified whenever a change is made to a GitHub repository. “When we launched Tines, our core product principal was that every single enterprise process could be broken down and automated using just seven types of action,” Tines cofounder Eoin Hinchy told VentureBeat. “With Tines, you take the process you want to automate, break it into steps, and decide which of these seven action types is best suited to perform that step. This makes the platform extraordinarily easy to learn — there are only seven things you need to know — and once you know them, you can automate anything.” Tines also serves up a bunch of prebuilt sample agent configurations to support some of the more common requirements — for example, to automatically send a Slack message whenever a specific type of security alert is triggered. Tines sports dozens of integrations out of the box, including CrowdStrike, Snyk, PagerDuty, Gmail, Box, Datadog, Okta, Facebook, Freshdesk, and more. However, Tines can in fact be integrated with any tool that has an API, something Hinchy says separates it from other players in the security orchestration, automation, and response (SOAR) space — its users are not restricted to the integrations it has created. The Dublin-based startup has amassed an impressive roster of clients since its founding just three years ago, including Box, Databricks, Sophos, OpenTable, Canva, and soon-to-be Okta-owned Auth0. At the time of its series A raise back in 2019, Hinchy said that while security would remain its core focus, some of its customers were already using the platform to support other workflows across IT, developer operations (DevOps), and even HR, raising the possibility that Tines could broaden its horizons in the future. Some 16 months on, Tines for now remains focused on the security realm. But with a fresh $26 million in the bank, the company is well-financed to expand its product “beyond the scope of security automation” over the next year or so. According to Hinchy, roughly one-quarter of its customers are using Tines outside of security, in places where “there’s a mission critical process that’s complex and time consuming,” he said. “Tines is flexible enough to support any workflow.” While Hinchy added that they were in “no hurry” to officially open the floodgates to the broader enterprise automation market, it will meander down that road. “We will stay focused on security for the next year while being opportunistic and selectively working with teams and companies in other parts of the enterprise,” he said. A typical example outside the security sphere might involve removing user-generated content that breaches company policy, updating help desk tickets, and employee onboarding. It’s worth noting that some of these use cases are supported by existing tools, but if a company is already paying to use Tines to orchestrate their security, then they may as well use it to automate other workflows across their business. Or to liven things up in a virtual office environment, they could also use Tines to randomly send dad-jokes to their colleagues as this user did — replete with SMS alerts.  In terms of pricing, Tines offers a basic free “community” edition, while its main hosted cloud product starts at $2,999 per month with support for 10 users and 250 actions. Elsewhere, the Tines platform can also be deployed on any public cloud or on-premises. “Tines can run anywhere,” Hinchy said. “Although the most common deployment mechanism for Tines is our hosted cloud, we have customers that run in Google Cloud, Azure, DigitalOcean, AWS, bare metal, private datacenters, and even FedRAMP-compliant facilities.” Tines said that it more than tripled its revenue last year. While it’s not clear how much of that can be directly attributed to the pandemic, Hinchy thinks that the increase in demand he’s seen for Tines is down to the fact that companies are trying to be more efficient. “They want to use their resources more intelligently and reduce margin for human error,” he said. “This has never been more true than in the last 12 months.” Moreover, as remote work has emerged as the “new normal” for millions of workers, this has perhaps created a more competitive landscape for technical talent. Jobs that are more rewarding and which involve fewer tedious tasks are likely to be more appealing, which is where Tines can help. “The sudden rise in remote, flexible work means that top talent can work for any company from almost anywhere,” Hinchy said. “As a result, we’ve had conversations with an increasing number of CISOs and CIOs that want to empower staff with the tools they need to automate their manual workloads end-to-end, allowing them to refocus on higher-impact, more rewarding, engaging projects. Ultimately reducing risk of burnout while also increasing job satisfaction.”"
https://venturebeat.com/2021/04/08/tasktop-nabs-100m-to-turn-devops-metrics-into-visualizations-at-scale/,Tasktop nabs $100M to turn DevOps metrics into visualizations at scale,"Value stream management (VSM) platform Tasktop today announced that it raised $100 million, bringing its total raised to over $129 million. The company says it plans to use the funding to accelerate growth while expanding the size of its customer base. As traditional businesses pour billions into digital transformation initiatives, they often struggle with the complexity of the teams, tools, and metrics at the core of those investments. Technical leaders deeply understand the software development process and business leaders know the investment strategies, but the two aren’t always aligned. In a study, Geneca found that 75% of executives surveyed admitted that their projects were either “always” or “usually” doomed right from the start. Vancouver, Canada-based Tasktop, which was founded in 2007, offers a VSM platform designed to reduce time to market and increase the velocity of software development. Sitting above the software development toolchain, Tasktop integrates with software development tools like Jira Software, ServiceNow, Azure DevOps, and more to allow organizations to see potential blockers.  VSM was born out of the frustration that most enterprises aren’t adequately adaptive. According to Forrester, only 16% say that they can release software more than once a month. For Tasktop’s part, the company asserts that VSM allows organizations to break down silos as well as identify and remove bottlenecks, eliminate waste, and accelerate delivery. Tasktop’s platform overlays the value stream to provide abstractions, visualizations, and diagnostics that measure all types of software delivery work. Connectors let customers send work between different dev tools, eliminating duplicate data entry and automating traceability. And Tasktop’s testing regimen runs 500,000 tests daily over 300 tool versions to ensure they work properly, handling tooling and API changes to minimize outages and delays. Coinciding with the new funding, Tasktop this morning launched a dashboard within its Tasktop Viz product — VSM Portfolio Insights — that rolls up analytics generated at the individual product value stream level to the executive plane. The dashboard presents consolidated insights into the performance, quality, value, and impact of delivery, including:  Since the birth of VSM, the market category has grown exponentially compared with the longer-tail development of agile and DevOps. The expansion speaks for itself with players like ServiceNow, IBM, Digital.ai, and of course Tasktop joining the fray. Last December, Tasktop announced record year-over-year growth with a 30% uptick in both revenue and customers. The company now claims to serve leading brands, including over half of the Fortune 100. Sumeru Equity Partners led Tasktop’s latest funding round, a strategic investment. Management and existing investors also participated."
https://venturebeat.com/2021/04/08/automated-corporate-spend-management-platform-ramp-nets-115m/,Automated corporate spend management platform Ramp nets $115M,"Corporate credit cards are pretty much a dime a dozen, as long-established players like American Express vie with newer VC-backed players such as Brex, Divvy, and Moss. To stand out against established players like Amex, it has become increasingly important to go beyond offering a simple piece of plastic and differentiate under the hood. Against this backdrop, New York-based Ramp launched in early 2020 as a comprehensive corporate credit card and automated spend-management platform that bypasses convoluted or duplicitous reward programs with the promise of saving companies money. To further this goal, Ramp today confirmed rumors that it has raised $115 million in fresh funding from notable names including Stripe and Goldman Sachs at a valuation of $1.6 billion. “Ramp’s unique differentiator is our savings-focused approach to spend management,” cofounder and CEO Eric Glyman told VentureBeat. “It is a well-known fact that complex rewards programs offered by incumbent corporate cards actually encourage businesses to spend more — ultimately hurting their bottom line. So we designed a corporate card aligned with the interests of our customers, which is time and money savings.” Ramp is in many ways a classic corporate card: Businesses distribute them to employees who use them for job-related purchases, and then everything is charged directly back to the company. Businesses can get an unlimited 1.5% cash back on their purchases, while Glyman says various automated smarts can save finance teams “hundreds” of hours each month. And it’s here, on the spend-management software side, that Ramp wants to make its mark. Admins and managers can set individual spend limits on a per-employee level, stipulate spending rules based on company policies and more. Using machine learning techniques, Ramp can identify duplicate spending — for example, if different teams are paying for the same software licences when a single licence should cover the whole organization. It can also flag “troublesome spend patterns,” surface partner rewards that might have gone unused, and even negotiate better rates with vendors, according to Glyman. More broadly, Ramp leverages ML to power its savings insights feature, which combines transaction data with customer feedback to “regularly deliver relevant and tailored savings insights for each customer,” according to Glyman. Companies can also automate many of the laborious processes involved in managing their accounts, such as chasing missing receipts for expenses — they set a policy and Ramp automatically messages the employee to ask them to submit a photo of their receipt. Ramp then uses optical character recognition (OCR) to verify and match the receipts to transactions automatically. The pandemic has transformed entire industries, and the corporate expense sphere is no different. Travel booking and management company TripActions essentially had to pivot last year to a broader spend management platform incorporating travel and expenses, and a few months back it raised $155 million at a $5 billion valuation. Ramp is a corporate credit card, but it also enables businesses to manage all of their corporate spending, powered by direct integrations with accounting software such as Quickbooks, NetSuite, Xero, and Sage Intacct. “The biggest problem Ramp solves is financial software consolidation,” Glyman said. “As organizations grow, the tools they use to manage payroll, out-of-pocket expenses, [and] AP/AR creates burdensome manual processes for finance teams and a fractured view of companywide spend for executives. Ramp combines corporate cards, expense management software, reimbursements, vendor management, and reporting and analytics — all in one free package.” The company said transaction volume on Ramp has grown fourfold over the past six months and is now at a run rate of $1 billion annually. But perhaps more interestingly, it said one-third of its customers, which include billion-dollar businesses such as Ro, Better, ClickUp, and Applied Intuition, switched from American Express, and more than 90% ditched existing spend management tools such as Expensify and Concur for Ramp’s offering. Like many others in this space, Ramp offers the card for free, and there are no transaction fees or interest. Ramp gets a cut from banks’ interchange fees, and it also plans to eventually make money from its own software platform as part of a SaaS subscription. “Ramp is currently free of charge — we are currently exploring seat-based pricing for our software offering,” Glyman said. “[There is] no set date on when we will launch new pricing.” Ramp has now raised a total of $320 million in debt and equity financing since it was founded in 2019. Its latest cash injection was spearheaded by D1 Capital Partners and Stripe, with participation from Goldman Sachs, Founders Fund, Coatue Management, Thrive Capital, Redpoint Ventures, Box Group, Neo, and Contrary Capital. Glyman said the company already has plans for allocating the funds. “We’re laser-focused on three areas of product development this year — more sophisticated workflow capabilities for mid-market and enterprise customers, payment consolidation for a wider swath of business expenses, and strategic partner integrations that will deliver customers a seamless experience across their IT, finance, and HR software stack,” he said."
https://venturebeat.com/2021/04/08/trisalus-life-sciences-and-md-anderson-announce-strategic-research-collaboration-to-evaluate-treatment-of-solid-tumors/,TriSalus Life Sciences and MD Anderson Announce Strategic Research Collaboration to Evaluate Treatment of Solid Tumors,"  HOUSTON & DENVER & CHICAGO–(BUSINESS WIRE)–April 8, 2021– The University of Texas MD Anderson Cancer Center and TriSalus Life Sciences®, an emerging immuno-oncology company committed to transforming outcomes for patients with liver and pancreatic tumors, today announced a strategic research collaboration to evaluate the treatment of tumors of the pancreas and liver by integrating interventional delivery of SD-101, an investigational toll-like receptor 9 (TLR9) agonist, in combination with checkpoint inhibition immunotherapy. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210408005178/en/ Under the agreement, MD Anderson and TriSalus will collaborate on studies evaluating the administration of investigational SD-101 intravascularly via TriSalus’ Food and Drug Administration (FDA) cleared, proprietary Pressure-Enabled Drug Delivery™ (PEDD™) technology across a range of liver and pancreatic solid tumors. The initial study will focus on liver metastases from uveal melanoma, followed by studies focused on metastatic disease from pancreatic ductal adenocarcinoma and colorectal cancer. Programs for hepatocellular carcinoma and locally advanced pancreatic ductal adenocarcinoma also are under development. TriSalus will provide funding and technology for the studies. “We’re pleased to collaborate with MD Anderson in pursuit of our collective goal to improve outcomes for patients with tumors of the liver and pancreas. The goal of these studies is to augment the potential of existing therapies through novel drug delivery technology and to investigate the strategic modulation of immune microenvironments with investigational candidate SD-101,” said Steven Katz, M.D., Chief Medical Officer, TriSalus Life Sciences. “Collaborations such as this are an integral part of our development strategy to evaluate treatments to help overcome the challenges inherent to solid tumors and enable a broader population of cancer patients to benefit from immunotherapy.” SD-101 has been evaluated in phase 2 studies in advanced cutaneous melanoma and head and neck cancer.1,2 Early data suggests SD-101 augmented responsiveness to checkpoint inhibitors through stimulation of innate immune cells, along with favorable programming of the T cell population.3 The planned studies under the research collaboration are intended to deliver SD-101 deep into the vasculature of solid tumors using PEDD™, a technique not previously possible using standard delivery approaches.4 Solid tumors continue to represent one of the single biggest hurdles to successful cancer treatment.5 High levels of pressure inside solid tumors prevent the delivery of oncology therapeutics, with less than 1% of therapy penetrating solid tumors in some circumstances.6,7 TriSalus developed PEDD to deliver immuno-oncology therapeutics directly into the vasculature of solid tumors. “Tumors in the pancreas and liver are notoriously difficult to treat effectively, and these patients need new therapeutic options. Our collaboration with TriSalus provides a unique opportunity to evaluate immunotherapy in combination with a novel delivery approach,” said Sapna Patel, M.D., associate professor of Melanoma Medical Oncology at MD Anderson. “We look forward to our work together to advance new treatments aimed at improving clinical outcomes and the lives of our patients.” About TriSalus TriSalus Life Sciences is a revenue generating, emerging immuno-oncology company dedicated to developing immunotherapy treatments for liver and pancreatic tumors using novel delivery technologies to improve patient outcomes. TriSalus intends to pursue multiple solid tumor indications with investigational SD-101 and acquire other immuno-oncology agents to combine with its proprietary Pressure-Enabled Drug Delivery technology for the administration of therapeutics intravascularly into visceral organ solid tumors. In combination with checkpoint inhibitors, TriSalus’ focus is to reprogram the dominant immunosuppressive cell population in liver and pancreatic tumors. This innovative approach in development has the potential to leverage multiple mechanisms that can work together with the goal to overcome inherent immune suppression within the solid tumor microenvironment. For more information, please visit www.trisaluslifesci.com. About MD Anderson The University of Texas MD Anderson Cancer Center in Houston ranks as one of the world’s most respected centers focused on cancer patient care, research, education and prevention. The institution’s sole mission is to end cancer for patients and their families around the world. MD Anderson is one of only 51 comprehensive cancer centers designated by the National Cancer Institute (NCI). MD Anderson is ranked No.1 for cancer care in U.S. News & World Report’s “Best Hospitals” survey. It has ranked as one of the nation’s top two hospitals for cancer care since the survey began in 1990 and has ranked first 16 times in the last 19 years. MD Anderson receives a cancer center support grant from the NCI of the National Institutes of Health (P30 CA016672).  View source version on businesswire.com: https://www.businesswire.com/news/home/20210408005178/en/ TriSalus Media and Investors Lisa DeScenzaVice President, Integrated CommunicationsLaVoieHealthScience(978) 395-5970ldescenza@lavoiehealthscience.com MD Anderson Media Clayton R. Boldt, Ph.D.(713) 792-9518crboldt@mdanderson.org"
https://venturebeat.com/2021/04/08/gupshups-chatbot-authoring-and-multichannel-messaging-platform-gets-100m-boost/,Gupshup raises $100M to expand its chatbot platform,"Gupshup, a messaging-focused customer engagement platform, today announced that it raised $100 million in funding from Tiger Global Management, valuing the startup at $1.4 billion. Gupshup says the proceeds will be put toward its go-to-market, product, and customer acquisition efforts as the company anticipates the close of a second, “significant” additional round. The ubiquity of smartphones and messaging apps — as well as the pandemic — have contributed to the increased adoption of conversational technologies. Fifty-six percent of companies told Accenture in a survey that conversational bots and other experiences are driving disruption in their industry. And a Twilio study showed that 9 out of 10 consumers would like the option to use messaging to contact a business. Gupshup, which was founded in 2014, offers a chatbot authoring service that allows developers to create, deploy, and publish chatbots across over 30 channels. The company claims to send more than 6 billion marketing, sales, and support messages between over 100,000 businesses and 300 million customers via Facebook Messenger, Telegram, Skype, Slack, and more per month.  India is one of Gushup’s largest markets. According to cofounder and CEO Beerud Seth, in recent years, business messaging has become more widespread there as text messages have begun to include hyperlinks and leverage newer IP-based messaging channels like WhatsApp, rich communications services, and Gupshup’s own Gushup IP (GIP). Gupshup pitches GIP as a “smarter,” “more conversational” messaging service that’s compatible with a range of mobile devices. It comes in two flavors, GIP Native (which embeds in native messaging apps) and GIP Widget (a device-agnostic chat widget), and features end-to-end encrypted messages with buttons and rich media like ecommerce flows, mini games, and cards. Gupshup also developed an on-device AI model for message classification and visualization. The model reads incoming messages and divides them into categories and sub-categories, after which it visualizes elements like amount and account number. Server-side AI models for natural language understanding field user queries sent to Gupshup’s chatbots. Gushup occupies a chatbot market that’s anticipated to be worth $142 billion by 2024, according to Insider Intelligence, up from $2 billion in 2019. Gartner predicts that over 50% of enterprises will spend more per annum on chatbot creation than mobile app development by this year. And Juniper Research expects that 75% to 90% of customer queries will be handled by chatbots within the next year.  Even before the pandemic, autonomous agents were on the way to becoming the rule rather than the exception, partly because consumers prefer it that way. According to research published last year by Vonage subsidiary NewVoiceMedia, 25% of people prefer to have their queries handled by a chatbot or other self-service alternative. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Perhaps reflecting the growing chatbot demand, Gushup had an annual run rate of around $150 million as of 2020. It’s expecting over 50% growth in for next few years. “The growth in business use of messaging and conversational experiences, transforming virtually every customer touchpoint, is an exciting secular trend,” Tiger Global Management partner John Curtius said in a statement. “Gupshup is uniquely positioned to win in this market with a differentiated product, a clear and sustainable moat, and an experienced team with a proven track record. In addition to its market leadership, Gupshup’s unique combination of scale, growth, and profitability attracted us.” This latest tranche brings San Francisco, California-based Gushup’s total raised to over $150 million. Propr to this in March 2010, the company, which has around 250 employees, closed a $12 million series D led by Globespan Capital Partners."
https://venturebeat.com/2021/04/08/canadian-banks-shrink-workforce-to-cover-big-tech-investments/,Canadian banks shrink workforce to cover big tech investments,"(Reuters) — Canada’s top banks are shedding workers for the second straight year, moving toward leaner operations to satisfy investors demanding returns on tens of billions of dollars that lenders have poured into new technologies. Five of Canada’s six biggest banks cut their workforces 4.4% from a year earlier to a combined total of 291,409 full-time equivalent employees as of January 31. That is down 5.2% from a peak in the third quarter of 2019. Despite growing optimism about a robust economic recovery, loan growth outside of mortgages has been stagnant due to the relatively slow pace of COVID-19 vaccinations in Canada and renewed lockdowns in some major cities. “It’s very difficult to grow” revenues, said Todd Johnson, chief investment officer at BCV Asset Management, which owns shares of all the big banks. Banks are likely to continue investing in technology at levels similar to the past few years, which will be “welcomed by investors as long as earnings and dividends continue to grow, and especially if tech investment displaces some labor costs,” he said. The pullback in headcounts follows combined quarterly year-on-year growth of 4% to 5% in 2018 and 2019 across the six big banks. The cuts have reduced efficiency ratios, or non-interest expenses as a proportion of revenues, by about 2 percentage points from a year ago at most banks, disclosures show. The phenomenon isn’t unique to Canada. U.S. and European banks last year joined Bank of Montreal and Canadian Imperial Bank of Commerce in announcing or resuming layoffs, with the former expected to shrink headcounts by an average of 5-10%. While job cuts at banks in other countries have included technology roles, Canadian lenders are still growing in this area because their digital shift has lagged. Toronto-Dominion Bank has been expanding its technology teams while redeploying employees from temporarily closed branches to other areas, CEO Bharat Masrani said in an interview. TD’s workforce has shrunk by about 0.7% from its peak in the fourth quarter of 2019, following quarterly growth of 4-6% over the prior year. “You should view this as the bank constantly adapting to evolving expectations,” Masrani said. TD declined to comment on its technology spending plans. Bank of Nova Scotia (Scotiabank), which has been divesting some international operations, and BMO, which has been working on improving efficiencies, have had the biggest year-on-year headcount reductions, 9.5% and 5.3%, respectively. Royal Bank of Canada, the country’s biggest lender, has been the only one to grow its workforce, by 1.9% from a year earlier, as it expands its wealth management divisions in the U.S. and Canada. A spokesperson said RBC continues to hire “selectively.” In February, CIBC executives said the bank had saved CA$800 million ($633.91 million) over the past five years by streamlining operations. It reinvested the funds in high-growth areas and accelerated technology spending. The other banks declined to comment. Much of the technology investment so far has gone into automating manual processes, such as enabling online mortgage applications and e-signing documents. Future investments will likely focus on beefing up cybersecurity, upgrading systems, and data and analytics, said Robert Colangelo, senior vice president for credit ratings at DBRS Morningstar. Headcounts are unlikely to “grind lower for years and years,” but they are expected to lag revenue growth, said Goodreid Investment Counsel portfolio manager Brian Madden, who estimates that lenders have invested a combined CA$10 billion annually in technology in the last few years. With labor the biggest part of non-interest expenses and the pandemic’s “unexpected turbo boost” to customer adoption of online banking, “most of the return on investment in tech spend is going to have to come from efficiency gains/headcount reductions,” he said."
https://venturebeat.com/2021/04/08/truelayer-raises-70m-to-build-the-worlds-most-valuable-open-banking-network/,TrueLayer Raises $70m to Build the World’s Most Valuable Open Banking Network,"  LONDON–(BUSINESS WIRE)–April 8, 2021– TrueLayer, Europe’s leading open banking platform, today announced it has secured a $70m Series D investment round led by new investor Addition. The latest raise reflects the growing demand for its open banking-based services and marks another significant milestone for TrueLayer on its mission to open up finance, building an open banking network that brings together payments, financial data, and identity to redefine how people spend, save, and transact online. Existing investors, including Anthemis Group, Connect Ventures, Mouro Capital, Northzone, and Temasek, also participated, with a significant increase to the company’s valuation. Additional investors in the round include Visionaries Club, Surojit Chatterjee (CPO Coinbase), Zack Kanter (CEO Stedi), Daniel Graf (ex-Uber, Google, Twitter) and David Avgi (ex-CEO SafeCharge, CEO UniPaaS). It brings the total investment to date in TrueLayer to $142m. The new funding will be used to fuel global expansion and accelerate the development of premium open banking-based services that will continue to drive innovation and revenue growth for clients. It will also be used to expand TrueLayer’s engineering, product and commercial teams to meet the increasing global demand for its open banking platform. TrueLayer’s API-first platform accounts for more than half of all open banking traffic in the UK, Ireland and Spain, processing billions of pounds in payments. It powers services for some of Europe’s fastest-growing brands, including Revolut, Trading 212 and Payoneer. TrueLayer has a market-leading payment conversion rate that is 22% higher than other providers, according to OpenBanking UK and other bank sources, and up to 40% higher than cards. Merchants offering TrueLayer as a payment method in their checkout have found that on average, 1 in 3 consumers chose to pay via TrueLayer. As a result, open banking is displacing other payment methods, such as cards, as the default payment option online. Over the past 12 months, TrueLayer has expanded its services across 12 European markets, growing payment volumes by 600x, and adding hundreds of new customers across digital banking, eCommerce, trading and investment, wealth management, crypto and iGaming. It has continued to innovate, for example, with the recent launch of PayDirect, combining instant pay-in capabilities with instant pay-outs, to deliver a higher converting, lower fraud method for online payments. “When Luca and I started TrueLayer in 2016, we imagined open banking becoming a new digital channel for solving cost and complexities around payments, digital identity, credit data and much more. We wanted to open up this newly built infrastructure to many businesses and consumers. It is such a joy to see our vision coming alive and open banking based payments quickly becoming the new normal,” commented Francesco Simoneschi, CEO and Co-Founder at TrueLayer. TrueLayer is rapidly expanding as demand for its open banking platform increases, largely driven by consumer demand for digital financial services that work better for them, and give them more control over their financial lives. “We have achieved this milestone thanks to the hard work of our stellar team. Bringing radical new products into the hands of consumers and businesses is incredibly exciting,” explained Luca Martinetti, Co-Founder and CTO at TrueLayer. “This new financial network we are building on top of open architectures has massive long term implications for the whole fintech ecosystem and we won’t compromise our vision in any way.” “That is why it is so important to select investors that can help you to plan for the next 15 years, not the next 15 months,” added Simoneschi. “The Addition team thinks very long term and it has been such a pleasure working together. They complement the incredibly strong group of experienced backers who align with our vision of how financial services are evolving.” Lee Fixel, Founder of Addition, commented: “TrueLayer is ideally positioned to benefit from the trends shaping the future of financial services as more and more companies embed digitally native payments into their platforms. We look forward to supporting the TrueLayer team as they scale their offering and drive continued innovation.” –ends– About TrueLayer TrueLayer provides global financial connectivity through open APIs. Our open banking platform empowers engineers, innovators and enterprises in every industry to create smarter financial services. Founded in 2016, TrueLayer is connected to major banks globally, backed by leading investors including Addition, Tencent, Temasek, Northzone, Anthemis Group, Mouro Capital and Connect Ventures, and trusted by some of the biggest names in fintech including Revolut, Trading 212 and Payoneer.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210408005064/en/ Geoff WhitehousePR and Communications Leadgeoff.whitehouse@truelayer.com +44 (0)7766 555077"
https://venturebeat.com/2021/04/08/survey-finds-96-of-execs-are-adopting-offensive-ai-against-cyberattacks/,Survey finds 96% of execs are considering adopting ‘defensive AI’ against cyberattacks,"“Offensive AI” will enable cybercriminals to direct attacks against enterprises while flying under the radar of conventional, rules-based detection tools. That’s according to a new survey published by MIT Technology Review Insights and Darktrace, which found that more than half of business leaders believe security strategies based on human-led responses are failing. The MIT and Darktrace report surveyed more than 300 C-level executives, directors, and managers worldwide to understand how they perceive the cyberthreats they’re up against. A high percentage of respondents (55%) said traditional security solutions can’t anticipate new AI-driven attacks, while 96% said they’re adopting “defensive AI” to remedy this. Here, “defensive AI” refers to self-learning algorithms that understand normal user, device, and system patterns in an organization and detect unusual activity without relying on historical data. Sixty-eight percent of the executives surveyed expressed concern about attacks employing AI for impersonation and phishing, while a smaller majority said they’re worried about more effective ransomware (57%), misinformation and the undermining of data integrity (56%), and the disruption of remote workers by targeting home networks (53%). Of the respondents, 43% underlined the damaging potential of deepfakes, or media that takes a person in an existing image, audio recording, or video and replaces them with someone else’s likeness using AI. As the report’s coauthors write, when offensive AI is thrown into the mix, “fake email” could become nearly indistinguishable from trusted contact messages. And with employees working remotely during the pandemic — without the security protocols of the office — organizations have seen successful phishing attempts skyrocket. Google registered over 2 million phishing websites since the start of 2020, when the pandemic began — a 19.91% increase compared with 2019. Businesses are increasingly placing their faith in defensive AI to combat the growing cyberthreats. Known as an autonomous response, defensive AI can interrupt in-progress attacks without affecting day-to-day business. For example, given a strain of ransomware an enterprise hasn’t encountered in the past, defensive AI can identify the novel and abnormal patterns of behavior and stop the ransomware even if it isn’t associated with publicly known compromise indicators (e.g., blacklisted command-and-control domains or malware file hashes). According to the survey, 44% of executives are assessing AI-enabled security systems and 38% are deploying autonomous response technology. This agrees with findings from Statista. In a 2019 analysis, the firm reported that around 80% of executives in the telecommunications industry believe their organization wouldn’t be able to respond to cyberattacks without AI. Reflecting the pace of adoption, the AI in cybersecurity market will reach $38.2 billion in value by 2026, Markets and Markets projects. That’s up from $8.8 billion in 2019, representing a compound annual growth rate of around 23.3%. “With the onset of AI-powered attacks, organizations need to reform their strategies quickly, be prepared to defend their digital assets with AI, and regain the advantage over this new wave of sophisticated attacks,” the report’s coauthors wrote. “By automating the process of threat detection, investigation, and response, AI augments human IT security teams by stopping threats as soon as they emerge, so people have the time to focus on more strategic tasks at hand.”"
https://venturebeat.com/2021/04/07/study-suggests-that-ai-model-selection-might-introduce-bias/,Study suggests that AI model selection might introduce bias,"The past several years have established that AI and machine learning are not a panacea when it comes to fair outcomes. Applying algorithmic solutions to social problems can magnify biases against marginalized peoples, and undersampling populations always results in worse predictive accuracy. But bias in AI doesn’t arise from datasets alone. Problem formulation, or the way researchers fit tasks to AI techniques, can also contribute. So can other human-led steps throughout the AI deployment pipeline. To this end, a new study coauthored by researchers at Cornell and Brown University investigates the problems around model selection — the process by which engineers choose machine learning models to deploy after training and validation. The team found that model selection presents another opportunity to introduce bias because the metrics used to distinguish between models are subject to interpretation and judgement. In machine learning, a model is typically trained on a dataset and evaluated for a metric (e.g., accuracy) on a test dataset. To improve performance, the learning process can be repeated. Retraining until a satisfactory model is produced is what’s known as a “researcher degree of freedom.” While researchers may report average performance across a small number of models, they often publish results using a specific set of variables that can obscure a model’s true performance. This presents a challenge because other model properties can change during training. Seemingly minute differences in accuracy between groups can multiply out to large groups, impacting fairness with regard to specific demographics. The coauthors underline a case study in which test subjects were asked to choose a “fair” skin cancer detection model based on metrics they identified. Overwhelmingly, the subjects selected a model with the highest accuracy even though it exhibited the largest gender disparity. This is problematic on its face, the researchers say, because the accuracy metric doesn’t provide a breakdown of false positives (missing a cancer diagnosis) and false negatives (mistakenly diagnosing cancer when it’s not actually present). Including these metrics could have biased the subjects to make different choices concerning which model was “best.” “The overarching point is that contextual information is highly important for model selection, particularly with regard to which metrics we choose to inform the selection decision,” the coauthors of the study wrote. “Moreover, sub-population performance variability, where the sub-populations are split on protected attributes, can be a crucial part of that context, which in turn has implications for fairness.” Beyond model selection and problem formulation, research is beginning to shed light on the various ways humans might contribute to bias in models. For example, researchers at MIT found just over 2,900 errors arising from labeling mistakes in ImageNet, an image database used to train countless computer vision algorithms. A separate Columbia study concluded that biased algorithmic predictions are mostly caused by imbalanced data but that the demographics of engineers also play a role, with models created by less diverse teams generally faring worse. In future work, the Cornell and Brown University team say they intend to see if they can ameliorate the issue of performance variability through “AutoML” methods, which divests the model selection process from human choice. But the research suggests new approaches might be needed to mitigate every human-originated source of bias."
https://venturebeat.com/2021/04/07/enterprise-software-devices-to-drive-2021-it-spending-to-4-1t/,"Enterprise software, devices to drive 2021 IT spending to $4.1T","Global information technology spending will grow 8.4% to $4.1 trillion in 2021, driven in part by enterprises accelerating their digital transformation plans, Gartner said in its latest IT spending forecast. IT spending will be driven by digital business plans that will be refined and completed in 2021, Gartner said. More digital initiatives are originating from business departments outside of IT, making IT a full participant in business value delivery, said John-David Lovelock, a distinguished research vice-president at Gartner. As a result, Gartner expects to see the source of funding to be charged as a cost of revenue or cost of goods sold (COGS). The source of funding changes “from an overhead expense that is maintained, monitored and sometimes cut, to the thing that drives revenue,” Lovelock said in the press release. Every IT spending category is expected to have positive growth through 2022, Gartner said. Device demand will show the most growth, at 14%, followed by enterprise software, at 10.8%. Every category will show solid growth in 2021, as “organizations focus on providing a more comfortable, innovative and productive environment for their workforce,” the company asserted.
For example, organizations are focusing on areas such as social software and collaboration platforms and human capital management (HCM) software to improve employee experience and well-being, Gartner said. Organizations will still focus on optimizing costs and other cost-savings efforts, but the IT spending focus in 2021 will be on revenue growth because there’s more economic certainty, according to the report. “Last year, IT spending took the form of a ‘knee jerk’ reaction to enable a remote workforce in a matter of weeks. As hybrid work takes hold, CIOs will focus on spending that enables innovation, not just task completion,” Lovelock said. IT spending took a hit in 2020, but Gartner estimated that IT spending in nearly every industry sector will recover and surpass 2019 levels within the next few years. Some sectors and regions will recover sooner than others, which will result in a “K-shape economic recovery,” Gartner said in its release. From an industry perspective, banks and financial services IT spending will reach 2019 levels as early as 2021. Retail and manufacturing IT spending will recover at a slower pace, and will not recover to pre-pandemic levels until about 2023. In an interview with VentureBeat, Lovelock was able to go in a bit more detail, noting that most sectors would return to 2019 levels of spending at some point in 2021. Government IT spending was back to 2019 levels exceptionally early, hitting that mark in mid-2020, Lovelock told VentureBeat. Transportation, however, is not expected to really recover until closer to 2025. There are also regional differences. China has already recovered, while Gartner estimates that North America and Western Europe will see IT spending recover in late 2021. Middle East/North Africa also has a quicker recovery path, Lovelock told VentureBeat. There were some variations in Asia, with “mature” Asia/Pacific markets expected to see their spending reach 2019 levels in early 2021, compared to “emerging” Asia/Pacific markets looking closer to 2022, Lovelock said. Latin America IT spending will grow much slowly, with Gartner predicting recovery around 2024. Regional recovery will also likely be influenced by what kind of industry is the most dominant in that region. Countries that may be more manufacturing-heavy will lag behind countries that have a mix of industry sectors, Lovelock said."
https://venturebeat.com/2021/04/07/why-company-culture-is-the-foundation-of-your-companys-growth-vb-live/,Why company culture is the foundation of your company’s growth (VB Live),"Presented by TriNet How do you create a company culture that empowers your employees to grow and gives you a competitive edge? Learn how to build a positive environment, align your culture with your org’s values and goals, and more in the first of this three-part webinar series, The Employee Experience: From Recruitment to Retention. Register here for free. As tragic as the pandemic has been in so many ways, this last year has provided a unique opportunity because companies simply haven’t had a choice – they’ve had to innovate. “There’s this window of opportunity where muscles have been stretched, particularly in the human capital space,” says Kristine Gunn, executive director of talent and organizational management at TriNet. “Companies have had to do things in six-week sprints that would have taken six years to get leadership buy-in.” Up until now, a common theme in organizational strategy many organizations have honed in on is efficiency and operational excellence to the detriment of innovation, Gunn says – not just R&D, but investing in reinventing the company. But this past year has blown the roof off disruption, and the need to keep reinventing and growing will continue to push companies forward, to not only survive the next disruption, but thrive. As part of that, some major employee trends have reared their heads in the past year. By way of example, a large tech CEO has said that digital adoption has moved five years in a period of just months, and digitization is putting tremendous pressure on organizations to adapt. Flexible work environments have become a huge topic of conversation in the human capital community, as employees realize the tremendous work-life benefits and companies learn that employees can work from home and stay productive, even in a fully remote hybrid environment. As part of that, people-centricity has taken center stage, the focus is shifting to the importance of employee well-being, and creating safety in organizations, especially for historically marginalized populations. The public focus on the fight for equity and equality for all has brought home the need to keep moving forward toward diversity and inclusion. These issues were important before, but creating inspiring organizations where people thrive is more important than ever, Gunn explains. “Research in talent and organization management has shown that those companies that have thrived through this trying past year have been those with strong missions, strong purpose – they’ve shown up in their communities,” she says. “Now more than ever, people are looking for organizations that have stepped up and risen to the challenge, with a very mission-driven, purpose-driven culture that is agile and can respond quickly to changes.” At its basic level, a mission-driven culture is one where people show up every day with purpose, and in which leaders have created spaces where their people can thrive. “It’s the environment, the behaviors, leadership,” Gunn says. “It’s how companies approach challenges. It’s how companies make decisions. It’s the unspoken rules about what people believe it takes to thrive – or what they believe it takes to get fired. It’s the conversations by the water cooler. It’s the seen and the unseen.” Culture is the secret sauce that drives employee engagement, innovation, business growth, and continuous reinvention. When the pandemic ends, there will be many more disruptions to come. It’s the companies that build resilience that will succeed and grow. Right now, the conversation has moved toward adapting more agile practices, like cross-functional teaming, and collaborative, empathetic types of leadership, where employees are empowered and equipped with the tools and resources to have authorship over projects, where they’re working together instead of in silos. It’s also about attracting people who have the ability to change, who are not afraid to take risks, and more importantly, who are not punished for taking risks, because there’s positive association to experimentation and failing forward. “When companies look at creating more agile organizations, they’re definitely looking at how to create entrepreneurism, authorship,” says Gunn. “How do they unleash people? How do they create a peer-to-peer environment instead of such a strong hierarchical environment?” In today’s landscape, leaders have to be agile enough to meet the current business challenges, and at the same time create stability and security for their members. It comes from the top down, and requires taking a stand, being intentional, knowing yourself, and knowing your company. It means telling a unique story and being authentic and thoughtful about what you stand for, while staying conscious of the fact that one size does not fit all. Companies have to find their own unique way – what works for Google or a daycare center, may actually derail your org. It also requires being highly in touch with your customer, and your customer promise and building the culture you need to deliver on that promise “Make sure that you have a value system that backs that up,” Gunn says. “Make sure that leaders are making decisions that align with that, particularly under pressure, when it’s easy to sway from your beliefs. Having that profound mission, knowing what’s important to your customers, and living that out – and being non-tolerant of folks that do not align with that culture and that vision that you’re looking to create.” As part of that, diversity and inclusion needs to be embedded into the company’s value system, and in the way decisions about talent are made – from fair and unbiased recruitment in order to attract the best candidates to how employees are onboarded, how they’re promoted, and how long they’re kept. Diversity isn’t just the right thing to do – it directly impacts your company’s attractiveness to talent, and your bottom line. More than two-thirds of active and passive job seekers said that a diverse workforce is an important factor when evaluating companies and job offers – and on average, the most diverse enterprises are the most innovative, achieving 19% higher innovation revenues and 9% higher EBIT margins. “You have to be super mindful of diversity and inclusion as the enabler of an organization’s culture,” says Gunn. “It’s deeply embedded in the values and how work gets done. Organizations have to focus on it. It doesn’t happen by accident. It takes conscious and very intentional effort to make sure that you’re educating people, embedding these practices, and reinventing and evolving. Leaders need to do everything in their power to create safe, happy, and healthy workplaces where everyone can bring their whole unique identity to work and feel valued and respected for their differences. It is more important than ever before.” Don’t miss out! Register here for free. Attendees will learn: Speakers: More to come!"
https://venturebeat.com/2021/04/07/experian-consumers-prefer-invisible-security-to-passwords/,Experian: Consumers prefer ‘invisible security’ to passwords,"Could the era of passwords be drawing to a close? Decades of fumbling around to remember the right password or constantly resetting or having to jump through multiple authentication hoops have made them dirty words for many consumers. All those headaches, and there’s still a good chance your personal information will wind up somewhere for sale on the internet. Perhaps it’s not surprising then, according to a new survey released by Experian, that consumers are embracing new methods of security that are physical and behavior-based. Indeed, according to the company’s 2021 Global Identity and Fraud Report, consumers did not rank passwords among the top 3 most secure ways to protect identity. Instead, the top 3 are so-called “invisible” methods: The Experian survey included 9,000 consumers and more than 2,700 businesses spread across 10 countries. The push to end passwords is getting greater attention from the security industry, enterprises, and venture capitalists. In December 2020, Beyond Identity raised $75 million for its solution that uses digital certificates to replace passwords. And just a few weeks ago, Identiq raised $47 million for a cryptographic network that can be used to confirm identity. The Experian study comes on the heels of a remarkable shift as COVID-19 drove a surge in online activity, from distance learning to remote work to ecommerce. Experian tracked a 20% increase in online consumer transactions over the past year. While that digital convenience became essential to helping businesses and consumers adapt to the pandemic, it also raised security concerns, with 55% of people surveyed ranking security as “the most important aspect of their online experience,” the report says. Of course, it’s a positive sign that consumers are taking security more seriously. The study found that 34% of consumers worry about privacy, up from 29% before the pandemic. Likewise, 33% worry about identity theft, up from 28% one year ago. And 49% have bigger concerns about fraud, compared to just 37% last year. These responses highlight a key challenge as businesses expand their digital footprint: how to securely authenticate real customers without making the process too burdensome, but still weeding out fraud? The answer would appear to be those invisible security strategies. In the survey, 48% of consumers under the age of 40 said they felt safer using biometric security now than before COVID-19, though that number drops to 37% for those over 40. “Consumers want to be recognized digitally without extra steps to identify themselves, and they don’t want to remember yet another password,” said Eric Haller, Experian EVP and general manager of Identity, Fraud and DataLabs, in a statement. “They are open to more practical solutions in today’s digital era.”"
https://venturebeat.com/2021/04/07/snorkel-ais-app-development-platform-lures-35m/,Snorkel AI’s app development platform lures $35M,"Snorkel AI, a startup developing data labeling tools aimed at enterprises, today announced that it raised $35 million in a series B round led by Lightspeed Venture Partners. The funding marks the launch of the company’s Application Studio, a visual builder with templated solutions for common AI use cases based on best practices from academic institutions. According to a 2020 Cognilytica report, 80% of AI development time is spent on manually gathering, organizing, and labeling the data that’s used to train machine learning models. Hand labeling is notoriously expensive and slow, with limited leeway for development teams to build, iterate, adapt, or audit apps. In a recent survey conducted by startup CloudFlower, data scientists said that they spend 60% of the time just organizing and cleaning data compared with 4% on refining algorithms. Snorkel AI hopes to address this with tools that let customers create and manage training data, train models, and analyze and iterate AI systems. Founded by a team spun out of the Stanford AI Lab, Snorkel AI claims to offer the first AI app development platform, Snorkel Flow, that labels and manages machine learning training data programmatically.  Application Studio will expand the Snorkel AI platform’s capabilities in a number of ways, the company says, by introducing prebuilt solution templates based on industry-specific use cases. Customers can leverage templates for contract intelligence, news analytics, and customer interaction routing as well as common AI tasks such as text and document classification, named entity recognition, and information extraction. Application Studio also provides packaged app-specific preprocessors, programmatic labeling templates, and high-performance open source models that can be trained with private data, in addition to collaborative workflows that decompose apps into modular parts. Beyond this, Application Studio offers a feature that versions the entire development pipeline from datasets to user contributions. With a few lines of code, apps can be adapted to new data or goals. And they keep training data labeling and orchestration in-house, mitigating data breach and data bias risks. Application Studio is in preview and will be generally available later this year within Snorkel Flow, Snorkel AI says. Palo Alto, California-based Snorkel AI’s latest fundraising round brings the startup’s total raised to date to $50 million, which 40-employee Snorkel AI says will be used to scale its engineering team and acquire new customers. Previous investors Greylock, GV, In-Q-Tel, and Nepenthe Capital, along with new investor Walden and funds and accounts managed by BlackRock, also participated in the series B."
https://venturebeat.com/2021/04/07/captivateiq-raises-46-million-to-automate-sales-commission-programs/,CaptivateIQ raises $46 million to automate sales commission programs,"Incentive compensation platform CaptivateIQ today announced that it raised $45 million in series B financing led by Accel. The company says it plans to use the funding to expand its reach, as well as to develop AI technologies that benefit sales planning and other parts of the business connected to sales. Management processes around commission programs can be both rigid and expensive. In fact, sales compensation represents the single largest investment for most business-to-business companies. U.S.-based enterprises alone spend over $800 billion on it each year, in aggregate — three times more than what they spend on advertising. CaptivateIQ, which launched in 2017 as part of Y Combinator’s 2018 winter cohort, claims to automate commission workflows using AI. The 90-employee startup was founded by Conway Teng, Hubert Wong, and Mark Schopmeyer, who has a fintech background in investment banking and private equity. Teng was a corporate finance analyst at McKinsey who later helped launch loyalty rewards startup Fivestars and worked on finance at Gusto. “Several years ago, when I was a finance lead at Brightroll, commissions was dumped on me and I was told I needed to build a new Excel model to pay out our team of 30 reps within 14 days,” Schopmeyer told VentureBeat via email. ” As a former banker, it seemed easy, but I spent the next 10 days working until 2 a.m. cleaning up the data, building the financial model, and sending out statements to the entire sales team … I later caught up with Teng and learned he was experiencing a similar pain. One thing led to another and we quit our jobs and brought on our college friend, Wong, and started building a new commissions solution that we wanted to use.” CaptivateIQ’s no-code software collates data from disparate sources like invoices and billing systems to power real-time calculations. Customers can use it to build, preview, and launch commission plans and share them in customized reports. Accel partner Ben Fletcher believes that CaptivateIQ’s value proposition lies in its digital, autonomous approach to tabulation. Companies typically manage commission programs using spreadsheets or costly legacy solutions. These are prone to data drift — a 2017 survey by Xactly found that four in five companies reported inaccuracies in their sales commission payouts. “CaptivateIQ is more than just commission software. Similar to UiPath, Ada, and Celonis, CaptivateIQ’s powerful and easy-to-use no-code platform is automating an important aspect of the sales and finance workflow,” Fletcher said in a press release. “When several of our other portfolio companies kept raving about how much time and headache it was saving their growing sales teams and businesses, we knew something special was happening ” There’s truth to this. While CaptivateIQ isn’t yet profitable, in 2020, the company says that revenue grew 6 times as companies like Affirm, Gong, Intercom, TripActions, and hundreds of other companies used its product. CaptivateIQ claims to have processed over $2 billion in commissions for tens of thousands of employees as of this year. “A big part of our growth is that we can help any company that offers a performanced-based compensation plan, so we don’t have any restrictions with the types of businesses we work with. We typically see conversations start with teams that have a minimum of 25 sales people, though we easily serve enterprises and public companies as well,” Schopmeyer said. “In terms of users, which we define as the number of payees, basically anybody who receives a payout is in our system, so that’s also grown significantly. December 2020 ended up 4 times from the prior year.” Ninety-employee CaptivateIQ says that a portion of the proceeds from this latest funding round will enable it to build AI models that help people understand how taking certain actions will impact future earnings. The idea is that, by being able to model and predict outcomes, companies will be able to drive behavior that benefits their bottom line while helping employees achieve their financial goals. “To start, we’re introducing more powerful data transformations, a richer set of formulas, and off-the-shelf templates,” Conway said. “Another goal of ours is to automate and streamline the end-to-end commissions process. We’re expanding our data integrations to support all major data systems and introducing new dashboarding capabilities that will let teams track performance and unlock business insights. We’re also enhancing existing collaboration workflows around approvals, inquiries, and contracts to enable every stakeholder in the commissions process to stay connected.” The series B in San Francisco, California-based CaptivateIQ announced today brings the company’s total raised to $63 million. Beyond Accel, Amity, S28 Capital, Sequoia, and Y Combinator participated."
https://venturebeat.com/2021/04/07/ataccama-expands-data-management-automation-tools-to-meet-pandemic-driven-needs/,Ataccama expands data management automation tools to meet pandemic-driven needs,"During its digital Ataccama Innovate 2021 event, Ataccama today announced general availability of the next major upgrade to its data management platform, which relies on an expert system and machine learning algorithms to automate processes. The company also released the results of a global survey of more than 1,000 executives and business users — from midsize to large enterprises. The survey finds 79% of executives and 75% of line-of-business users are contending with data quality issues at a time when more than three-quarters of respondents (78%) are collecting and processing more data now than they were prior to the pandemic. Conducted by ResearchScape, the survey also finds over half of users (55%) require additional help to transform data to fit their purpose, with 44% having to wait a day or more to get help from a technical user or IT team. The top three impacts of poor data quality identified by respondents are slower times to data insight (46%), negative impacts to business performance and decision-making (42%), and negative impacts on strategic initiatives (40%). But close to half of survey respondents (43%) admit they have implemented data governance strategies at either a low maturity level or not at all. Specifically, survey respondents are looking to implement processes to better organize data (59%), consolidate documented data (55%), and ensure data is only used for purposes that are compliant with regulations (40%). The survey results suggest executives are now a lot more conscious of the need to consistently manage data as they invest in AI-enabled digital business transformation initiatives that require access to massive amounts of data, Ataccama CEO Michal Klaus said. In fact, the top priority technologies survey respondents identified for the next 12 months are AI or machine learning (43%), data governance (41%), data security (41%), and data quality management (37%). The Ataccama ONE Gen2 platform is an expert system that has been optimized to manage large volumes of disparate data, Klaus said. It enables organizations to create data profiles, track data quality monitoring, set up data previews to better understand the relationship between different datasets, and enforce the policies that determine which end users are allowed to access what data. Earlier this year, Ataccama acquired Tellstory to add data visualization capabilities to the data catalog that has been revamped as part of the latest release of its platform. Tasks and processes that are being enhanced using AI capabilities include data classification, rules suggestion, relationship and data lineage discovery, anomaly detection, pattern matching, data quality monitoring, and integration with master data management (MDM) tools Ataccama provides. As organizations invest more in data lakes and data warehouses, they are discovering the limitation of their existing data management processes. In many cases, the return on investment in those data lakes never materializes because there’s no way to navigate all the data being dumped into them, Klaus notes, adding “They become data swamps.” Ultimately, the goal is to provide a data management platform that makes it simpler for IT teams to enable end users to self-service their own data needs at scale in a way that complies with privacy regulations and other considerations, Klaus said. Most IT organizations are not especially proficient when it comes to data management, and data is typically managed within the context of the application employed to create it. This means applications wind up creating data that is often conflicting because, for example, the way a customer is described varies from one application to the next. Applying advanced analytics to data depends on IT teams addressing data quality issues that stem from the way different application silos have rendered data. It not uncommon for much of that data to also be incomplete or simply wrong. Regardless of the tools employed to clean up that mess, data management will need to become a lot more automated than it is today to address the true scale of the challenges ahead."
https://venturebeat.com/2021/04/07/email-is-the-answer-to-the-death-of-cookies-for-digital-publishers/,Email is the answer to the death of cookies for digital publishers,"Presented by Jeeng For years, digital publishers have relied on third-party cookies to help them learn about audience behavior in a sort of ad hoc, outsourced data strategy. By relying on ad targeting through cookies, they could also infer a bit about their site visitors’ preferences and behavior. But publishers didn’t actually own that data; they only borrowed it from third-party ad servers. And, thanks to privacy concerns — most notably the fact that site visitors had no idea they were being tracked — cookies have now come under fire and will soon be obsolete as the web’s mostly widely-used browsers have moved to eliminate cookies to protect users’ privacy. This leaves publishers in a lurch. Consumers increasingly expect personalized content and experiences, across every experience — from TV to shopping to news and entertainment. And, without it, they’ll take their business elsewhere. Obviously, this is something no publisher can afford right now, especially as Google and Facebook gobble up a progressively larger share of the advertising pie. Publishers need all the well-targeted eyeballs they can possibly get onto their site to drive revenue. Facing tremendous pressure from the death of cookies and a tight budget squeeze, publishers need a new solution — a way to connect with their audiences directly, to cut out the middleman and deliver the personalized content and experiences audiences expect. Enter: email. Converting site visitors to logged-in subscribers through an email address opens a whole new world of opportunity in 1:1 engagement. From personalized content delivery over multiple channels to precise advertising that drives revenue, email is the answer to the death of cookies for digital publishers. Here’s why: Cultivating a direct relationship with known subscribers is vital for publishers to survive in the post-cookie era. In an ironic twist, as cookies die, email — the channel many called dead a decade ago — is the answer to publishers’ needs. Email is not dead, and in fact is breathing new life into the trusted relationships publishers need with their audiences. As digital publishing adapts to a cookie-less world, the winners will be those who can leverage email-based tracking, targeting, and delivery into a 1:1, personalized, automated multichannel experience for their subscribers. To learn more about how digital publishers can leverage email and multi-channel messaging to engage subscribers and drive more revenue, visit www.jeeng.com. Jeff Kupietzky is CEO of Jeeng, formerly PowerInbox. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/07/synthesis-ai-emerges-from-stealth-with-4-5m-to-create-synthetic-face-datasets/,Synthesis AI emerges from stealth with $4.5M to create synthetic face datasets,"Synthesis AI, a synthetic data company, today emerged from stealth with the announcement that it closed a $4.5 million funding round. The startup says that the capital will allow it to expand its R&D team and develop new synthetic data technologies. Self-driving vehicle companies alone spend billions of dollars per year collecting and labeling data, according to estimates. Third-party contractors enlist hundreds of thousands of human data labelers to draw and trace the annotations machine learning models need to learn. (A properly labeled dataset provides a ground truth that the models use to check their predictions for accuracy and continue refining their algorithms.) Curating these datasets to include the right distribution and frequency of samples becomes exponentially more difficult as performance requirements increase. And the pandemic has underscored how vulnerable these practices are, as contractors have been increasingly forced to work from home, prompting some companies to turn to synthetic data as an alternative. Synthesis AI’s platform leverages generative machine learning models, image rendering and composition, and other techniques to create and label images of objects, scenes, people, and environments. Customers can modify things like geometries, textures, lighting, image modalities, and camera locations to produce varied data for training computer vision models. Synthesis AI offers datasets containing 10,000 to 200,000 scenes for common use cases including head poses and facial expressions, eye gazes, and near infrared images. But what the company uniquely provides is an API that generates millions of images of realistic faces captured from different angles in a range of environments. Using the API, customers can submit a job in the cloud to synthesize as much as terabytes of data. Synthesis AI says its API covers tens of thousands of identities spanning genders, age groups, ethnicities, and skin tones. It procedurally generates modifications to faces to reflect changes in expressions and emotions, as well as motions like head turns and features such as head and facial hair. Built-in styles adorn subjects with accessories like glasses, sunglasses, hats and other headwear, headphones, and face masks. Other controls enable adjustments in camera optics, lighting, and post-processing. Synthesis AI makes the claim that its data is unbiased and “perfectly labeled,” but the jury’s out on the representativeness of synthetic data. In a study last January, researchers at Arizona State University showed that when an AI system trained on a dataset of images of engineering professors was tasked with creating faces, 93% were male and 99% white. The system appeared to have amplified the dataset’s existing biases — 80% of the professors were male and 76% were white. On the other hand, startups like Hazy and Mostly AI say that they’ve developed methods for controlling the biases of data in ways that actually reduce harm. A recent study published by a group of Ph.D. candidates at Stanford claims the same — the coauthors say their technique allows them to weight certain features as more important in order to generate a diverse set of images for computer vision training.   Despite competition from startups like Datagen and Parallel Domain, Synthesis AI says that “major” technology and handset manufacturers are already using its API to generate model training and test datasets. Among the early adopters is Affectiva, a company that builds AI it claims can understand emotions by analyzing facial expressions and speech. “One of our teleconferencing customers leveraged synthetic data to create more robust facial segmentation models. By creating a very diverse set of data with more than 1,000 individuals with a wide variety of facial features, hairstyles, accessories, cameras, lighting, and environments, they were able to significantly improve the performance of their models,” founder and CEO Yashar Behzadi told VentureBeat via email. “[Another one] of our customers is building a car driver and occupant sensing systems. They leveraged synthetic data of thousands of individuals in the car cabin across various situations and environments to determine the optimal camera placement and overall configuration to ensure the best performance.” In the future, 11-employee Synthesis AI plans to launch additional APIs to address different computer vision challenges. “It is inevitable that simulation and synthetic data will be used to develop computer vision AI,” Behzadi continued. “To reach widespread adoption, we need to continue to build out 3D models to represent more of the real world and create scalable cloud-based systems to make the simulation platform available on-demand across a broad set of use cases.” Existing investors Bee Partners, PJC, iRobot Ventures, Swift Ventures, Boom Capital, Kubera VC, and Leta Capital contributed to San Francisco, California-based Synthesis AI’s seed round announced today."
https://venturebeat.com/2021/04/07/trifacta-expands-data-preparation-tools-with-databricks-integration/,Trifacta expands data preparation tools with Databricks integration,"Trifacta today announced it has integrated its data preparation tools with a data warehouse platform based on the open source Apache Spark framework provided by Databricks. This is in addition to repositories based on an open source data built tool (DBT) that is maintained by Fishtown Analytics. In both cases, Trifacta is extending the reach of tools it provides for managing data pipelines to platforms that are widely employed in the cloud to process and analyze data, Trifacta CEO Adam Wilson said. Trifacta traces its lineage back to a research project that involved professors from Stanford University and the University of California at Berkley and resulted in a visual tool that enables data analysts without programming skills to load data. In effect, Trifacta automated extract, transform, and load (ETL) processes that had previously required an IT specialist to perform. There is no shortage of visual tools that let end users without programming skills migrate data. But Trifacta has extended its offerings to a platform that enables organizations to manage the data pipeline process on an end-to-end basis as part of its effort to meld data operations (DataOps) with machine learning operations (MLOps). The goal is to enable data analysts to self-service their own data requirements without requiring any intervention on the part of an IT team, Wilson noted. Google and IBM already resell the Trifacta data preparation platform, and the company has established alliances with both Amazon Web Services (AWS) and Microsoft. Those relationships enable organizations to employ Trifacta as a central hub for moving data in and out of cloud platforms. The alliance with Databricks and the support for DBT further extend those capabilities at a time when organizations have begun to more routinely employ multiple cloud frameworks to process and analyze data, Wilson said. In general, data engineering has evolved into a distinct IT discipline because of the massive amount of data that needs to be moved and transformed. While visual tools make it possible for data analysts to self-service their own data requirements, organizations are now also looking to programmatically move data to clouds as part of a larger workflow. Many individuals that have ETL programming expertise, often referred to as data engineers, are now in even higher demand than data analysts, Wilson said. Once considered the IT equivalent of a janitorial task that revolved mainly around backup and recovery tasks, data engineering is now the discipline around which all large-scale data science projects revolve, Wilson noted. In fact, IT professionals with ETL skills have reinvented themselves to become data engineers, Wilson added. “In the last 12 months, data engineering has become the hottest job in all of IT,” Wilson said. It remains to be seen just how automated data engineering processes can become in the months and years ahead. Not only is there more data to be processed and analyzed than ever, the types of data that need to be processed have never been more varied. Going forward, a larger percentage of data will be processed and analyzed on edge computing platforms, where it is created and consumed. But the aggregated results of all that data processing will still need to be shared with multiple data warehouse platforms residing in the cloud and in on-premises IT environments. Regardless of where data is processed, the sheer volume of data moving across the extended enterprise will continue to increase exponentially. The issue now is figuring out how to automate the movement of that data in a way that scales much more easily."
https://venturebeat.com/2021/04/07/whitesource-raises-75m-to-move-beyond-open-source-security-and-compliance-management/,WhiteSource raises $75M to move beyond open source security and compliance management,"WhiteSource, a platform that companies such as Microsoft, IBM, and Comcast use to secure their open source software components, has raised $75 million in a series D round of funding. Founded in 2011, WhiteSource automatically identifies every open source component in a company’s technology stack, then identifies and prioritizes vulnerabilities, issuing real-time alerts on genuine risks it detects. “In order to mitigate open source risks, it’s essential to remediate open source vulnerabilities as soon as they are discovered,” WhiteSource CEO and cofounder Rami Sass told VentureBeat. “However, in most cases it’s impractical to fix all vulnerabilities, and some require major development work. WhiteSource research shows that only 15% to 30% of vulnerabilities are effective — the majority of open source vulnerabilities are not called by the proprietary code.” There is a strong case to support the widely uttered mantra that open source has eaten the world. All the major tech companies not only use open source software, but contribute back to the communities and even open-source their own internal tools. Indeed, most modern software relies on at least some open source components, as it saves the companies that build it time and resources in having to develop and maintain everything themselves. A recent IBM-commissioned study called The Value of Open Source in the Cloud Era noted that most of the respondents (developers and managers) used open source software in some aspect of their operations, while in its recent State of Enterprise Open Source report, Red Hat found that 90% now use enterprise open source in their organizations — up from 89% last year. “We certainly have noticed the trend as well,” Sass said. “Over the past three years, we have seen the numbers of our enterprise customers triple and seen our revenue grow by 800%, underscoring the enormous demand by organizations developing software to effectively manage their use of open source components. In our view, the current pace of enterprise software development, using modern application architecture like microservices and containers, is only sustainable through a high-level of reliance on open source.” Although there are counter arguments that posit the exact opposite, open source software has often been plagued by the notion that it is less secure than its proprietary counterpart. Equifax, for example, blamed its mega 2017 security breach on the open source server framework Apache Struts. In its recent State of Software Security: Open Source Edition report, app security company Veracode found that “open source libraries are ubiquitous and risky,” with 70% of applications containing a security flaw in an open source library, while WhiteSource rival Sonatype recently reported a 430% surge in cyberattacks aimed at “infiltrating open source software supply chains.” Elsewhere, a joint report produced by WhiteSource and the Ponemon Institute found that “more than 70% of enterprise application portfolios have become more vulnerable to attack” in the past year. “There are a number of reasons for the increase of vulnerability in enterprise applications,” Sass explained. “A misalignment between risk-levels and the level of annual spending across different protection layers. The gap is most evident in the application layer, where the percentage of allocated budget is significantly lower compared with the perceived level of risk.” Sass also cited a “lack of a formal approach” to securing the software development life cycle, as well as limited collaboration between development and security teams as other reasons why enterprise applications may have become more vulnerable. Moreover, matters have been compounded by faster software release cycles, with developers expected to ship more code and faster, leading companies to play a delicate balancing act between security and speed. Developers can integrate WhiteSource with many of the popular development environments, including IDEs, so they can see immediately whether any open source component has security vulnerabilities before they make a pull request (i.e., before the component enters a live code base). WhiteSource offers four core plans which offer incrementally more features: free; Essentials, at $2,400 per year; Teams, at $10,000 per year; and Enterprise, which starts at $28,000 per year. The platform includes a dashboard that serves up an overview of an organization’s open source dependencies and license risks, among other data points. Users can dig down into specific vulnerabilities to see where they exist and how they can go about remediating them (such as upgrading to a new version). Although open source is generally free for developers to use, it often has some restrictions in terms of how third-parties are allowed to use it — as such, WhiteSource also helps companies adhere to any licensing policies that are in place. Other notable players in the space, a sector that is commonly referred to as software composition analysis (SCA), include Black Duck, which Synopsys bought for $547 million in 2017; the aforementioned Sonatype, which was acquired by Vista Equity Partners in 2019; and Snyk, which just last month closed a $300 million round of funding at a whopping $4.7 billion valuation. So, what would a world look like without such automated tools? Well, the onus would likely fall on security teams to manually review and approve all the open source components in their tech stack, which is a lengthy, never-ending process of checking and testing. “Sometimes, information security teams may enforce open source security standards and block components from use, without consideration for the implication on development teams,” Sass said. “Other times, developers would use their own tools to detect and avoid open source vulnerabilities, and manage the findings using spreadsheets, with limited visibility to other teams or external auditors.” Prior to now, WhiteSource had raised around $46 million, the chunk of which arrived through its series C round in 2018. With its latest $75 million cash injection — which attracted existing investors including Microsoft’s M12, 83North, and Susquehanna Growth Equity, in addition to Pitango Growth, which led the round — WhiteSource is gearing up to broaden its reach beyond the SCA sphere and into the wider application security testing (AST) space. “This will go beyond detection to offer prioritization and auto-remediation of open source vulnerabilities to cover all threats and all application attack vectors,” Sass said. “Our vision is not limited to open source code, and we will announce more exciting developments in the near future.”"
https://venturebeat.com/2021/04/07/malomo-raises-5m-to-drive-repeat-purchases-through-shipment-tracking/,Malomo raises $5M to drive repeat purchases through shipment tracking,"Shipment tracking platform Malomo today announced it has raised $5 million in seed funding. The company says it will use the capital to make strategic hires, develop new big data tools, and broaden international carrier supports while expanding its customer base. Malomo says the funds will also enable it to integrate its product with marketing automation platform Klaviyo to give merchants greater control over their post-purchase experience. While year-over-year consumer spending in the U.S. dipped last month, the pandemic has on the whole supercharged ecommerce. According to data from IBM’s U.S. Retail Index, business closures and shelter-in-place orders accelerated the shift to digital shopping by five years, with online shopping projected to have grown nearly 20% in 2020. Based on the survey data from BMC and Mercatus, ecommerce grocery orders alone totaled $5.9 billion, up 3.6% from $5.7 billion in August. Indianapolis, Indiana-based Malomo, which was founded in 2018, uses shipment tracking technology to drive repeat sales. Using its solution, retailers can share shipping updates with buyers via webpages, app notifications, and emails. When customers check their order tracking, they’re treated to products, ads, and other content Malomo serves to spur additional purchases. Driving repeat purchases remains a challenge for retailers in the exploding ecommerce space. It’s estimated that 73% of first-time purchasers won’t make a second purchase from the same online seller. But according to Malomo CEO Yaw Aning, delivery experiences can have a major impact on sentiment. Ninety-six percent of respondents to a MyCustomer survey said a positive shipment experience would encourage them to shop with a retailer again.  “The number one question for the billions of people that buy things online is ‘Where is my order?'” Aning said in a press release. “While Amazon spends billions of dollars on [its] post-purchase experience to ease customer anxiety around that question, independent retailers who sell their products directly through their own website to consumers don’t have the same resources to compete. We level the playing field by helping retailers own the customer experience from the buy button all the way to the doorstep and beyond. We help them activate post-purchase as a channel to drive growth, retention, and trust with their fans.” Malomo says on average its clients see support tickets cut in half and a 2% to 3% increase in repeat purchase rate. Customer Grace Eleyae says its click rate climbed 18.7% from its tracking pages, and repeat purchases through its emails and pages were up 8 times. “When COVID forced small businesses to rely entirely on online sales, it became challenging for merchants to maintain meaningful customer relationships that build trust,” said Base10 Partners’ Chris Zeoli, a Malomo investor. “Malomo’s transparent and proactive post-purchase shipment tracking platform absolutely delights customers throughout the purchase and delivery experience and is a no-brainer return on investment for merchants looking to acquire and retain customers.” Beyond Base10, participants in Malomo’s funding round included existing investors Harlem Capital, High Alpha, Hyde Park Venture Partners, Paul Singh, and new strategic investors The Vaan Group, CX Collective, Curtis Cheng, Jeremy Cai, and Alexandre Perrier. The company’s total raised now stands at over $8 million."
https://venturebeat.com/2021/04/07/ourcrowd-50-index-fund-is-now-available-on-the-bny-mellon-pershing-alternative-investment-network/,OurCrowd 50 Index Fund is Now Available on the BNY Mellon Pershing Alternative Investment Network," Advisors on the Pershing platform to gain access to privately held companies at lower minimums than via traditional VC firms  NEW YORK & JERUSALEM–(BUSINESS WIRE)–April 7, 2021– OurCrowd, Israel’s most active venture investor, announced today that its flagship portfolio index fund OurCrowd 50 (“OC50”) has been added to BNY Mellon Pershing’s (“Pershing”) Alternative Investment Network. OC50 is a hyper-diversified investment vehicle enabling investors to participate in the next 50 OurCrowd opportunities in which OurCrowd invests at least $1M. The addition of OurCrowd 50 to the Pershing platform is part of OurCrowd’s initiative enabling Registered Investment Advisors (“RIAs”) and other financial intermediaries in the United States to offer OurCrowd’s venture capital opportunities to their clients for a minimum as low as $50,000 per fund. Ilana Odess, OurCrowd Partner and Managing Director of North America leading the RIA initiative said: “We are very pleased that Pershing has onboarded our flagship OC50 Fund. Pershing is one of the leading clearing and custodial platforms and we are glad that RIAs who clear through Pershing will be able to offer the OC50 Fund to their clients.” “Private companies are offering their shares to the public through an IPO much later in their lifecycle than they did in the late 1990s through early 2000s. RIAs who offer OurCrowd’s venture capital opportunities enable their clients to access privately-held companies at much lower minimums than traditional venture capital firms,” Odess said. OC50 was created as a hyper-diversified investment vehicle to break open the gates and offer more investors access to highly vetted startups across a variety of sectors, stages, and geographies, enabling them to participate in the next 50 OurCrowd investment opportunities across a spectrum of today’s most dynamic growth industries, including healthcare technology, transportation, agriculture, communication, enterprise, robotics, and AI. Through the first three series of the OC50 Fund, investors have benefited from OurCrowd companies entering the public equity markets – such as food company Beyond Meat and insurance provider Lemonade – or others being acquired by companies like Vimeo, H&R Block, Uber, and NetApp. The original OC50 Series I Fund launched in 2017. Six companies in the fund have completed exits, generating on average 2.6 times gross return on invested capital as of the end of Q4 2020.* Financial advisors interested in learning more about this opportunity can contact RIA@OurCrowd.com. END About OurCrowd: OurCrowd is a global venture investment platform that empowers institutions and individuals to invest and engage in emerging companies. With $1.5 billion of committed funding, and investments in more than 240 portfolio companies and 25 venture funds, OurCrowd offers access to its membership of 80,000 individual accredited and institutional investors, family offices, and venture capital partners from over 195 countries to invest alongside, at the same terms. Rated by PitchBook as the most active venture investor in Israel, OurCrowd portfolio companies have been acquired by some of the most prestigious brands in the world, including Microsoft, Uber, Canon, Oracle, Nike, and Intel. To register visit www.ourcrowd.com. *Note: Nothing contained in and accompanying this communication shall be construed as an offer to sell, a solicitation of an offer to buy, or a recommendation to purchase any security by OurCrowd, its portfolio companies or any third party. Information regarding OurCrowd’s limited partnerships and/or portfolio companies and the investment opportunities on OurCrowd’s website is intended and available for accredited investors only (criteria at http://www.ourcrowd.com/). OurCrowd urges potential investors to consult with licensed legal professionals and investment advisors for any legal, tax, insurance, or investment advice. Please be aware that investments in early-stage companies or in venture capital funds contains a high level of risk and you should consider this prior to making any investment decisions. Past performance is not indicative of future results.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210407005063/en/ PRESS CONTACT: OurCrowd: Leah Stern, Partner, Global Communications / Europe: +44 747 0196826 / E: leah@ourcrowd.com"
https://venturebeat.com/2021/04/07/sverica-capital-management-announces-strategic-investment-in-winwire-technologies/,Sverica Capital Management Announces Strategic Investment in WinWire Technologies,"SAN FRANCISCO–(BUSINESS WIRE)–April 7, 2021– Sverica Capital Management LP (“Sverica”), a private equity investment firm, announced today that it has made a strategic investment in WinWire Technologies, Inc. (“WinWire” or the “Company”), a leading data-driven digital engineering company specializing in Microsoft Azure and other Microsoft cloud platforms. Based in Santa Clara, California, WinWire was founded in 2007 with the mission of “stitching the digital fabric” to help organizations gain competitive advantage through innovative software solutions. As a Microsoft Managed Partner and AI Inner Circle Partner with numerous Gold partner accreditations, WinWire has demonstrated extensive familiarity with Microsoft cloud platforms and positioned itself as a technology leader with enterprise clients. With expertise and solution offerings across a range of cloud platforms, data analytics, artificial intelligence, and other technology domains and a deeply embedded “People First” culture, WinWire serves as a trusted, long-term partner to enterprises seeking to modernize their businesses and accelerate their digital transformations. “WinWire is excited to partner with Sverica as this strategic investment allows us to accelerate our growth, expand our global presence and focus on building domain competencies and solution accelerators in the areas of Healthcare, Retail, Hi-Tech and Manufacturing to deliver world-class customer experiences. WinWire thanks its customers and partners for their commitment and partnership and its employees who, especially during these unprecedented times, continue to make WinWire a Great Place to Work®,” said Ashu Goel, CEO of WinWire. “Ashu has done a remarkable job building WinWire into one of the leading providers of Azure-centric services in the Microsoft ecosystem today,” said Frank Young, Managing Partner at Sverica. “Beyond WinWire’s impressive growth and clear, customer-focused strategy, we particularly admire the manner in which he has humbly built the business through a strong, authentic, people-centric culture. We are thrilled to partner with him and look forward to working together to ensure WinWire accelerates its high growth trajectory in the years to come.” Ryan Harstad, Partner at Sverica, added: “As part of Sverica’s dedicated focus on cloud services, our team actively sought to invest with a proven leader in Microsoft Azure services. With deep technical expertise and significant momentum, WinWire is an exciting company that we enthusiastically welcome to the Sverica family. The ‘People First’ foundation upon which Ashu has built WinWire resonates closely with Sverica’s partnership-driven approach, and we’re eager to get started in collaborating with him and his great team to drive the company’s continued growth.” About WinWire WinWire Technologies is a data-driven digital engineering company that supports enterprises across Healthcare, Retail, Hi-Tech and Manufacturing and several other industry domains in navigating their digital transformation journey. WinWire enables its customers to drive business growth and gain competitive advantage by aligning business value and digital technologies, calling this process “Stitching the Digital Fabric.” WinWire has extensive expertise across a range of digital technologies and delivers large enterprise solutions leveraging cloud, AI, machine learning, mixed/augmented reality, Internet of Things (IoT), mobility, security, and UI/UX to help clients harness business value. For more information, please visit www.winwire.com. About Sverica Capital Management Sverica Capital Management is a leading growth-oriented private equity firm that has raised over $1.1 billion across five funds. The firm acquires, invests in and actively builds companies that are, or could become, leaders in their industries. Since inception, Sverica has followed a “business builder” approach to investing and takes an active supporting role in its portfolio companies. Sverica devotes significant internal time and resources to help its management teams develop and execute growth strategies and proactively looks for levers to pull to accelerate growth by reinvesting back into those companies. Sverica firmly believes in building businesses collaboratively that can endure for the long term by starting with a strong foundation and bringing the right people and playbook to drive reinvestment and ultimately strong returns for our investors. For more information, please visit www.sverica.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210407005268/en/ Media Inquiries: Sverica Capital Management LPNathalie Allen415-249-4906nathalie@sverica.com"
https://venturebeat.com/2021/04/07/5-jobs-that-are-honestly-as-awesome-as-they-sound/,5 jobs that are honestly as awesome as they sound,"Looking for a new job can be really exciting, but also tricky. It doesn’t happen overnight (well, most of the time anyway). People usually say that looking for a job is like a job in itself — and we want to make that search of yours a little bit easier. That’s why we have a brilliant job board on our site. It’s a one-stop shop if you’re looking for an exciting new role in the U.S. (and beyond, if you fancy packing your bags!) Take a look here at a handful of what’s on offer. Klavyio is looking for a product designer who will lead proposed and design solutions that influence direction and outcomes. They will evaluate UX for one or more domains and apply designs to create new opportunities, while determining UX direction with respect to competition. The successful candidate will have a Bachelor’s degree in arts, science, new media, design, psychology, or any other related field, or foreign academic equivalent, and at least one year of experience in the job offered or in digital product design using software such as Figma, Sketch, Adobe CC, Framer, and/or Invision. As a Software Engineer, Server, you will be working to improve the onboarding experience for Zwift customers. Onboarding new customers is a non-trivial problem as new customers learn what Zwift is, what they need to Zwift, and help in walking through the steps to get set up and become an active customer. You will collaborate in cross-functional teams including teammates from engineering, product, design, operations, and support disciplines. They are looking for someone comfortable in this highly collaborative environment, working with ambiguity, and continuously looking to proactively make improvements, to build a world-class experience for their customers. As a Computer Engineer at Micron Technology, Inc., you will be working in a fast-paced, collaborative, production support role evaluating, developing, enhancing, and debugging both in-house and commercial EDA tools and flows for the design, physical layout, and verification of CMOS integrated circuits. The successful candidate will innovate to develop tools, flows, and methodologies to increase the productivity and reliability of memory designs. They will own production level flows across a worldwide company, while providing training, documentation, and support to end-users on new tools and methods. Inpixon is looking for a Pre-Sales Engineer to design, architect, and detail customized solution demonstrations and proofs-of-value (POV) for their Aware product prospects. This is an exciting opportunity to bridge your technical experience with your client-facing ability, dive into the details for product offerings, and play a critical role in the success of the Inpixon sales efforts and resulting growth. You will work with sales, development, and product to understand the needs of the prospect and rapidly convert those requirements into interactive, customized demonstrations, providing the full Inpixon experience and the extent of technical expertise and capabilities. The Crypsis Group is seeking a Network Traffic Analyst to join their growing team. The Network Traffic Analyst will work in tandem with the Threat Intelligence Analyst and Mobile Application Analyst teams to investigate suspected malicious mobile applications. Suspected malicious mobile applications will be tested in a sandboxed environment with proxies capturing all network traffic. The Network Traffic Analyst will work to identify anomalous activities within captured logs, as well as supplemental information discovered via Threat Intelligence."
https://venturebeat.com/2021/04/07/blue-dot-raises-32m-for-ai-that-helps-companies-comply-with-tax-codes/,Blue Dot raises $32M for AI that helps companies comply with tax codes,"Tax compliance platform Blue Dot (previously VatBox) today announced it has raised $32 million, bringing its total raised to over $96 million. The firm says it will put the funds toward product R&D and expanding the size of its globlal workforce. The tax compliance burden for enterprises can be significant. In 2019, half of companies responding to an EY Americas survey indicated that their biggest compliance challenge would be staying current on legislative and regulatory developments. Another 23% said modeling the tax impacts of business and legislative change was their next biggest barrier to overcome. Blue Dot offers software designed to reduce companies’ tax liabilities by addressing a few of these challenges. Leveraging a knowledge base, as well as AI and machine learning algorithms, it looks for anomalies in evidence, supplier data, and reports, consolidating data sources and processes into a single workflow and flagging anything that might be amiss. Blue Dot was founded in 2013 by Isaac Saft and Noam Guzman, who sought to streamline the recovery of value-added taxes (VAT), or taxes levied on products at each stage of production, distribution, and sale. Along with over 100 employees, the two cofounders built a business-to-business software-as-a-service solution that relies on the cloud and automation to chase higher returns.  Blue Dot’s algorithms monitor transactional data across entities and jurisdictions, identifying unclaimed tax returns, avoidable costs, and more. The software extracts, matches, and analyzes invoices, employing an access point for global and local management of corporate tax policies. Using Blue Dot, companies can adjust the strictness of their tax returns based on rulings with local authorities while maintaining regulatory compliance. For example, Blue Dot can tell customers how much VAT has been spent across their business, as well as the likely amount of VAT that can be reclaimed — by country, date, and expense type. Blue Dot’s optimization features show things like the difference between potential and actual VAT returns, in addition to information on lost VAT refunds. “Our ecosystem combines hundreds of extractors and algorithms, using semantic networks, statistical relationships, anomaly detection, and many other models,” a company spokesperson told VentureBeat via email. “Blue Dot harnesses the power of AI and machine learning while leveraging external data sources and historical data to create an end-to-end story of each employee-driven transaction. [The company’s] architecture ensures the data’s integrity via a continuously updated knowledge base of tax rules and a configuration wizard that allows each client to apply their own policies to optimize the end-to-end tax compliance process.” The global tax management market is expected to grow from $15.5 billion in 2019 to $27.0 billion by 2024. Factors driving the climb include an increasing volume of transactions due to digitalization, the complex nature of existing tax systems, and the increased vigilance of tax administrators, according to Markets and Markets. Blue Dot is based in Amstelveen, Netherlands and claims more than 3,000 multinational clients, including brand leaders and Fortune 500 companies such as Dell, BCD Travel, and Michelin."
https://venturebeat.com/2021/04/07/eu-may-probe-facebooks-kustomer-acquistion-over-antitrust-concerns/,EU may probe Facebook’s Kustomer acquisition over antitrust concerns,"(Reuters) — Facebook’s acquisition of customer service startup Kustomer may be subjected to European Union antitrust scrutiny after Austria asked EU enforcers to take over the task, the European Commission said on Tuesday. The move comes as the EU competition regulator girds up to vet more tech, pharma, and biotech startup deals, sending a warning to tech giants criticized by some for so-called killer acquisitions, where startups buy nascent rivals with the goal of shutting them down. The world’s largest social network announced the deal in November. It could help the company scale up its instant messaging app WhatsApp, which has seen usage jump during the COVID-19 pandemic. Facebook sought approval for the deal from the Austrian competition agency on March 31. “We can confirm that we have received a request for referral from Austria,” a Commission spokesperson said. Other national watchdogs have 15 working days to inform the EU competition enforcer whether they too want it to review the deal. “Following the expiry of the deadline for other Member States to join the referral, the Commission will have 10 working days to decide whether to accept or reject the referral,” the spokesperson said. Facebook said the deal would bring more innovation to businesses and consumers in a dynamic and competitive space. “We look forward to demonstrating to regulators that Facebook and Kustomer would offer more choices and better services through this pro-competitive deal,” a Facebook spokesperson said."
https://venturebeat.com/2021/04/06/intel-touts-latest-xeon-processor-for-scaling-5g-networks/,Intel touts latest Xeon processor for scaling 5G networks,"Intel launched its latest datacenter platform in the form of the 3rd Gen Intel Xeon Scalable processors. The Santa Clara, California-based chipmaker said that the processors deliver a 46% performance increase on datacenter workloads. The server chips with integrated AI will power cloud-native datacenters and applications such as 5G networks, cryptography, drug discovery, and confidential computing. For 5G, the new chips deliver on average 62% more performance on network and 5G workloads. In an online briefing, Intel executive vice president Navin Shenoy said Intel has added advanced security with Intel Software Guard Extension and Intel Crypto Acceleration. Intel has shipped more than 200,000 chips for revenue in the first quarter, and it boasts more than 250 design wins for the chips with 50 partners, 15 telecom equipment and communications firms, and 20 high-performance computing labs. AT&T said it is seeing 1.9 times higher throughput and 33% more memory capacity with the combination of the Intel Xeon Scalable processors and Intel Optane Persistent Memory, so the network can serve the same number of subscribers at higher resolution or a greater number of subscribers at the same resolution. Verizon and Vodafone also said they’re using the new Xeons. With the chips, Intel said communication service providers can increase 5G user plane function performance by up to 42%. The chip uses Intel’s 10-nanometer manufacturing process (equivalent to the 7-nanometer process of rivals based on nomenclature), and it delivers up to 40 cores per processor and up to 2.65 times higher average performance gain compared to five-year-old systems. Intel CEO Pat Gelsinger said in an online briefing that over the past year companies have been forced to undertake a warp-speed cloudification of infrastructure to serve remote workforces, and he said the new processors have flexible architecture for advanced security and built-in AI to handle processing from the edge to the cloud. “Technology is like magic,” he said. “It has the power to improve the lives of every person on the planet. It’s a new day at Intel. We are no longer just the CPU company.” He said Intel combines software, silicon, and manufacturing to differentiate itself from rivals. The company will operate internal factories, strategically use foundry services to make Intel chips with the help of outside contract manufacturers, and offer its own foundry services to others. “With a backdrop of fierce competition, Intel is leading with its strengths with its 3rd Gen Xeon processors,” said Patrick Moorhead, an analyst at Moor Insights & Strategy, in a message to VentureBeat. “The company is offering a platform approach to provide its partners solutions incorporating CPUs, storage, memory, FPGAs, and networking ASICs. This is in addition to its ability to leverage resources for co-marketing and co-development. I also believe the company is differentiated with its on-chip ML inference and cryptographic capabilities versus its closest competitors.” The latest hardware and software optimizations deliver 74% faster AI performance compared with the prior generation and provide up to 1.5 times higher performance across a broad mix of 20 popular AI workloads versus AMD Epyc 7763 and up to 1.3 times higher performance on a broad mix of 20 popular AI workloads versus Nvidia A100 GPU, Intel said. Shenoy said its security-focused SGX protects sensitive code and data with the smallest potential attack surface within the system. It is now available on two-socket Xeon Scalable processors with enclaves that can isolate and process up to a terabyte of code and data to support the demands of mainstream workloads. And Shenoy said Intel Crypto Acceleration delivers performance across a variety of important cryptographic algorithms. Businesses that run encryption-intensive workloads, such as online retailers who process millions of customer transactions per day, can leverage this protection without impacting user response times or overall system performance. Intel said that more than 800 of the world’s cloud service providers run on Intel Xeon Scalable processors, and all of the largest cloud service providers are planning to offer cloud services in 2021 powered by the newest chips. HP Enterprise said it has launched new computers across eight different models with the new Xeons, and it uses AMD’s latest Epyc processors as well."
https://venturebeat.com/2021/04/06/google-launches-lyra-codec-in-beta-to-reduce-voice-call-bandwidth-usage/,Google launches Lyra codec in beta to reduce voice call bandwidth usage,"Google today open-sourced Lyra in beta, an audio codec that uses machine learning to produce high-quality voice calls. The code and demo, which are available on GitHub, compress raw audio down to 3 kilobits per second for “quality that compares favorably to other codecs,” Google says. While mobile connectivity has steadily increased over the past decade, the explosive growth of on-device compute power has outstripped access to reliable, fast internet. Even in areas with reliable connections, the emergence of work-from-anywhere and telecommuting have stretched data limits. For example, early in the pandemic, nearly 90 out of the top 200 U.S. cities saw internet speeds decline as bandwidth became strained, according to BroadbandNow. It’s Google’s assertion that Lyra can make a difference in these scenarios. Lyra’s architecture is separated into two pieces, an encoder and decoder. When someone talks into their phone, the encoder captures distinctive attributes, called features, from their speech. Lyra extracts these features in 40-millisecond chunks and then compresses and sends them over the network. It’s the decoder’s job to convert the features back into an audio waveform that can be played out over the listener’s phone. According to Google, Lyra’s architecture is similar to traditional audio codecs, which form the backbone of internet communication. But while these traditional codecs are based on digital signal processing techniques, the key advantage for Lyra comes from the ability of its decoder to reconstruct a high-quality signal. Google believes there are a number of applications Lyra might be uniquely suited to, from archiving large amounts of speech and saving battery to alleviating network congestion in emergency situations. “We are excited to see the creativity the open source community is known for applied to Lyra in order to come up with even more unique and impactful applications,” Google Chrome engineers Andrew Storus and Michael Chinen wrote in a blog post. “We [want] to enable developers and get feedback as soon as possible.” The Lyra code is written in C++ using the Bazel build framework. The core API provides an interface for encoding and decoding at the file and packet levels, and the complete signal processing toolchain is provided, which includes filters as well as transforms. Google’s example code integrates with the Android NDK to show how Lyra can work with Java-based Android apps, and Google has also provided the weights and vector quantizers necessary to run Lyra. “This release provides the tools needed for developers to encode and decode audio with Lyra, optimized for the 64-bit ARM android platform, with development on Linux,” Storus and Chinen continued. “We hope to expand this codebase and develop improvements and support for additional platforms in tandem with the community.”"
https://venturebeat.com/2021/04/06/segment-founder-on-future-of-customer-data-management-and-acquisition-by-twilio/,Segment founder on future of customer data management and acquisition by Twilio,"A little more than eight years ago, Segment almost died an early death when it seemed to be out of money and ideas. Last November, the company capped its improbable rebound and rise to stardom when it was acquired by Twilio in late 2020 for $3.2 billion. The deal to put two customer data companies together seemed like a good fit on the surface. Both focus on developers and delivering solutions by leveraging APIs. But Segment founder Peter Reinhardt told VentureBeat recently that there is much more to the deal, and he remains even more bullish about that potential almost four months later. Particularly with the pandemic reshaping daily life, it’s become more important than ever for companies to have insight into customers and to be able to deliver an engaging digital experience. “Ultimately what we’re both trying to do is improve customer engagement and help companies engage better with their customer,” Reinhardt said. “We’re like the two halves of the coin that handle the two halves of that problem.  One actually delivering communications and the other the data to figure out what to send in the first place.” When Reinhardt last spoke with VentureBeat in late 2019, the company was experiencing meteoric growth. After a couple of early ideas misfired and almost put them out of business, the Segment founders posted a JavaScript library they had created, dubbed simply “analytics.js”, to GitHub that allowed developers to funnel all their data through any analytics service of their choice and then monitor the results to get better insight into customer behavior. That proved to be a hit, and Segment quickly began developing its own commercial services on top of that on the way to raising $300 million in venture capital for a $1.5 billion valuation. Those services include an API that enhances the customer data centralization and expands the range of services where the information could be sent for further analysis or personalization. At the start of 2020, Reinhardt said Segment getting 500 billion API calls per month. By the end of the year, that had grown to 1 trillion per month. Meanwhile, Twilio CEO Jeff Lawson had started to make overtures. Twilio had built a big business around messaging APIs that help brands connect with consumers and manage interactions. “He had been coming to me about once a quarter and making a pitch for how Segment and Twilio would be better together,” Reinhardt said. “And at first, I kind of just gently and politely dismissed it out of hand. But he’s very persistent.” Over time, as Reinhardt listened to Lawson’s pitch, he also began to notice a pattern among Segment customers. Reinhardt would be discussing how they were going to use Segment data and customers would bring up the desire to have more integration with messaging. “Some of my conversations with [Jeff Lawson] started to stick in my head, like, ‘Oh, yeah, I see they’re using the data to drive the communication channels,” Reinhardt said. “And then mid-last year, Jeff and I had yet another quarterly check-in and I was like, ‘You know what? I see it. And I actually think it is a much, much bigger opportunity.'” After the intense entrepreneurial journey, Reinhardt has undergone some personal and professional adjustments following the decision to sell. “I’ve been through the whole emotional roller coaster that every founder goes through in this kind of situation,” he said. “Relief, sadness, letting go of, excitement, fear. Like, everything all bundled into one two-week period. But now we’re super excited to be on the other side of that and have sort of a fresh beginning.” The business is now dubbed Twilio Segment. Going forward, the goal is to develop a customer engagement tool that offers developers a more flexible approach to managing customer data than the customer management software tools that have been dominant. That tool is now available as a beta. The idea is to blend the Segment and Twilio experience into a seamless customer data management system. “I’m sending my customer an email and sending them an SMS and I’m doing a voice call with them,” Reinhardt said. “Whatever the mode of interaction, I’m doing that to engage them. The question is: ‘How do I do that better?’ If you’re going to do that better, it means sending a more personalized message that’s better targeted and that’s more interesting to them. You do that with data.” For that reason, Reinhardt argues that Twilio’s communications tools and Segment’s data management tools “sit extremely adjacent to each other in the higher-level problem of customer engagement.” To take advantage of these underlying capabilities, Reinhardt said the new product will become a feedback loop where data continually flows to improve communications and engagement, which in turn generates new data that can be used to shape the next engagements. “The first thing is actually getting feedback out of the communications,” he said. “And that is already in beta. It takes data out of Twilio and sends it downstream for usage in Segment but also more broadly joining the concept of building blocks.” The plan is to unveil more details around this evolution at Twilio’s Signal conference in late October. Meanwhile, Reinhardt has felt a greater sense of urgency to expand these tools over the past year, both before and after the Twilio deal. As the pandemic put the digital experience front and center for companies, it became a huge area of investment. But it’s also one where many are struggling to stand out from competitors while meeting the expectations — and increasingly higher standards — of customers. That has become even more challenging in a world where companies can’t rely on face-to-face meetings or other forms of traditional interaction. The answer is to get more sophisticated about how that data is being used. “In a world where you need to compete primarily in a digital world, the question is, ‘How do you meaningfully differentiate?'” Reinhardt said. “It’s going to be something in that customer experience. And if it’s going to be different, then that means you’ll have to build something. You have to have an engineering team that’s building something and that’s pulling together some aspects of data about the customer experience that allows marketers to send more creative messaging than competitors. Fundamentally, at its root, that relies on developers. That’s why Twilio and Segment are so focused on being tools for marketers but are ultimately built for developers.”"
https://venturebeat.com/2021/04/06/ex-akamai-cso-guide-security-startups-on-strategy-as-yl-ventures-partner/,Ex-Akamai CSO will guide security startups on strategy as new YL Ventures partner,"Andy Ellis, the former CSO of Akamai Technologies, has joined YL Ventures as an operating partner. Ellis will draw upon his experiences as a security decision-maker to advise startups on a broad range of services, including product development, go-to-market strategies, and customer pipeline management. YL Ventures funds Israeli cybersecurity companies from “seed to lead,” but the support goes beyond just funding, Ellis said in an interview with VentureBeat. YL Ventures provides strategic and operational guidance to the companies in its portfolio “in the time that YL’s going to be part of their journey,” Ellis added. The firm also publishes the CSO Circuit, a newsletter that tells startup founders what CSOs need and what the market is currently looking for to help shape product roadmaps and sales strategies. The support could be as straightforward as access to a marketing or press team before the company is big enough to have its own. It is also about time and practical advice, with YL acting as an advisor on anything the founders and their teams need help with. Ellis makes himself available “anywhere in the pipeline that I can be useful,” such as joining the company on a customer call, providing feedback on product design, advising on the product roadmap, helping develop the marketing presentation, and advising on how to recruit and develop talent.  Ellis spent 20 years at Akamai and grew the security business to more than $1 billion in annual revenue. As the CSO, he dealt with the challenges enterprise security leaders face in developing a security program, as well as deploying and integrating multiple platforms and technologies. One thing he regularly encountered was the question of how to protect as many people as possible — security at scale. “I can bring some of the lessons about solutions that didn’t always work because they were great on paper and in the pilot [but not across the entire organization],” Ellis said. He also has the vendor perspective, thanks to “secondhand experiences across thousands of CSOs” while working with Akamai customers. He knows enterprise leaders have budget constraints and integration challenges and can advise security companies on how to address those specific needs. On the selling side, he understands how different dynamics play out in different marketplaces and knows the difference between selling to financial services, retail, and manufacturing. One of the challenges early startups face is shifting their focus from investors to customers. The first slide deck a startup creates typically leads with how much it has already raised and is designed to sell the company to VCs. You almost have to throw that entire deck out and start over with a presentation that considers the target market and what the customer cares about, Ellis said. A cloud-native business, for example, will want to know how the technology will solve the problem it is having. “How much money you raised is a signal that says, ‘Maybe you have a great idea, but the idea [technology] is what you want to talk about and should always be what you’re talking about in selling the business,'” Ellis said. YL Ventures consults with CSOs — there are over 90 CSOs on the advisory board, as well as a less formal network of a thousand security leaders — to “get market feedback before making an investment,” Ellis said. These conversations help YL Ventures stay up to date with the challenges organizations are facing and understand any needs and gaps in the industry. This collaboration helps YL Ventures decide which areas to focus on and which security companies to add to their investment portfolio. Ellis joined that advisory board about four years ago. “Sometimes the input was ‘Wow, brilliant people, but this technology is never going to sell in the market,'” Ellis said. “Other times it was, ‘Oh my goodness, I want to do this one.'” YL Ventures currently manages over $300 million and is investing from its fourth $135 million fund. Its portfolio is currently focused on the following areas: application security and securing code; security controls for software-as-a-service applications; extended detection and response (XDR) capabilities; next-generation cloud security solutions; and data security. YL Ventures is betting on these technologies as the areas enterprise leaders are most concerned about. Over the last 15 years, organizations have built out extensive security platforms to protect the applications and data they have, but those security platforms are left behind when organizations move their applications or data to the cloud, Ellis said. Most public cloud platforms already have “fantastic” security tools, but the challenge is finding them and knowing how to use them, Ellis said. So he will be looking at ways to make it easier to deploy security services in the cloud. This kind of thinking applies to SaaS applications as well, since security teams have to make sure the employees are using them safely and that information remains secure. “It really does come down to ‘How do we make security easy? How can we give security teams scale?'” Ellis said. Another area where Ellis sees a lot of CSO interest is in application development, in helping developers build secure code. “That said, any idea is on the table,” Ellis said. “If a company comes with a brilliant idea and it’s not in those three spaces, I’m totally excited about that too.” While YL Ventures didn’t provide aggregate totals on how much it has invested in each of these areas, it offered some insights into recent funding decisions. The sheer number of attacks against applications and the ever-widening attack surface is driving organizations to allocate more resources to application security and secure software development. One emphasis is on security solutions to “shift left” to implement security earlier in the software development lifecycle. The firm led the seed round in application security startup Enso Security, vulnerability management company Vulcan Cyber, and source code protection startup Cycode. The growing attack landscape means organizations are also increasingly looking for new ways to detect threats and respond quickly. XDR is a new approach that collects and automatically correlates data across multiple security layers so threats can be detected faster and security analysts can improve investigation and response times. YL Ventures led a seed round in autonomous threat intelligence startup Hunters and participated in follow-on rounds. The shift to the cloud, the growing remote workforce, and rapid adoption of digital transformation initiatives mean enterprise leaders are willing to spend to protect cloud platforms and software-as-a-service applications. This is especially true as more employees are connecting remotely to corporate assets and accessing sensitive data from private devices and networks. YL Ventures led the seed round in Orca Security, a security startup that reached a unicorn valuation in less than two years. Data security and governance is another big area for YL Ventures, especially as enterprises try to manage the massive amount of data being generated. Organizations have to figure out how to comply with a growing slate of regulations, such as the European Union’s General Data Protection Regulation and the California Consumer Privacy Act. As more states follow California’s example, organizations have to ensure their governance processes and data management practices keep up with each regulation’s requirements. YL Ventures led a seed round in data protection and management startup Satori in 2019. Other areas YL Ventures invested in recently include authorization (build.security), medical device security (Medigate), and embedded security for connected systems (Karamba). VCs often straddle the fine line between investing in companies that address the needs of the market and trying to anticipate future problems and find startups that are beginning to tackle those issues. There is an educational opportunity for companies to prepare for the moment when organizations realize they have a particular problem. Ellis gave the example of how organizations started buying technologies to deal with distributed denial-of-service (DDoS) attacks. Akamai was already working on the technology in 2004, before most organizations were even thinking about it. When Operation Payback — a series of coordinated DDoS attacks against financial services and other major organizations — hit in 2010, Akamai was ready. Vulcan Security, a company in the YL Ventures portfolio, is another example. When the company was raising its seed round, very few organizations were thinking about vulnerability remediation orchestration. Just a handful of years later, it is something organizations are actively looking at. “My favorite part of this business is helping,” Ellis said. “How do you make the internet safer by finding things that scale as solutions that the market needs?”"
https://venturebeat.com/2021/04/06/how-intel-and-burger-king-built-an-order-recommendation-system-that-preserves-customer-privacy/,How Intel and Burger King built an order recommendation system that preserves customer privacy,"The pandemic has placed enormous strains on the restaurant and fast food industries. Within a month of the health crisis, 3% of restaurants had closed for good and another 11% anticipated doing so within the following month, according to a National Restaurant Association study. While fine dining and casual dining establishments suffered the bulk of the impact, the fast food industry wasn’t immune. A Datassential survey found that sales among fast food operators declined 42% during the first few weeks of the pandemic. As more customers began relying on take-out and drive-thru options instead of indoor dining, fast food retailers like Burger King turned to AI and machine learning for solutions. In collaboration with Intel, Burger King developed an AI system that uses touchscreen menu boards to recommend items to customers as they’re about to order. It can predict whether a customer will order a hot or cold drink or a light or large meal, potentially saving time and leading to a better customer experience. Burger King and Intel say the solution has already been piloted in over 1,000 Burger King locations. Burger King isn’t the first fast food chain to experiment with AI in customer service. McDonald’s has been using AI in its drive-thrus since acquiring tech company Dynamic Yield in 2019. Dunkin’ Donuts is testing drive-thrus that can recognize a loyalty member as soon as they pull up. Some Sonic drive-ins recently got AI-powered menu kiosks. And Chick-fil-A is using AI to spot signs of foodborne illness from social media posts. As Luyang Wang, director of advanced analytics and machine learning at Burger King, explained to VentureBeat via email, fast food recommendation has its own set of unique challenges. There’s no easy way to identify customers and retrieve their profiles because all of the recommendations happen offline. Moreover, context features like location, time, and weather conditions have to be preprocessed before they can be loaded into a model. To solve these challenges, TxT was built with what’s called a “double” Transformer architecture that learns real-time order sequence data, as well as features like location, weather, and order behavior. TxT leverages all data points available in a restaurant without having to identify customers prior to the order-taking process. For example, if a customer puts a milkshake as the first item in their basket, that will influence what TxT suggests — based on what’s been sold in the past, what’s selling today, and what is sold at that location. TxT was developed within Analytics Zoo, Intel’s open source platform for big data analytics workloads running in datacenters. Intel and Burger King collaborated to create an end-to-end recommendation pipeline, which includes distributed Apache Spark data processing and Apache MXNet training on an Intel Xeon cluster. The TxT model was deployed using Intel’s RayOnSpark library, which allows enterprises to directly run programs on existing clusters. According to Wang, TxT has already led to surprising sales insights. For one, Burger King customers will order milkshakes in any weather — even when it’s cold out. And people are much more willing to add a dessert when they have a high-calorie basket versus a low-calorie basket. “At Burger King, we are always looking to improve our guests’ experience,” Wang said. “This AI recommender system — Transformer Cross Transformer (TxT) — allows Burger King to better learn customer habits and, essentially, better communicate with guests.”"
https://venturebeat.com/2021/04/06/tvscientific-launches-connected-tv-performance-advertising-platform/,tvScientific Launches Connected TV Performance Advertising Platform," The New Platform combines the Power of TV Advertising with the Control and Measurability of Digital Platforms  PASADENA, Calif.–(BUSINESS WIRE)–April 6, 2021– tvScientific announced the launch of its Connected TV (CTV) buying, measurement, and attribution platform, built for businesses of all sizes that value performance media. CTV represents the fastest growing segment of a $72 billion TV advertising market, which is dominated by roughly 300 national advertisers. tvScientific aims to eliminate the high barriers of entry around TV advertising by making it accessible and measurable for all businesses. The tvScientific platform was designed for performance marketers, media agencies and businesses that want to take advantage of the rapidly growing and engaged CTV audience. By combining the powerful viewing experience of TV advertising with the capabilities of SmartTVs, tvScientific can help businesses buy and measure advertising performance. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210406005332/en/ An Idealab company, tvScientific co-founders include Bill Gross, founder of GoTo.com and father of the paid search business model, and Jason Fairchild, an early executive at GoTo.com and cofounder of OpenX. The company raised $1.5 million in seed funding from IdealabX and MathCapital in addition to well-known leaders of the advertising tech community including Tim Cadogan (GoFundMe, OpenX, Yahoo, and Overture), Tom Chavez (super{set}, Krux), John Gentry (OpenX and Overture), and Kent Wakeford (Integral Ad Science and Kabam) among others. “There were two reasons GoTo.com and the paid search model were so revolutionary. First, paid search campaigns were measurable so advertisers could see the clicks and sales that search ads generated. Second, any business could set up an account and start driving clicks in 10 minutes. The combination of these two simple dynamics attracted millions of businesses to search advertising. I believe tvScientific will do the same thing for TV advertising, where we already have 200 million CTV viewers in the US, but with only a few hundred participating advertisers,” said Bill Gross, Co-Founder of tvScientific, and Founder and Chairman of Idealab. “Until now, Connected TV advertising has been complex, inefficient, and not measurable in the way that digital is, preventing most businesses from advertising on the most influential screen in the house,” said Jason Fairchild, Co-Founder and CEO of tvScientific. “Our new platform allows businesses to buy and measure Connected TV media on one simple platform, and evaluate it the way they do with search and social.” tvScientific is the only CTV platform to offer a self-serve solution that combines fully optimized buying with comprehensive measurement and attribution. The platform has direct access to over 90% of premium CTV publishers, bypassing unnecessary technology layers, which allows businesses to buy premium CTV inventory at efficient rates. The tvScientific platform also features advanced audience targeting, measurement, attribution solutions, and optimization: tvScientific connects buyers to hundreds of premium CTV inventory sellers and data providers across a wide variety of platforms, CTV OEMs and tech infrastructure providers, including SpotX, Magnite, OpenX, Samsung, running on CTV devices like Samsung, Vizio, Roku, and more. This unique end-to-end platform empowers businesses to quickly create an account and launch measurable CTV advertising campaigns in minutes. tvScientific is available now, visit tvscientific.com for more information. About tvScientific tvScientific is a TV advertising technology company that finally brings the power of digital advertising to television. tvScientific’s industry-first buying and attribution platform is the only CTV platform to offer a self-serve solution that combines fully optimized media buying with comprehensive measurement and attribution. An Idealab company, tvScientific co-founders include Bill Gross, founder of GoTo.com and father of the paid search business model, Jason Fairchild, an early executive at GoTo.com and co-founder of OpenX, in addition to David Koye, a veteran advertising and media executive from Cox Media and Kent Wakeford, co-founder of Integral Ad Science and COO of Kabam.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210406005332/en/ Media Contact: Kristen Morquechopress@tvscientific.com"
https://venturebeat.com/2021/04/06/ai-observability-startup-aporia-nabs-5m-to-provide-guardrails-for-ai/,AI observability startup Aporia nabs $5M to provide guardrails for AI,"Machine learning observability startup Aporia today emerged from stealth with $5 million in venture capital funding. The company says the proceeds will support the unveiling of its platform for AI models, which enables companies to monitor AI running in cloud environments. Businesses are investing upwards of $50 billion annually on AI adoption, a recent report from the MIT Sloan School of Management and Boston Consulting Group found. But a lack of ability to detect issues in models as they enter production could be undermining investments. “AI needs guardrails,” Aporia CEO Liran Hason told VentureBeat via email. “Companies need to have confidence in their machine learning models, and the only way to get there is by robust monitoring to ensure they’re doing what they’re supposed to do.” Hason, a veteran of the Israel Defense Forces’ elite 81 intelligence unit, was one of the first employees at cloud security company Adallom, which Microsoft acquired for $320 million in 2015. At Adallom, he led the machine learning production architecture, which served as many as millions of users.  “I founded the company in late 2019 after leading machine learning production architecture at Adallom and then working at Vertex Ventures VC where I was involved in dozens of startup investments,” Hason told VentureBeat via email. “It seemed natural to use my development best practices, hard-learned lessons with data science challenges, and my gravitation towards startups to start a company that would apply best practices from production engineering and adjust them to machine learning, in the hope of transforming doubt into trust and build what analysts often call ‘responsible AI.'” Aporia lets data scientists create, maintain, or modify monitors for models and set alerts that trigger notifications via email, Slack, and other channels.  The Aporia platform can be installed with a few lines of code and set to monitor billions of daily model predictions asynchronously. Alongside its public cloud offering, Aporia provides a managed on-premises solution for enterprises with data privacy and security requirements. Machine learning models can work perfectly in the experimentation phase but start to drift in production over time due to changes in their datasets, Hason explained. Something as routine as a company expanding into a new market can affect the performance of a model. Customers and businesses typically suffer the consequences — predictions based on the wrong data are flawed, resulting in unintended outcomes and in turn lost revenue.  “Companies are struggling to keep watch of their AI in the ways that matter for their specific machine learning model and use case,” Hason added. “Aporia’s platform contains three pillars: (1) visibility, allowing data scientists to explore production data easily, (2) monitoring, the beating heart of the system, where users can implement any monitoring logic they’d like and adapt it to their use case and investigation, and (3) toolbox, for root cause analysis. Aporia aims to be the place where organizations manage the reliability of their models, and ensure responsible usage, whether in regard to performance or bias and fairness matters.” Sixteen-employee Aporia has rivals in data reliability startup Monte Carlo and WhyLabs, a startup developing a solution for model monitoring and troubleshooting. There’s also Domino Data Lab, a company that claims to prevent AI models from mistakenly exhibiting bias or degrading. But according to Hason, Aporia’s differentiator is its experienced team. Already, the company’s platform is being used by roughly a dozen users across over 11 “multi-billion-dollar” companies. One organization is tapping Aporia to monitor a model that predicts whether an applicant will be able to repay a loan without defaulting.  “We had one case where the credit history data provider had changed the schema of the data without notifying anyone, resulting in a significant drift in model’s behavior, leading it to approve or deny loans unjustifiably,” Hason explained. “Without a proper monitoring system in place, it would only have been discovered a few months later once loans were starting to be defaulted on and there was major revenue loss. However, with Aporia, they got an alert about that drift on the very same day the problem had started, which allowed them to react quickly and avoid potential deficiencies.” When asked about Aporia’s fundraising, Rona Segev, managing partner at investor TLV Partners, said, “Monitoring production workloads is a well-established software engineering practice, and it’s past time for machine learning to be monitored at the same level. Aporia’s team has strong production-engineering experience, which makes their solution stand out as simple, secure, and robust.” Vertex Ventures and TLV Partners led Tel Aviv, Israel-based Aporia’s seed round."
https://venturebeat.com/2021/04/06/catch-nvidia-gtc-21-live-right-here/,Catch Nvidia GTC 21 live right here,"This article is part of the VB Lab / Nvidia GTC insight series. Nvidia GTC, the conference for AI innovators, developers, technologists, startups, and creatives, is offering over 1,600 sessions online this year. Running from April 12 – 16, 2021, the opening keynote will be livestreamed free right here. The keynote starts at 8:30 am PDT on April 12 with Nvidia CEO Jensen Huang – bookmark this page and tune in to watch live. Be sure to also register to attend any of the 1,600 free sessions offered during the conference featuring the world’s brightest researchers and thought leaders from top companies covering breakthroughs in AI, data center, accelerated computing, autonomous vehicles, health care, intelligent networking, game development, and more.  VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/06/onestream-raises-200m-to-bringing-intelligent-financial-planning-to-the-modern-enterprise/,OneStream raises $200M to bring intelligent financial planning to the enterprise,"OneStream, a corporate performance management (CPM) company that unifies customers’ financial planning processes and reporting through a single platform, has raised $200 million in a series B round of funding at a whopping $6 billion valuation. CPM encapsulates all of the processes, systems, and methodologies an enterprise might use to track its business performance and usually involves gleaning data from myriad sources, such as enterprise resource planning (ERP), customer relationship management (CRM), and human capital management (HCM). Let’s say a business that collects global sales transaction data through its ERP tool wants to transform that information into something more meaningful and useful on a day-to-day basis, rather than waiting for monthly or quarterly reviews. OneStream essentially uses the data to generate “performance signals” that are delivered each day to help execs make “mid-stream” decisions. “You can think of OneStream as the ‘financial brain’ for modern business,” CFO Bill Koefoed told VentureBeat. “OneStream helps organizations turn the data they are collecting into actionable, financially intelligent information. This is critical because traditionally it might take 30 days or more to pull insights from financial reporting. Companies have realized — especially in the midst of COVID-19 — that they need to access critical data insights on company performance much faster than that. We make that intelligence available daily, and that’s game-changing.” OneStream typically integrates data from general ledger/ERP systems for financial data, though it also links to systems spanning HCM, CRM, and data warehouses, among other sources. Founded out of Rochester, Michigan in 2010, OneStream occupies a space that includes legacy players such as Oracle Hyperion, SAP, and IBM, as well as products focused on more specific business problems, such as those from Anaplan and Blackline. OneStream said it notched up 85% growth in annual recurring revenue (ARR) in 2020 and increased its customer base — which includes UPS, CapitalOne, and Costco — by 40% to more than 650 enterprises globally. It added that the majority of these customers were replacing legacy CPM applications. “OneStream is replacing multiple cumbersome and antiquated legacy systems with a single, unified platform to help organizations better and more efficiently support their financial operations, such as financial close, consolidation, planning, reporting, and analysis,” Koefoed said, adding that CPM constitutes “just a small part” of its competitors’ businesses, while newer cloud-based solutions only address a subset of CPM requirements. “OneStream goes beyond this, addressing all aspects of corporate finance, local finance, and diverse operational business units in a single application — streamlining financial processes and planning while also maintaining compliance and regulatory reporting.” OneStream had bootstrapped its way to profitability ahead of a $500 million-plus investment from KKR in 2019 that valued the company at more than $1 billion. A new $6 billion valuation puts OneStream alongside Anaplan and Blackline, which currently hold similar valuations on the public markets. Indeed, with a fresh $200 million in the bank from backers such as D1 Capital Partners, Tiger Global, and Investment Group of Santa Barbara (IGSB), Koefoed confirmed that OneStream is now contemplating the transition from being a private to a public company, though he stopped short of specifying a time frame."
https://venturebeat.com/2021/04/06/servicenow-partners-with-qualtrics-on-feedback-loop-using-sentiment-data-from-surveys/,ServiceNow partners with Qualtrics on feedback loop using sentiment data from surveys,"ServiceNow and Qualtrics today announced they are working together to integrate sentiment data captured using surveys within workflows running on the Now platform. The feature will become available sometime in the second half of this year. The two providers of software-as-a-service (SaaS) platforms will integrate their offerings for managing both IT and customer service. The goal is to create a feedback loop within those workflows that will enable organizations to better optimize workflows over time, said Michael Ramsey, vice president of product management for customer workflow products at ServiceNow. In some cases, that feedback loop will be created by manually reviewing survey data, while in others it might be automatically integrated with workflows that could be dynamically adjusted based on the feedback captured by a Qualtrics survey, Ramsey noted. “It’s about making feedback actionable,” Ramsay said. “We’re enabling a modern digital experience.” ServiceNow already has a similar relationship with SurveyMonkey to capture survey data. But Qualtrics is evolving into an experience management platform that makes it possible to, for example, trigger escalations within workflows based on feedback gathered, Qualtrics chief product officer Jay Choi said. As organizations embrace digital business transformation more aggressively in the wake of the COVID-19 pandemic, many of them have no way of knowing whether customers and end users are viewing these changes in a positive light, Choi noted. SaaS platforms such as Qualtrics provide a simple way to capture that feedback at a time when many organizations are relying on online processes to engage customers. “There’s an experience gap,” Choi said. In the last year, many organizations have accelerated digital business transformation initiatives out of necessity. But in the rush to roll those processes out, many of the organizations didn’t have the time to properly vet them. As the pandemic eventually subsides, most organizations will need to revisit digital processes that were hastily created to simply stay engaged with customers. At the same time, internal IT teams created self-service portals for employees working from home to help combat the COVID-19 pandemic. Most of those internal IT teams, however, don’t have any direct feedback from end users on the value of those efforts. Following the rollout of COVID-19 vaccinations, employees will be split between home and the office, depending on their personal preference and company mandates. Internal IT teams will once again need to adjust the IT services they provide to those end users — hopefully, with more input this time. Organizations have been launching customer surveys for decades. Platforms like Qualtrics and SurveyMonkey simply make it easier for certain types of businesses to launch them without having to contract a dedicated customer research team. Of course, the feedback any organization collects is only as good as the questions asked, so savvy organizations will still need to ask questions in a way that doesn’t bias survey results. In the meantime, a lot of organizations are about to discover just how well-received their digital business transformation efforts actually are. A digital business process might be more cost-effective for organizations to deliver, but it’s not uncommon for end customers to resent the level of data entry that has been shifted their way to enable that process. In fact, as organizations are increasingly judged by the online experience they provide as much as by the products and services they sell, many are about to discover to what degree online experiences negatively impact customer loyalty. But the first step toward determining what customers think is capturing their sentiment at the point where those digital services are being consumed."
https://venturebeat.com/2021/04/06/sendbird-secures-100m-to-help-businesses-add-chat-voice-and-video-calling-to-their-apps/,"Sendbird secures $100M to help businesses add chat, voice, and video calling to their apps","Sendbird, a platform that makes it easier for businesses to add messaging, voice, and video chat functionality to their apps, has raised $100 million in a series C round of funding at a $1.05 billion valuation. The raise comes while the so-called API economy is thriving as businesses across the spectrum have been forced to embrace digital transformation, be that through extending online customer service channels or expanding into video-based telehealth. The API economy was on an upward trajectory long before the global pandemic took hold, though, driven in part by a gradual shift from monolithic on-premises software to the cloud and microservices-based applications. Smaller, function-based components are easier to develop and maintain, with individual teams or developers taking responsibility for a single service — and APIs are integral to joining them all together. Moreover, consumers and end-users increasingly expect to be able to engage with companies directly through their mobile apps. But a company that offers an app-based food delivery service, for example, doesn’t really want to consume resources building their own communications infrastructure to enable customers to chat with their driver — it’s much easier if they can leverage platforms that were custom-built for that purpose. This is where Sendbird comes into play. “Even before Covid, there has been a shift to more and more of the tasks we accomplish in our lives occurring within mobile apps — online purchases, entertainment, food delivery, and lots of others,” Sendbird cofounder and CEO John S. Kim told VentureBeat. “Brands are increasingly choosing in-app chat over SMS as the way of connecting with users and connecting users with each other within the mobile and sometimes Web experience. These interactions facilitate purchases, provide support, and build loyalty. This is what’s been driving Sendbird’s growth for the last five years and continues to do so as the shift from offline to online continues.” There has been a flurry of activity across the API sphere of late: MessageBird acquired Pusher to expand its real-time communication APIs; Idera, meanwhile, acquired Apilayer, a startup that provides cloud-based APIs to big names such as Amazon, Apple, and Facebook; and RapidAPI acquired Paw to help developers build, test, and manage APIs. And in the funding sphere, companies including MessageBird, Postman, and Kong have all raised large sums of money at multi-billion dollar valuations over the past year. Founded out of Korea in 2013, Sendbird had largely focused on offering chat and messaging services to developers, but last March it expanded to offer real-time voice- and video-calling too. Although businesses can already choose from a wide array of free existing tools to connect with clients or customers, they don’t provide sufficient control over the experience, which is why many prefer to create custom solutions themselves in-house. “Smaller companies typically rely on free services like Zoom or WhatsApp to connect with their customers,” Kim said. “But brands who want to control the branding and user experience, get the benefits of the data and analytics, and integrate conversations into a core workflow — such as connecting a seller with a buyer who has questions — those businesses are going to invest in a great mobile experience and that experience is going to need chat, voice, and video interactions as a core piece.” Sendbird’s typical customer is a mobile-first digital company rather than traditional enterprise clients such as banks or insurance companies. For example, it counts several mobile wallets as customers, such as Indian super app Paytm. That said, Sendbird does have traditional enterprise clients too, including Korea Telecom and ServiceNow. “We do have traditional industries that did not start cloud first or mobile first as our customers, but going after those companies proactively is not a focus for us,” Kim said. Prior to now, Y Combinator alum Sendbird had raised around $121 million in funding, the bulk of which arrived via its series B round of funding which closed in 2019. The company’s latest cash injection was spearheaded by Steadfast Capital Ventures, with participation from Emergence Capital, Softbank Vision Fund 2, World Innovation Lab, Iconiq Growth, Tiger Global Management, and Meritech Capital, and it said that it plans to use the funds to “aggressively accelerate its R&D efforts” and hire across its key hubs in San Mateo (California), New York, London, Munich, Singapore, Bengaluru, and Seoul."
https://venturebeat.com/2021/04/06/upsolver-raises-25m-to-enable-no-code-data-analytics-in-the-cloud/,Upsolver enables no-code data analytics in the cloud with $25M funding,"Upsolver, a no-code startup that enables analytics on cloud data lakes, this morning announced that it raised $25 million in financing led by Scale Venture Partners. The company, which also today launched a free community edition of its product, says the funds will be used to hire engineers and scale its go-to-market efforts. Enterprises are rapidly adopting the cloud — 68% of CIOs ranked migrating to the cloud as their top IT spending driver in 2020, according to a Deloitte survey. But building an analytics-ready cloud data lake can be complex and expensive. A recently published Statista report found that around 83% of cloud practitioners considered security, managing cloud spend, governance, and a lack of resources to be significant barriers to entry. Upsolver develops software designed to prime data lakes for analytics, CEO Ori Rafael told VentureBeat via email. Founded in 2014 by Rafael and Yoni Eini, the company’s platform offers a visual structured query language (SQL) interface and automations for data optimization, tuning, and orchestration. “We wanted to store data affordably in the cloud without analytics vendor lock-in,” Rafael said. “Unfortunately, what used to take three hours using SQL turned into 30 days of hand-coding and complex Spark configuration. We created Upsolver to bridge this gap between raw cloud data and analytics-ready data.”  Upsolver’s product lets companies perform analytics using a range of query engines and data systems including PrestoDB, Trino, Athena, Snowflake, Redshift, Synapse Analytics, Splunk, and Elastic. Ultimately, the goal is to replace all a customer’s code-heavy approaches with Upsolver’s compute layer, which sits between the customer’s cloud storage and their preferred tools, engines, and apps. “Upsolver is a critical component for successfully implementing a cloud data lake for analytics, which is a popular approach due to the affordability that data lakes provide. We complement cloud data warehouses, search engines, and other purpose-built data stores as well,” Rafael said. “Upsolver is often used to output prepared data to those platforms making data available to standalone query engines. Data engineers use Upsolver’s visual user interface to build any data transformations and preparation tasks. This generates SQL that the data engineer can also directly edit to fine-tune the processing.” Rafael says that Upsolver will soon add the ability to replicate databases into data lakes while keeping them up to date. The platform recently launched on Azure and is set to become available in a community edition delivering “free-forever” capabilities to those with smaller workloads, providing a proving ground for companies who might want to work with Upsolver on larger use cases. Scale partner Ariel Tseitlin asserts that Upsolver benefits from having a foot in two fast-growing markets: big data analytics and data lakes. The global data lake market size was valued at $7.6 billion in 2019, according to Grand View Research. And in 2017, Forbes reported that 53% of companies had adopted big data analytics, with a large portion opting to run workloads in the cloud. Thirty-employee Upsolver says that revenue tripled 2020 as brands including Cox Automotive, Wix, and AppsFlyer joined its customer base. “Monolithic analytics platforms are a thing of the past. Today’s organizations require a variety of analytics tools to fully capitalize on their data,” Tseitlin said in a press release. “Data lakes originally promised this variety and openness but also required a large, ongoing investment in engineering. Upsolver eliminates this trade-off.” Existing investors Vertex Ventures US, Wing Venture Capital, and JVP also participated in Upsolver’s series B round. It comes on the heels of a $13 million series A and brings the company’s total raised to date to $42 million."
https://venturebeat.com/2021/04/06/threat-intelligence-platform-threatquotient-secures-22-5m/,Threat intelligence platform ThreatQuotient secures $22.5M,"ThreatQuotient, a security operations platform provider, today announced that it raised $22.5 million, a mix of debt and $13 million in equity. The company says it’ll use the proceeds to expand its workforce and the availability of its products globally. More than three quarters of IT security leaders anticipate a major breach involving a critical infrastructure organization in the near future, according to a Black Hat USA survey. It’s estimated that 31% of organizations have experienced cyberattacks on operational technology infrastructure. Perhaps unsurprisingly, Gartner found that organizations planned to invest $17.48 billion in infrastructure protection in 2020. ThreatQuotient was founded in 2013 by Ryan Trost and Wayne Chiang as a part of an effort to build a centrally managed source of cybersecurity solutions. The company’s product, ThreatQ, is designed to serve as a threat intelligence platform with an integrated, self-tuning library that features a workbench allowing for threat detection and response. ThreatQ can score and prioritize cyber threats based on customers’ parameters as well as facilitate aggregation, operationalization across systems and teams. It offers configuration and integrations in addition to workload and enrichment, coordinating inefficiencies that span security operations. Using ThreatQ, team leaders can direct actions, assign tasks, and see the results in real time. They can also import and aggregate data sources and export intelligence to third-party tools via integrations with feeds and security systems. “ThreatQ [uses] analytics and machine learning to determine relevance and priority for specific companies. This is the basis for our dynamic scoring and self-tuning capabilities. As more data and context is captured in the threat library, the platform will reprioritize the data to ensure priority is known and appropriate actions are taken,” CEO and president John Czupak explained to VentureBeat via email. “ThreatQuotient also offers [a]  cybersecurity situation room designed for collaborative threat analysis, shared, accelerated understanding, and coordinated response. Built on top of the ThreatQ platform, [it] allows for the capturing, learning and sharing of knowledge.” ThreatQ can be deployed solely on-premises or in cloud-based environments, plus software-only distributions for virtual machines. ThreatQuotient also offers a family of appliances to meet various performance requirements. In 2020, 115-employee ThreatQuotient says it secured a “record” number of new customers in 12 countries, bringing its customer base to over 100 companies. The startup also claims to have achieved a “record” number of transactions following the launch of a fully managed version of its platform. “As a result of strong performance in 2020, we welcomed an opportunity to secure additional funding and add new investors to our syndicate. ThreatQuotient is meeting a critical need for security operations solutions, and we have significant expansion plans to continue this momentum,” Czupak continued. “Fortunately, we are in a position where the products and services we provide are critical to our customer’s security operations. In some cases, opportunities accelerated because security became higher on the list of priorities for organizations during the pandemic.” New Enterprise Associates, Adams Street Partners, Escalate Capital, Blu Ventures, Cisco Investments, and Gaingels participated in the latest funding round. It brings the company’s total raised to over $60 million."
https://venturebeat.com/2021/04/05/google-triumphs-after-epic-java-api-copyright-battle-with-oracle/,Google triumphs after epic Java API copyright battle with Oracle,"(Reuters) — The U.S. Supreme Court handed Alphabet’s Google a major victory on Monday, ruling that its use of Oracle’s software code to build the Android operating system that runs most of the world’s smartphones did not violate federal copyright law. In a 6-2 decision, the justices overturned a lower court’s ruling that Google’s inclusion of Oracle’s software code in Android did not constitute fair use under U.S. copyright law. Justice Stephen Breyer, writing for the majority, said that allowing Oracle to enforce a copyright on its code would harm the public by making it a “lock limiting the future creativity of new programs. Oracle alone would hold the key.” Oracle and Google, two California-based technology giants with combined annual revenues of more than $175 billion, have been feuding since Oracle sued for copyright infringement in 2010 in San Francisco federal court. Google had appealed a 2018 ruling by the U.S. Court of Appeals for the Federal Circuit in Washington reviving the suit. The ruling spares Google a potentially massive damages verdict. Oracle had been seeking more than $8 billion, but renewed estimates went as high as $20 billion to $30 billion, according to two people with knowledge of the situation. “The decision gives legal certainty to the next generation of developers whose new products and services will benefit consumers,” said Kent Walker, Google’s senior vice president of global affairs. Oracle’s lawsuit accused Google of plagiarizing its Java software by copying 11,330 lines of computer code, as well as the way it is organized, to create Android and reap billions of dollars in revenue. Android, for which developers have created millions of applications, now powers more than 70% of the world’s mobile devices. Google has said it did not copy a computer program but rather used elements of Java’s software code needed to operate a computer program or platform. Federal copyright law does not protect mere “methods of operation.” The companies also disputed whether Google made fair use of Oracle’s software code, making it permissible under the 1976 Copyright Act. Dorian Daley, Oracle’s executive vice president and general counsel, said that with the ruling “the Google platform just got bigger and market power greater” and “the barriers to entry higher and the ability to compete lower.” “They stole Java and spent a decade litigating as only a monopolist can. This behavior is exactly why regulatory authorities around the world and in the United States are examining Google’s business practices,” Daley said. Technology industry trade groups cheered the ruling, saying an Oracle victory in the case would have inhibited competition by making it harder to use programming elements to ensure computer interoperability. “The high court’s decision that fair use extends to the functional principles of computer code means companies can offer competing, interoperable products,” said Matt Schruers, president of the Computer & Communications Industry Association. Shares in Oracle rose nearly 4%, and Alphabet gained 4.4% in mid-afternoon trading. In Monday’s ruling, Breyer wrote, “Google’s copying was transformative,” adding that the company repurposed Oracle’s code in a way that helps developers create programs. The ruling sidestepped the question of whether Oracle’s code was entitled to copyright protection in the first place. In a dissenting opinion, Justice Clarence Thomas, joined by Justice Samuel Alito, said the court should have found that Oracle’s work deserved a copyright and Google’s use was “anything but fair.” Noting that Apple and Microsoft did not resort to copying like Google did to create mobile operating systems, Thomas said the ruling will harm competition. If “companies may now freely copy libraries of declaring code whenever it is more convenient than writing their own, others will likely hesitate to spend the resources Oracle did to create intuitive, well-organized libraries that attract programmers and could compete with Android,” Thomas wrote. Google twice lost at the Federal Circuit, in 2014 and 2018. A jury cleared Google in 2016. The Federal Circuit overturned that decision in 2018, finding that Google’s incorporation of elements of Oracle’s “application programming interfaces” was not permitted under the fair use doctrine, rejecting Google’s argument that by adapting them to a mobile platform it transformed them into something new. Justice Amy Coney Barrett did not participate in the ruling. She had not yet joined the court when arguments were held on October 7."
https://venturebeat.com/2021/04/05/m-files-acquires-hubshare-to-gain-access-to-content-sharing-portal/,M-Files acquires Hubshare to gain access to content-sharing portal,"M-Files today announced it has acquired Hubshare as part of an effort to make it easier to share files and data stored in its enterprise content management (ECM) platform. Terms of the deal were not disclosed. Hubshare presents end users with a portal through which they can access data and files without requiring IT teams to move everything into one central repository, said M-Files CEO Antti Nivala. M-Files plans to continue to offer Hubshare as a standalone platform while simultaneously working to tighten integration between Hubshare and the ECM platform provided by M-Files, Nivala added. In general, most Hubshare users have been people working for different organizations who needed an easier way to collaborate and share files, documents, and other classes of data, noted Nivala. As the number of digital business transformation initiatives involving multiple organizations continues to expand, the need to seamlessly share information will only increase in the months ahead, Nivala said. In effect, Hubshare is evolving into the front-end portal through which customers can centrally access multiple backend platforms for storing data via a portal that keeps track of which users are authorized to access specific files and documents, Nivala said. M-Files earlier this year raised an additional $80 million in funding. Ultimately, the company’s goal is to apply AI to metadata independently of the ECM platform it was captured from. Armed with AI capabilities, it should become simpler to optimize workflows spanning multiple ECM platforms. Despite the rise of massive data lakes in the cloud, Nivala said there will never be a single source of truth in any enterprise. Data will continue to be created and managed within the context of a wide range of application silos that will each continue to have their own set of master data. “There are going to be a number of master locations for data,” he said. The challenge IT organizations face is finding a way to affordably make all that data accessible to end users inside and outside of an organization in a way that doesn’t compromise security and, just as importantly, can be audited from a compliance perspective, Nivala added. Most IT organizations don’t have a great track record when it comes to centralizing data management. However, there are now more users than ever that need to directly access data that was created outside of an application environment they have express permission to use. As such, the need for a portal that makes it easier to navigate data residing in multiple master data repositories is rising. Data is now being stored independently of the application first employed to create it. In fact, many organizations will soon find themselves judged based on how easy they are to work with while, paradoxically, being required to make sure that content stays secure. As critical as security might be, productivity is still the most critical of all metrics applied to any workflow. Of course, competition among providers of ECM platforms is already fierce. These platforms are racing to embed AI capabilities that will make it simpler to access and integrate a wide range of content. It’s not clear to what degree those capabilities might dissuade organizations from launching massive data warehouse projects as it becomes simpler for both end users and third-party applications to access content wherever it happens to reside. Regardless of approach, however, the one thing that is apparent is that legacy approaches to managing content will no longer suffice in an era where the value of data rises in direct proportion to how often and widely it’s employed."
https://venturebeat.com/2021/04/05/government-audit-of-ai-with-ties-to-white-supremacy-finds-no-ai/,Government audit of AI with ties to white supremacy finds no AI,"In April 2020, news broke that Banjo CEO Damien Patton, once the subject of profiles by business journalists, was previously convicted of crimes committed with a white supremacist group. According to OneZero’s analysis of grand jury testimony and hate crime prosecution documents, Patton pled guilty to involvement in a 1990 shooting attack on a synagogue in Tennessee. Amid growing public awareness about algorithmic bias, the state of Utah halted a $20.7 million contract with Banjo, and the Utah attorney general’s office opened an investigation into matters of privacy, algorithmic bias, and discrimination. But in a surprise twist, an audit and report released last week found no bias in the algorithm because there was no algorithm to assess in the first place. “Banjo expressly represented to the Commission that Banjo does not use techniques that meet the industry definition of artificial Intelligence. Banjo indicated they had an agreement to gather data from Twitter, but there was no evidence of any Twitter data incorporated into Live Time,” reads a letter Utah State Auditor John Dougall released last week. The incident, which VentureBeat previously referred to as part of a “fight for the soul of machine learning,” demonstrates why government officials must evaluate claims made by companies vying for contracts and how failure to do so can cost taxpayers millions of dollars. As the incident underlines, companies selling surveillance software can make false claims about their technologies’ capabilities or turn out to be charlatans or white supremacists — constituting a public nuisance or worse. The audit result also suggests a lack of scrutiny can undermine public trust in AI and the governments that deploy them. Dougall carried out the audit with help from the Commission on Protecting Privacy and Preventing Discrimination, a group his office formed weeks after news of the company’s white supremacist associations and Utah state contract. Banjo had previously claimed that its Live Time technology could detect active shooter incidents, child abduction cases, and traffic accidents from video footage or social media activity. In the wake of the controversy, Banjo appointed a new CEO and rebranded under the name safeXai. “The touted example of the system assisting in ‘solving’ a simulated child abduction was not validated by the AGO and was simply accepted based on Banjo’s representation. In other words, it would appear that the result could have been that of a skilled operator as Live Time lacked the advertised AI technology,” Dougall states in a seven-page letter sharing audit results. According to Vice, which previously reported that Banjo used a secret company and fake apps to scrape data from social media, Banjo and Patton had gained support from politicians like U.S. Senator Mike Lee (R-UT) and Utah State Attorney General Sean Reyes. In a letter accompanying the audit, Reyes commended the results of the investigation and said the finding of no discrimination was consistent with the conclusion the state attorney general’s office reached because there simply wasn’t any AI to evaluate. “The subsequent negative information that came out about Mr. Patton was contained in records that were sealed and/or would not have been available in a robust criminal background check,” Reyes said in a letter accompanying the audit findings. “Based on our first-hand experience and close observation, we are convinced the horrible mistakes of the founder’s youth never carried over in any malevolent way to Banjo, his other initiatives, attitudes, or character.” Alongside those conclusions are a series of recommendations for Utah state agencies and employees involved in awarding such contracts. Recommendations for anyone considering AI contracts include questions they should be asking third-party vendors and the need to conduct an in-depth review of vendors’ claims and the algorithms themselves. “The government entity must have a plan to oversee the vendor and vendor’s solution to ensure the protection of privacy and the prevention of discrimination, especially as new features/capabilities are included,” reads one of the listed recommendations. Among other recommendations are the creation of a vulnerability reporting process and evaluation procedures, but no specifics were provided. While some cities have put surveillance technology review processes in place, local and state adoption of private vendors’ surveillance technology is currently happening in a lot of places with little scrutiny. This lack of oversight could also become an issue for the federal government. The Government by Algorithm report Stanford University and New York University jointly published last year found that roughly half of algorithms used by federal government agencies come from third-party vendors. The federal government is currently funding an initiative to create tech for public safety, like the kind Banjo claimed to have developed. The National Institute of Standards and Technology (NIST) routinely assesses the quality of facial recognition systems and has helped assess the role the federal government should play in creating industry standards. Last year, it introduced ASAPS, a competition in which the government is encouraging AI startups and researchers to create systems that can tell if an injured person needs an ambulance, whether the sight of smoke and flames requires a firefighter response, and whether police should be alerted in an altercation. These determinations would be based on a dataset incorporating data ranging from social media posts to 911 calls and camera footage. Such technology could save lives, but it could also lead to higher rates of contact with police, which can also cost lives. It could even fuel repressive surveillance states like the kind used in Xinjiang to identify and control Muslim minority groups like the Uyghurs. Best practices for government procurement officers seeking contracts with third parties selling AI were introduced in 2018 by U.K. government officials, the World Economic Forum (WEF), and companies like Salesforce. Hailed as one of the first such guidelines in the world, the document recommends defining public benefit and risk and encourages open practices as a way to earn public trust. “Without clear guidance on how to ensure accountability, transparency, and explainability, governments may fail in their responsibility to meet public expectations of both expert and democratic oversight of algorithmic decision-making and may inadvertently create new risks or harms,” the British-led report reads. The U.K. released official procurement guidelines in June 2020, but weeks later a grading algorithm scandal sparked widespread protests. People concerned about the potential for things to go wrong have called on policymakers to implement additional legal safeguards. Last month, a group of current and former Google employees urged Congress to adopt strengthened whistleblower protections in order to give tech workers a way to speak out when AI poses a public harm. A week before that, the National Security Commission on Artificial Intelligence called on Congress to give federal government employees who work for agencies critical to national security a way to report misuse or inappropriate deployment of AI. That group also recommends tens of billions of dollars in investment to democratize AI and create an accredited university to train AI talent for government agencies. In other developments at the intersection of algorithms and accountability, the documentary Coded Bias, which calls AI part of the battle for civil rights in the 21st century and examines government use of surveillance technology, started streaming on Netflix today. Last year, the cities of Amsterdam and Helsinki created public algorithm registries so citizens know which government agency is responsible for deploying an algorithm and have a mechanism for accountability or reform if necessary. And as part of a 2019 symposium about common law in the age of AI, NYU professor of critical law Jason Schultz and AI Now Institute cofounder Kate Crawford called for businesses that work with government agencies to be treated as state actors and considered liable for harm the way government employees and agencies are."
https://venturebeat.com/2021/04/05/bank-software-provider-alkami-chases-2b-ipo/,Bank software provider Alkami chases $2B IPO,"(Reuters) — Alkami Technology is aiming for a valuation of up to $2.08 billion in a U.S. initial public offering (IPO), the banking software provider said in its prospectus on Monday. The company said it was looking to raise as much as $150 million by selling 6 million shares at a price range of $22 to $25 per share. Reuters reported on the Plano, Texas-based firm’s IPO plans in February. Backed by investors including General Atlantic and D1 Capital, Alkami supplies cloud-based platforms that banks and credit unions can use to set up digital offerings for their retail and business customers. The company’s listing plans come as the COVID-19 pandemic has shown that financial services companies need secure and effective digital platforms. Smaller banks typically do not have the resources to invest in developing such systems internally and instead turn to third-party providers like Alkami. Alkami’s website shows its services are used by more than 160 financial institutions. The company earlier said its shares would be listed on the Nasdaq under the symbol “ALKT.” Goldman Sachs, JP Morgan, and Barclays are the lead underwriters for the offering."
https://venturebeat.com/2021/04/04/what-is-a-streaming-database/,What is a streaming database?,"The internet of things is everywhere, and the data is piling up. The new tiny, embeddable computers are energizing managers and engineers with the possibility of using all of this data to control everything from industrial plants to personal homes. The database administrators, though, aren’t as thrilled because they are expected to gather, store, and analyze this often unceasing firehose of bits. Some programmers and DBAs are developing pipelines that can accept, analyze, and store the important bits. These so-called streaming databases are tools designed to handle both the unstoppable incoming flow as well as the endless queries from tools that want to make decisions based upon the data. Streaming databases are close cousins to other new classes of tools like time-series databases or log databases. All are designed to track a series of events and enable queries that can search and produce statistical profiles of blocks of time. The streaming databases can respond to queries for data and also statistics about the data, generate reports from these queries, and populate all of the dashboards that track what’s happening to allow the users to make smart decisions about the telemetry. The tools are essentially pipelines that start out analyzing the incoming data flow and end up storing aggregated data in a database that’s easily queried. Some think of the streaming database as the entire system, and some imagine that the system is created by attaching the pipeline to a more traditional database. In both cases, the entire system is ready to answer questions. Some good examples of important use cases include: The data inside is often split, at least philosophically, into two tiers. The raw input, often called “streams,” are immutable, append-only sequences of events. They’re meant to be a historical record of what happened and when. The second tier is built from watching the streams and constructing summaries, often statistical, about the events. They might, for instance, count the number of times that an event happened each day over the last month or find the average value over each week in a year. The analysis is usually stored in tables that are often similar in structure and behavior to traditional relational databases. Indeed, it’s not uncommon for developers to connect a traditional database for these results. Some streaming databases are designed to dramatically reduce the size of the data to save storage costs. They can, say, replace a value collected every second with an average computed over a day. Storing only the average can make long-term tracking economically feasible. Streaming opens up some of the insides of a traditional database. Standard databases also track a stream of events, but they’re usually limited to changes in data records. The sequence of INSERTs, UPDATEs, and DELETEs are normally stored in a hidden journal or ledger inside. In most cases, the developers don’t have direct access to these streams. They’re only offered access to the tables that show the current values. Streaming databases open up this flow and makes it simpler for developers to adjust how the new data is integrated. Developers can adjust how the streams from new data are turned into tabular summaries, ensuring that the right values are computed and saved while the unneeded information is ignored. The opportunity to tune this stage of the data pipeline allows streaming databases to handle markedly larger datasets. The traditional databases are finding a role in streaming applications, but usually as a destination that lies downstream. The data flows through another tool that analyzes it and generates more concise values for more permanent storage in a traditional database. The legacy software and reporting tools can work easily with it. For instance, Oracle Streams can be deployed either as a service or as on-premises installation. It will gather and transform data from a variety of sources and then deposit it with other services that can include their own databases. The message format is designed to be compatible with Apache Kafka, an open standard, allowing it to be integrated with other Kafka applications. IBM’s product, also called Streams, emphasizes the analytical power of the pipeline integrated with some of the machine learning products. It is also compatible with Kafka and can deposit the results in a number of destinations, including IBM’s own data warehouses. Microsoft’s Stream Analytics also emphasizes the analytics that can occur along the path from the event’s first appearance to its eventual destination, which can be any of Azure’s storage solutions including the SQL databases. The processing, which can be written in an SQL-like language and incorporate other common languages like JavaScript, also may train machine learning models via Azure’s ML Service. The SQL dialect includes temporal constraints used to transform the incoming data, which is usually tracking the time and date. The Azure Stream Analytics service is also tightly integrated with Microsoft’s AI services to use machine learning and video analytics to deconstruct the data stream. It offers an SQL-like syntax that can be extended with code written in JavaScript or C#. New companies are tackling the challenge by either building entirely integrated tools or simply creating a stream-handling layer that works with existing databases. Those that integrate with established infrastructure can leverage all of the other compatible tools, while the entirely new versions have the advantage of building everything from scratch. Many of the tools that integrate with existing databases are built on Apache’s Kafka, an open source message handing framework that’s often used to link together multiple software packages. Kafka itself handles the chores of buffering and delivering the messages containing the events. This buffering, incidentally, requires storing the stream of events, making Kafka a kind of very basic database that eventually delivers the data to another. Equalum, for instance, offers a tool for transforming a data stream en route to a data warehouse or data lake using more traditional databases. It’s built upon an open source foundation of Apache’s Kafka and Spark and offers a simplified, visual coding framework that allows the data pathway to be defined as a flowchart. Developers who enjoy working in SQL will appreciate ksqlDB, a tool for ingesting and storing data that uses a version of SQL to specify major tasks. “Use a familiar, lightweight syntax to pack a powerful punch,” the sales literature promises. “Capture, process, and serve queries using only SQL. No other languages or services are required.” The tool is tightly integrated with Kafka to ensure it is simpler to install in existing applications that use it. Amazon calls its major offering Kinesis and offers special, preconfigured pathways for working with video feeds. It’s integrated with some of AWS’ AI tools like the Rekognition for video analysis and SageMaker for basic machine learning. Others are starting to build open source projects like Debezium that can transform data from event streams managed by Kafka or other pipelines. In many ways, streaming databases are just supersets of the traditional model. If you think of the standard INSERTs and DELETEs as events, then any of the standard applications can be handled by the streaming cousins. Much of the overhead, though, may be wasted if the application does not require constantly evolving analysis. Many streaming databases also offer fewer of the traditional functions or APIs because their first job is taming the endless flow of data. They may not offer the complex views or elaborate joins, at least not for the incoming data. If the results are stored in a more traditional relational database, it will have all of the features associated with it. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/04/bounding-your-ml-models-dont-let-your-algorithms-run-wild/,Bounding your ML models: Don’t let your algorithms run wild,"The purpose of designing and training algorithms is to set them loose in the real world, where we expect performance to mimic that of our carefully curated training data set. But as Mike Tyson put it, “everyone has a plan, until they get punched in the face.” And in this case, your algorithm’s meticulously optimized performance may get punched in the face by a piece of data completely outside the scope of anything it encountered previously. When does this become a problem? To understand, we need to return to the basic concepts of interpolation vs. extrapolation. Interpolation is an estimation of a value within a sequence of values. Extrapolation estimates a value beyond a known range. If you’re a parent, you can probably recall your young child calling any small four-legged animal a cat, as their first classifier only used minimal features. Once they were taught to extrapolate and factor in additional features, they were able to correctly identify dogs too. Extrapolation is difficult, even for humans. Our models, smart as they might be, are interpolation machines. When you set them to an extrapolation task beyond the boundaries of their training data, even the most complex neural nets may fail. What are the consequences of this failure? Well, garbage in, garbage out. Beyond the deterioration of model results in the real world, the error can propagate back to training data in production models, reinforcing erroneous results and degrading model performance over time. In the case of mission critical algorithms, as in healthcare, even a single erroneous result should not be tolerated. What we need to adopt, and this is not a unique problem in the domain of machine learning, is data validation. Google engineers published their method of data validation in 2019 after running into a production bug. In a nutshell, every batch of incoming data is examined for anomalies, some of which can only be detected by comparing training and production data. Implementing a data validation pipeline had several positive outcomes. One example the authors present in the paper is the discovery of missing features within the Google Play store recommendation algorithm — when the bug was fixed, app install rates increased by 2 percent. Researchers from UC Berkeley evaluated the robustness of 204 image classification models in adapting to distribution shifts arising from natural variation in data. Despite the models being able to adapt to synthetic changes in data, the team found little to no adaptation in response to natural distribution shifts, and they consider this an open research problem. Clearly this is a problem for mission critical algorithms. Machine learning models in healthcare bear a responsibility to return the best possible results to patients, as do the clinicians evaluating their output. In such scenarios, a zero-tolerance approach to out-of-bounds data may be more appropriate. In essence, the algorithm should recognize an anomaly in the input data and return a null result. Given the tremendous variation in human health, along with possible coding and pipeline errors, we shouldn’t allow our models to extrapolate just yet. I’m the CTO at a health tech company, and we combine these approaches: We conduct a number of robustness tests on every model to determine whether model output has changed due to variation in the features of our training sets. This training step allows us to learn the model limitations, across multiple dimensions, and also uses explainable AI models for scientific validation. But we also set out of bound limitations on our models to ensure patients are protected. If there’s one takeaway here, it’s that you need to implement feature validation for your deployed algorithms. Every feature is ultimately a number, and the range of numbers encountered during training is known. At minimum, adding a validation step that ascertains whether a score in any given run is within the training range will increase model quality. Bounding models should be fundamental to trustworthy AI. There is much discussion on design robustness and testing with adversarial attacks (which are designed specifically to fool models). These tests can help harden models but only in response to known or foreseen examples. However, real world data can be unexpected, beyond the ranges of adversarial testing, making feature and data validation vital. Let’s design models smart enough to say “I know that I know nothing” rather than running wild. Niv Mizrahi is Co-founder and CTO of Emedgene and an expert in machine learning, big data, and large-scale distributed systems. He was previously Director of Engineering at Taykey, where he built an R&D organization from the ground up and managed the research, big data, automation, and operations teams."
https://venturebeat.com/2021/04/04/how-cloud-architectures-defend-against-the-cyber-attack-surge/,How cloud architectures defend against the cyber attack surge,"As we look to a post-pandemic world, we can expect to see companies invest in building resilience to destructive-type attacks. 2020 saw a record number of distributed denial-of-service (DDoS) and ransomware attacks, and the numbers are expected to remain high through the rest of this decade. The cloud — and cloud-native architectures — can help deliver resilience due to three key attributes: Distributed applications and services: If your applications are leveraging a distributed delivery model, for example leveraging cloud-based services such as content delivery networks (CDNs), then you have to worry less about DDoS attacks, as these attacks work best by concentrating their firepower in one direction. Immutable data sets: If your applications are leveraging solutions that do not modify records but rather are “append-on-write,” in other words your data set is immutable, then you have to worry less about attacks on the integrity of that data, as it is easier to detect and surface such attacks. Ephemeral workloads: Finally, if your applications are ephemeral in nature then you may worry less about attackers establishing persistence and moving laterally. And the value of confidential information (such as tokens associated with that application instance) is reduced, as those assets simply get decommissioned and new ones get instantiated within a relatively short frame of time. By leveraging modern cloud-native architectures that are distributed, immutable and ephemeral, you help address the issues of confidentiality, integrity and availability that have been the foundational triad of cybersecurity. So how are companies manifesting these attributes in their applications? Modern cloud architectures are moving from monolithic, tiered models to distributed microservices-based architectures, where each microservice can scale independently, within a geographic region or across regions. And each microservice can have its own, optimized storage and database, thereby allowing that service to run stateless (or perhaps more accurately using a shared-state model where the state is shared amongst the running instances via the storage/database layer). This allows those services to become truly ephemeral and distributed. This brings us to a concept that has seen quite a bit of discussion already in the context of the cloud — pets vs. cattle. Pets have a cute name and can be recognized individually. If a pet falls ill, the owner takes it to the vet. Owners give their pets a lifetime of caring and make sure they live healthy lives for as long as possible. Traditional applications are like pets. Each instance is unique. If the application gets infected, it is taken to the cyber vet. “Patch in place” is common with traditional applications, which make these instances unique. IT’s job is to keep the applications up and running for as long as possible. Cattle on the other hand, don’t have names, they have numbers. You generally cannot distinguish the cattle in the herd, and you don’t build relationships with them. If cattle fall ill or get infected, you cull the herd. Modern cloud applications are like cattle. You create many running instances of the services, and each instance is indistinguishable from the other. They are all manifested from a golden repository. You never patch-in-place, i.e. you never make the instances bespoke. Your job is to make the instances ephemeral, killing them quickly and creating new ones. In doing so, you build resilient systems rather than fragile ones. The cloud offers many tools to help build systems that follow this paradigm. For example, Amazon recently announced “chaos engineering” as-a-service, which allows organizations to introduce elements of chaos into their production workloads, such as taking down running instances, to ensure that the overall performance isn’t impacted and the workloads over time become resilient in the face of these types of operational setbacks. Getting to this point is a journey, and companies may need to take multiple steps to get there. For example, if you move your pets from an on-premises world to the cloud world without significantly altering the architecture of the applications, that’s just one step. The common term for this is “lift and shift.” Once your applications are in the cloud and you have started building familiarity with cloud native tools, you can work on re-architecting those pets into modern architectures that are distributed, immutable and ephemeral (i.e. cattle). In other words, you can move from pets-in-the-cloud to cattle-in-the-cloud. When you get to that point, you need to make sure you don’t regress and move back to creating pets again. In other words, don’t patch-in-place or keep instances up and running longer than necessary. Shehzad Merchant is CTO at Gigamon."
https://venturebeat.com/2021/04/03/ibm-bets-homomorphic-encryption-is-ready-to-deliver-stronger-data-security-for-early-adopters/,IBM bets homomorphic encryption is ready to deliver stronger data security for early adopters,"The topics of security and data have become almost inseparable as enterprises move more workloads to the cloud. But unlocking new uses for that data, particularly driving richer AI and machine learning, will require next-generation security. To that end, companies have been developing confidential computing to allow data to remain encrypted while it is being processed. But as a complement to that, a security process known as fully homomorphic encryption is now on the verge of making its way out of the labs and into the hands of early adopters after a long gestation period. Researchers like homomorphic encryption because it provides a certain type of security that can follow the data throughout its journey across systems. In contrast, confidential computing tends to be more reliant upon special hardware that can be powerful but is also limiting in some respects. Companies such as Microsoft and Intel have been big proponents of homomorphic encryption. Last December, IBM made a splash when it released its first homomorphic encryption services. That package included educational material, support, and prototyping environments for companies that want to experiment. In a recent media presentation on the future of cryptography, IBM director of strategy and emerging technology Eric Maass explained why the company is so bullish on “fully homomorphic encryption” (FHE). “FHE is a unique form of encryption, and it’s going to allow us to compute upon data that’s still in an encrypted state,” Maass said. First, some context. There are three general categories of encryption. The two classic ones are encryption for when data is at rest or stored and then “data in transit” that protects the confidentiality of data as it’s being transmitted over a network. The third one is the piece that has been missing: the ability to compute on that data while it’s still encrypted. That last one is key to unlocking all sorts of new use cases. That’s because until now, for someone to process that data, it would have to be unencrypted, which creates a window of vulnerability. That makes companies reluctant to share highly sensitive data involving finance or health. “With FHE, the ability to actually keep the data encrypted and never exposing it during the computation process, this has been somewhat akin to a missing leg in a three-legged crypto stool,” Maass said. “We’ve had the ability to encrypt the data at rest and in transit, but we have not historically had the ability to keep the data encrypted while it’s being utilized.” With FHE, the data can remain encrypted while being used by an application. Imagine, for instance, a navigation app on a phone that can give directions without actually being able to see any personal information or location. Companies are potentially interested in FHE because it would allow them to apply AI to data, such as from finance and health, while being able to promise users that the company has no way to actually view or access the underlying data. While the concept of homomorphic encryption has been of interest for decades, the problem is that FHE has taken a huge amount of compute power, so much so that it has been too expensive to be practicable. But researchers have made big advances in recent years. For instance, Maass noted that in 2011, it took 30 minutes to process a single bit using FHE. By 2015, researchers could compare two entire human genomes using FHE in less than an hour. “IBM has been working on FHE for more than a decade, and we’re finally reaching an apex where we believe this is ready for clients to begin adopting in a more widespread manner,” Maass said. “And that becomes the next challenge: widespread adoption. There are currently very few organizations here that have the skills and expertise to use FHE.” During the presentation, AI security group manager Omri Soceanu ran an FHE simulation involving health data being transferred to a hospital. In this scenario, an AI algorithm was used to analyze DNA for genetic issues that may reveal risks for prior medical conditions. That patient data would typically have to be decrypted first, which could raise both regulatory and privacy issues. But with FHE, it remains encrypted, thus avoiding those issues. In this case, the data is sent encrypted and remains so while being analyzed, and the results are also returned in an encrypted state. It’s important to note that this system was put in place using just a dozen lines of code, a big reduction from the hundreds of lines of code that have been required until recently. By reducing that complexity, IBM wants to make FHE more accessible to teams that don’t necessarily have cryptography expertise. Finally, Soceanu explained that the simulation was completed in .069 seconds. Just five years ago, the same simulation took a few hours, he said. “Working on FHE, we wanted to allow our customers to take advantage of all the benefits of working in the cloud while adhering to different privacy regulations and concerns,” he said. “What only a few years ago was only theoretically possible is becoming a reality. Our goal is to make this transition as seamless as possible, improving performance and allowing data scientists and developers, without any crypto skills, a frictionless move to analytics over encrypted data.” To accelerate that development, IBM Research has released open source toolkits, while IBM Security launched its first commercial FHE service in December. “This is aimed at helping our clients start to begin to prototype and experiment with fully homomorphic encryption with two primary goals,” Maass said. “First, getting our clients educated on how to build FHE-enabled applications and then giving them the tools and hosting environments in order to run those types of applications.” Maass said in the near term, IBM envisions FHE being attractive to highly regulated industries, such as financial services and health care. “They have both the need to unlock the value of that data, but also face extreme pressures to secure and preserve the privacy of the data that they’re computing upon,” he said. But he expects that over time a wider range of businesses will benefit from FHE. Many sectors want to improve their use of data, which is becoming a competitive differentiator. That includes using FHE to help drive new forms of collaboration and monetization. As this happens, IBM hopes these new security models will drive wider enterprise adoption of hybrid cloud platforms. The company sees a day, for instance, when due diligence for mergers and acquisitions is done online without violating the privacy of shareholders and when airlines, hotels, and restaurants use FHE to offer packages and promotions without giving their partners access to details of closely held customer datasets. “FHE will allow us to secure that type of collaboration, extracting the value of the data while still preserving the privacy of it,” Maass concluded."
https://venturebeat.com/2021/04/03/all-ai-driven-crm-platforms-are-not-created-equal/,All AI-driven CRM platforms are not created equal,"Customer relationships are changing in unpredictable ways compared to a year ago, creating unique data-driven challenges for marketers. The pandemic has made digital convenience a high priority with consumers who want a contextually rich, safe customer experience on any mobile device and are willing to switch brands and products to get it. The shift to ecommerce for everything was accompanied by the expectation of having all daily transactions be digital and touchless. As marketers struggle to decipher how changes in customer data can affect current and future campaigns, they are looking at what AI and machine learning can do to improve customer relationship management. Salesforce Research’s Sixth Annual State of Marketing Report found that 40% of B2B marketing leaders (manager level or higher) and 38% of B2C marketing leaders planned to increase their use of AI in 2020. That is on top of the 35% of B2B and B2C marketing leaders who said they are already using AI, according to the report. Salesforce surveyed 6,950 full-time marketing leaders from B2B (business-to-business), B2C (business-to-consumer), and B2B2C (business-to-business-to-consumer) companies around the world and found that 84% of marketers said they were using AI in 2020, up from 29% in 2018. CMOs and their marketing teams face the challenge of delivering results that drive revenue even as the markets redefine themselves. B2B marketing leaders rely on AI to improve their customer segmentation and lookalike audience modeling, according to Drift and the Marketing Artificial Intelligence Institute’s 2020 Marketing Leadership Benchmark Report. Other high priorities include personalizing channel experiences, discovering new data insights, and driving next-best actions, including offers in real time, automating customer interactions, and personalizing the overall customer journey. It stands to reason that AI-based CRM applications promise to improve existing processes’ speed and efficiency, increase revenue, and help find new services to sell. Too often, however, the apps are providing more process automation than AI-driven results. The demand for AI-based apps and platforms is extremely high, leading some CRM vendors to overstate the AI capabilities in their applications, which creates more hype across the CRM landscape. For example, process automation is often sold as AI, when what it does is independently perform simple, repetitive actions and tasks. If it can’t learn from datasets and initiate new workflows or ways of doing work, it isn’t AI. To provide some context, CRM is big business. Gartner’s latest market forecasts peg the worldwide CRM market at $56.5 billion in 2019, placing it as the largest segment of the enterprise software market, at 11.7% of global software revenue. Software rating site G2 Crowd lists 14 different types of AI-based CRM applications. One way to evaluate whether the CRM application actually utilizes AI or is just marketing hype is to look at feature areas individually. Each feature area is assigned a grade in the below list based on how much value AI delivers to marketers and the companies relying on these applications. AI-based sales assistants or bots
Despite an impressive amount of hype in the CRM community about AI-based sales assistants or bots increasing revenue, they are most often used to automate data entry and scheduling tasks and carry out routine sales force automation (SFA) tasks. Sales assistants or bots in their current generation often only rely on CRM datasets, drastically reducing their possible use cases. Bots are also purpose-built for specific tasks and are often process automation engines. Any organization considering these should give the product area a few generations to get a more integrated foundation in place. (Grade: B- / C) Configure, price, and quote (CPQ)
Dominated by rules- and constraint-based product configuration and process automation engines, CPQ is another area that’s overhyped when it comes to AI. CPQ benefits most from AI when it comes to guided selling and optimizing revenue management. For CPQ to deliver the maximum value it’s capable of, it needs to at least be integrated with an ERP and CRM system. Constraint-based configurators have been around for decades, as have process automation engines, two technologies that have at times been sold as low-end AI. It’s much easier to use product configuration rules from an existing configurator than to train configuration models, which is what a true AI-based configuration requires. (Grade: C- / D) Cross-sell and up-sell
Often sold as an integrated app within a configure, price, and quote (CPQ) or account-based marketing (ABM) system or platform, cross-sell and up-sell apps have progressed from relatively simple apps that integrate with product catalogs or product data to more advanced rules- and constraint-based scenarios, including AI-based apps that factor in customers’ personalized preferences. Cross-sell and up-sell apps have become the go-to option in ABM to expand sales into existing accounts yet are limited in how much business value they can deliver using AI. Process automation-based apps are sometimes sold as AI-based in this area of CRM. (Grade: B) Data intelligence solutions for sales
Vendors providing apps in this category move away from contact and company information and toward contextual intelligence using AI and ML. Vendors’ goals in transitioning to contextual intelligence include supporting sales prospects and selling scenarios with real-time data. Like many of the CRM apps mentioned, vendors in this category do not provide their own datasets. They have limited expertise in improving their data quality to get the most value out of this application. (Grade: B-) Sales predictive analytics (includes lead scoring)
AI’s impact on improving sales predictive analytics is evident in how effective these applications are in guiding sales rep, sales leader, and sales operations decision-making to improve margins and revenue. The best apps in this category are using machine learning to find new insights in account, sales history, and revenue data. Predictive forecasting, pipeline inspection, opportunity, and lead scoring are a few of the many areas where sales predictive analytics’ AI-based capabilities can contribute. (Grade: A+) Quota planning
AI- and ML-based quota planning apps are sold as part of an integrated sales performance management (SPM) platform or as a standalone product. The majority of apps in this category today support collaboration and workflows to define accurate, optimal sales quotas. The best apps in this category support various mathematical modeling approaches for assigning quotas across an organization. AI and ML algorithms are being used to set optimal quotas that are in turn distributed across an organization. (Grade: B) What’s limiting AI’s potential to deliver value in CRM today is the lack of consistent, high-quality data. How much of a contribution AI-based apps and platforms make as part of any CRM system is more dependent on the quality and availability of integrated data and less on the features of the app itself. Marketing organizations are renowned for having data quality problems, as data governance often isn’t a core strength of the department. There can be conflicting data structures, taxonomies, metatags, and a lack of consistency across all databases. Overcoming all of these obstacles and improving the data’s quality needs to come first, yet this can be a hurdle too high for marketers to clear. But each of these areas has the potential to deliver greater value in CRM once businesses overcome data quality challenges."
https://venturebeat.com/2021/04/03/pivoting-to-privacy-first-why-this-is-an-adapt-or-die-moment/,Pivoting to privacy-first: Why this is an adapt-or-die moment,"Operating in the digital advertising ecosystem isn’t for the faint of heart, and that’s never been truer than it is in 2021. The landscape is undergoing unprecedented transitions right now as we make a much-needed pivot to a privacy-first reality, and a lot of business models, practices, and technologies are not going to survive the upheaval. That said, I’m not here to make doomsday predictions. In fact, there are a lot of reasons for genuine optimism right now. As an industry, we’re heading in the right direction, and when we emerge on the other side of important transitions — including Google’s removal of third-party cookie support in Chrome and Apple’s limitations on IDFA — our industry will be stronger as a whole, as will consumer protections. Let’s take a look at the principles that will define the digital advertising and marketing world of the future, as well as the players that operate within it. Google gave the industry more than two years’ warning of its plans to end third-party cookie support on Chrome in 2022. Since then, a number of companies and industry organizations have rolled up their sleeves and started planning for what has long been an inevitability. Those that leaned into the conversation, digesting Google’s position and anticipating how the cookieless future would look, weren’t surprised when Google clarified in March 2021 that it isn’t planning to build or use alternate identifiers within its ecosystem. The simple fact is that burying your head in the sand or digging your heels in as it relates to changes of this magnitude isn’t an option. Industry consternation, and even legal pushbacks, might delay implementation of certain policy shifts, but that’s all they will do — delay the inevitable. The writing is on the wall: Greater privacy controls are coming to the digital landscape, and the companies that succeed in the future will be the ones that embrace — and even help to accelerate — this transition. If the panic that followed Google’s cookieless announcement taught us anything, it should have been this: The digital marketing ecosystem can’t allow itself to become overly reliant on any single technology or provider. The future belongs to those that put interoperability at the heart of their approach. Moving forward from the cookie, there are a few truths we must recognize. One is that there’s no single universal identifier that’s going to step forward to fill the entirety of the void left by third-party cookies. A number of companies are moving forward with plans for their own universal identifiers, and taken together, these identifiers will help to illuminate user identity on a portion of the open web (i.e., non-Google properties). They will be an important part of the ecosystem but by no means a silver bullet to comprehensive cross-channel, personalized advertising. Another massive component of the post-cookie landscape will be behavioral cohorts, embodied most prominently in Google’s Federated Learning of Cohorts (FLoC) construct. Through FLoC, Google will be creating targetable groups of anonymous users who navigate the internet in similar ways. The good news is that, through FLoC, nearly all of Chrome’s users will become addressable in a fully private manner, whereas only a portion of them were addressable via cookies. As such, marketers and their partners will need to build solutions that accommodate FLoC and other cohort-driven approaches. But at the same time, they also need to look beyond what Google’s putting into the marketplace in order to continue effective cross-channel marketing and personalization across the broader landscape. Ultimately, companies that can bring their own ground truth of consumer understanding to the table — and then extend their insights through the most important identifiers and behavioral cohort solutions — will prove the most adaptable to future marketplace shifts. The time of putting all your digital eggs into one ecosystem basket are long gone. The next 12 months are going to be transformative in our industry. In 24 months, we’ll all be a lot wiser. We will have taken universal IDs and behavioral cohorts for a few laps around the track, and we’ll have a much stronger sense of the role that they can and will play in furthering our consumer connections and understanding. Likewise, the innovators of our industry will have gotten to work on rewriting the internet economy around the new privacy-first reality, and we’ll all be reaping the benefits of their novel ideas and solutions. Along the way, of course, we will see a lot of companies pivoting. This might be a period of rapid transformation, but there’s no reason to believe a period of stagnation awaits us on the other side. The future, as always, belongs to the nimble — the ones that anticipate and adapt while others resist. Now is the time to be fearless in building the future of our industry in a way that is sustainable for companies and consumers alike. Tom Craig is CTO at Resonate."
https://venturebeat.com/2021/04/03/these-are-the-ai-risks-we-should-be-focusing-on/,These are the AI risks we should be focusing on,"Since the dawn of the computer age, humans have viewed the approach of artificial intelligence (AI) with some degree of apprehension. Popular AI depictions often involve killer robots or all-knowing, all-seeing systems bent on destroying the human race. These sentiments have similarly pervaded the news media, which tends to greet breakthroughs in AI with more alarm or hype than measured analysis. In reality, the true concern should be whether these overly-dramatized, dystopian visions pull our attention away from the more nuanced — yet equally dangerous — risks posed by the misuse of AI applications that are already available or being developed today. AI permeates our everyday lives, influencing which media we consume, what we buy, where and how we work, and more. AI technologies are sure to continue disrupting our world, from automating routine office tasks to solving urgent challenges like climate change and hunger. But as incidents such as wrongful arrests in the U.S. and the mass surveillance of China’s Uighur population demonstrate, we are also already seeing some negative impacts stemming from AI. Focused on pushing the boundaries of what’s possible, companies, governments, AI practitioners, and data scientists sometimes fail to see how their breakthroughs could cause social problems until it’s too late. Therefore, the time to be more intentional about how we use and develop AI is now. We need to integrate ethical and social impact considerations into the development process from the beginning, rather than grappling with these concerns after the fact. And most importantly, we need to recognize that even seemingly-benign algorithms and models can be used in negative ways. We’re a long way from Terminator-like AI threats — and that day may never come — but there is work happening today that merits equally serious consideration. Deepfakes are realistic-appearing artificial images, audio, and videos, typically created using machine learning methods. The technology to produce such “synthetic” media is advancing at breakneck speed, with sophisticated tools now freely and readily accessible, even to non-experts. Malicious actors already deploy such content to ruin reputations and commit fraud-based crimes, and it’s not difficult to imagine other injurious use cases. Deepfakes create a twofold danger: that the fake content will fool viewers into believing fabricated statements or events are real, and that their rising prevalence will undermine the public’s confidence in trusted sources of information. And while detection tools exist today, deepfake creators have shown they can learn from these defenses and quickly adapt. There are no easy solutions in this high-stakes game of cat and mouse. Even unsophisticated fake content can cause substantial damage, given the psychological power of confirmation bias and social media’s ability to rapidly disseminate fraudulent information. Deepfakes are just one example of AI technology that can have subtly insidious impacts on society. They showcase how important it is to think through potential consequences and harm-mitigation strategies from the outset of AI development. Large language models are another example of AI technology developed with non-negative intentions that still merits careful consideration from a social impact perspective. These models learn to write humanlike text using deep learning techniques that are trained by patterns in datasets, often scraped from the internet. Leading AI research company OpenAI’s latest model, GPT-3, boasts 175 billion parameters — 10 times greater than the previous iteration. This massive knowledge base allows GPT-3 to generate almost any text with minimal human input, including short stories, email replies, and technical documents. In fact, the statistical and probabilistic techniques that power these models improve so quickly that many of its use cases remain unknown. For example, initial users only inadvertently discovered that the model could also write code. However, the potential downsides are readily apparent. Like its predecessors, GPT-3 can produce sexist, racist, and discriminatory text because it learns from the internet content it was trained on. Furthermore, in a world where trolls already impact public opinion, large language models like GPT-3 could plague online conversations with divisive rhetoric and misinformation. Aware of the potential for misuse, OpenAI restricted access to GPT-3, first to select researchers and later as an exclusive license to Microsoft. But the genie is out of the bottle: Google unveiled a trillion-parameter model earlier this year, and OpenAI concedes that open source projects are on track to recreate GPT-3 soon. It appears our window to collectively address concerns around the design and use of this technology is quickly closing. AI may never reach the nightmare sci-fi scenarios of Skynet or the Terminator, but that doesn’t mean we can shy away from facing the real social risks today’s AI poses. By working with stakeholder groups, researchers and industry leaders can establish procedures for identifying and mitigating potential risks without overly hampering innovation. After all, AI itself is neither inherently good nor bad. There are many real potential benefits that it can unlock for society — we just need to be thoughtful and responsible in how we develop and deploy it. For example, we should strive for greater diversity within the data science and AI professions, including taking steps to consult with domain experts from relevant fields like social science and economics when developing certain technologies. The potential risks of AI extend beyond the purely technical; so too must the efforts to mitigate those risks. We must also collaborate to establish norms and shared practices around AI like GPT-3 and deepfake models, such as standardized impact assessments or external review periods. The industry can likewise ramp up efforts around countermeasures, such as the detection tools developed through Facebook’s Deepfake Detection Challenge or Microsoft’s Video Authenticator. Finally, it will be necessary to continually engage the general public through educational campaigns around AI so that people are aware of and can identify its misuses more easily. If as many people knew about GPT-3’s capabilities as know about The Terminator, we’d be better equipped to combat disinformation or other malicious use cases. We have the opportunity now to set incentives, rules, and limits on who has access to these technologies, their development, and in which settings and circumstances they are deployed. We must use this power wisely — before it slips out of our hands. Peter Wang is CEO and Co-founder of data science platform Anaconda. He’s also the creator of the PyData community and conferences and a member of the board at the Center for Human Technology."
https://venturebeat.com/2021/04/02/ai-weekly-heres-how-enterprises-say-theyre-deploying-ai-responsibly/,AI Weekly: Here’s how enterprises say they’re deploying AI responsibly,
https://venturebeat.com/2021/04/02/why-ai-cant-solve-unknown-problems/,Why AI can’t solve unknown problems,"When will we have artificial general intelligence, the kind of AI that can mimic the human mind in all aspect? Experts are divided on the topic, and answers range anywhere between a few decades and never. But what everyone agrees on is that current AI systems are a far shot from human intelligence. Humans can explore the world, discover unsolved problems, and think about their solutions. Meanwhile, the AI toolbox continues to grow with algorithms that can perform specific tasks but can’t generalize their capabilities beyond their narrow domains. We have programs that can beat world champions at StarCraft but can’t play a slightly different game at amateur level. We have artificial neural networks that can find signs of breast cancer in mammograms but can’t tell the difference between a cat and a dog. And we have complex language models that can spin thousands of seemingly coherent articles per hour but start to break when you ask them simple logical questions about the world. In short, each of our AI techniques manages to replicate some aspects of what we know about human intelligence. But putting it all together and filling the gaps remains a major challenge. In his book Algorithms Are Not Enough, data scientist Herbert Roitblat provides an in-depth review of different branches of AI and describes why each of them falls short of the dream of creating general intelligence. The common shortcoming across all AI algorithms is the need for predefined representations, Roitblat asserts. Once we discover a problem and can represent it in a computable way, we can create AI algorithms that can solve it, often more efficiently than ourselves. It is, however, the undiscovered and unrepresentable problems that continue to elude us. Throughout the history of artificial intelligence, scientists have regularly invented new ways to leverage advances in computers to solve problems in ingenious ways. The earlier decades of AI focused on symbolic systems. This branch of AI assumes human thinking is based on the manipulation of symbols, and any system that can compute symbols is intelligent. Symbolic AI requires human developers to meticulously specify the rules, facts, and structures that define the behavior of a computer program. Symbolic systems can perform remarkable feats, such as memorizing information, computing complex mathematical formulas at ultra-fast speeds, and emulating expert decision-making. Popular programming languages and most applications we use every day have their roots in the work that has been done on symbolic AI. But symbolic AI can only solve problems for which we can provide well-formed, step-by-step solutions. The problem is that most tasks humans and animals perform can’t be represented in clear-cut rules. “The intellectual tasks, such as chess playing, chemical structure analysis, and calculus are relatively easy to perform with a computer. Much harder are the kinds of activities that even a one-year-old human or a rat could do,” Roitblat writes in Algorithms Are Not Enough. This is called Moravec’s paradox, named after the scientist Hans Moravec, who stated that, in contrast to humans, computers can perform high-level reasoning tasks with very little effort but struggle at simple skills that humans and animals acquire naturally. “Human brains have evolved mechanisms over millions of years that let us perform basic sensorimotor functions. We catch balls, we recognize faces, we judge distance, all seemingly without effort,” Roitblat writes. “On the other hand, intellectual activities are a very recent development. We can perform these tasks with much effort and often a lot of training, but we should be suspicious if we think that these capacities are what makes intelligence, rather than that intelligence makes those capacities possible.” So, despite its remarkable reasoning capabilities, symbolic AI is strictly tied to representations provided by humans. Machine learning provides a different approach to AI. Instead of writing explicit rules, engineers “train” machine learning models through examples. “[Machine learning] systems could not only do what they had been specifically programmed to do but they could extend their capabilities to previously unseen events, at least those within a certain range,” Roitblat writes in Algorithms Are Not Enough. The most popular form of machine learning is supervised learning, in which a model is trained on a set of input data (e.g., humidity and temperature) and expected outcomes (e.g., probability of rain). The machine learning model uses this information to tune a set of parameters that map the inputs to outputs. When presented with previously unseen input, a well-trained machine learning model can predict the outcome with remarkable accuracy. There’s no need for explicit if-then rules. But supervised machine learning still builds on representations provided by human intelligence, albeit one that is more loose than symbolic AI. Here’s how Roitblat describes supervised learning: “[M]achine learning involves a representation of the problem it is set to solve as three sets of numbers. One set of numbers represents the inputs that the system receives, one set of numbers represents the outputs that the system produces, and the third set of numbers represents the machine learning model.” Therefore, while supervised machine learning is not tightly bound to rules like symbolic AI, it still requires strict representations created by human intelligence. Human operators must define a specific problem, curate a training dataset, and label the outcomes before they can create a machine learning model. Only when the problem has been strictly represented in its own way can the model start tuning its parameters. “The representation is chosen by the designer of the system,” Roitblat writes. “In many ways, the representation is the most crucial part of designing a machine learning system.” One branch of machine learning that has risen in popularity in the past decade is deep learning, which is often compared to the human brain. At the heart of deep learning is the deep neural network, which stacks layers upon layers of simple computational units to create machine learning models that can perform very complicated tasks such as classifying images or transcribing audio. But again, deep learning is largely dependent on architecture and representation. Most deep learning models needs labeled data, and there is no universal neural network architecture that can solve every possible problem. A machine learning engineer must first define the problem they want to solve, curate a large training dataset, and then figure out the deep learning architecture that can solve that problem. During training, the deep learning model will tune millions of parameters to map inputs to outputs. But it still needs machine learning engineers to decide the number and type of layers, learning rate, optimization function, loss function, and other unlearnable aspects of the neural network. “Like much of machine intelligence, the real genius [of deep learning] comes from how the system is designed, not from any autonomous intelligence of its own. Clever representations, including clever architecture, make clever machine intelligence,” Roitblat writes. “Deep learning networks are often described as learning their own representations, but this is incorrect. The structure of the network determines what representations it can derive from its inputs. How it represents inputs and how it represents the problem-solving process are just as determined for a deep learning network as for any other machine learning system.” Other branches of machine learning follow the same rule. Unsupervised learning, for example, does not require labeled examples. But it still requires a well-defined goal such as anomaly detection in cybersecurity, customer segmentation in marketing, dimensionality reduction, or embedding representations. Reinforcement learning, another popular branch of machine learning, is very similar to some aspects of human and animal intelligence. The AI agent doesn’t rely on labeled examples for training. Instead, it is given an environment (e.g., a chess or go board) and a set of actions it can perform (e.g., move pieces, place stones). At each step, the agent performs an action and receives feedback from its environment in the form of rewards and penalties. Through trial and error, the reinforcement learning agent finds sequences of actions that yield more rewards. Computer scientist Richard Sutton describes reinforcement learning as “the first computational theory of intelligence.” In recent years, it has become very popular for solving complicated problems such as mastering computer and board games and developing versatile robotic arms and hands. But reinforcement learning environments are typically very complex, and the number of possible actions an agent can perform is very large. Therefore, reinforcement learning agents need a lot of help from human intelligence to design the right rewards, simplify the problem, and choose the right architecture. For instance, OpenAI Five, the reinforcement learning system that mastered the online video game Dota 2, relied on its designers simplifying the rules of the game, such as reducing the number of playable characters. “It is impossible to check, in anything but trivial systems, all possible combinations of all possible actions that can lead to reward,” Roitblat writes. “As with other machine learning situations, heuristics are needed to simplify the problem into something more tractable, even if it cannot be guaranteed to produce the best possible answer.” Here’s how Roitblat summarizes the shortcomings of current AI systems in Algorithms Are Not Enough: “Current approaches to artificial intelligence work because their designers have figured out how to structure and simplify problems so that existing computers and processes can address them. To have a truly general intelligence, computers will need the capability to define and structure their own problems.” “Every classifier (in fact every machine learning system) can be described in terms of a representation, a method for measuring its success, and a method of updating,” Roitblat told TechTalks over email. “Learning is finding a path (a sequence of updates) through a space of parameter values. At this point, though, we don’t have any method for generating those representations, goals, and optimizations.” There are various efforts to address the challenges of current AI systems. One popular idea is to continue to scale deep learning. The general reasoning is that bigger neural networks will eventually crack the code of general intelligence. After all, the human brain has more than 100 trillion synapses. The biggest neural network to date, developed by AI researchers at Google, has one trillion parameters. And the evidence shows that adding more layers and parameters to neural networks yields incremental improvements, especially in language models such as GPT-3. But big neural networks do not address the fundamental problems of general intelligence. “These language models are significant achievements, but they are not general intelligence,” Roitblat says. “Essentially, they model the sequence of words in a language. They are plagiarists with a layer of abstraction. Give it a prompt and it will create a text that has the statistical properties of the pages it has read, but no relation to anything other than the language. It solves a specific problem, like all current artificial intelligence applications. It is just what it is advertised to be — a language model. That’s not nothing, but it is not general intelligence.” Other directions of research try to add structural improvements to current AI structures. For instance, hybrid artificial intelligence brings symbolic AI and neural networks together to combine the reasoning power of the former and the pattern recognition capabilities of the latter. There are already several implementations of hybrid AI, also referred to as “neuro-symbolic systems,” that show hybrid systems require less training data and are more stable at reasoning tasks than pure neural network approaches. System 2 deep learning, another direction of research proposed by deep learning pioneer Yoshua Bengio, tries to take neural networks beyond statistical learning. System 2 deep learning aims to enable neural networks to learn “high-level representations” without the need for explicit embedding of symbolic intelligence. Another research effort is self-supervised learning, proposed by Yann LeCun, another deep learning pioneer and the inventor of convolutional neural networks. Self-supervised learning aims to learn tasks without the need for labeled data and by exploring the world like a child would do. “I think that all of these make for more powerful problem solvers (for path problems), but none of them addresses the question of how these solutions are structured or generated,” Roitblat says. “They all still involve navigating within a pre-structured space. None of them addresses the question of where this space comes from. I think that these are really important ideas, just that they don’t address the specific needs of moving from narrow to general intelligence.” In Algorithms Are Not Enough, Roitblat provides ideas on what to look for to advance AI systems that can actively seek and solve problems that they have not been designed for. We still have a lot to learn from ourselves and how we apply our intelligence in the world. “Intelligent people can recognize the existence of a problem, define its nature, and represent it,” Roitblat writes. “They can recognize where knowledge is lacking and work to obtain that knowledge. Although intelligent people benefit from structured instructions, they are also capable of seeking out their own sources of information.” But observing intelligent behavior is easier than creating it, and, as Roitblat told me in our correspondence, “Humans do not always solve their problems in the way that they say/think that they do.” As we continue to explore artificial and human intelligence, we will continue to move toward AGI one step at a time. “Artificial intelligence is a work in progress. Some tasks have advanced further than others. Some have a way to go. The flaws of artificial intelligence tend to be the flaws of its creator rather than inherent properties of computational decision making. I would expect them to improve over time,” Roitblat said. Ben Dickson is a software engineer and the founder of TechTalks. He writes about technology, business, and politics. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/02/how-ai-powered-bi-tools-will-redefine-enterprise-decision-making/,How AI-powered BI tools will redefine enterprise decision-making,"Value-creation in business intelligence (BI) has followed a consistent pattern over the last few decades. The ability to democratize and expand the addressable user base of solutions has corresponded to large value increases. Enterprise BI arguably started with highly technical solutions like SAS in the mid-’70s, accessible only to a small fraction of highly specialized employees. The BI world began to open up in the ’90s with the advent of solutions like SAP Business Objects, which created an abstraction layer on top of query language to allow a broader swath of employees to run business intelligence. BI 3.0 came in the last decade, as solutions like Alteryx have provided WYSIWYG interfaces that further expanded both the sophistication and accessibility of BI. But in many cases, BI still involves analysts writing SQL queries to analyze large data sets so that they can provide intelligence for non-technical executives. While this paradigm for analysis continues to increase, I believe that a new BI paradigm will emerge and grow in importance over the next few years — one in which AI surfaces relevant questions and insights, and even proposes solutions. This fourth wave of BI will leverage powerful AI advancements to further democratize analytics so that any line of business specialist can supervise more insightful and prescriptive recommendations than ever before. In this fourth wave, the traditional order of BI will be inverted. The traditional method of BI generally begins with a technical analyst investigating a specific question. For example, an electronics retailer may wonder if a higher diversity of refrigerator models in specific geographies will likely increase sales. The analyst blends relevant data sources (perhaps an inventory management system and a billing system) and investigates whether there is a correlation. Once the analyst has completed the work, they present a conclusion about past behavior. They then create a visualization for business decision makers in a system like a Tableau or Looker, which can be revisited as the data changes. This investigation method works quite well, assuming the analyst asks the right questions, the number of variables is relatively well-understood and finite, and the future continues to look somewhat similar to the past. However, this paradigm presents several potential challenges in the future as companies continue to accumulate new types of data, business models and distribution channels evolve, and real-time consumer and competitive adjustments cause constant disruptions. Specifically: AI-enabled platforms that will define the fourth wave of BI start by crunching and blending massive amounts of data to find and surface patterns and relevant statistical insights. A data analyst applies judgment to these myriad insights to decide which patterns are truly meaningful or actionable for the business. After digging into areas of interest, the platform suggests potential actions based on correlations that have been seen over a more extended period — again validated by human judgment. The time is ripe for this methodology to proliferate — AI advancements are coming online in conjunction with the growth of cloud-native vendors like Snowflake. Simultaneously, businesses are increasingly feeling the strain that business complexity and data proliferation are putting on their traditional BI processes. The data analytics space has spawned some incredible companies capable of tackling this challenge. In the last six months, Snowflake vaulted into the top 10 cloud businesses with a valuation above $70 billion, and Databricks raised $1 billion at a $28 billion valuation. Both of these companies (along with similar offerings from AWS and Google Cloud) are vital enablers for modern data analytics, providing data warehouses where teams can leverage flexible, cloud-based storage and compute for analytics. Industry verticals such as ecommerce and retail that are under the most strain from the three challenges outlined above are starting to see industry-specific platforms emerge to deliver BI 4.0 capabilities — platforms like Tradeswell, Hypersonix, and Soundcommerce. In the energy and materials sector, platforms like Validere and Verusen are helping to address these challenges by using AI to boost margins of operators. In addition, broad technology platforms like Outlier, Unsupervised, and Sisu have demonstrated the power to pull exponentially more patterns from a dataset than a human analyst could. These are examples of intuitive BI platforms that are easing the strains, old and new, that data analysts face. And we can expect to see more of them emerging over the next couple of years. Steve Sloane is a Partner at Menlo Ventures."
https://venturebeat.com/2021/04/02/what-is-a-decentralized-database/,What is a decentralized database?,"A decentralized database splits the workload up among multiple machines and uses sophisticated algorithms to balance the incoming and outgoing requests for the best response time. This type of database is useful for those times when there is more data that needs to be stored in the database than can physically saved on one physical machine. The bits — like log files, data collected by tracking click-throughs in the application, and the data generated by internet of things devices — pile up and need to be stored somewhere. They are also frequently referred to as distributed databases. There are several good reasons for splitting up a database: One approach to simplify the architecture is to split the dataset into smaller parts and assign the parts to certain machines. One computer might handle all people whose last name begins with A through F, another G through M, etc. This splitting, often called “sharding,” can inspire strategies that range from simple to complex. The greatest challenge with splitting up the database is ensuring that the information remains consistent. For example, in the case of a hypothetical airline booking system, if one machine responds to a database query that an airplane seat has been sold, then another machine shouldn’t respond to a query by saying that the seat is open and available. Some distributed databases enforce the rules on consistency carefully so that all queries receive the same answer, regardless of which node in the cluster responded to the query. Other distributed databases relax the consistency requirement in favor of “eventual consistency.” With eventual consistency, the machines can be out-of-sync with each other and return different answers, so long as the machines eventually catch up to each other and return the same results. In some narrow cases, one machine may not hear about the new version of the data stored on another machine for some time. Machines in the same datacenter tend to reach consistency faster than those separated by longer distances or slower networks. Database developers must choose between fast responses and consistent answers. Tight synchronization between the distributed versions will increase the amount of computation and slow the responses, but the answers will be more accurate. Allowing data to be out of sync will speed up performance, but at the expense of accuracy. Choosing whether to prioritize speed or accuracy is a business decision that can be an art. Banks, for instance, know their customers want correct accounting more than split-second responses. Social media companies, however, may choose speed because most posts are rarely edited and small differences in propagation aren’t essential. The major database companies offer elaborate options for distributing data storage. Some support large machines with multiple processors, multiple disks, and large blocks of RAM. The machine is technically one computer, but the individual processors coordinate their responses in similar ways as if the processors were separated by continents. Many organizations run their Oracle and SAP deployments on Amazon Web Services in order to take advantage of the computing power. AWS’ u-24tb1.metal, for instance, may look like one machine on the invoice, but it has 448 processors inside, along with 24 terabytes of RAM. It is optimized for very large databases like SAP’s HANA, which stores the bulk of the information in RAM for fast response. All of the major databases have options for replicating the database to create distributed versions that are split between more distinct machines. Oracle’s database, for instance, has long supported a wide range of replication strategies across collections of machines that can even include non-Oracle databases. Lately, Oracle has been marketing a version with the name “autonomous” to signify that it’s able to scale and replicate itself automatically in response to loads. MariaDB, a fork of MySQL, also supports a variety of replication strategies that allow the data from one primary node to pass copies of all transactions to replicas that are commonly set up to be read-only. That is, the replica can answer queries for information, but it doesn’t store new data. In a recent presentation, Max Mether, one of the cofounders of MariaDB, says his company is working hard at adding autonomous abilities to its database. “The server should know how to tune itself better than you,” he explained. “That doesn’t mean you shouldn’t have the option to tune the server, but for many of these variables, it’s really hard as a user to figure out how to tune them optimally. Ideally you should just let the server choose, based on the current workload, what makes sense.” The rise of cloud services hides some of the complexity of distributing the databases, at least for configuring the server and arranging for the connection. DigitalOcean, for instance, offers managed versions of MySQL, PostgreSQL, and Redis. Clusters can be created with a certain size with a single control panel to offer storage and failover. Some providers have added the ability to spread out clusters in different datacenters around the world. Amazon’s RDS, for instance, can configure clusters that span multiple areas called “availability zones.” Online file storage is also starting to offer much of the same replication. While the services that offer to store blocks of data in buckets don’t provide the indexing or complex searching of databases, they do offer replication as part of the deal. Some approaches work to merge more complex calculations with distributed data sets. Tools like Hadoop and Spark, for instance, are just two of the popular open source constellations of tools that match distributed computation with distributed data. There are a number of companies that specialize in supporting versions that are installed in house or in cloud configurations. Databricks’ Delta Lake, for instance, is one product that supports complex data mining operations on distributed data. Groups that value privacy are also exploring complicated distributed operations like the Interplanetary File System, a project designed to spread web data out among multiple locations for speed and redundancy. Not all work requires the complexity of coordinating multiple machines. Some projects may be labeled “big data” by project managers who feel aspirational, even though the volume and computational load is easily handled by a single machine. If a fast response time is not essential and if the size is not too large and won’t grow in an unpredictable way, a simpler database with regular backups may be sufficient. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/02/nanoms-nanotech-makes-more-efficient-batteries-that-last-at-least-9-times-longer/,Nanom’s nanotech makes more efficient batteries that last at least 9 times longer,"Nanom is unveiling a patented technology that uses nanoparticles to make batteries that are far more efficient than the ubiquitous lithium-ion version used today. If it works, the technology could revolutionize batteries used in everything from electric cars to laptops and smartphones. Nanom claims its batteries last 9 times longer than the nickel-iron batteries Thomas Edison invented and more than than 9 times longer than lithium-ion batteries. They also weigh 5 times less and improve energy density, recharge rates, and battery disposal. Nanom isn’t disclosing the direct comparison to lithium-ion batteries yet. But it is saying that an electric car with Nanom tech could drive across the United States on a single battery charge. That’s pretty mind-boggling. And the company has raised $3 million from investors in Silicon Valley and the Nordics, thanks to backers like Village Global, whose network includes Bill Gates, Jeff Bezos, and Reid Hoffman. A big advantage of the nanotechnology Nanom uses is that it can be applied to existing lithium-ion batteries, creating more surface area to generate energy with the battery cells. Nanom can make the tech in its labs and easily add production to existing manufacturing plants. “The problem has always been that nanotechnology has been a little bit stuck in the lab,” CEO Armann Kojic said in an interview with VentureBeat. “It’s like a concept that has been great for making a million-dollar battery, but not one that actually goes in your car. And what’s unique about our approach is the fact that our core method of creating nanoparticles is a mass production method. This is very much about bringing it out of the lab and into a product.” The Edison nickel-iron battery is more than 120 years old. Using iron and nickel particles and carbon fiber, Nanom makes the battery usable again. The company says its materials are 80% cheaper than those in a regular battery — and more disposable. The nickel-iron battery is used in large-scale energy storage devices. “The Edison battery is kind of a showcase for us … to show what could be done to take a green battery and counter the weaknesses,” Kojic said. Nanom is talking to a wide range of transportation, stationary storage, and battery companies about immediately adopting its nanoparticles into their own proprietary designs. “We all know that this battery problem exists,” Kojic said. “And there’s a lot of factors that need to come together, like the environmental factor, the cost factor, and the effectiveness of the battery.” Nanom’s nanoparticle technology enables any battery formulation to be fundamentally altered for the better by dramatically increasing every aspect of its performance. It does so by immersing the battery materials in nanoparticles that increase the surface area that generates energy on a microscopic level. These particles are many orders of magnitude more effective in increasing energy surface area than the current solutions, which makes them ideal for this generation of battery energy storage. And Nanom immediately enhances the energy density of any battery, Kojic said. “It’s all about the ultra-high surface area, which affects the charging rates and the capacity,” Kojic said. “If I take a business card, I have the surface area as the front and the back of the business card. But if I cut it into a billion pieces, it becomes several football fields of a surface area, I’m using the sides and everything in between. And we can use all of that unused area for storing energy.” Nanom has the ability to convert existing battery materials into nanoparticle size, enabling the same benefits for any battery process without needing to retool — the nanoparticles are simply mixed into the slurry that is a standard part of all battery manufacturing lines. Nanom has achieved massive scale in its manufacturing process and can already satisfy the requirements of key battery markets. In that sense, Kojic said the invention is industrially scalable. The critical barrier to the utilization of the full potential of nanotechnology has always been the inability to economically manufacture nanoparticles in sufficient quantities. Nanom’s method allows for mass production of nanoparticles produced through friction by transverse introduction of feedstock material into alternating supersonic flow regimes in various controlled atmospheres. Controlling flow speed, atmosphere, repetitions, and after treatment allows the company to uniquely shape the particle’s size and surface areas, resulting in a bespoke nanostructure with unique properties. Nanom said it can convert micron-sized particles to nano-sized particles in high volume. This provides unique nanostructures that enable longer-lasting batteries without changing anything in the batteries’ manufacturing flow. By replacing key materials with nano-enhanced materials, Nanom says it gives customers a competitive advantage. On top of the other benefits, the company said it can build the batteries into the materials used to make products, like car bodies, airplane seats, boat hulls, and more. A structural battery is one that can be used in the material of a product by embedding the nanoparticles into a structure. In a car, it’s a fast-charging battery that is safe to use, as it doesn’t explode on impact. It also allows engineers to decentralize a power source. Even a wall in your house can become a giant, safe battery that takes you off the grid. Nanom enables any structure or surface to become a battery storage device. For example, the company has already created a pilot project in which an electric boat was constructed such that the actual hull of the boat became the battery (15 meters of that type of hull has the energy storage capacity of five Tesla vehicles). Any material or structure Nanom turns into a battery is completely solid, uses no harmful chemicals, is green for the planet, and does not explode upon impact (as lithium-ion batteries have a distressing tendency to do). And Nanom doesn’t have to convince customers to go with a different material. “You can take existing technologies and get more out of them so that we don’t deplete the environment of raw materials,” he said. “We can create these high surface area nanoparticles that are super relevant to the battery industry. You can make batteries with a lower cost per kilowatt-hour than you can do otherwise.” Kojic was born in Iceland and moved to the San Francisco Bay Area to join the StartX mentor labs incubator at Stanford University. He fell in love with the region and met his wife in the Bay Area. As he built the company, he also had an easier time finding investors in the region than he would have back home. Nanom has applied for patents on the tech. The company has 17 people working in Palo Alto and Reykjavik, Iceland and has raised $3 million to date. The employees have worked at companies including Decode, Maersk, and Total S.A. Backers include Village Global, Iceland Venture Studio, Perkins Coie, and the European Union’s Green Deal project. The EU Green Deal is a very ambitious action plan with the target of making Europe carbon-neutral by 2050. The Nanom Greenvolt project was selected as an EU Green Deal technology, which came with Green Deal funding. Kojic said the company will disclose more in a few months when it announces its partners. The name “Nanom” combines the words “nano” and “om,” and it reflects the company’s vision of using the power of nano, or matter, to impact the universe, or om, the entirety of the universe, truth, and knowledge."
https://venturebeat.com/2021/04/02/a-developers-inside-look-at-where-slack-started-and-where-its-headed/,A developer’s inside look at where Slack started — and where it’s headed,"Presented by OutSystems In 2020, Slack nearly doubled its paying customer base over the previous year, thanks to the pandemic, and was recently acquired by Salesforce for over $27 billion. But according to Justin Hardin, senior software engineer at Slack, the product originally started as a gaming platform that failed to take off. “They unfortunately ran out of money and had to lay people off, so they pivoted by asking, ‘Which piece of our product works?’ And that was the chat aspect,” Hardin says on the latest episode of Decoded, OutSystems’ podcast for the next generation of developers. But the app’s friendly human tone was inspired by its gaming roots. “They kept the writer on who was creating the dialogues for the games, and instead had her do the dialogues for the product,” he says. “That’s how you have this enterprise chat platform with help messages and onboarding that’s in a more conversational tone, which helped define the product experience.” Listen to the conversation with Justin Hardin right here. Since Slack’s early pivot, Hardin says that Slack has been very intentional in the way it has expanded internationally. In addition to creating server caches around the world to reduce latency, Slack localizes completely when entering new markets. The product team, marketing team, enterprise team, app store team, and others all localize the entire Slack experience to ensure it’s relevant to every user, no matter where they are. “We localize not just the language itself, but also the imagery and graphics to make sure it’s contextual to the countries we’re in,” he says. “We make sure the blog isn’t only localized, but empower marketers from each country to create their own articles targeted at their specific market.” Hardin believes this reflects the fact that technology is becoming less centered around Silicon Valley. “Slack is a very American-based company, but that’s not how the world is going to see tech moving on in the future,” he explains. “Tech should be a global entity, not just a Silicon Valley thing. In a post-pandemic world, the whole notion of Silicon Valley should exist on the internet. It shouldn’t be a specific place.” Thanks to its commitment to creating an intuitive UI and human experience, Slack originally found success among developer teams at companies. This led to adoption at businesses ranging from small startups to Amazon, and then spread through other functions like marketing, sales, and finance. To ensure Slack is relevant and useful to all types of users, Hardin says their team focuses on providing a consistent experience above all else. “How can we make our front end through the product and the marketing site consistent? How can we have a design system? How can we utilize components?” he says. “That’s where I see Slack as being kind of an innovator in this space: how can you bring consistency? Some of it is through getting everyone on the same page, but other times it’s through how you create toolings that allow developers to not think about this.” With more people than ever using Slack as they work from home, Hardin says this consistency, along with rigorous testing, is what has helped the company scale to meet growing demand. The result is a product that people love so much and has been so successful that it’s news when the product has an issue. “When things don’t go well, we’re trending on Twitter.” Check out this week’s Decoded podcast to learn much more about how Justin Hardin started his career in software development, his work at Slack, and his work as the co-founder of Climatebase.org, a platform for climate action, education, and impact. Listen now, and subscribe to future episodes today. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/02/american-construction-source-acquires-foley-lumber-milaca-building-center/,American Construction Source Acquires Foley Lumber & Milaca Building Center," The Ninth Transaction by ACS Expands LBM Distribution Capacity in Minnesota  SPRINGFIELD, Mo.–(BUSINESS WIRE)–April 2, 2021– American Construction Source (“ACS”), a leading national building materials distribution platform for custom home builders and repair and remodel contractors, backed by Angeles Equity Partners, LLC (“Angeles”) and Clearlake Capital Group, L.P. (together with its affiliates “Clearlake”), today announced the acquisition of Foley Lumber (“Foley”) and Milaca Building Center (“Milaca”). Foley and Milaca will operate under their existing local brand names already established in Minnesota. The transaction marks the ninth acquisition by ACS under sponsorship from Clearlake and Angeles. “The ACS team welcomes Foley and Milaca to our national platform of service-oriented lumber and building materials locations,” said James Drexinger, CEO of ACS. “The Kotsmith family has built a great business over generations, and we respect the customer relationships they have fostered for the last 90 years.” “We are confident ACS, Angeles, and Clearlake are the best partners to help accelerate growth by delivering more value-added products and an expanded portfolio of services in North Central Minnesota and the greater Minneapolis St. Paul metro area,” said Milaca and Foley owners Chris Kotsmith and Randy Kotsmith. “ACS’s technology investments and national scale will enable Foley and Milaca to better serve our customers for generations to come.” Simpson Thacher & Bartlett LLP provided legal counsel to ACS. CliftonLarsonAllen Wealth Advisors, LLC served as financial advisors to the sellers. The financial terms of the transaction were not disclosed. About Foley Lumber and Milaca Building Center Serving customers since 1932, Foley Lumber and Milaca Building Center are widely known for quality products, including common and not so common lumber and building materials. Our sales, support, and yard staff include knowledgeable, professional individuals with many years of hands-on experience. The most important aspect of our business is servicing our builder and contractor customers. About American Construction Source American Construction Source is an LBM distributor with 70+ locations in 9 states serving the needs of custom home builders, repair & remodel contractors, and DIY consumers. ACS provides lumber and building materials businesses the resources, leverage, and focus to make their ideas happen. Recognizing the value and heritage of deep, local customer relationships as a strong foundation for growth, ACS’s best practices are designed to leverage shared strengths, drive operational excellence, and motivate performance to create a leading building products distributor with a national footprint and the industry’s best customer experience. ACS is backed by Angeles Equity Partners and Clearlake Capital Group. Learn more online at www.acs-lbm.com. About Angeles Equity Partners, LLC Angeles Equity Partners, LLC is a private equity firm that invests in companies across a wide range of sectors and specifically targets businesses which it believes can directly benefit from the firm’s strategic, operational, and M&A capabilities. The Angeles skill set drives the firm’s investment philosophy and, in its view, can help businesses reach their full potential. Learn more online at www.angelesequity.com. About Clearlake Founded in 2006, Clearlake Capital Group, L.P. is an investment firm operating integrated businesses across private equity, credit and other related strategies. With a sector-focused approach, the firm seeks to partner with experienced management teams by providing patient, long term capital to dynamic businesses that can benefit from Clearlake’s operational improvement approach, O.P.S.® The firm’s core target sectors are technology, industrials and consumer. Clearlake currently has approximately $35 billion of assets under management, and its senior investment principals have led or co-led over 300 investments. The firm has offices in Santa Monica and Dallas. More information is available at www.clearlake.com and on Twitter @ClearlakeCap.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210402005103/en/ Trenton Waterhouse at +1 623 523 1672 or email trent@acs-lbm.com"
https://venturebeat.com/2021/04/01/optipulse-non-coherent-laser-offers-optical-fiber-installers-a-wireless-extension-cord/,OptiPulse Non-Coherent Laser Offers Optical Fiber Installers a Wireless Extension Cord,"ALBUQUERQUE, N.M.–(BUSINESS WIRE)–April 1, 2021– OptiPulse is testing a new light source that sends invisible eye-safe infrared light wirelessly from one building or pole to another at 10Gbps. The light source is a miniature chip that costs ~$1 to produce in large volumes and has tested error-free at 25Gbps. The Light is about the same wavelength as your TV remote. It is turning on and off at 25 billion times a second to send data over a beam of invisible light with extremely high bandwidth. The company has designed the compound semiconductor chip, developed, and produced it at a commercial foundry. The chips were installed in a “binocular” type 3rd prototype that emulates a wireless infrastructure link. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210401005871/en/ OptiPulse is trying to prove that light is superior to microwaves for sending high speed wireless data. Their first product will be beta tested within 6 months. The links use invisible light about the same wavelength as your TV remote’s infrared light. The demonstrated 10Gbps upload and download systems can reduce the cost and increase the performance of those links. The 3rd prototype used less than ¼ of the energy a typical microwave link. These combined factors may ultimately lead to a significant reduction in energy use worldwide even as the bandwidth demands increase. OptiPulse’s first goal is to make infrastructure links that can be a kind of wireless extension cord for fiber optic deployments. Many times, fiber installers run into rock, streams or highway crossings that are difficult to span. Plugging the fiber into one device at the obstacle can send the same bandwidth wirelessly across the difficult area much cheaper than the alternatives. It is hoped by many current clients waiting for the links that it will reduce the cost and time for fiber installs. The links can also be used in metro areas to not disturb businesses by tearing up roads in major metro areas. The founder, John Joseph, believes the way to prevent this work from being taken over is by getting the public to fund and oversee its development. The company has started with a WeFunder crowd funding campaign where the public can invest as little as $100. https://wefunder.com/optipulse  View source version on businesswire.com: https://www.businesswire.com/news/home/20210401005871/en/ John Joseph CEO OptiPulse480-652-0717jjoseph@optipulse.com https://optipulse.com"
https://venturebeat.com/2021/04/01/global-chip-shortage-affects-more-than-cars/,Global chip shortage affects more than cars,"(Reuters) — From delayed car deliveries to a supply shortfall in home appliances to costlier smartphones, businesses and consumers across the globe are facing the brunt of an unprecedented shortage in semiconductor microchips. The shortage stems from a confluence of factors as carmakers, which shut plants during the COVID-19 pandemic last year, compete against the sprawling consumer electronics industry for chip supplies. Consumers have stocked up on laptops, gaming consoles and other electronic products during the pandemic, leading to tighter inventory. They also bought more cars than industry officials expected last spring, further straining supplies. Sanctions against Chinese tech companies have further exacerbated the crisis. Originally concentrated in the auto industry, the shortage has now spread to a range of other consumer electronics, including smartphones, refrigerators and microwaves. With every company that uses chips in production panic buying to shore up stocks, the shortage has squeezed capacity and driven up costs of even the cheapest components of nearly all microchips, increasing prices of final products. Automobiles have become increasingly dependent on chips — for everything from computer management of engines for better fuel economy to driver-assistance features such as emergency braking. The crisis has forced many to curtail the production of less profitable vehicles. General Motors Co and Ford Motor Co are among the big carmakers who said they would scale down production, joining other automakers including Volkswagen AG, Subaru Corp, Toyota Motor Corp and Nissan Motor Co. A shortage of auto semiconductor chips could impact nearly 1.3 million units of global light vehicle production in the first quarter, according to data firm IHS Markit. IHS said a fire at a Japanese chip-making factory owned by Renesas Electronics Corp, which accounts for 30% of the global market for microcontroller units used in cars, has worsened the situation. Severe winter weather in Texas has also forced Samsung Electronics Co Ltd, NXP Semiconductors and Infineon to shut down factories temporarily. Infineon and NXP are major automotive chip suppliers, and analysts expect the disruptions to add to the shortfalls in the ailing sector. At the root of the squeeze is the under-investment in 8-inch chip manufacturing plants owned mostly by Asian firms, which means they have struggled to ramp up production as demand for 5G phones and laptops picked up faster than expected. Qualcomm Inc, whose chips feature in Samsung phones, is one major chipmaker struggling to keep up with demand. Apple Inc’s major supplier Foxconn also warned of the chip shortage affecting supply chains to clients. The majority of chip production occurs in Asia currently, where major contract manufacturers such as Taiwan Semiconductor Manufacturing Co Ltd (TSMC) and Samsung handle production for hundreds of different chip companies. U.S. semiconductor companies account for 47% of global chip sales, but only 12% of global manufacturing is done in the United States. Factories that produce wafers cost tens of billions of dollars to build, and expanding their capacity can take up to a year for testing and qualifying complex tools. U.S. President Joe Biden has sought $37 billion in funding for legislation to supercharge chip manufacturing in the country. Currently, four new factories are slated in the country, two by Intel Corp and one by TSMC in Arizona, and another by Samsung in Texas. China has also offered a myriad of subsidies to the chip industry as it tries to reduce its dependence on Western technology."
https://venturebeat.com/2021/04/01/study-finds-that-even-the-best-speech-recognition-systems-exhibit-bias/,Study finds that even the best speech recognition systems exhibit bias,"Even state-of-the-art automatic speech recognition (ASR) algorithms struggle to recognize the accents of people from certain regions of the world. That’s the top-line finding of a new study published by researchers at the University of Amsterdam, the Netherlands Cancer Institute, and the Delft University of Technology, which found that an ASR system for the Dutch language recognized speakers of specific age groups, genders, and countries of origin better than others. Speech recognition has come a long way since IBM’s Shoebox machine and Worlds of Wonder’s Julie doll. But despite progress made possible by AI, voice recognition systems today are at best imperfect — and at worst discriminatory. In a study commissioned by the Washington Post, popular smart speakers made by Google and Amazon were 30% less likely to understand non-American accents than those of native-born users. More recently, the Algorithmic Justice League’s Voice Erasure project found that that speech recognition systems from Apple, Amazon, Google, IBM, and Microsoft collectively achieve word error rates of 35% for African American voices versus 19% for white voices. The coauthors of this latest research set out to investigate how well an ASR system for Dutch recognizes speech from different groups of speakers. In a series of experiments, they observed whether the ASR system could contend with diversity in speech along the dimensions of gender, age, and accent. The researchers began by having an ASR system ingest sample data from CGN, an annotated corpus used to train AI language models to recognize the Dutch language. CGN contains recordings spoken by people ranging in age from 18 to 65 years old from Netherlands and the Flanders region of Belgium, covering speaking styles including broadcast news and telephone conversations. CGN has a whopping 483 hours of speech spoken by 1,185 women and 1,678 men. But to make the system even more robust, the coauthors applied data augmentation techniques to increase the total hours of training data “ninefold.” When the researchers ran the trained ASR system through a test set derived from the CGN, they found that it recognized female speech more reliably than male speech regardless of speaking style. Moreover, the system struggled to recognize speech from older people compared with younger, potentially because the former group wasn’t well-articulated. And it had an easier time detecting speech from native speakers versus non-native speakers. Indeed, the worst-recognized native speech — that of Dutch children — had a word error rate around 20% better than that of the best non-native age group. In general, the results suggest that teenagers’ speech was most accurately interpreted by the system, followed by seniors’ (over the age of 65) and children’s. This held even for non-native speakers who were highly proficient in Dutch vocabulary and grammar. As the researchers point out, while it’s to an extent impossible to remove the bias that creeps into datasets, one solution is mitigating this bias at the algorithmic level. “[We recommend] framing the problem, developing the team composition and the implementation process from a point of anticipating, proactively spotting, and developing mitigation strategies for affective prejudice [to address bias in ASR systems],” the researchers wrote in a paper detailing their work. “A direct bias mitigation strategy concerns diversifying and aiming for a balanced representation in the dataset. An indirect bias mitigation strategy deals with diverse team composition: the variety in age, regions, gender, and more provides additional lenses of spotting potential bias in design. Together, they can help ensure a more inclusive developmental environment for ASR.”"
https://venturebeat.com/2021/04/01/arms-confidential-computing-uses-hardware-to-ensure-security/,Arm’s confidential computing uses hardware to ensure security,"Arm introduced its Armv9 chip platform this week as the first major upgrade for its architecture in a decade. And one of the key pillars was “confidential computing,” a hardware-based security initiative. Arm is a chip architecture company that licenses its designs to others, and its customers have shipped more than 100 billion chips in the past five years. Nvidia is in the midst of acquiring Cambridge, United Kingdom-based Arm for $40 billion, but the deal is waiting on regulatory approvals. During Arm’s press event, CEO Simon Segars said that Armv9’s roadmap introduces the Arm Confidential Compute Architecture (CCA). Confidential computing shields portions of code and data from access or modification while in use, even from privileged software, by performing the computation in a hardware-based secure environment, he said. More details will be released over time. The processor can have secure enclaves, and that can create better security throughout the system. Usually, the model for software is to inherently trust the operating system and the hypervisor the software is running on, and that the highest tiers of software are allowed to see into the execution of the lower tiers. But if the operating system or hypervisor is compromised, that’s a risk. CCA introduces a new concept of dynamically created “realms,” which can be viewed as secured containerized execution environments that are completely opaque to the OS or hypervisor. The hypervisor would still exist, but be solely responsible for scheduling and resource allocation. The realms instead would be managed by a new entity called the realm manager, which is supposed to be a new piece of code roughly a tenth the size of a hypervisor. “The Arm Confidential Compute architecture will introduce the concept of dynamically created realms, usable by ordinary programs in a separate computation world from either the non-secure or secure world that we have today,” said Richard Grisenthwaite, chief architect at Arm, in a press briefing. “Realms use a small amount of trust and testable management software that is inherently separated from the operating system.” Segars said that Realms are much like software containers, which isolate code in certain ways, but with hardware support. “People are realizing that it matters,” said Mike Bursell, chief security architect at Red Hat, in a press briefing. “Confidential computing is about protecting your applications, your workloads from a host which is compromised or malicious or from external hackers. Keeping your workloads safe using hardware controls is how we think about confidential computing. People realize there are some workloads that they’re not happy about putting on the cloud or which are not safe on the edge, maybe because their boxes aren’t physically secure.” Realms can protect commercially sensitive data and code from the rest of the system while it is in use, at rest, and in transit. In a recent survey of enterprise executives, more than 90% of the respondents believe that if confidential computing were available, the cost of security could come down, enabling them to dramatically increase their investment in engineering innovation. Overall, the chain of trust required for an application to run can be more limited, protecting the overall system if part of the system is compromised. Henry Sanders, chief technology officer of Azure Edge and Platforms at Microsoft, said in a statement that the complexity of edge-to-cloud computing means that one-size-fits-all solutions don’t work. He believes more synergy between hardware and software with the Confidential Compute architecture is necessary to foster innovation. Lee Caswell, vice president of marketing at VMware’s cloud platform business, said in a statement that Arm’s SmartNICs with VMware Project Monterey introduce a zero-trust security model with the goal of both improved security and better performance across a hybrid cloud. “Arm is positioning itself as a high-performance and highly secure platform, stepping up its competition with x86 and to stay ahead of RISC-V,” said Kevin Krewell, an analyst with Tirias Research, in an email to VentureBeat. “The System Ready program is designed to improve the standardization of Arm-based chips to ease software compatibility. Arm is also preparing for an eventual merger with Nvidia, with its Mali graphics adding new features that mirror Nvidia’s RTX family.” Patrick Moorhead, an analyst at Moor Insights & Strategy, said confidential computing is the next frontier in datacenter security, where every link in the chain has “zero trust” in each other. Armv9 incorporates many elements of confidential computing, and so he thinks Realms is a differentiator. “It’s all about security against many different attack scenarios from a security perspective,” said Ron Martino, executive vice president and general manager of edge computing at NXP. “This includes both the data and the software IP, dealing with multiple entities, some trusted, some that aren’t trusted. And it also includes ensuring security against physical and remote attacks. So when you think about this whole computing concept and deploying devices, it’s this edge-to-cloud computing concept that is applying confidential computing.” Dave Kleidermacher at Google said that confidential computing applies both to the cloud as well as mobile devices. He said one of the uses for confidential computing in the cloud is to stop fraud: Data can be extracted from each domain in a chain of payments, and that data that can point to evidence of fraud in a privacy-preserving way. Richard Searle at Fortanix said the Linux Foundation has been trying to educate the tech community about confidential computing, but there’s still some confusion around it. “There’s still work to be done,” he said. “It’s a new market. But events like this can help get the message about what this new technology can bring to data and application security.”"
https://venturebeat.com/2021/04/01/fangraphs-advanced-baseball-analytics-has-a-new-cloud-home-mariadb/,FanGraphs’ advanced baseball analytics has a new cloud home: MariaDB,"With the 2021 Major League Baseball season opening today, fans will be filling out their scorecards as they return to stadiums for the first time since the COVID-19 pandemic took hold last spring. Of course, the data that is now regularly made available by the MLB goes well beyond the hits, runs, and errors fans typically record in a scorecard they purchase at a game. MLB has made the Statcast tool available since 2015. It analyzes player movements and athletic abilities. The Hawk-Eye service uses cameras installed at ballparks to provide access to instant video replays. Fans now regularly consult a raft of online sites that uses this data to analyze almost every aspect of baseball: top pitching prospects, players who hit the most consistently in a particular ballpark during a specific time of day, and so on. One of those sites is FanGraphs, which has transitioned the SQL relational database platform it relies on to process and analyze structured data to a curated instance of the open source MariaDB database that has been deployed on the Google Cloud Platform (GCP) as part of a MariaDB Sky cloud service. MariaDB provides IT organizations with an alternative to the open source MySQL database Oracle gained control over when it acquired Sun Microsystems in 2009. MariaDB is a fork of the MySQL database that is now managed under the auspices of a MariaDB Foundation that counts Microsoft, Alibaba, Tencent, ServiceNow, and IBM among its sponsors, alongside MariaDB itself. FanGraphs uses the data it collects to enable its editorial teams to deliver articles and podcasts that project, for example, playoff odds for a team based on the results of the SQL queries the company crafts. These insights might be of particular interest to a baseball fan participating in a fantasy league, someone who wants to place a more informed wager on a game at a venue where gambling is, hopefully, legalized, or those making baseball video games. The decision to move from MySQL to MariaDB running on GCP was made after a few false starts involving attempts to lift and shift the company’s MySQL database instance into the cloud, FanGraphs CEO David Appelman said. One of the things that attracted FanGraphs to MariaDB is the level of performance that it could attain using a database-as-a-service (DBaaS) platform based on MariaDB and that it provides access to a columnstore storage engine that might one day be employed to drive additional analytics, Appelman said. In addition, MariaDB now manages the underlying database FanGraphs uses. Appleman said he previously handled most of the IT functions for FanGraphs, including the crafting of SQL queries. Now he will have more time to create SQL queries and monitor the impact they have on the performance of the overall database, Appelman said. “I like to see where the bottlenecks created by a SQL query are,” he added. FanGraphs plans to eventually take advantage of the data warehouse service provided by MariaDB, Appelman noted. It’s not likely any of the analytics capabilities provided by FanGraphs and similar sites will one day be able to predict which baseball team will win on any given day. However, the insights they surface do serve to make the current generation of baseball fans a lot more informed about the nuances of the game than Abner Doubleday probably could have imagined."
https://venturebeat.com/2021/04/01/amazon-introduces-choreographed-motions-with-alexa-presentation-language-1-6/,Amazon introduces choreographed motions with Alexa Presentation Language 1.6,"Amazon today unveiled a new version of its Alexa Presentation Language (APL), the visual design framework that allows developers to build visual apps on Alexa-enabled devices. This new version includes support for three choreographed motions on the Echo Show 10, the first-party Alexa-enabled device with a motorized base that was released last month. Also in tow is improved support for Fire tablets and custom on-screen transitions, as well as faster animations. The pandemic appears to have supercharged voice app usage, which was already on an upswing. According to a study by NPR and Edison Research, the percentage of voice-enabled device owners who use commands at least once a day rose between the beginning of 2020 and the start of April. Just over a third of smart speaker owners say they listen to more music, entertainment, and news from their devices than they did before, and owners report requesting an average of 10.8 tasks per week from their assistant this year compared with 9.4 different tasks in 2019. According to a new report from Juniper Research, consumers will interact with voice assistants on 8.4 billion devices by 2024. APL became generally available in September 2019, and it supports all Alexa devices with screens, including Fire TV devices. This latest release better supports multimodal apps at a time when multimodal app usage on Alexa devices is on the rise. Amazon says that on average, APL-based multimodal apps are seeing more than 3 times the number of monthly active users compared with voice-only apps. Moreover, apps that have implemented APL video have nearly double the customer engagement of voice-only apps, according to the company.  The new choreographed motions in APL (version 1.6), which Amazon calls “choreos,” are: Amazon says the choreos were created by designers working from home during the pandemic. The inspiration, testing, and final designs were completed outside the office, starting from sketches, and came together in about six weeks. APL 1.6 also introduces a change to the APL authoring tool that enables it to convert Lottie files into Alexa Vector Graphics (AVG). (Lottie is an open source animation file format popular among web designers,)  Once converted, the AVGs can be used in an app’s visual responses. Amazon has also increased the limit for visual app responses from 24KB to 120KB to support “even more complex and engaging customer experiences.” Beyond this, the company says it has expanded APL beyond Show Mode on supported Fire tablet — the mode that effectively takes Fire tablet displays full-screen. APL 1.6 lets developers create custom layouts for new screen sizes and adapt their responses as devices flip between portrait and landscape orientations. APL 1.6 is generally available starting today."
https://venturebeat.com/2021/04/01/open-banking-is-big-heres-why-open-finance-is-bigger/,Open banking is big. Here’s why open finance is bigger.,"Presented by Envestnet | Yodlee The open banking movement is part of a much larger open finance trend. In this VB Live event, learn how open finance and access to alternative data can revolutionize customer experiences, spur innovation, improve efficiencies, increase demand, and more. Register here for free. Open banking starts with the premise that consumers have the right to access the data that’s held by their financial institutions — and permit that data to be used by third-parties for the consumers’ benefit. But this is only the beginning, says Brian Costello, VP, Data Strategy & Governance at Envestnet | Yodlee, as the open banking movement becomes part of a much larger open finance trend. In the U.S., open banking has been about giving third parties access to retail banking data, while in the U.K., it’s been limited to just payment accounts. Open finance is simply taking the notion of open banking a step further: giving consumers the ability to share access to all of their financial data online, including mortgages, savings accounts, 401Ks, bills, payroll data, insurance, and more. Envestnet | Yodlee has been doing a commercial model of open finance for 20 years, Costello explains, giving them a front-row seat to the benefits when this data can be shared. “We know how important access to all these data types is for good advice, for proper lending decisions, for helping consumers understand and nudge their financial behaviors in the right direction,” he explains. The first major government-backed implementation of open banking around the globe was in the U.K., coming at the same time as PSD2, the Revised Payment Service Directive, which meant only payment accounts were in its scope. Non-payment accounts were behind security controls, and financial institutions couldn’t make the data available via the API. That’s had unintended negative consequences for consumers in the U.K. — but the issue can be addressed with the shift to open finance, Costello says. “The Financial Conduct Authority in the U.K. is engaging, very sincerely, in the evolution to open finance based on the hard lessons learned from their open banking implementation,” he says. Australia started first with their Consumer Data Rights. The Austrailia Parliament passed a consumer data law, the CDR, which affirms the open data concept: a consumer has a right to their data, and the right to permission it to accredited, appropriate third parties. The institution holding the data has the obligation to protect it, and the obligation to release it, to participate in the scheme. They started a phased implementation of open banking, beginning with checking and savings accounts and credit cards. The second and third implementations of CDR are telecommunications and energy, two of the most impactful discretionary spends that people have. In the U.S., the technical standard used for the open banking initiative is being evangelized and implemented by the Financial Data Exchange. Their model includes mortgages, loans, and tax forms.  The Finacial Data and Technology Association (FDATA) advocates that policy makers and regulators begin with this  end in mind as they lobby regulators on Parliament Hill in Canada and Capitol Hill in the U.S. “An open banking regime in the U.S. that’s just limited to payment accounts would be a major step back,” Costello says. “We’re not going to repeat the mistake that was made in the U.K.” Open finance will improve the experience for customers in the U.S., but they won’t really notice it directly, Costello says. However, under the hood, they’ll benefit from more reliability and more symmetric customer protection end to end — and that will make a big difference. This move to a regulated open finance experience will give customers not just uniform access to all of the data, but under the same umbrella of symmetric customer protection, their payment account data will be as safe as their loan data, payroll data, and so on. “The customer experience for the person who needs to use these services but is reluctant to is going to be incredibly positively impacted,” he says. “Now they’re going to have enough trust in these tools and services to know that if they’re harmed in some way, if there’s a breach in the system or a bad actor, they’re going to be protected.” “As this ecosystem takes off, the data that is being generated, correlated, and used is beneficial not just for the consumer and their direct third-party service providers, but by all thirdparty service providers,” Costello says For the banking industry, open finance levels the playing field. It allows all entities to participate in the ecosystem offering products and services that will either make better customers, attract more deposits, attract more lenders, or at least make less risky customers. “They get top-line growth and they can have bottom-line savings,” Costello says. “As well, there are efficiencies in the infrastructure as we move from screen scraping to APIs. That’s a huge operational savings.” Broker-dealers, RIAs, lenders, lending networks, and insurance companies will all benefit from the operational cost benefits of open finance, as well as the ability to participate in the ecosystem and compete with the startups. Insurance companies in particular will benefit because they’re going to have access to more data and be able to make better underwriting decisions. “There’s an opportunity for improved financial well-being of consumers and families but also it’s going to be a virtuous cycle for competition and customer protection,” he says. “If you’re a bank or a brokerage, now is not the time to be limited by your current technical debt or your current legacy systems.” To learn more about how open finance is evolving around the globe, the benefits to consumers and companies alike, and what financial institutions and fintechs need to do now to prepare for the shift, don’t miss this VB Live event. Don’t miss out. Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/01/curri-nabs-6m-for-ai-powered-last-mile-logistics-for-construction/,Curri nabs $6M for tech-powered last-mile construction logistics,"Curri, a Y Combinator-backed logistics startup delivering construction supplies and materials, today announced the closing of a $6 million funding round. The company says the proceeds will be used to expand its services as well as its market reach. Last-mile delivery logistics tends to be the most expensive and time-consuming part of the shipping process. According to one estimate, last-mile accounts for 53% of total shipping costs and 41% of total supply chain costs. With the rise of ecommerce in the U.S., retail providers are increasingly focusing on fulfilment and distribution at the lowest cost. Particularly in the construction industry, the pandemic continues to disrupt wholesalers, highlighting the need for flexible and reliable delivery. Curri claims to solve this problem in construction with an “Uber-like” last-mile delivery model. The company makes available to customers a fleet of drivers with trucks, flatbeds, cars, and other vehicles who can deliver items like pipe bundles, water heaters, and lumber. Curri users arrange an order, open the Curri app, and enter pickup and dropoff locations to book the service. Curri’s drivers then pick up the supplies and ensure the order is correct before fulfilling the delivery. Curri offers live updates via the app to let customers follow and share the status of their deliveries. It also provides proof-of-delivery signature and photos for tracking, regulatory, and compliance purposes. Curri competes with a number of startups in a last-mile delivery market that’s anticipated to reach $66 billion by 2026, including Bond, Bringg, Onfleet, DispatchTrack, and Deliverr. But Curri claims its secret sauce is something that cofounder and CEO Matthew Lafferty calls “elastic scale.” Basically, it’s a concept where customers only pay for what they need. While traditional fleets can underutilize trucks or idle drivers as they wait for orders to come in, Curri says it delivers loads faster thanks to a deep layer of predictive machine learning. According to Lafferty, thousands of customers use Curri to deliver shipments throughout the U.S. “Suppliers who don’t have the ability to make urgent, on-demand, or long distance deliveries are leaving sales on the table and risk losing customers and business to suppliers who do,” he said in a press release. “Fleet augmentation is the secret weapon of suppliers who care about getting material in their customer’s hands, fast.” Los Angeles, California-based Curri’s series A funding announced today was led by Brick & Mortar Ventures and included participation from existing backer Initialized Capital in addition to new investor Rainfall Ventures. It brings four-year-old Curri’s total raised to date to nearly $9 million following a $150,000 seed round in August 2019."
https://venturebeat.com/2021/04/01/aclaimant-a-data-driven-safety-and-risk-management-platform-for-the-workplace-raises-15m/,"Aclaimant, a data-driven safety and risk management platform for the workplace, raises $15M","Aclaimant, an “insight-driven” safety and risk management platform businesses use to monitor and report injuries and incidents in the workplace, has raised $15 million in a series B round of equity and debt funding. While remote work has been a prominent fixture for most companies over the past year, many businesses are now preparing to return to physical offices in some capacity. This is the world Aclaimant and its investors — including Next Coast Ventures — are betting on. Founded in 2013, Chicago-based Aclaimant offers tools that support tracking and the follow-up actions required to improve safety in the workplace and serves clients in industries such as construction, hospitality, insurance, and HR. Aclaimant’s service includes electronically submitting incident reports with support for capturing videos, images, and other documentation. Underpinning all of this are analytics that deliver insights into incidents and related claims activity, organizing all the data into a structured format and helping companies create reports to share with management and other stakeholders. The company in January launched its AI-powered Aclaimant Insights feature, which it said uses predictive modeling to help organizations “evaluate and understand every facet of risk in their environment,” spanning safety, activities, incidents, and claims. Other notable players in the space include Riskconnect, which was acquired by private equity giant Thoma Bravo a few years back. Aclaimant had previously raised $13 million, and with its fresh $15 million cash injection the company is well-positioned to support businesses as they welcome staff back to the office in the coming months. Indeed, tech giants Google and Amazon both confirmed this week that they will shortly start transitioning their staff back to the office, with Amazon noting that it expects most of its staff to be back by this fall."
https://venturebeat.com/2021/04/01/device-monitoring-and-management-startup-memfault-nabs-8-5m/,Device monitoring and management startup Memfault nabs $8.5M,"Memfault, a startup developing software for consumer device firmware delivery, monitoring, and diagnostics, today closed an $8.5 million series A funding round. CEO François Baldassari says the capital will enable Memfault to scale its engineering team and make investments across product development and marketing. Slow, inefficient, costly, and reactive processes continue to plague firmware engineering teams. Often, companies recruit customers as product testers — the first indication of a device issue comes through users contacting customer service or voicing dissatisfaction on social media. With 30 billion internet of things (IoT) devices predicted to be in use by 2025, hardware monitoring and debugging methods could struggle to keep pace. As a case in point, Palo Alto Networks’ Unit 42 estimates that 98% of all IoT device traffic is unencrypted, exposing personal and confidential data on the network. Memfault, which was founded in 2019 by veterans of Oculus, Fitbit, and Pebble, offers a solution in a cloud-based firmware observability platform. Using the platform, customers can capture and remotely debug issues as well as continuously monitor fleets of connected devices. Memfault’s software development kit is designed to be deployed on devices to capture data and send it to the cloud for analysis. The backend identifies, classifies, and deduplicates error reports, spotlighting the issues likely to be most prevalent. Baldassari says that he, Tyler Hoffman, and Christopher Coleman first conceived of Memfault while working on the embedded software team at smartwatch startup Pebble. Every week, thousands of customers reached out to complain about Bluetooth connectivity issues, battery life regressions, and unexpected resets. Investigating these bugs was time-consuming — teams had to either reproduce issues on their own units or ask customers to mail their watches back so that they could crack them open and wire in debug probes. To improve the process, Baldassari and his cofounders drew inspiration from web development and infrastructure to build a framework that supported the management of fleets of millions of devices, which became Memfault. By aggregating bugs across software releases and hardware revisions, Memfault says its platform can determine which devices are impacted and what stack they’re running. Developers can inspect backtraces, variables, and registers when encountering an error, and for updates, they can split devices into cohorts to limit fleet-wide issues. Memfault also delivers real-time reports on device check-ins and notifications of unexpected connectivity inactivity. Teams can view device and fleet health data like battery life, connectivity state, and memory usage or track how many devices have installed a release — and how many have encountered problems. “We’re building feedback mechanisms into our software which allows our users to label an error we have not caught, to merge duplicate errors together, and to split up distinct errors which have been merged by mistake,” Baldassari told VentureBeat via email. “This data is a shoo-in for machine learning, and will allow us to automatically detect errors which cannot be identified with simple heuristics.”  IDC forecasts that global IoT revenue will reach $742 billion in 2020. But despite the industry’s long and continued growth, not all organizations think they’re ready for it — in a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a major challenge. That’s perhaps why Memfault has competition in Amazon’s AWS IoT Device Management and Microsoft’s Azure IoT Edge, which support a full range of containerization and isolation features. Another heavyweight rival is Google’s Cloud IoT, a set of tools that connect, process, store, and analyze edge device data. Not to be outdone, startups like Balena, Zededa, Particle, and Axonius offer full-stack IoT device management and development tools. But Baldassari believes that Memfault’s automation features in particular give the platform a leg up from the rest of the pack. “Despite the ubiquity of connected devices, hardware teams are too often bound by a lack of visibility into device health and a reactive cycle of waiting to be notified of potential issues,” he said in a press release. “Memfault has reimagined hardware diagnostics to instead operate with the similar flexibility, speed, and innovation that has proven so successful with software development. Memfault has saved our customers millions of dollars and engineering hours, and empowered teams to approach product development with the confidence that they can ship better products, faster, with the knowledge they can fix bugs, patch, and update without ever disrupting the user experience.” Partech led Memfault’s series A raise with participation from Uncork Capital, bringing the San Francisco, California-based company’s total raised to $11 million. In addition to bolstering its existing initiatives, Memfault says it’ll use the funding to launch a self-service of its product for “bottom-up” adoption rather than the sales-driven, top-down approach it has today."
https://venturebeat.com/2021/04/01/scratchpad-command-lets-you-update-salesforce-from-anywhere-on-the-web/,Scratchpad Command lets you update Salesforce from anywhere on the web,"Scratchpad, a fledgling startup that has built a modern productivity workspace directly on top of the Salesforce CRM, has launched a new feature that lets users update Salesforce from whatever website they are currently on, without switching tabs. The problem Scratchpad is looking to fix is that while Salesforce is the dominant CRM tool for sales teams, it’s not always popular from a usability perspective. Scratchpad develops a suite of tools that integrate with Salesforce — spanning notes, spreadsheets, tasks, search, and more — bundles them under a nice interface, and sells them as a freemium SaaS subscription. Notable early enterprise customers include Adobe, Autodesk, Box, Snowflake, Splunk, and Twilio. With Scratchpad Command, which is available for free as of this week, the company is now setting out to help sales teams avoid having to constantly switch between Salesforce and the various web apps they need to do their job. The key benefit Scratchpad Command offers is that users can not only access Salesforce data from any tab (e.g. to check whether a LinkedIn prospect they’re researching is already in their Salesforce instance), they can now update their Salesforce instance from wherever they are. First, the user has to install the Scratchpad extension for Chrome and authenticate themselves with their Salesforce credentials. They then have to activate Scratchpad Command during the setup process by checking the box. After that, the user can hit Ctrl J (Windows), Command J (Mac) and the Scratchpad icon on the browser toolbar to be presented with a search bar that lets them search their Salesforce instance or create a new note, task, account, lead, opportunity, and more. Scratchpad Command more or less brings the full functionality of Salesforce to wherever you are on the web with a single click. So if you’re in Slack and a potential lead pops up, you can pounce on it without switching tabs. Scratchpad is just one of several startups in the Salesforce ecosystem to garner investor attention recently. According to some estimates, the burgeoning ecosystem is at least 4 times larger than Salesforce itself. Just this week, Sonar raised $12 million for a platform that monitors companies’ Salesforce tech stack for changes, and this came shortly after Copado’s $96 million raise for Salesforce-native DevOps. OwnBackup, meanwhile, secured $167.5 million at a $1.4 billion valuation in January to power cloud data backups for Salesforce. Founded out of San Francisco in 2019, Scratchpad recently raised $13 million in a series A round of funding from Accel and David Sacks’ Craft Ventures, with Sacks now sitting on Scratchpad’s board of directors."
https://venturebeat.com/2021/04/01/ibm-and-red-hat-form-strategic-partnership-with-process-mining-startup-celonis/,IBM and Red Hat form strategic partnership with process mining startup Celonis,"(Reuters) — Celonis, a fast-growing German process mining software startup, has struck a strategic partnership with IBM to help companies make the most of the digital transformation that many are undergoing at speed. IBM’s Global Business Services consulting arm will weave the Celonis Execution Management System into its offering, adding the ability to analyse data thrown off by processes like supply-chain management, finance, or procurement to identify weaknesses and recommend fixes. Celonis will also shift its software stack to IBM’s Red Hat OpenShift platform, which enables companies to operate in an open “hybrid” setting that can include public or private cloud datacenters, on-premise servers, and mainframe computers. That represents a big step for the Munich startup, which last raised funds from investors at a valuation of $2.5 billion in 2019 and counts Coca-Cola, Siemens, Uber, and Vodafone as clients. “This is the most comprehensive and committal partnership that I’ve been part of,” said Miguel Milano, chief revenue officer and co-owner of Celonis, who joined from Salesforce a year ago. “We took the risk because we saw the opportunity and the value to re-engineer our full stack to run on Red Hat OpenShift. That is massive,” Milano, a 28-year software industry veteran, told Reuters in an interview. A typical large company runs more than 1,000 processes with the use of technology, often in different computing environments, creating complexity that makes it hard to bring about change. Weaving Celonis into IBM’s consulting offering can help clients identify where the value can be found across a range of applications, making it possible to benchmark performance, deploy artificial intelligence, or trigger automation. “Every workflow in every organization is now fair game,” said Mark Foster, senior vice president at IBM Services. Celonis has doubled bookings and revenues over the past year, and Milano said the partnership offered an opportunity to accelerate its growth further. “Our breakthrough from a strategic perspective was to realise that Celonis alone cannot take our platform to every company, every industry, every process,” he said, explaining the attraction of the partnership."
https://venturebeat.com/2021/03/31/cisco-is-bringing-individual-and-team-insights-to-webex-video-calls/,Cisco is bringing individual and team insights to Webex video calls,"Starting this summer, Cisco’s Webex will begin serving up insights for video calls to a select group of individuals, teams, and organizations. Engagement insights include things like how often you had your video on or showed up on time and the people or teams within an organization you speak with most often. The goal, Cisco VP Jeetu Patel told VentureBeat in a phone interview, is to make video calls better for people living in the hybrid world between in-person meetings in the office and virtual meetings at home. The tricky part, he said, is considering what information is useful for an individual to know while not raising concern that Webex is, for example, alerting managers to employees who are routinely late to meetings.  “Let’s say you did 12 meetings today, and in six of those meetings with four people or fewer, you actually spoke for 90% of the time. That would be a really bad thing to give your boss but a really good thing for you to have so you can say, ‘Oh, I should probably do a better job listening,'” he said. “The privacy on that front is not at the organizational level. It’s at the individual level. So when we provide insights like that to an individual, the individual owns the data, not the organization, because we don’t believe that without your explicit permission, you’d want to have your boss see that.” Webex has introduced a series of new features in recent months, some powered by artificial intelligence, to change how people share information in video calls. Toward this end, Patel said, “We’ve probably invested about a billion dollars or so in the past two years in AI.” Gesture recognition means people in video calls can now raise their hand to ask to speak or give a thumbs up or thumbs down to register feedback. Another AI-powered feature on the way will crop the faces of people who attend in-person meetings for anyone who’s working remotely. “Even though there are three people sitting in a conference room, we’ll actually break the stream into three separate boxes and show it to you, and our hardware will actually do that,” Patel said. Patel has overseen the acquisition of three companies since joining Cisco last summer, after serving as chief product officer at Box. Last month, Cisco closed its acquisition of IMImobile for $730 million, in part to beef up its AI capabilities. Last summer, Cisco announced plans to acquire BabbleLabs, an AI startup focused on filtering audio so the sound of someone doing dishes, a lawnmower running, or other loud background noise can be reduced or eliminated. And earlier this year, Cisco acquired Slido, a startup that makes engagement features for video calls, like word clouds and upvoting questions. Such features can allow a meeting to take the structure of a town hall, with transparency around the top questions for employees within an organization, since everyone can see the questions being posted. But Patel acknowledges that there are limits to how far the technology should go. “Engagement should not be measured based on having a judgment on someone saying, ‘I’m judging that you look sad, and therefore I’m going to do certain things’ … at that point in time, in my mind, you could cross a boundary where there’s more bad that can come out of that than good,” he said. In 2019, Cisco acquired Voicea to power speech-to-text transcription of meetings. Closed captioning and live translation are also available in Webex calls. Deciding where to draw the line on which AI-powered features or insights to introduce in video calls can be a challenge. Earlier this year, Microsoft Research did a study with AffectiveSpotlight on AI for recognizing confusion, engagement, and head nods in meetings. If taken in the aggregate, picking up cues from the audience could be really helpful, particularly for large organizations. But if affective AI for video calls leads to a critique of how often a person smiles or has a certain expression on their face, it could be considered invasive or counterproductive or even biased toward groups of people. Video analysis of expression today can have major shortcomings. A group of journalists in Germany recently demonstrated that placing a bookshelf in the background or wearing glasses can change affective AI evaluations of a person in a video. And it shouldn’t matter whether a person is an extrovert or prefers not to talk in group settings as long as they fulfill their job duties. Some people talk a lot but have nothing much say, while others speak less often but deliver sharp insights or sage advice. It all depends on the team, role, and scenario. Monitoring such information also raises the question of consent. “There’s a fine line between ‘This is super productive’ and ‘We can’t do this because it violates my privacy or it’s just outright creepy,'” Patel said.  Cisco plans to roll out Webex People Insights globally over the next year, starting with select users in the U.S. this summer. The company announced the news today as part of its Cisco Live virtual event. In other Cisco Live news, on Tuesday the company announced plans to combine networking, security, and IT infrastructure offerings and to work with the Duo authentication platform it acquired in 2018."
https://venturebeat.com/2021/03/31/cresta-which-uses-ai-to-mentor-customer-service-agents-in-real-time-raises-50m/,"Cresta, which uses AI to mentor customer service agents in real time, raises $50M","Cresta, an AI-powered platform that offers real-time support to help customer service agents respond to inquiries on calls or in chats, has raised $50 million in a series B round of funding. The company’s latest investment, which was led by Sequoia Capital, with participation from Greylock Partners, Andreessen Horowitz, Allen & Company, and Porsche Ventures, comes after a year of growth that saw its revenues quadruple. It’s difficult to read too much into any first-year revenue growth metrics, but it’s clear that companies are hankering for technology that helps them optimize their customer-facing operations. Contact centers have proven fertile ground for AI, with a slew of companies emerging to offer their own take on how automation can improve companies’ interactions with their customers. Just today, Uniphore announced a fresh $140 million investment to analyze emotion and engagement in both voice and video-based calls, while Talkdesk launched a new “human-in-the-loop” AI trainer for contact centers. Cresta shares common ground with many of these companies, though it’s placing a specific focus on learning from what the best-performing agents do and passing this knowledge to colleagues while nudging them with suggested responses. The San Francisco-based company officially launched last year with $21 million in funding, and it has amassed an impressive roster of clients so far, including Intuit, Adobe, and Dropbox. Cresta recently introduced Cresta for Voice to target phone-based sales and contact center teams, and it launched an integration with Amazon Web Services’ Amazon Connect cloud contact service platform last year."
https://venturebeat.com/2021/03/31/cloudera-expands-enterprise-data-platform-to-google-cloud/,Cloudera expands enterprise data platform to Google Cloud,"Enterprise data cloud company Cloudera has announced that its data platform is now available on Google Cloud. The Palo Alto-based company launched the Cloudera Data Platform (CDP) back in 2019, following its merger with rival Hortonworks. The CDP combined technologies from both companies, serving to bring a powerful data and analytics platform to hybrid and multicloud enterprises. At launch, the only public cloud CDP supported was AWS, though Azure was added to the mix shortly afterward. So today’s news has been a long time coming. This is a big deal for several reasons, though perhaps chiefly because it positions Cloudera as a truly platform-agnostic data platform, one that could help it capture a bigger piece of the burgeoning multicloud market. While similar services from cloud rivals such as Microsoft and Google do offer some support for competing public (and private) clouds, they were ultimately designed for their respective owner’s cloud. Cloud infrastructure spending has skyrocketed over the past year, and reports suggest businesses are eager to segue from single cloud platform providers, lured by the promise of flexibility, a way to avoid vendor lock-in, and the inherent task-specific strengths different clouds offer. Flexera’s recent State of the Cloud report noted that 93% of enterprises now use multiple cloud service providers, a factor that has led Microsoft and Google to embrace a multicloud world, though Amazon’s AWS still has some way to go on that front. The CDP is generally available on Google Cloud from today."
https://venturebeat.com/2021/03/31/cloud-management-startup-striim-plans-new-integrations-and-streaming-analytics-with-50m-funding/,Cloud management startup Striim plans new integrations and streaming analytics with $50M funding,"Striim, a startup specializing in streaming data integration, today closed a $50 million series C funding round led by Goldman Sachs Growth Equity. Striim founder and CEO Ali Kutay says the funding will be used to scale the company’s offerings that power cloud integration, edge processing, and streaming analytics. Data modernization continues to be a major push, with Gartner predicting that over 75% of midsize and large companies will deploy a multi-cloud or hybrid IT strategy by 2021. Reflecting this uptick, Markets and Markets anticipates that the multi-cloud management sector will climb from $1.17 billion in value in 2017 to $4.49 billion by 2022. Kutay, Steve Wilkes, and Sami Akbay cofounded Palo Alto, California-based Striim in 2012 to address the needs of enterprises managing data across multiple clouds. Each had experience with deep data and infrastructure and shared successes building GoldenGate Software, which was acquired by Oracle in 2009, and WebLogic, which merged with BEA. Striim continuously ingests data from databases, log files, messaging systems, cloud apps, and internet of things devices, performing processing including filtering, transformations, aggregations, masking, and metadata enrichment. The platform offers monitoring and validation features that help to trace and confirm the collection and delivery of streaming data, as well as a wizard that leads customers through the process of defining data flows and connections.  Striim customers can connect with their data in public and private clouds and build custom pipelines with routing and rules defined in an SQL-like language. Out-of-the-box dashboards show table-level metrics and latency of data delivery, with tools admins can use to configure performance and uptime alerts and self-healing pipelines with remediation workflows. These dashboards can also incorporate AI for deeper insights and expose metrics on connector components including read and write rates, lag, latency, and CPU usage. Striim appears to be well-positioned for growth in an increasingly lucrative market. As cloud computing usage explodes during the pandemic, enterprises are embracing multi-cloud strategies. Ninety-three percent are using multiple clouds and 87% have a hybrid cloud setup, according to a Flexera survey. And while 20% of organizations already spend over $12 million a year on public clouds, 59% expect their cloud usage to exceed projections due to pandemic headwinds. Striim recently announced a preview release of StreamShift, a managed service offering that helps enterprises migrate databases from on-premise to the cloud. (A full release is expected in Q2 2021.) Building on this, the company says it’s continuing to hire “aggressively” and now has over 100 full-time employees across offices in California as well as Chennai, India. Striim’s customers include brands in financial services, health care, transportation and logistics, and retail, including UPS, Macy’s, Equifax, and HSBC. Strimm’s latest funding round brings the company’s total raised to date to over $108 million. It follows on the heels of a $16.5 million extension to a series B round that closed in March 2017."
https://venturebeat.com/2021/03/31/world-leading-ai-research-and-inclusion-at-the-forefront-of-this-years-nvidia-gtc/,World-leading AI research and inclusion at the forefront of this year’s NVIDIA GTC,"This article is part of the VB Lab / NVIDIA GTC insight series. “The story of GTC is in many ways the story of NVIDIA, and it’s also the story of what’s happening in technology,” says Greg Estes, VP of corporate marketing and developer programs at NVIDIA. Twelve years ago, GTC began as a conference focused squarely on GPUs, and at that time, that meant primarily graphics and gaming. “But then people figured out that GPUs are the perfect architecture for AI,” says Estes. GTC is now billed as the conference for AI innovators, developers, technologists, startups and creatives, and this year it will offer over 1,500 sessions covering breakthroughs in AI, data center, accelerated computing, autonomous vehicles, health care, intelligent networking, game development, and more. This year’s event will take place online April 12 – 16, 2021, free for registered participants. The week-long event kicks off with a keynote on April 12 at 8:30 a.m. PDT with NVIDIA’s CEO and founder Jensen Huang. Along with exciting announcements, he’s set to share the company’s vision for computing that scales from the edge to the data center to the cloud. Following the keynote are panels from the world’s leading researchers along with thought leaders from top companies, including Adobe, Amazon, Facebook, GE Renewable Energy, Google, MIT, Microsoft, Salesforce, and Stanford University. “We bring together the people who are taking the leading edge of technology and making it deployable across businesses and industries of all kinds,” Estes says. “You’ll hear everything from bleeding edge, just-invented-last-week technology to the platform solutions that can be securely deployed in your data center. There’s no other place you’ll find this huge range of insight and expertise.” That’s why GTC has grown so large, from its 2009 debut in a San Jose hotel ballroom to the sprawling behemoth that will attract more than 100,000 attendees and an A-list line-up of speakers this year. That includes Turing Award winners like Geoffrey Hinton, Yoshua Bengio, and Yann LeCun; AI pioneers like Juergen Schmidhuber; MacArthur Fellowship Award winner Daphne Koller; Gordon Bell Award winners for COVID research Rommie Amaro and Lillian Chong; autonomous vehicle pioneer Raquel Urtasun; and many more, in addition to a number of NVIDIA’s own researchers, including Anima Anandkumar and Sanja Fidler. “GTC has now become a magnet for business leaders, developers, and startups to talk about what they’re doing as they implement AI and data science across their work, from health care, automotive and transportation to energy research, retail, media, and entertainment,” says Estes. “The Da Vincis and Curies of our time are at GTC,” he adds. “Where else do you get to hang out with those people?” In addition to the vast breadth of topics, the conference is putting emphasis on two major tech topics this year: autonomous vehicle technology and health care. Autonomous vehicle tech is evolving rapidly, and so is the industry. AI is creating an opportunity for new entrants into the field, shaking up a playing ground that had long been dominated by the major auto players, and opening new business models to automakers. This year, leading thinkers from two dozen autonomous vehicle startups including Aurora, Cruise, and Zoox will be presenting, Estes says, alongside the world’s largest automakers, including Audi, Ford, Toyota, and Volkswagen. “When you think about the toughest problems in AI, autonomous vehicle technology is far and away at the top of the list,” he says. “As a research area, dealing with the complexity of the neural networks, sensors, and data, plus keeping things secure, is very rich.” In the health care arena, AI has played a major role in everything from COVID-19 research to cancer detection, genomics, and more, all the way through climate studies and other essential research on the many things that impact human health. Several NVIDIA Inception startups, the company’s AI startup acceleration platform, are leading the charge in health care. “The diversity of health care research comes together at GTC — the combination of researchers, doctors, and companies focused on drug discovery is unique compared with other health care conferences,” Estes says. NVIDIA is committed to making GTC a forum for all communities to engage with the leading-edge of AI, data science, and other ground-breaking technologies, Estes says. “These world-changing technologies affect everyone, which means it’s more important than ever to ensure everyone’s voices are heard.” Overall, GTC attracts attendees from 165 countries. “We’ve been doing a lot of work to make GTC more inclusive for women, underrepresented communities, and developers from emerging countries,” Estes says. “We have an amazing and diverse group of speakers to inspire the next generation regardless of what they look like or where they come from.” That includes several hundred women speakers, including Hildegard Wortmann, on the board of management at Audi; Victoria Uti, director and principal research engineer at Kroger Technology, and Amy Bunszel, senior vice president of product at Autodesk. NVIDIA is also creating pathways to learning for those new to AI and deep learning, which includes free seats for day-long certification classes at the NVIDIA Deep Learning Institute. Partners comprise minority-serving institutions such as Norfolk State, Howard University, North Carolina A&T University, San Jose State University, and Bowie State University, and tech organizations like Black in AI, Black Tech Nation, Data Science Salon, LatinX in AI, and Qwasar. NVIDIA’s inclusion efforts also encompass outreach to developers in emerging technology centers, including Africa, Latin America, and southeast Asia, for which they will feature the work of startups and rising developers building AI in their regions. GTC will also highlight innovative work being done by scientists such as Dr. Nashlie Sephus, who is transforming 12 acres of abandoned land in Mississippi into a $25M tech hub, as well as Black futurist Justin Shaifer, and the Hip Hop MD, Maynard Okereke, as they co-moderate a panel on the importance of educating youth to be more AI literate. There will also be networking opportunities for underrepresented communities, including GTC’s popular Dinner with Strangers series. The series provides an opportunity to meet new people who share similar interests, build connections, and foster career growth. Get all the details about GTC 2021 right here. Registration is free. VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/31/edge-computing-orchestration-startup-zededa-raises-12-5m/,Edge computing orchestration startup Zededa raises $12.5M,"Edge virtualization startup Zededa today announced it has raised $12.5 million in an oversubscribed extension to its series A round. The company says the funding will be used to bolster its customer acquisition efforts as it continues to expand the size of its workforce. Global internet of things (IoT) revenue hit an estimated $1.7 trillion in 2019, when the number of edge devices connected to the internet exceeded 23 billion, according to CB Insights. But despite the industry’s growth, not all organizations think they’re ready for it. In a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a significant blocker. Zededa was cofounded in 2016 by Erik Nordmark, Roman Shaposhnik, Said Ouissal, and Vijay Tapaskar to solve the edge computing adoption challenges plaguing the enterprise. Based in Santa Clara, California and India, the company’s hardware- and cloud-agnostic software enables app deployment over most types of edge networks. “Edge computing is critical in not just driving efficiencies that may lead to cost savings, but more importantly in building new business models and creating new customer experiences,” Ouissal told VentureBeat via email. “Organizations are quickly identifying new use cases where edge computing will provide tangible benefits, such as remote orchestration to reduce the need for employees to travel to edge locations, self-service kiosks, and computer vision to maintain safe indoor experiences [during the pandemic].” Zededa says its centralized, subscription-based technology is built on open standards and plays nicely with devices from Advantech, Lanner, SuperMicro, Scalys, and other vendors. It supports Docker containers, Kubernetes clusters, and virtual machines and works in concert with Eve, a bare-metal Linux-based operating system designed for edge computing that counts among its contributors Intel and General Electric. Leveraging apps, their dependencies, and core operating system bits that periodically communicate with the cloud, Zededa’s platform attempts to ensure edge installations behave as they should, according to a spokesperson. “Zededa [addresses] the complexity of today’s solutions through an open and purpose-built IoT edge orchestration framework that breaks down silos and delivers the agility customers need to evolve their connected operations with a multicloud strategy,” they said. There’s an abundance of tools promising to simplify IoT analytics and management at the edge, including Google’s Cloud IoT Edge, Amazon’s Amazon Web Services (AWS) IoT, Microsoft’s Azure Sphere, and Baidu’s OpenEdge. But Zededa asserts that it has an advantage in the open source nature of its approach. “We made our product generally available earlier this year in January, after nearly a year of working with a very select group of advanced and leading customers and partners to build and refine the product features. Today we have over a dozen paying customers, including several Fortune 500 companies spread across verticals like manufacturing, oil and gas, and renewable energy, telco, and health care,” Ouissal said. “We anticipate bringing on several new high-profile customers this year and are excited to see customers begin to scale their deployments. We expect to see the number of edge nodes under management more than triple by the middle of 2021 from the beginning of the year.” Zededa recently launched a marketplace hosting data ingestion, AI and machine learning, networking, and security apps and joined the Linux Foundation’s EdgeX Foundry, an ongoing vendor-neutral open source IoT project. Zededa is also part of the Foundation’s LF Edge umbrella organization, where it’s incubating Eve and the telecom-oriented Akraino Edge Stack. New strategic investors Rockwell Automation, Juniper Networks, and EDF North America Ventures participated in Zededa’s expanded round, along with investors Almaz Capital, Energize Ventures, Lux Capital and HBAM. This brings the company’s total raised to date to $28.5 million."
https://venturebeat.com/2021/03/31/ai-experts-warn-facebooks-anti-bias-tool-is-completely-insufficient/,AI experts warn Facebook’s anti-bias tool is ‘completely insufficient’,"Facebook today published a blog post detailing Fairness Flow, an internal toolkit the company claims enables its teams to analyze how some types of AI models perform across different groups. Developed in 2018 by Facebook’s Interdisciplinary Responsible AI (RAI) team in consultation with Stanford University, the Center for Social Media Responsibility, the Brookings Institute, and the Better Business Bureau Institute for Marketplace Trust, Fairness Flow is designed to help engineers determine how the models powering Facebook’s products perform across groups of people. The post pushes back against the notion that the RAI team is “essentially irrelevant to fixing the bigger problems of misinformation, extremism, and political polarization [on Facebook’s platform],” as MIT Tech Review’s Karen Hao wrote in an investigative report earlier this month. Hao alleges that the RAI team’s work — mitigating bias in AI — helps Facebook avoid proposed regulation that might hamper its growth. The piece also claims that the company’s leadership has repeatedly weakened or halted initiatives meant to clean up misinformation on the platform because doing so would undermine that growth. According to Facebook, Fairness Flow works by detecting forms of statistical bias in some models and data labels commonly used at Facebook. Here, Facebook defines “bias” as systematically applying different standards to different groups of people, like when Facebook-owned Instagram’s system disabled the accounts of U.S.-based Black users 50% more often than accounts of those who were white. Given a dataset of predictions, labels, group membership (e.g., gender or age), and other information, Fairness Flow can divide the data a model uses into subsets and estimate its performance. The tool can determine whether a model accurately ranks content for people from a specific group, for example, or whether a model under-predicts for some groups relative to others. Fairness Flow can also be used to compare annotator-provided labels with expert labels, which yields metrics showing the difficulty in labeling content from groups and the criteria used by the original labelers. Facebook says its Equity Team, a product group within Instagram focused on addressing bias, uses “model cards” that leverage Fairness Flow to provide information potentially preventing models from being used “inappropriately.”  The cards include a bias assessment that could be applied to all Instagram models by the end of next year, although Facebook notes the use of Fairness Flow is currently optional. Mike Cook, an AI researcher at the Queen Mary University of London, told VentureBeat via email that Facebook’s blog post contains “very little information” about what Fairness Flow actually does. “While it seems that the main aim of the tool is to connect the Facebook engineers’ expectations with the model’s output, … the old adage ‘garbage in, garbage out’ still holds. This tool just confirms that the garbage you’ve gotten out is consistent with the garbage you’ve put in,” he said. “In order to fix these bigger problems, Facebook needs to address the garbage part.” Cook pointed to language in the post suggesting that because groups might have different positive rates in factual (or “ground truth”) data, bias isn’t necessarily present. In machine learning, a false positive is an outcome where a model incorrectly predicts something, while a true positive measures the percentage of the model’s correct predictions. “One interpretation of this is that Facebook is fine with bias or prejudice, as long as it’s sufficiently systemic,” Cook said. “For example, perhaps it’s reasonable to advertise technology jobs primarily to men, if Facebook finds that mostly men click on them? That’s consistent with the standards of fairness set here, to my mind, as the system doesn’t need to take into account who wrote the advert, what the tone or message of the advert is, what the state of the company it’s advertising is, or what the inherent problems in the industry the company is based in are. It’s simply reacting to the ‘ground truth’ observable in the world.” Indeed, a Carnegie Mellon University study published last August found evidence that Facebook’s ad platform discriminates against certain demographic groups. The company claims its written policies ban discrimination and that it uses automated controls — introduced as part of the 2019 settlement — to limit when and how advertisers target ads based on age, gender, and other attributes. But many previous studies have established that Facebook’s ad practices are at best problematic. Facebook says Fairness Flow is available to all product teams at the company and can be applied to models even after they’re deployed in production. But Facebook admits that Fairness Flow, the use of which is optional, can only analyze certain types of models — particularly supervised models that learn from a “sufficient volume” of labeled data. Facebook chief scientist Yann LeCun recently said in an interview that removing biases from self-supervised systems, which learn from unlabeled data, might require training the model with an additional dataset curated to unteach specific biases. “It’s a complicated issue,” he told Fortune. University of Washington AI researcher Os Keyes characterized Fairness Flow as “a very standard process,” as opposed to a novel way to address bias in models. They pointed out that Facebook’s post indicates the tool compares accuracy to a single version of “real truth” rather than assessing what “accuracy” might mean to, for instance, labelers in Dubai versus in Germany or Kosovo. “In other words, it’s nice that [Facebook is] assessing the accuracy of their ground truths … [but] I’m curious about where their ‘subject matter experts’ are from, or on what grounds they’re subject matter experts,” Keyes told VentureBeat via email. “It’s noticeable that [the company’s] solution to the fundamental flaws in the design of monolithic technologies is a new monolithic technology. To fix code, write more code. Any awareness of the fundamentally limited nature of fairness … It’s even unclear as to whether their system can recognise the intersecting nature of multiple group identities.” Exposés about Facebook’s approaches to fairness haven’t done much to engender trust within the AI community. A New York University study published in July 2020 estimated that Facebook’s machine learning systems make about 300,000 content moderation mistakes per day, and problematic posts continue to slip through Facebook’s filters. In one Facebook group that was created last November and rapidly grew to nearly 400,000 people, members calling for a nationwide recount of the 2020 U.S. presidential election swapped unfounded accusations about alleged election fraud and state vote counts every few seconds. Separately, a May 2020 Wall Street Journal article brought to light an internal Facebook study that found the majority of people who join extremist groups do so because of the company’s recommendation algorithms. And in an audit of the human rights impact assessments (HRIAs) Facebook performed regarding its product and presence in Myanmar following a genocide of the Rohingya people in that country, Carr Center at Harvard University coauthors concluded that the third-party HRIA largely omitted mention of the Rohingya and failed to assess whether algorithms played a role. Accusations of fueling political polarization and social division prompted Facebook to create a “playbook” to help its employees rebut criticism, BuzzFeed news reported in early March. In one example, Facebook CEO Mark Zuckerberg and COO Sheryl Sandberg have sought to deflect blame for the Capitol Hill riot in the U.S., with Sandberg noting the role of smaller, right-leaning platforms despite the circulation of hashtags on Facebook promoting the pro-Trump rally in the days and weeks beforehand. Facebook doesn’t perform systematic audits of its algorithms today, even though the step was recommended by a civil rights audit of Facebook completed last summer. “The whole [Fairness Flow] toolkit can basically be summarised as, ‘We did that thing people were suggesting three years ago, we don’t even make everyone do the thing, and the whole world knows the thing is completely insufficient,'” Keyes said. “If [the blog post] is an attempt to respond to [recent criticism], it reads as more of an effort to pretend it never happened than actually address it.”"
https://venturebeat.com/2021/03/31/windfall-nabs-21m-for-ai-that-aids-nonprofit-fundraising/,Windfall nabs $21M for AI that aids nonprofit fundraising,"Windfall, a startup developing an AI platform to help nonprofits engage donors, today raised $21 million in venture capital. The company says the funds will be used to invest in product R&D, the expansion of its team, and the scaling up of Windfall’s marketing and sales operations. Securing donations remains among the top challenges philanthropic organizations face on a regular basis. Eight percent of nonprofits saw the number of philanthropies competing for dollars as one of the biggest barriers to their charitable impact, a 2019 Statista survey found. Crises like the pandemic threaten to exacerbate this. According to the Association of Fundraising Professionals, more than half of U.S. nonprofits expected to raise less money in 2020 than they did in 2019, and an equal percentage believe the same will occur in 2021. Founded in 2016 by Arup Banerjee, Cory Tucker, and Dan Stevens, Windfall offers estimates of potential donors’ net worth at the household level, leveraging data and machine learning to make projections. The platform also delivers contextual analysis, putting donor databases in a framework that can then be used for data science. With Windfall, nonprofits can prioritize donors and apply insights to segment the donors and focus outreach. The company claims its platform can sync millions of records with Salesforce, HubSpot, Shopify, and other apps on a weekly basis. “With a focus on the affluent, Windfall delivers a precise net worth figure, not a range, and a spectrum of deterministic consumer attributes at the individual household level,” the company writes on its website. “Windfall’s proprietary data set is completely rebuilt on a weekly basis, giving customers the best and latest insights on their constituents.” Windfall’s current round follows a seed round in 2019 that raised $9 million and allowed the company to continue building its consumer financial data products. Windfalls says it had a 3 times year-over-year growth rate in 2020 and that its customer base exceeds 500 organizations, including the Environmental Defense Fund, Make-A-Wish, and the University of Michigan. “At Windfall, our focus is empowering our customers to access the most reliable data and insights possible, but that’s just the beginning,” CEO Banerjee said. “We’ve built artificial intelligence and machine learning into our platform to drive better decision-making, enable teams to develop comprehensive data-driven strategies and build that into their existing workflows. It’s a truly transformative approach for our customers.” David Lamond led San Francisco, California-based Windfall’s series A with participation from EPIQ Capital Group and existing investors Bonfire Ventures, Bullpen Capital, Cherubic Ventures, and ValueStream Ventures. It brings the company’s total raised to date to $30 million."
https://venturebeat.com/2021/03/31/dataikus-new-ai-tools-reduce-dependency-on-data-science-teams/,Dataiku’s new AI tools reduce dependency on data science teams,"Dataiku today expanded its effort to make AI accessible to the average business user with an update that makes it possible to run what-if simulations of AI models to determine how changes to the data they are based on will impact them. The goal is to make it easier for business analysts to experiment with AI models based on machine learning algorithms they can create with the help of a data scientist team, Dataiku CEO Florian Douetteau said. As part of that effort, Dataiku 9 adds a Model Assertions tool that enables a subject matter expert to inject a known condition or sanity-checks into a model to prevent a certain outcome or conclusion from ever being reached. There is also now a Visual ML Diagnostics tool that will generate error messages if the platform determines a model will fail and a Model Fairness Report tool that provides access to a dashboard through which companies can assess the bias or fairness of an AI model. Finally, there is also now a Smart Pattern Builder and Fuzzy Joins capability that makes it easier to work with more complex or even incomplete datasets without having to write code or manually clean or prep data. Most organizations are investing in AI to make better data-driven decisions, which Douetteau says makes it essential for business analysts to create AI models without having to wait for a data science team to construct them. “Business users should be able to create AI, not just consume it,” he said. That doesn’t necessarily mean an organization shouldn’t hire a data science team to address more complex challenges, but it does reduce the dependency an organization would otherwise have a on small team of specialists, Douetteau noted. When it comes to AI, most organizations are employing the approach they typically used to make previous generations of analytics applications available to end users. A team of IT specialists would create a series of dashboards based on data marts that would be exposed to business users via a self-service portal. But that approach is insufficient when it comes to AI because the rate of change in the underlying data has now substantially increased, Douetteau said. Business analysts need to be able to dynamically create AI models that enable organizations to respond faster to rapidly changing business conditions. Of course, there is no better example of the need for that capability than the COVID-19 pandemic. Most of the AI models organizations had in place prior to the arrival of the pandemic were rendered obsolete almost overnight. It has taken most of those organizations months to construct new AI models, but even now it’s difficult to make assumptions that are not subject to change as new data becomes available. Unfortunately, the average data science team today is fortunate if they can get an AI model into a production environment in a few months. Businesses clearly need to get data science tools into the hands of end users to enable AI to live up to its full potential, Douetteau said. The challenge is ensuring there are enough checks and balances in place to make sure any AI model that is implemented is properly vetted. After all, the scale at which an AI model typically works means that any potential error could represent a considerable risk for any business. At this point, however, the proverbial AI genie is out of the bottle, regardless of the level of risk. Tumultuous economic times are pushing many business executives to accept higher levels of risk in an effort to either reduce costs or maximize revenues. But whatever outcomes AI eventually enables, the way business processes are constructed and maintained is about to change utterly."
https://venturebeat.com/2021/03/31/moveworks-helps-enterprises-automate-it-self-service-tasks/,Moveworks helps enterprises automate IT self-service tasks,"IT issues take time to solve, which can cost enterprises money. A PagerDuty survey found 38.4% of organizations that take more than 30 minutes to resolve IT incidents see an impact on customer-facing services. Moreover, nearly one-third of departments regularly affected by technical problems say that an hour of downtime costs them $1 million or more. That’s where Moveworks aims to make a difference. The Mountain View, California-based company, which was founded in 2016, is developing an AI platform that can resolve IT support issues automatically. Today marks the launch of Moveworks’ newest product, the Employee Service Platform, which brings together AI and natural language understanding technologies to get employees help across departments. Moveworks says that the system can handle human resources, finance, and facilities issues end-to-end, from the initial request to the final resolution. According to CEO Bhavin Shah, Moveworks has been laying the groundwork for the Employee Service Platform since the company’s earliest days. Eighteen months ago, after experiencing success in the IT segment — Moveworks counts among its customers Palo Alto Networks, Slack, and LinkedIn — the company began building the platform. More recently, they started inviting customers in early access. “Everything we do at Moveworks is inspired by a simple idea: It shouldn’t take days to get help at work,” Shah said. “Today, after half a decade, Moveworks … delivers instant help to all lines of business.” Beyond answering questions about unlocking accounts, resetting passwords, and provisioning software, the Employee Service Platform helps surface forms, pull answers from knowledge bases, and route requests to the right subject-matter experts. The platform’s engine, which was trained on over 100 million real-world issues, combines domain recognition, semantic search, and deep integrations to address questions with answers from departments’ knowledge bases. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. “We engineered a unique approach to understanding the language used in the enterprise, which we deployed prior to this product expansion to resolve IT issues — without predefining specific intents or hard-coding rigid workflows. That approach is our multifaceted intent system,” CTO Vaibhav Nivargi told VentureBeat via email. “At a high level, it is a generalized natural language understanding system. Rather than predefining specific user intents, our multifaceted intent system determines the overarching action and resource type needed to resolve each issue. Once we’ve established this generalized intent, we then evaluate the utility of potential resources.” The Employee Service Platform also transforms resources to display information in a conversational format inside collaboration tools like Slack, Microsoft Teams, and more. For example, users can fill out IT forms without leaving the Moveworks interface in Teams or receive only the pertinent paragraph of a human resources policy after asking Moveworks a question in Slack. As part of the Employee Service Platform, Moveworks released the Employee Communications module, which enables company leaders to send messages via a cross-platform chatbot. The engine ingests knowledge articles and documents several times per day, enabling the chatbot to answer follow-up questions about messages autonomously. The chatbot market is expected to reach $1.23 billion by 2025, according to Grand View Research, and there’s reason for its continued growth. Fifty-three percent of service organizations expect to use chatbots within 18 months, according to a Salesforce survey. And Gartner estimates that chatbots were powering 85% of all customer service interactions as of last year. “Immediately following the pandemic, we saw a significant increase in the overall volume of tickets submitted to Moveworks — approximately twice as many in March 2020 than in February. Employees across industries needed to learn to use new collaboration tools, order new devices for the home office, look up colleagues’ contact information, troubleshoot Zoom, stay abreast of business continuity plans, and more,” Nivargi said. “Perhaps the most enduring challenge for companies in this work-from-anywhere economy is keeping their employees up-to-date and on the same page. … We responded to the demand by accelerating the creation of our new solution for employee communications. Our customers regularly achieve 50% to 70% engagement with communications campaigns done through Moveworks, compared to around 10% for the average mass email.”"
https://venturebeat.com/2021/03/31/oracle-offers-free-cloud-migration-to-lure-new-customers/,Oracle offers free cloud migration to lure new customers,"(Reuters) — Oracle announced on Wednesday it will migrate companies’ most complicated computer programs to its cloud for free as it tries to catch a new wave of potential cloud-computing clients by aiming to save them time and money. The pandemic prompted many businesses and governments to shift from in-house digital storage and computing to leased cloud servers from Amazon Web Services and other providers. A laggard in the cloud industry, Oracle is counting on its free support to persuade organizations that have not made a switch — or done so only partially — to opt for a move to its infrastructure. More than 100 customers have taken advantage of what Oracle is calling its Cloud Lift Services over the last six months, and the program is opening globally on Wednesday. “I’ll be surprised if any customer says ‘I don’t need this,'” Vinay Kumar, a senior vice president at Oracle, told Reuters. Over the last nine months, Oracle has shifted 1,000 workers to focus on Cloud Lift Services, Kumar said. He expects the offer will pay off for Oracle, but he declined to say how much the company has budgeted for it. Oracle’s expertise in using its own tools efficiently and setting up applications to work smoothly on the cloud speeds transitions and reduces the risk of customers encountering errors, he said. Rival cloud providers discount transition support but typically only for key customers or those that agree to long-term deals or minimum spending. Kumar said Oracle is not requiring commitments, but the program is limited to what he called the “handful of applications” that would be most difficult for a customer to migrate and excludes those requiring a thorough rewrite. Among early testers was new U.S. agribusiness client Cargill. With Oracle’s help, Cargill transitioned in weeks, not months, Kumar said."
https://venturebeat.com/2021/03/31/auditoria-raises-15-5m-to-automate-repetitive-accounting-tasks/,Auditoria raises $15.5M to expand AI platform automating repetitive accounting tasks,"Auditoria, a startup offering AI-driven automation products for corporate finance teams, today announced that it raised $15.5 million in series A funding. Cofounder and CEO Rohit Gupta says that the proceeds will shore up Auditoria’s investments in data science and the expansion of its sales, marketing, and customer success teams. . It’s estimated that accountants lose thousands of hours to repetitive tasks, follow-ups, error checking, and data entry. But studies show that the vast majority of these workloads can be automated. That may be why over 50% of respondents in a survey conducted by the Association of Chartered Certified Accountants said they anticipate the development of automated and intelligent systems will have an impact on finance over the next 30 years. Auditoria, which was founded in 2019, offers software that aims to streamline collections and procurement processes through the use of AI. A combination of machine learning, computer vision, robotic process automation, optical character recognition, and natural language processing enable Auditoria’s platform to augment the work of human accountants. The company claims it can handle certain tax obligations and expense management as well as vendor onboarding via auditable journal entries.  Auditoria’s workflow orchestration engine draws on data from clients’ email inboxes, apps, and other financial systems of record, leveraging algorithms to bring in human experts where necessary. Auditoria says its software can communicate with stakeholders using natural language while monitoring high-risk accounts with incentives and promotions for clients. Beyond this, Auditoria can respond to payment inquiries, capture supplier data, and automate the forecasting of financial key [performance indicators (KPIs). While 25-employee Auditoria competes with Botkeeper and Zeni in an accounting software market projected to be worth $19.59 billion by 2026, according to Mordor Intelligence, the company is among the first that’s enterprise resource planning-centric in Gupta’s mind. He says that more than 200 organizations are using Auditoria’s tools. “Despite launching out of stealth during the pandemic, Auditoria has seen incredible growth and traction,” Gupta said. “The team’s focused efforts to provide a purpose-built solution for corporate finance struck a chord with the forward-looking, next-gen finance executives looking to adopt software to drive greater efficiencies and increase resiliency across the back office.” Brian Ascher, partner at investor Venrock, added, “Finance operations automation is the future for corporate finance. We have been on the hunt to invest in the right team with the experience and innovative technology necessary to transform corporate finance, and we have found that in Auditoria. We envision a day when every global finance team is using Auditoria to streamline and automate back-office operations.” Venrock led Santa Clara, California-based Auditoria’s oversubscribed round announced today, which saw participation from Workday Ventures and existing investors including B Capital Group, Engineering Capital, Firebolt Ventures, and Neotribe Ventures."
https://venturebeat.com/2021/03/31/uniphore-nabs-140-million-for-automated-analysis-of-voice-and-video-calls/,Uniphore nabs $140 million for automated analysis of voice and video calls,"Uniphore, an AI-powered platform that helps businesses understand, analyze, and automate their voice-based customer service, has raised $140 million in a series D round of funding. The company said it plans to use the investment to expand its existing conversational AI and machine learning technologies deeper into the enterprise, with a particular focus on video-based applications. The genesis for this expansion actually dates back a couple of months to its acquisition of Emotion Research Lab, a Spanish startup that determines emotion and engagement levels through video-based interactions by tracking facial expressions and eye movement. Founded out of India in 2008, Uniphore offers a platform built around four core services: U-Self-Serve, designed to give businesses quick setup access to a conversational AI assistant; U-Analyze, which uses natural language processing (NLP) to glean insights and generate analytics from customer conversations; U-Trust, an automated voice authentication tool that helps companies verify an agent’s identity in the remote-working world; and U-Assist, which serves up real-time call transcriptions and in-call alerts. Uniphore, which opened a new U.S. HQ in Palo Alto in 2019, had previously raised $81 million and claims a roster of major enterprise clients, including BNP Paribas. Its latest investment was led by Sorenson Capital Partners, with participation from notable enterprise backers such as Cisco Investments. By adding video to its existing automated voice monitoring smarts, Uniphore is essentially looking beyond the customer service realm and into sales, marketing, and HR, among other business verticals. It’s focused anywhere companies may come face to face with people over video, which is particularly pertinent as the world has had to rapidly embrace remote work. In addition to expanding into video-based applications, Uniphore said it will invest in other areas around trust, security, and robotic process automation (RPA). This comes shortly after it acquired an exclusive third-party RPA license from NTT Data."
https://venturebeat.com/2021/03/31/productiv-which-develops-software-that-helps-enterprises-manage-saas-apps-raises-45m/,"Productiv, which develops software that helps enterprises manage SaaS apps, raises $45M","Productiv, a platform for aggregating real-time engagement data and insights for apps and management, today announced that it raised $45 million in a series C funding round, bringing its total raised to date to over $73 million. The company says it’ll use the funding to bolster growth after a year in which Productiv more than doubled its headcount and added customers including DocuSign, Kayak, PagerDuty, and Robinhood. Enterprise software-as-a-service (SaaS) adoption has never been higher. Companies use 16 SaaS apps on average, driving the global industry to an estimated $157 billion. But coinciding with this climb is a decline in app usage transparency. A recent survey of IT leaders conducted by Numerify found that 45% don’t have a complete picture of key apps and business health services, with 57% saying they lacked an overview of IT performance across projects and employees. The market’s relative opaqueness motivated Jody Shapiro, formerly head of product management for Google Analytics, to investigate a metrics-driven solution. Unable to find one, he developed his own in Palo Alto, California-based Productiv. “When I was at Google, I witnessed first-hand how Google Analytics’ quantification of marketing efforts transformed the CMO role, but marveled that nothing like that existed for CIOs,” Shapiro told VentureBeat via email. “I got together with my two cofounders Ashish Aggarwal, former engineering lead at Postmates and Amazon, and Munish Gandhi, former COO for LinkedIn Sales Navigator, to start Productiv.” Productiv’s cloud-based dashboard integrates with single sign-on tools to track login activity and extract purchase and license data from contracts, finance, and expense reporting systems. The platform surfaces usage data and over 50 different engagement dimensions that can highlight redundant apps, offering an organization-wide view of agreements and expired software. Configurable rules enable admins to reclaim licenses automatically, and usage logs — including charts that plot the number of engaged users, teams, and locations over time — make it easier to compare stats to industry benchmarks and to determine best practices that might boost productivity. “At the core of [the] platform is a complex, real-time analytics engine that automatically joins billions of data points from multiple sources, including user-level, feature-level engagement data from SaaS applications, login data from single sign-on providers, spend data from ERP providers, expense data from expense management providers, contract data from contract management providers, data from security providers, and more,” Shapiro explained. “Ingesting, normalizing, and joining all of this data is all automated.” The endgame is to empower teams to make profit-boosting rightsizing decisions from app analytics. Rather than determining whether a division has, say, dozens or hundreds of Dropbox licenses and how many team members used those licenses in the past fiscal quarter, with Productiv, CIOs can drill down into the productivity impact and estimate the potential cost savings of choosing to cancel, upgrade, or downgrade service. “Companies spend enormous amounts providing people with the tools they need to get their work done — more than $10,000 per employee annually — but these are open-loop investments without data to show how business value is delivered … Productiv gives CIOs actionable insights and intelligent recommendations, enabling them to focus on real, measurable business outcomes,” Shapiro continued. “Productiv helps enterprises understand exactly how their employees are using SaaS. Better usage and adoption data coupled with customizable automation of everyday tasks allow CIOs and their teams to focus on the projects that actually grow their business.” Productiv, which has 75 employees, claims to have hundreds of enterprise customers."
https://venturebeat.com/2021/03/31/opswat-expands-infrastructure-protection-with-anti-malware-tools-raises-125m/,"Opswat expands infrastructure protection with anti-malware tools, raises $125M","Infrastructure protection services company Opswat today announced that it raised $125 million from Brighton Park Capital, the startup’s first external funding round. Opswat says the capital will support its R&D and hiring efforts as it looks to acquire new customers. More than three quarters of IT security leaders anticipate a major breach involving a critical infrastructure organization in the near future, according to a Black Hat USA survey. It’s estimated that 31% of organizations have experienced cyberattacks on operational technology infrastructure. Perhaps unsurprisingly, Gartner found that organizations planned to invest $17.48 billion in infrastructure protection in 2020. San Francisco, California-based Opswat, which was founded in 2002,  protects critical infrastructure by attempting to eliminate malware and zero-day attacks. Its products focus on threat prevention and process creation for secure data transfer and safe device access. Toward this end, Opswat offers two platforms — MetaDefender for threat prevention and MetaAccess for cloud access control and endpoint compliance. MetaDefender leverages what Opswat calls content disarm and reconstruction — “deep CDR” – to remove threats from files by reconstructing the files while stripping out potentially malicious content. The platform also delivers multiscanning with over 30 antimalware engines, file-based vulnerability assessment, and proactive data loss prevention technologies, which spot and redact sensitive information like names, companies, subjects, GPS locations, authors, and more in emails and other messages. As for MetaAccess, it allows access to software-as-a-service apps and cloud data based on device health and compliance, helping to block potentially risky devices. MetaAccess and MetaDefender draw on Opswat’s endpoint compliance technology, which detects and classifies over 5,000 apps to enable monitoring, management, and real-time remediation. It’s complemented by anti-keylogger and screen capture prevention features that intercept and conceal keystrokes from malware and block unauthorized screengrabs by users, web collaboration tools, and apps. Opswat competes with Confluera, Erikos, IP Access, and others in a global cybersecurity market that’s valued at over $156.5 billion. But Opswat claims to have over 1,000 customers, including IBM, Comcast, American Express, the U.S. Department of Homeland Security, and 98% of U.S. nuclear power facilities. The company recently announced that its channel partner program now covers over 40 countries and will expand by 50% this year. Opswat would appear to be primed for growth when taking into account the recent spate of attacks on critical infrastructure including electricity, gas, and water systems,. Just last month, hackers attempted to poison a Florida city water supply by remotely accessing a server. And in 2015 and 2016, cyberattacks caused large-scale power outages in Ukraine. Founder and CEO Benny Czarny claims Opswat’s revenue grew 40% from 2019 to 2020 while annual recurring revenue hit 30%. With “record sales” in 2020, he says that Opswat intends to be ready for a stock market offering by 2021, which would make it among the first companies focused on infrastructure protection to go public. “Opswat’s mission is to protect the world’s critical infrastructure. This is an extremely important objective since it literally means helping ensure people worldwide can continue their daily lives uninterrupted,” Czarny told VentureBeat via email. “While we have built an amazing business over the past 18 years, I realize a financial partner can help us accelerate our progress toward achieving our mission.” Opswat employs over 400 employees across its 10 offices worldwide. Last December, the company relocated its headquarters to Tampa, Florida and plans to hire 100 employees in the city over the next three years."
https://venturebeat.com/2021/03/30/looking-for-a-new-job-then-you-need-to-check-these-out/,Looking for a new job? Then you need to check these out,"Are you on the hunt for a new job at the moment? First of all, good for you — it’s a pretty exciting decision to make for yourself. However, it can also be pretty scary, especially if you’re living through a global pandemic, am I right? The good news is that companies are very much hiring at the moment, despite everything that’s happening, so there’s no shortage of exciting roles to apply for. Here are just a handful of the kinds of roles up for grabs right now. This position is for a Staff Software Engineer with solid development experience who will focus on creating new capabilities for the Visa AI Platform while maturing the code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything. You will be an integral part of the development team, sometimes investigating new requirements and design and at times refactoring existing functionality for performance and maintainability, but always working on ways to make Visa more efficient and provide better solutions to customers. The Bizinfra team that is part of the core development of Outbrain’s Business Technology, is looking for a leader! As a Bizinfra Team Lead you will lead backend developers (local and offshore) and QA engineers. You will be working closely with Randamp;D, MIS (Information System), Solution Architect, Business and Finance stakeholders, data infra, and more. The successful candidate will be responsible for designing, developing, and maintaining processes that affect both customer-facing applications and integrations with back-office systems while also building and maintaining processes to support billing systems. The role of the Redbull Communications function is to develop the overarching messaging and communications strategy for the brand, and to get people to talk about Red Bull through media coverage, opinion leader engagement, and partner content amplification. The function grows media relationships, brings opinion leaders into the Red Bull world through experiences, and works with partners to share messages. Reporting to the Director of Marketing of the Southwest Region, you will lead the Communications efforts for the region, with the goal of increasing awareness of and affinity for the Red Bull brand and its local activations, events, athletes, and media projects. You will develop the annual Southwest Regional Communications plan, ensuring all communications efforts are accomplished on-budget and achieve their planned engagement targets. Zurich Instruments is the technology leader for advanced test and measurement instruments. As a marketing team, they are proud of their products and passionate about finding creative and effective ways of how to promote these to researchers and industrial customers around the world. Are you a self-starter keen to work in an international team? Then apply now as a Marketing Specialist who will strengthen the Marketing & Sales team in Cambridge, MA. Supported by teams from across three continents, you will become the first marketer in the U.S. office and will therefore have the unique chance to drive and further develop marketing initiatives in the region. You will join McKinsey’s Technology & Digital organization as a core member of both the Product Management function and as part of the Data, BI and Analytics Group committed to solving problems for colleagues and solutions using technology. You will focus on products designed to enable data-driven decisions and deliver insights. The Product Managers work closely in teams alongside experts in various disciplines — designers, researchers, engineers, analysts, and others — and together, the team relentlessly strives to deliver outcomes that matter to people. You and your team will work together to discover opportunities, experiment, test, and learn, and deliver solutions using a mix of methodologies, including design thinking to help understand people’s needs, wants, and problems; lean methodologies to experiment fast and learn a lot; and agile approaches to reduce uncertainty by working in short dev cycles and pausing to reflect and adapt. Head on over to VentureBeat jobs now for even more brilliant opportunities."
https://venturebeat.com/2021/03/30/armv9-is-arms-first-major-architectural-update-in-a-decade/,Armv9 is Arm’s first major architectural update in a decade,"Arm, the leader in chips used in everything from mobile devices to supercomputers, has unveiled Armv9, the company’s first major architectural change in a decade. The new designs should result in 30% faster performance over the next two chip generations. Arm is a chip architecture company that licenses its designs to others, and its customers have shipped more than 100 billion chips in the past five years. Nvidia is in the midst of acquiring Cambridge, United Kingdom-based Arm for $40 billion, but the deal is waiting on regulatory approvals. In a press briefing, Arm CEO Simon Segars said Armv9 will be the base for the next 300 billion Arm-based chips. Arm’s customers have shipped more than 180 billion chips to date, and those chips touch more than 70% of the world’s population, Segars said. “We’re extremely excited to be sharing Arm’s vision of the next decade of computing with you,” Segars said. The new architecture has processing that balances economics, design freedom, and accessibility advantages of general-purpose computing devices with specialized processors that handle tasks like digital signal processing and machine learning. The company says Armv9 also takes security and artificial intelligence features to new levels. Arm previously launched its Armv8 architecture in 2011, and that became its most successful platform in history as the foundation for smartphone chips, internet of things (IoT) devices, and a wide range of industrial devices. Arm has more than 6,500 employees, about 80% of whom are engineers. At the current rate, 100% of the world’s shared data will soon be processed on Arm; either at the endpoint, in the data networks or the cloud, Segars said. Such pervasiveness conveys a responsibility on Arm to deliver more security and performance, along with other new features in Armv9, he added. The new capabilities in Armv9 will accelerate the move from general-purpose to more specialized compute across every application as AI, IoT, and 5G gain momentum globally. Back in 2011, Arm launched its 64-bit processing architecture, enabling Arm devices to make the leap from low-power mobile devices to high-end supercomputers. “The Arm architecture is not a static thing. We keep on innovating and evolving to meet the ever changing needs of the computing world,” said Richard Grisenthwaite, chief architect, in a press briefing. “In our increasingly connected world, we’re seeing Arm processors being used at all stages. The collection of data often starts with ultra-low-power IoT devices based on the Arm profile processes, or from the Arm-based smartphones that virtually all of us carry all of the time. … It continues to be the processor of choice.” To address the greatest technology challenge today — securing the world’s data — the Armv9 roadmap introduces the Arm Confidential Compute Architecture (CCA). Confidential computing shields portions of code and data from access or modification while in use, even from privileged software, by performing computation in a hardware-based secure environment. The Arm CCA will introduce the concept of dynamically created Realms, usable by all applications, in a region that is separate from both the secure and non-secure worlds. Segars said that Realms are much like software containers, which isolate code in certain ways, but with hardware support. For example, in business applications, Realms can protect commercially sensitive data and code from the rest of the system while it is in use, at rest, and in transit. In a recent Pulse survey of enterprise executives, more than 90% of the respondents believe that if confidential computing were available, the cost of security could come down, enabling them to dramatically increase their investment in engineering innovation. “The Arm Confidential Compute architecture will introduce the concept of dynamically created Realms, usable by ordinary programs in a separate computation world from either the non-secure or secure world that we have today,” Grisenthwaite said. “Realms use a small amount of trust and a testable management software that is inherently separated from the operating system.” The ubiquity and range of AI workloads demands more diverse and specialized solutions. For example, it is estimated there will be more than eight billion AI-enabled voice-assisted devices in use by the mid-2020s, and 90% or more of on-device applications will contain AI elements along with AI-based interfaces like vision or voice. To address this need, Arm partnered with Fujitsu to create the Scalable Vector Extension (SVE) technology, which is at the heart of Fugaku, the world’s fastest supercomputer. Building on that work, Arm has developed SVE2 for Armv9 to enable enhanced machine learning (ML) and digital signal processing (DSP) capabilities across a wider range of applications. “I am excited about the new generation of Arm instruction sets and technological capabilities,” said Patrick Moorhead, an analyst at Moor Insights & Strategies. “Performance-wise, Arm is making it easier to integrate ML capabilities into the end product. It’s important to recognize that for most performance cases, especially CPU, it’s more about the architecture of the design versus the instruction set. So in other words, chip designers still need to architect something performant. Security is dramatically improving too, and if we had these technologies fully enabled today, it could ward off most all of the known attacks. I also think Arm thought about the future with ‘Realms’ even though it won’t be out day one.” SVE2 enhances the processing ability of 5G systems, virtual and augmented reality, and ML workloads running locally on CPUs, such as image processing and smart home applications. Over the next few years, Arm will further extend the AI capabilities of its technology with substantial enhancements in matrix multiplication within the CPU, in addition to ongoing AI innovations in its Mali graphics processing units (GPUs) and Ethos network processing units (NPUs). Segars noted that one customer, Johnson Controls, has been working on automation and control equipment in buildings for more than a century. “They’re a major user of Arm-based chips, and now we’re talking to them about the enhanced AI and security features coming with the new Armv9 architecture being launched today,” Segars said. “One upgrade JC is considering is the use of AI-powered digital twins monitoring key equipment in real time within the company, as well as aggregating data in the cloud.” Johnson Controls has already used Arm chips to manage chiller systems and cut energy use by more than 50%. Segars also said that Arm-based devices could prove that someone has been vaccinated against COVID-19. The smartphone could be used for that, and it could store medical information, but to be comfortable with that, Segars said he would want advanced encryption running on the device beyond what is possible today. He would want features like memory tagging to help eliminate memory cybersecurity issues. “Our first smartphone product with an Armv9 CPU will be commercially available by the end of this year,” Segars said. Besides security, Armv9 supports specialized AI, DSP, and XR workloads. Segars said he also expects Arm’s combination with Nvidia will advance areas such as graphics computing. Over the past five years, Arm designs have increased CPU performance annually at a rate that outpaces the industry, Segars said. He added that Arm will continue this momentum into the Armv9 generation with expected CPU performance increases of more than 30% over the next two generations of mobile and infrastructure CPUs. However, as the industry moves from general-purpose computing toward ubiquitous specialized processing, annual double-digit CPU performance gains are not enough. Along with enhancing specialized processing, Arm’s Total Compute design methodology will accelerate overall compute performance through focused system-level hardware and software optimizations and increases in use-case performance. By applying Total Compute design principles across its entire IP portfolio of automotive, client, infrastructure, and IoT solutions, Armv9 system-level technologies will span the entire IP solution, as well as improving individual IP. Additionally, Arm is developing several technologies to increase frequency, bandwidth, and cache size, and reduce memory latency to maximize the performance of Armv9-based CPUs. “There was very little detail in the disclosures. Realms should improve security, particularly for multiuser cloud systems (such as Amazon Web Services),” said Linley Gwennap, principal analyst at the Linley Group, in an email. “The memory protection stuff sounded interesting, but it seems like it might be years away. In summary, v9 offers a much smaller improvement than v8.” Grisenthwaite said that addressing the demand for more complex AI-based workloads is driving the need for more secure and specialized processing, which will be the key to unlocking new markets and opportunities. “It’s been an amazing, tragic, and enlightening year, no matter where we’ve been living or working,” Segars said. “Now, it’s time to rebuild a world that’s inherently more resilient. In computers, one of the most urgent needs is expanding the data processing capacity in the cloud. We can’t just do that at any cost. Transforming the cloud isn’t just about the more. It’s about different, especially when it comes to the performance per watt of traditionally power-hungry datacenter chips.” Arm collected supporting comments from customers including Ampere Computing, Cadence, Crytek, Foxconn, Fujitsu, Google, Marvell, MediaTek, Nvidia, NXP, Oppo, Red Hat, Renesas Electronics, Samsung, Siemens, Synopsys, Unity Technologies, Vivo, VMware, and Xiaomi Group. “We’re not just focusing on the CPU and GPU either, but looking at all of compute, as well as maximizing performance by deploying new system technologies that provide additional gains,” Segars said. “And we are broadening the architecture to execute even more compute, such as DSP and AI on the CPU.”"
https://venturebeat.com/2021/03/30/elistair-the-pioneer-in-tethered-uavs-announces-a-e5m-series-b-round-to-accelerate-its-international-expansion/,"Elistair, The Pioneer in Tethered UAVs, Announces a €5M Series B Round to Accelerate Its International Expansion","   LYON, France & BOSTON–(BUSINESS WIRE)–March 30, 2021– Elistair, a market leader in tethered drones, has raised €5M from Omnes and Starquest Capital, its historic investor. This Series B round follows an initial €2M funding round in 2018. Since its creation in 2014, Elistair’s solutions have already been deployed by military forces and national security agencies in over 65 countries. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210330005817/en/ Automated tethered drones to protect military bases, surveil borders and provide security for major events Elistair counts national security and defense organizations amongst its customers, offering them two patented product ranges: an automated tethered drone: Orion 2 and tether stations capable of transforming free-flying drones into tethered drones. With their ability to detect vehicles more than 6 miles away, The Orion 2 drone acts as an aerial reconnaissance mast, substantially increasing the field of vision of operating forces and reducing vulnerable entry points. Orion 2, provides clear, and live information which is key for border surveillance and protection of perimeters and events. They are also used to deploy tactical communication networks, greatly increasing the range and coverage of military radios. Thanks to its micro-tether technology, Elistair has solved the problem of a drones limited battery life. When connected to a generator or the electrical grid, the micro-tether powers the drone continuously, allowing it to function for 24 hours without interruption. It also provides secure data transfer between the drone and its operator through the tether. This means that the camera feed cannot be saturated and provides protection against hacking and other interference. “Our tethered drones let units on the ground expand their field of vision considerably, as well as their understanding of their nearby environment during operations, which is often a critical factor. This funding round will allow us to consolidate our range of products for the national security and defense segment and to handle the significant amount of orders from recent months.” – Guilhem de Marliave, Elistair CEO International presence and investment in R&D The reliability and sturdiness of Elistair’s products have already earned recognition in the US, where the company conducts more than 40% of its business. After opening a subsidiary in Boston, the French start-up will be increasing its production capacity in order to assert its foothold in the American market, while reinforcing its presence in the European market. “We plan to accelerate the industrialization of our production processes and build up our R&D capacity. Our technological roadmap aims to develop the next generations of tethered drones capable of responding to any and all kinds of situations and conditions. We will also be boosting our solutions’ intelligence and automation.” – Timothée Penet, Elistair CTO Elistair is launching a campaign to recruit around 20 people for its Lyon and Boston sites. The goal is to retain its initial approach – a combination of technological innovation and knowledge of conditions in the field – thanks to a multidisciplinary R&D team and former defense operatives (from the Royal Air Force, Navy SEALs and the French Navy). “Elistair is a leading name on the tethered drone market, which is still a relatively new market, but one that is growing quickly. Guilhem and Timothée have demonstrated an impressive execution capacity from the outset, and the company displays a rare level of business maturity after only a few years in operation. This funding will allow the company to shore up its technological leadership, and we are very eager to see the next innovations to be released on the market.” – Fabien Collangettes, Venture Capital Principal, Omnes About ElistairCreated in 2014 and based in Lyon (France), and Boston (USA), Elistair designs and manufactures tethered drone solutions for tactical surveillance and civil defense missions. Elistair’s products are used by armed forces, civil security services and a number of private companies, in more than 60 countries. Elistair was co-founded by Guilhem de Marliave and Timothée Penet, both of them alumni of the scientific graduate school Centrale Lyon.https://elistair.com/ About OmnesOmnes is a leading private equity and infrastructure investor. With €5 billion in assets under management, it provides companies with the capital they need to fund their growth, in three key areas: venture capital, growth & buyout capital and infrastructure. Omnes is wholly-owned by its employees. It is committed to ESG issues and has founded the Fondation Omnes to fund initiatives in favour of children and young people. It is a signatory to the United Nations Principles for Responsible Investment (PRI). www.omnescapital.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005817/en/ Press Contact – Elistair Gwenaelle Le Cocguen – g.lecocguen@elistair.com – +33 (0)6 67 10 62 22 Press Contact – Omnes Aurélie Blanchard-Massoni – aurelie.blanchard-massoni@omnescapital.com – +33 (0)7 63 13 65 74"
https://venturebeat.com/2021/03/30/cisco-integrates-networking-security-it-management-for-the-enterprise/,"Cisco integrates networking, security, IT management for the enterprise","At its an online Cisco Live! event, Cisco today advanced an ambitious effort to further unify the management of networking, security, and IT infrastructure across the extended enterprise using hardware and software that can now be acquired under a single subscription license dubbed Cisco Plus. In addition, Cisco is making it possible to acquire all the elements of its Secure Access Service Edge (SASE) portfolio via a single offering that will also soon be made available via a subscription. And Cisco announced that the Duo authentication platform it acquired in 2018 can now be configured to enable users to log into cloud applications using biometrics or security keys that eliminate the need to employ passwords, in addition to updating a Cisco Secure X platform through which it unifies the management of its security portfolio. Finally, Cisco has — as expected — integrated the ThousandEyes monitoring service it acquired last year with the Cisco Catalyst 9000 switches and the Cisco AppDynamics Dash Studio that provides visibility into an observability platform the company acquired in 2017. As the management of networking and security becomes flatter across an extended enterprise, it’s clear Cisco is making a case for integrating the management of networking and security to reduce total cost at a time when more highly distributed applications that tend to be latency-sensitive are being deployed. In some cases, networking platforms based on proprietary processors will be required to maximize throughput, while platforms based on commodity processors will suffice in others, said Todd Nightingale, senior vice president and general manager for the Enterprise Networking and Cloud business at Cisco. Cisco makes available a series of management overlays and control planes that — in addition to AppDynamics and SecureX — includes Cisco Intersight to manage IT operations, regardless of the underlying class of processors or platforms employed. The company, for example, has extended Cisco Intersight to also manage cloud infrastructure resources across a hybrid cloud computing environment that now includes a growing number of edge computing platforms. In effect, Cisco is making a case for lowering the total cost of IT by relying on a single vendor to unify as much of the underlying IT and security infrastructure as possible. ThousandEyes is a critical element of that strategy because the platform enables IT teams to gain insights into network bottlenecks that are impacting application performance even when that network infrastructure is managed by a third-party telecommunications carrier. “It provides full visibility from the user to the application,” Nightingale said. It’s not clear to what degree IT organizations are employing the entire Cisco portfolio. Organizations may, for example, employ Cisco networking hardware while relying on a platform other than AppDynamics to monitor applications. The decision to acquire those platforms is often made by completely different teams within an enterprise. Via various subscription services, Cisco is trying to entice organizations to standardize on a wider range of offerings. Those subscription offerings can over time reduce the total cost of IT. At the same time, it reduces any economic incentive an organization might have to swap in a rival platform because future Cisco upgrades are now included as part of the subscription service. It remains to be seen how many enterprise IT organizations are willing to subscribe to offerings from a single vendor. But there is no doubt more organizations are acquiring IT technologies via subscription services that make it possible to treat the acquisition of IT as an operating rather than capital equipment expense. In effect, the same consumption-based pricing models that are commonly found in the cloud are now being applied to all IT infrastructure and security offerings. The tradeoff is that while those underlying platforms and systems have arguably never been more open, the mechanism that locks an IT organization into one vendor versus another is now embedded within the subscription."
https://venturebeat.com/2021/03/30/github-launches-secret-scanning-for-private-repositories-into-general-availability/,GitHub’s secret scanning for private repositories enters general availability,"GitHub has announced that its enterprise-focused secret scanning tool for private repositories is now generally available. The Microsoft-owned code-hosting platform first debuted secret scanning for private repositories last May as part of its advanced security program. This was introduced in beta alongside a new native code-scanning tool that automatically scans every git push for vulnerabilities. Code scanner launched in general availability in September and is followed today by secret scanning. In related news, GitHub also announced the beta launch of a new “security overview” tool that gives security teams a single interface to view all the risks detected by GitHub’s advanced security tools. These span code scanning, secret scanning, and Dependabot. The overview highlights known and unknown security risks, where teams haven’t fully configured their security features. “Secrets” refers to authentication credentials such as API tokens, passwords, and keys that protect access to applications, services, and other sensitive areas of a company’s digital infrastructure. GitHub first launched secret scanning — then known as “token scanning” — for public repositories back in 2018. It’s designed to help companies identify sensitive data hidden inside their public code before it’s found by bad actors. There has been a flurry of activity in the secrets management space of late, with GitGuardian raising $12 million in funding a few months back to help companies detect sensitive data hidden in their code repositories and Doppler raising $6.5 million in a round of funding led by Alphabet’s GV to expand into the enterprise. Recent data from GitGuardian indicates a 20% rise in secrets hidden in public GitHub repositories last year, a trend driven in part by a broader push toward code collaboration platforms as developers and businesses rapidly embraced remote work. Businesses that use GitHub for private (i.e. non-open source) projects can buy a GitHub advanced security license as part of their Enterprise Cloud (hosted) or Enterprise Server (self-hosted) subscription, which gives them access to secrets scanning. In the 10 months since it first arrived in beta, GitHub said it has helped organizations find and revoke more than 5,000 secrets. Since its beta launch last year, GitHub has added a bunch of new features, though some are currently only available for the GitHub Enterprise Cloud edition. These include an API and support for webhooks to set up secret scanning alerts, while GitHub has also expanded its secret scanning pattern coverage to incorporate tokens from more than 35 companies, including Shopify, Stripe, AWS, Azure, SendGrid, Twilio, and Slack. Earlier today, GitHub also launched new granular controls for the GitHub mobile app, designed to boost developers’ productivity by helping them manage notifications and pause them at the end of a shift."
https://venturebeat.com/2021/03/30/pinterest-open-sources-big-data-analytics-tool-querybook/,Pinterest open-sources big data analytics tool Querybook,"Pinterest today open-sourced Querybook, a data management solution for enterprise-scale remote engineering collaboration. The company says the tool, which it uses internally, can help engineers compose queries, create analyses, and collaborate with one another via a notebook interface. Querybook started in 2017 as an intern project at Pinterest. The development team early on decided on a document-like interface where users could write queries and analyses in one place, with collocated metadata and the simplicity of a note-taking app. Released internally in March 2018, Querybook became the go-to solution for big data analytics at Pinterest. It now averages 500 daily active users and 7,000 daily query runs. “With Querybook, Pinterest engineers have brought together the power of metadata with the simplicity of a note-taking app for a better querying interface, where teams can compose queries and write analyses all in one place,” a spokesperson told VentureBeat. “Querybook can be set up and deployed in minutes.” Every query executed on Querybook gets analyzed to extract metadata like referenced tables and query runners. Querybook uses this information to automatically update its data schema and search ranking, as well as to show a table’s frequent users and query examples. The more queries in Querybook, the better documented the tables become. Querybook also features an admin interface that lets companies configure query engines, table metadata ingestion, and access permissions. From this interface, admins can make live Querybook changes without going through code or config files. And they can create visualizations, including lines, bars, stacked areas, pies, donuts, scatter charts, and table charts. “The common starting point for any analysis at Pinterest is an ad-hoc query that gets executed on the internal Hadoop or Presto cluster. To continuously make these improvements, especially in an increasingly remote environment, it’s more important than ever for teams to be able to compose queries, create analyses, and collaborate with one another,” Pinterest wrote in a blog post. “We built Querybook to provide a responsive and simple web user interface for such analysis so data scientists, product managers, and engineers can discover the right data, compose their queries, and share their findings.” Pinterest previously open-sourced Teletraan, a tool that can deploy code onto virtual machines, such as those available from public cloud Amazon Web Services. Prior to this, the company released Terrapin, software designed to more efficiently push data out of the Hadoop open source big data software and make it available for other systems to use."
https://venturebeat.com/2021/03/30/sonar-which-monitors-companies-salesforce-tech-stack-for-changes-raises-12m/,"Sonar, which monitors companies’ Salesforce tech stack for changes, raises $12M","Salesforce has emerged as a formidable force in the cloud-based enterprise software sphere — a $200 billion colossus upon which countless others have built their own billion-dollar businesses. In the past couple of months alone, we’ve seen OwnBackup raise $167.5 million at a $1.4 billion valuation to power cloud data backups for Salesforce; Scratchpad lock down $13 million to develop a productivity workspace for Salesforce teams; and Copado secure $96 million for Salesforce-native DevOps. According to some estimates, the Salesforce ecosystem could be at least 4 times larger than the company itself, a factor that has played no small part in Salesforce’s success over the past two decades. Against this backdrop, Sonar today announced it has raised $12 million from a slew of big-name investors, including David Sacks’ Craft Ventures and Slack’s venture capital fund, to bring “X-ray vision” to Salesforce by helping sales teams visualize how all their data is connected and used across related systems. “It’s essentially a living, searchable dictionary that shows how your entire tech stack works together and automatically documents every change to your data,” Sonar CEO and cofounder Brad Smith told VentureBeat. “This allows teams to scope and execute their work fast, work confidently without the risk of taking critical systems offline due to breakages, and collaborate effortlessly around change-management projects, digital transformation, systems integrations, building new processes, and more.” Although Sonar is now focused purely on Salesforce, and showing how other systems are mapped to Salesforce fields and processes, later this year it plans to “start serving more operations teams across the enterprise,” according to Smith. This will include expanding to marketing platforms such as Marketo, HubSpot, and Pardo, as well as finance systems such as Netsuite. “Change management” is perhaps the key to understanding what Sonar actually does. When someone in a company alters any fields or automations in a critical system, it can be difficult to see how this might impact other processes that rely on it — but the smallest changes can break integrations, lead to inaccurate reports, and — perhaps more importantly — prevent sales teams from closing deals. For a platform like Salesforce that thrives on integrations, Sonar shows its users how all their data is joined together and issues real-time alerts when anything that might impact them is changed. Sonar can be used before changes are made to determine how (or whether) they should be implemented, and it can also provide visibility into problems that occur as a result of changes after they have been made. This is particularly important for teams that are transitioning any of their existing workflows or integrations, as Sonar helps them reverse-engineer processes that may have been set up a while ago by someone who is no longer at the company and understand how everything fits together. “Sonar gives you complete situational awareness so you can understand the impact of changes before they happen and correct problems if and when they occur,” Smith said. “This helps teams prioritize their work, work faster, and execute changes safely. In the event someone does make a change that causes processes to break, Sonar will alert you to exactly what broke and why.” At its core, Sonar is designed to help revenue and operations teams avoid making blind changes and then scrambling to fix problems after. According to Smith, Sonar does something “similar to what GitHub does for software engineers,” in terms of how it documents every change you make and records the full history and context around it. But rather than focusing on codebases, Sonar is concerned with integrations across a company’s tech stack, allowing non-technical teams to handle at least some of the heavy lifting behind the scenes. Sonar is available as a standalone web application, though the company also offers a browser extension that overlays data on top of Salesforce to help visualize dependencies across every field. Founded out of Atlanta, Georgia in 2018, Sonar had previously raised $3.7 million. It said that over the past year its customer count and revenue have increased by 1,000% and 3,000%, respectively. It has also signed up notable enterprise clients, including OneLogin and Carta."
https://venturebeat.com/2021/03/30/docebo-launches-multi-product-learning-suite-to-address-challenges-across-entire-learning-lifecycle/,Docebo Launches Multi-Product Learning Suite to Address Challenges Across Entire Learning Lifecycle,"TORONTO–(BUSINESS WIRE)–March 30, 2021– Leading AI-powered learning platform, Docebo Inc. (“Docebo”) (Nasdaq:DCBO; TSX:DCBO), announced today the launch of a multi-product learning technology suite. Previously, Docebo has focused on solving the problem of how organizations deliver training with its flagship learning management system (LMS). With the launch of Docebo Learning Suite, Docebo will go beyond content delivery and address challenges across the entire learning lifecycle, from content creation and management to measuring learning impact and key business drivers. The launch of Docebo Learning Suite coincides with the launch of Docebo Shape, a content creation product built on AI. Developed internally, Docebo Shape enables businesses to bring more internal experts into their elearning content strategy by leveraging AI to create engaging learning content in minutes. Including Docebo Shape, the core products that come together to transform the company’s offering into a cohesive learning suite include: “Our vision is to build a learning suite for the future that addresses every enterprise learning requirement so our customers have a one-stop shop for all their learning needs,” said Docebo’s Chief Executive Officer, Claudio Erba. “Today’s announcement is the result of both a previous acquisition and internal R&D, and it’s a testament to the innovation that exists in Docebo’s DNA.” To learn more about Docebo’s Enterprise Learning Suite, visit www.docebo.com. About Docebo Docebo is redefining the way enterprises leverage technology to create content, deliver training, and understand the business impact of their learning experiences. With Docebo’s multi-product learning suite, enterprises around the world are equipped to tackle any learning challenge and create a true learning culture within their organization. Forward-Looking Information This press release may contain “forward-looking information” and “forward-looking statements” (collectively, “forward-looking information”) within the meaning of applicable securities laws, including, without limitation, statements regarding: Company’s business; future business strategy; the launch of new products and features; the learning management industry; our growth rates and growth strategies; addressable markets for our solutions; the achievement of advances in and expansion of our platform; expectations regarding our revenue and the revenue generation potential of our platform and other products; our business plans and strategies; and our competitive position in our industry. This forward-looking information is based on our opinions, estimates and assumptions that, while considered by the Company to be appropriate and reasonable as of the date of this press release, are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause the actual results, level of activity, performance or achievements to be materially different from those expressed or implied by such forward-looking information, including, without limitation: risks related to the COVID-19 pandemic and its impact on Docebo, economic conditions, and global markets; and other unforeseen events, developments, or factors causing any of the aforesaid expectations, assumptions, and other factors ultimately being inaccurate or irrelevant, and those factors discussed in greater detail under the “Risk Factors” section of our Annual Information Form for the year ended December 31, 2020, available under our profile on SEDAR at www.sedar.com, and should be considered carefully by prospective investors. If any of these risks or uncertainties materialize, or if the opinions, estimates or assumptions underlying the forward-looking information prove incorrect, actual results or future events might vary materially from those anticipated in the forward-looking information. Although we have attempted to identify important risk factors that could cause actual results to differ materially from those contained in forward-looking information, there may be other risk factors not presently known to us or that we presently believe are not material that could also cause actual results or future events to differ materially from those expressed in such forward-looking information. There can be no assurance that such information will prove to be accurate, as actual results and future events could differ materially from those anticipated in such information. No forward-looking statement is a guarantee of future results. Accordingly, you should not place undue reliance on forward-looking information, which speaks only as of the date made. The forward-looking information contained in this press release represents our expectations as of the date specified herein, and are subject to change after such date. However, we disclaim any intention or obligation or undertaking to update or revise any forward-looking information whether as a result of new information, future events or otherwise, except as required under applicable securities laws. All of the forward-looking information contained in this press release is expressly qualified by the foregoing cautionary statements.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005315/en/ Press: Nesh PillayDirector of PR and Communicationspress@docebo.com Investors: Dennis FongInvestor Relationsinvestors@docebo.com"
https://venturebeat.com/2021/03/30/github-boosts-developer-productivity-with-new-mobile-notification-controls/,GitHub boosts developer productivity with new mobile notification controls,"GitHub is rolling out a handful of new updates to its mobile and desktop apps, including “enhanced” push notifications with more granular controls and the ability to pause them altogether. The Microsoft-owned code-hosting platform said the update is part of its push to support the burgeoning hybrid and remote workforce, which relies on asynchronous communications. Nicole Forsgren, VP of research and strategy at GitHub, recently wrote about developer productivity in a co-authored article published in ACM Queue. The paper notes that ensuring efficient software development and the well-being of developers has “never been more important,” with the rapid shift to remote work creating a potential disconnect between developers and their usual workspaces and teams. “This forced disruption and the future transition to hybrid remote/colocated work expedites the need to understand developer productivity and well-being, with wide agreement that doing so in an efficient and fair way is critical,” the coauthors wrote. GitHub launched its mobile app for Android and iOS a year ago, but at the time it only supported push notifications for messages that include a direct mention of the developer — and with good reason. “Push notifications [were] one of the very first features we added via a cross-team hack week with the GitHub notifications team,” Ryan Nystrom, senior director of engineering at GitHub, told VentureBeat. “From that work, we created early versions of pushes for any type of activity, but we knew that without controls this could overwhelm users. “Notification fatigue is real, so we decided to start at a very high signal with lower volume through the initial direct mentions notifications.” In other words, developers could end up drowning under a deluge of alerts, particularly when they’re supposed to be offline. And so over the past year, GitHub has been taking on feedback from developers to figure out what additional notifications and controls could help them manage their time and productivity. With this latest update, developers can toggle push notifications on and off not only for when they’ve been directly mentioned, but when they’ve been asked to review a pull request, assigned a task, or asked to approve a deployment for a protected branch. This is important because a manager or senior developer might need to approve key stages in a project when they’re on the move or otherwise not at their desktop. “One of the core principles of the mobile app is that we’re helping unblock people,” Nystrom said. “Deploy approvals are a new flow for GitHub — for developers using GitHub mobile, we knew immediately it’d be valuable to get notified when your review is requested so you can unblock a deploy without the need to be at your computer.” Related to this, GitHub for mobile also now lets developers set custom working hours, meaning users can specify when push notifications will be sent to their phone. This fits a push across the technology spectrum to foster a healthier work-life balance — Google, for example, rolled out “focus mode” in 2019 to help users minimize and control alerts on their mobile devices. Elsewhere, the GitHub mobile app now lets developers view releases natively inside the app, rather than linking the user through to a web view. “This was also one of our most-requested features,” Nystrom added. Along similar lines, GitHub users can also now customize their repository “watch” settings from mobile. Much as it works on the browser version, they can now opt in to a very specific subset of actions they’d like notifications for in their inbox, such as issues, pull requests, releases, and discussions. Over in the desktop realm, GitHub launched version 2.7 of its desktop app that makes it easier for developers to copy individual or multiple commits between branches (known as “cherry-picking”) using a drag-and-drop tool. According to GitHub staff engineering manager Billy Griffin, developers would previously have to go to the command line and look up the Git cherry-pick documents to remember the correct syntax to copy the commits, but the drag-and-drop option makes this process more visual and intuitive."
https://venturebeat.com/2021/03/30/cloudera-foundation-merger-strengthens-data-science-and-ai-support-for-nonprofits/,Cloudera Foundation merger strengthens data science and AI support for nonprofits,"Today, the Patrick J. McGovern Foundation and the Cloudera Foundation, software company Cloudera’s philanthropic arm, announced a merger that will launch an initiative to accelerate data and AI maturity within nonprofits. In a definitive agreement, Silicon Valley-based Cloudera Foundation will merge its staff, $9 million endowment, and $3 million in grants with the Patrick J. McGovern Foundation, a $1.5 billion data science and AI philanthropic organization. The companies say the transaction is expected to close in the second quarter of 2021, subject to regulatory approval. Nonprofits face significant barriers to leveraging big data analytics and AI tools for driving strategy, growth, and global impact. The gaps in knowledge and technical expertise translate to opportunity costs in efficiency, impact, and scalability. A 2019 survey by PwrdBy found that nonprofit-specific AI is reaching fewer than 23% of organizations worldwide. In that same report, 83% of respondents said they believe ethical frameworks need to be defined before the nonprofit sector sees “full adoption” of AI. “Data science and AI-based tools, deployed with purpose and social conscience, could improve nearly every part of the human experience. Enabling civil society to access these technologies will unlock transformational approaches to many of the world’s greatest challenges,” Patrick J. McGovern Foundation president Vilas Dhar said in a press release. “Backed by philanthropic capital and deep technical expertise, we are working together with social changemakers to advance a tech-enabled, human-centered future.” The Cloudera Foundation, which was founded in 2017 with the mission of providing technology and mentorship to foster data expertise in the civil sector, has piloted its approach with seven nonprofits. The Patrick J. McGovern Foundation plans to broaden the foundation’s efforts post-merger to up to 100 organizations addressing the United Nations (UN) Sustainable Development Goals, the collection of 2030 goals set by the UN General Assembly as “a blueprint to achieve a better and more sustainable future for all.” Cloudera Foundation CEO Claudia Juech will direct activities around data enablement for nonprofits as VP of Data and Society at the Patrick J. McGovern Foundation, a new program. Data and Society will offer a range of services, including public workshops, multi-year data science development collaborations, and accelerator partnerships focusing on extracting actionable insights from datasets. For example, Data and Society could help organizations combating climate change better manage their resources. In a report on the civil sector’s adoption of AI, the Brookings Institute cites a partnership between The Nature Conservancy and the Red Cross to create a dashboard incorporating social media data on floods for city planning — an effort that might assist cities in becoming more sustainable. “AI can enable nonprofits to manage a broad range of activities. In conjunction with machine learning and data analytics, it is a way to control costs, handle internal operations, and automate routine tasks within the organization,” the report’s coauthors wrote. “Adoption of these tools can help groups with limited resources streamline internal operations and external communications and thereby improve the manner in which they function.” Cloudera cofounder and Cloudera Foundation chair Mike Olson added: “Our missions couldn’t be better matched. We have seen how data and analytics enable nonprofits to work more effectively and increase their impact. With the leadership and resources of the Patrick J. McGovern Foundation, our team can do even more to help mission-driven organizations change the world.”"
https://venturebeat.com/2021/03/30/celential-ai-which-matches-software-engineers-with-jobs-raises-9-5m/,"Celential.ai, which matches software engineers with jobs, raises $9.5M","AI-powered software engineering recruitment platform Celential.ai today announced that it raised $9.5 million in series A funding. It comes as Celential appoints Amer Akhtar, former Yahoo small business president and ex-CEO of ADP China, as the company’s new CEO. Ncube estimates that sometime this year, the software industry will experience a shortage of 1.4 million engineers. Meanwhile, the employment of software developers is projected to grow 21% from 2018 to 2028. The cost of hiring will likely remain substantial as a result. On average, it takes 42 days to fill a job position, during which companies can face a productivity loss of $33,251. Celential, which is headquartered in Sunnyvale, California and was founded by former product leads at Salesforce, VMware, and Zynga, develops a platform designed to match hiring managers with engineering candidates by essentially “simulating” human experts. The startup’s AI-driven virtual recruiter finds, vets, and engages passive candidates behind the scenes, resulting in what Celential claims is a 70% present-to-interview ratio. Celential clients upload the job description for their engineering, data science, or project manager roles to the platform. Then, Celential lines up candidates by engaging them with an email engine that tracks open rates, automatically follows up, and gauges sentiment. To personalize the emails, Celential’s algorithms draw on talent and company graphs vertically focused on over 3 million engineers and the employers for whom they’ve worked. The company says it pulls in thousands of data sources to train its natural language processing, computer vision, and machine learning recommendation algorithms, aiming for a mutual fit between engineers and hiring companies, teams, and opportunities.  For job seekers, Celential searches job boards and career sites. Because it’s referral-based, only matches with roles and companies that are actively hiring bubble up to the top. Celential’s competitors include Plum, which has job candidates fill out problem-solving and personality tests that award points for “talents” like adaptation, communication, inclusion, and innovation. Another rival, Vervoe, offers AI tools that test would-be employees’ on-the-job skills with a mix of general assessments, coding challenges, and personality quizzes. There’s also Headstart, which recently raised $7 million for AI that can mitigate recruitment bias; Xor, a startup developing an AI chatbot platform for recruiters and job seekers; and Phenom People, a human resources platform that taps AI to help companies attract new talent. But 40-employee Celential’s focus on software engineering talent recruitment has allowed it to stand out in the crowded field. It counts among its over 50 customers Affinity, Paradigm, Twin Health, and more than 100 other companies. And Celential says it’s on track to notch “double digit millions” in annual recurring revenue by 2022, after growing revenue by 7 times in 2021. “Celential is on a mission to fundamentally transform recruiting and career development,” Akhtar told VentureBeat via email. “With proven customer success, our vertically focused virtual recruiter, powered by AI, enables fast-growing companies to compete for talent cost-effectively, with little friction, and without having to build and scale up a recruiting team and infrastructure. Over time, we’ll leverage the same vertical approach and deep insights into talent, industry, companies and opportunities to accelerate talent’s career development, through a virtual personal career agent solution.” GSR Ventures led Celential’s investment round announced today. Spider Capital, TSVC, and undisclosed individual investors participated."
https://venturebeat.com/2021/03/30/gryps-an-rpa-platform-focusing-on-construction-raises-1-5m/,"Gryps, an RPA platform focusing on construction, raises $1.5M","Gryps, a robotic process automation (RPA) startup focused on the construction industry, today announced it has raised $1.5 million. The company says it will use the funding to support product R&D and hire new employees, particularly engineers. RPA — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that RPA and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than four hours savings per employee each week. Gryps, which was founded in 2020, connects to multiple email, project management database, and other systems to automatically scrape and organize documents during the construction process. The platform ingests things like manuals, warranty certificates, contracts, change orders, invoices, lien waivers, and other close-out documents from different sources and then applies machine learning to categorize the files and label them correctly so that they can be shared with various stakeholders. “We met in 2013 working on the same team at a construction management firm in New York City,” Gryps founders Dareen Salama and Amir Tasbihi told VentureBeat. “One thing was always top of mind: The industry is ready for and needs AI that helps process the volume of data generated, and it must be easy to adopt for users. We started Gryps to provide an amazing product with the best user experience and to create an environment where young professionals in construction can grow and find a fulfilling career.” Gryps employs APIs and digital robots to process the documents it collects. Leveraging a combination of natural language processing, machine learning, computer vision, and document understanding, the platform canvasses, ingests, and transforms construction project data. “We use cutting-edge transfer learning techniques to transfer AI knowledge from best-in-class models to boost our accuracy. We also use layout-based, Transformer-based deep learning models to extract information from documents,” Salama and Tasbihi explained. “Our RPA agents are rule-based software robots performing actions to get our information ingestion jobs done more accurately and faster than people can … For example, [we apply] computer vision and machine learning to extract and analyze trends such as companies, products used, services rendered, and costs involved from documents.” Gryps has a number of competitors in a global intelligent process automation sector that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere last secured a $290 million investment from SoftBank at a $6.8 billion valuation. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. But Gryps, which has three paying customers and several in pilots, asserts that specializing in construction gives it a leg up over rivals focused on the broader market. To this end, one of the company’s first clients was the Javits Convention Center in New York. Gryps claims its software automatically ingested over 20,000 documents and 100,000 data points, collated them, and handed them over to the Javits team, with estimates putting the savings at hundreds of hours of staff time. Salama and Tasbihi say the pandemic made apparent the need for and speed of digitization adoption, with construction teams desiring faster access to information remotely, in-office, and on-site. “With teams working from home, they needed more robust tools than what exists today to access project data such as contracts, financials, and other documentation as fast as possible,” they continued. “Project managers are typically extremely busy responding to project needs, which typically slows down technology adoption because they have no time to spend on digitizing their processes. The pandemic paused a lot of projects and provided executives and teams time to rethink their policies, procedures, and ways of doing business. Now these projects are coming back and the teams are determined to increase efficiency, and integrating new technologies is key to that goal.” LDV Capital led Gryps’ seed round with participation from Pear VC and Harvard Business School Graduate Syndicate. A group of angel investors also contributed."
https://venturebeat.com/2021/03/30/6sense-which-uses-ai-to-power-account-engagement-raises-125m/,"6Sense, which uses AI to power account engagement, raises $125M","Account engagement startup 6Sense today announced that it closed a $125 million series D funding round led by D1 Capital Partners, valuing the company at $2.1 billion post-money. 6Sense says that the investment will bolster its growth and product initiatives, particularly in the areas of machine next-best-action prediction, data insights, and AI-powered orchestration capabilities. Business-to-business buyers are typically 57% of the way to a buying decision before they engage with sales departments. Moreover, only 23% of executives are confident in the speed at which they’re gaining accurate insights. Motivated by the idea that AI might have a role to play in helping seal the deal, five entrepreneurs — Amanda Kahlow, Dustin Chang, Premal Shah, Shane Moriah, and Viral Bajaria — cofounded 6Sense in 2013. 6Sense’s product captures intent signals from known and anonymous sources including the web, creating customer segments by account, behavioral intent, or a combination of those two factors. 6Sense identifies contacts and builds out targeted buyer lists, helping to prioritize outreach sales efforts and boost conversions with machine learning-based fit scores. The platform also triggers marketing communications through apps like Marketo and Eloqua in response to sales prospects’ demands. Moreover, it enables salespeople to engage with buying teams via multichannel, multitouch campaigns. Automation is now seen as essential among marketers to bolster the outreach of campaigns, in part because of its ability to better target customer communications. According to a recent HubSpot survey, email automation campaigns are among the top three tactics used by marketers to improve performance. And in 2017, Salesforce reported that 67% of sales leaders used a marketing automation platform. Under the hood of 6Sense’s platform is a demand graph that captures signals and automatically connects them to sales prospects. Algorithms ingest historical intent data to reconstruct account-based buyer journeys for any given business, monitoring the demand graph and analyzing changes in intent to score hundreds of millions of accounts and people every day. 6Sense recently launched Segment Performance Reports and Custom Talking Points, two features that enable marketers to analyze changes in account engagement and progression through buying stages and provide guided conversation points based on buyer intent, role, and fit. February 2020 saw the introduction of Next Best Actions, which leverages AI to present business development representatives with a prioritized list of actions to engage buying teams within a target account. 6Sense competes to a degree with ZoomInfo. Other startups operating in the segment include Demandbase, which has raised over $150 million with backing from high-profile investors, as well as Lattice Engines and Leadspace. But 6Sense backers aren’t concerned, and they have some reason to be optimistic. The year 2020 was the company’s third straight year of 100% revenue growth, 6Sense says, driven by “significant increases” in pipeline, revenue, average sale price, and deal velocity and a twofold increase in customer base size. (Brightcove and Cognizant are among 6Sense’s clients.) While global ad spend was predicted to have fallen 10.2% year-on-year in 2020, ad agencies including Magna say they expect to see ad spend to rise over 7% in 2021 to around $612 billion total, with digital media seeing growth exceeding 10%. “We’re grateful for our success leading the account-based sales and marketing category — and humbled by the confidence our customers and investors have in 6Sense — but our vision has always been bigger and bolder,” said CEO Jason Zintak. “There is an enormous opportunity to redesign the way business-to-business companies go to market. We believe we have the platform, data, team, and investment partners to be the foundation for business-to-business revenue technology.” Forrester predicts that spend for marketing automation tools will grow “vigorously” over the next few years, reaching $25.1 billion annually by 2023 from $11.4 billion in 2017. It’s estimated that 55% of marketing decision-makers plan to increase their spending on marketing technology including AI and machine learning, with one-fifth of the respondents expecting to increase by 10% or more. “We invest heavily in sales and marketing technology, and 6Sense is truly one-of-a-kind,” Sapphire Ventures partner Rajeev Dham, a 6Sense investor, told VentureBeat. “We’ve always viewed 6Sense as a market leader with the ability to execute on their bold vision of transforming sales and marketing with data-driven insights and orchestration capabilities. 6Sense is already the leading account-based sales and marketing platform, and they are poised to define and deliver the future of revenue technology that every B2B organization needs.” Beyond D1 Capital Partners and Sapphire Ventures, Insight Partners and Tiger Global participated in 6Sense’s latest funding round. It brings the 300-employee, San Francisco, California-based company’s total raised to date to over $225 million."
https://venturebeat.com/2021/03/30/rivery-raises-16m-to-help-enterprises-manage-and-transform-data/,Rivery raises $16M to help enterprises manage and transform data,"Rivery, a data management platform geared toward enterprises, today closed a $16 million series A round, bringing its total raised to date to over $22 million. The company says the funds will be put toward expanding the size of its workforce and growing its international footprint. Enterprise data collection is expected to increase substantially over the next few years, but a chunk of data remains unleveraged, due to challenges in management and security. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends, while a separate Veritas report found that 52% of all information stored by organizations is of unknown value. Rivery, which was founded in 2018 and has offices in New York and Tel Aviv, offers a fully managed, serverless product that lets teams ingest, extract, and transform raw data. The platform offers APIs and connectors that allow customers to automate orchestration in the cloud, from both in-house and third-party databases. The idea is to turn data into business-ready inputs inside data stores to power insights and analysis. “Our mission is to give 100% of staff in organizations control over 100% of their data. In return, this gives teams the freedom to access the data they need to make incisive business decisions, whenever they need it,” cofounder and CEO Itamar Ben Hemo told VentureBeat via email. “By automating and streamlining data processes, we democratize access to data with operational efficiency that creates a single source of truth … Just like DevOps change the way businesses synchronize IT and development processes, DataOps will be essential to running an efficient data operation that is perfectly automated and connected to the business.” With Rivery, organizations can build, test, and deploy multiple data models, as well as aggregating data from sources such as social media data lakes and on-premise datacenters. The opportunity cost of this data, if unused, is substantial. The aforementioned Veritas report pegged it at a cumulative $3.3 trillion in 2020. That’s perhaps why the global enterprise data management market is expected to surpass $136.4 billion by 2026, according to a Research Dive report. Since launching in 2019, 40-employee Rivery says it has seen 500% growth with enterprise companies, including Bayer and the American Cancer Society. Hemo says the company’s annual recurring revenue stands at seven figures and is projected to reach eight figures within the next year. In the near-term, Rivery plans to turn its focus to a feature called Rivery Kits, which is designed to give data teams access to ready-made machine learning models. Rivery Kits are predefined templates for common use cases that live under a marketplace. Hemo describes this as a repository of data model templates with pipelines, connections, and logic. “[We have] hundreds of customers worldwide, ranging from data-driven tech unicorns like WalkMe or Riskified to established enterprises such as Bayer or American Cancer society … We’re also seeing increasing interest from finance and insurance companies and direct-to-consumer brands like NectarSleep,” Hemo said. “As a cloud-native company, Rivery works alongside all main cloud data warehouses with enterprise-grade solutions for businesses that need to unify, align, and automate all their data processes, ultimately to control and centralize the management of data flows.” Entree Capital led the series A round in Rivery, with participation from existing investor State of Mind Ventures."
https://venturebeat.com/2021/03/30/what-you-need-to-know-about-your-thought-leadership-strategy-post-pandemic/,"What you need to know about your thought leadership strategy, post pandemic","The term “thought leadership” was coined decades ago by Joel Kurtzman, former editor-in-chief of Strategy and Business magazine and Harvard Business Review and was defined as one who “possesses a distinctively original idea, a unique point of view, or an unprecedented insight into their industry.” Since then, it’s been proven that it’s not just a buzzword, but that it has real impact on influencing decisions. I asked Michael Norton, Harvard Business School Professor, how he would define thought leadership in today’s digital age. Michael shared, “Unfortunately, most self-proclaimed thought leaders tend to be thought followers — they reflect back the current zeitgeist and conventional wisdom rather than shine new light. Especially in times of dramatic social change, thought leaders who actually lead our thinking in new directions are sorely needed.” If new ways of thinking is the new way of owning thought leadership, how can you ensure your strategy is effective? If done right, thought leaders can be powerfully influential. Studies show that B2B companies with demonstrable expertise are those that will get noticed by other influencers and their core audience, and will realize real ROI. Now more than ever, in a post-pandemic world, business leaders are facing many unknowns, and thought leaders have an opportunity to influence and help guide their decision-making process. So what does this mean for B2B and B2BC marketers? I connected with industry-leading marketing and PR experts who have partnered with VentureBeat Lab, a thought leadership consultancy, and asked them to share how they defined thought leadership and how it’s become even more important post pandemic. Here’s what they shared: “Thought leadership is a powerful way to engage an audience that prefers to be educated versus sold to. Companies often make the mistake of trying to sell through the lens of what’s best for them, versus what’s best for their audience. Thought leadership helps bring your audience on a journey of shared understanding and insights, in order to drive toward impactful solutions. Born from necessity, the pandemic catalyzed the shift to a more digitally driven selling process, and consequently thought leadership has become an increasingly important top of the funnel marketing tool.” –Leah Hardy, Head of Gaming Marketing, Americas, Facebook “The pandemic has forced everyone to rethink how they do business, from doctors to data centers. After a year of fear and uncertainty, people will look to those who clearly understand how technology can fuel the recovery and deliver exciting new capabilities. Thought leadership is all the more important during these transitional times, to show the way forward.” — Ken Brown, Director of Corporate Communications at Nvidia. In this digital age, marketers are continuously adapting to the changing needs of the marketplace, and the continually evolving cycle of consumer behavior. The ways content is consumed is also constantly changing, and your thought leadership strategy should recognize this evolution. Whether you’re a marketer for an emerging start-up that’s looking to find its way into building awareness, or you’re representing a Fortune 500 company that’s globally well- known, thought leadership is an ongoing venture that keeps delivering results. For marketers in the start-up stage, your go-to-market strategy falls in what we can call “Phase 1” of your marketing efforts. Phase 1 means your company, services, and story is still “unknown” and has limited exposure and visibility in the marketplace, and your goal should be  focusing on building awareness thoughtfully before you ramp up any lead generation efforts. The takeaway: make sure your audience gets to know you organically first before you attempt to schedule a meeting with prospects. High-impact brand awareness and digital marketing is going to be part of the strategy, but a combination of branding and a well-conceived thought leadership strategy is what will really build meaningful awareness — you can’t do one without the other if you want to establish your brand. To summarize, in Phase 1, thought leadership is fundamental to taking the steps towards getting noticed and getting in front your top prospects — and again, if done right, that strategy is going to drive real ROI. For marketers who are developing a strategy for a corporation that is already well known, the need for thought leadership is continuous as it’s important to stay top of mind and highly engaged with your community, in an authentic way. Building authority as a thought leader is never ending, and those who have a thoughtful approach that puts the audience’s needs first will experience real turn-key engagement points with their target markets. This is why we still see experts from big brands and some of the most innovative companies speaking at events, publishing new content, and engaging with new and growing communities — and those who are authentic in their communication and approach will create value for those who are in the market for their services. So how can you stand out as an authentic thought leader and bring new value? For B2B companies, you want to create value by showing how your expertise is helping business decision-makers make more informed decisions. According to a 2020 Edelman study, 89% of decision-makers surveyed believe that thought leadership is effective in enhancing their perceptions of an organization — yet only 17% of them rate the quality of most of the thought leadership they read as very good or excellent. This is a real challenge for marketers, particularly in the B2B space, where marketers are facing a growing saturated market where it’s difficult to get noticed. As the marketplace evolves, it may seem hard to get it right. This is what inspired the creation of VentureBeat Lab, a thought leadership consultancy for brand partners. VB Lab has a proven process that helps brand partners establish their authority and influence as a global thought leader. Because VentureBeat reaches an influential audience of key enterprise business decision makers, partners have a real chance of influencing decision-makers and engaging with top prospects. VB Lab’s Thought Leadership Platform helps B2B marketers identify the most innovative go-to-market solutions and get in front of influencers and their core audience in a thoughtful way. It starts with identifying what makes you different. Hone in on your differentiator, offer new insights, and understand where the value lies in your target audience. Through high-touch collaboration, at VB Lab we’re focused on marketing strategies that nail your most important differentiator — and from there, we create new, disruptive partnership solutions, and custom content strategies, and then thoughtfully execute on your key core objectives. Thought leadership isn’t just the core part of building awareness and staying relevant, but a necessary tool for quality lead generation. When done right, the opportunities with thought leadership are endless. Visit VB Lab to learn more about VentureBeat Lab’s Thought Leadership Platform or email us at partners@venturebeat.com. Gina Joseph is Co-Founder of VB Lab and VP of Strategic Partnerships, VentureBeat"
https://venturebeat.com/2021/03/30/recuro-health-announces-oversubscribed-funding-round-founding-ceo-of-teladoc-leads-launch-of-new-integrated-digital-solutions-company/,Recuro Health Announces Oversubscribed Funding Round: Founding CEO of Teladoc Leads Launch of New Integrated Digital Solutions Company,"DALLAS–(BUSINESS WIRE)–March 30, 2021– Recuro Health (Recuro), an integrated digital health solution that transitions the U.S. healthcare system from a reactive, disease-focused model to a population health, outcomes approach, today announces the closing of its oversubscribed funding round led by OLSF Ventures. Michael Gorton, founding CEO of Teladoc, leads the Recuro team which includes several Teladoc leaders: the original Chief Technology Officer, along with the marketing team that perfected Teladoc’s go-to-market strategy. The company is also adding healthcare luminaries and experienced business executives such as Jay Sanders, MD, founder of the American Telemedicine Association and acknowledged as the “Father of Telemedicine.” “This oversubscribed round of funding and interest in our subsequent round demonstrate the support of our investors and confidence in our management team to build yet another industry-transforming company in the digital health space,” says Gorton, CEO and founder of Recuro Health. “The platform combines digital health with a patient-centered medical home. We are creating an easy, personal and economic solution for patients, employers and payers.” According to Dr. William Paiva, managing partner at OLSF Ventures and lead investor in the round, “Given the unprecedented interest in the Recuro business model, we will quickly follow this company formation and funding round with a larger round of funding which will further accelerate Recuro’s journey to build a new industry category.” Recuro is focused on empowering patients with tools, education and guidance to live healthier, longer and happier lives. Grounded in a suite of digital health offerings, analytics and services, Recuro touches patients along their entire health journeys. Gorton says, “We intend to disrupt the traditional healthcare delivery system by bridging the gap between the current system and digital health. This requires a deep understanding of the current challenges facing healthcare, along with a proven ability to impact the industry. We are building infrastructure and partnerships to support care delivery in every setting, across the entire patient journey, minimizing the need for acute, episodic care by providing the digital keys to member health.” Dr. Paiva continues, “There are rare moments in a venture capitalist career when you are presented an industry transforming opportunity, at the right moment in time, with a management team that has created the industry category. This is one of those moments.” In addition to OLSF, participants in the current round of funding include Cortado Ventures, 1843 Capital and Sage Venture Partners. About Recuro Health Serving employers, providers and managed care organizations, Recuro Health delivers value throughout the healthcare ecosystem. Digital and virtual services leverage aggregated data to direct users to achieve effective and efficient offerings. By evaluating outcomes and compliance together, Recuro provides appropriate referrals and recommendations. www.recurohealth.com About OLSF Ventures With over 20 years of industry experience working in management consulting, large pharma and venture capital, OLSF contributes many diverse perspectives to our portfolio companies. OLSF I is invested in five start-ups that have created over 100 jobs with a mean income that is three times greater than the state average. These companies have attracted over $210 million in venture capital from regional and national venture capital firms after OLSF’s first investment. These entrepreneurial success stories have continued with our second round, OLSF II, which has already invested in five promising start-ups in Oklahoma, which have attracted over $74.3 million in outside venture capital. Since its inception in 2000, The Oklahoma Life Science Fund has been the single largest driver of private equity capital to Oklahoma. www.olsfventures.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005442/en/ Media: CPR for Recuro HealthHolly McKennaHmckenna@cpronline.com 1.518.461.8207"
https://venturebeat.com/2021/03/30/climacell-rebrands-as-tomorrow-io-and-raises-77m-to-bring-weather-data-intelligence-to-enterprises/,ClimaCell rebrands as Tomorrow.io and raises $77M to bring weather data intelligence to enterprises,"Weather data holds a degree of interest for most people — it informs our plans for the weekend, whether to take an umbrella to the local grocery store or to pack sunscreen on a day walking in the hills. But for enterprises, procuring accurate weather data can be mission critical, which is why ClimaCell has built a platform over the past five years that enables companies across insurance, supply chain, energy, and more to predict future weather events and limit the impact it might have on their business. The Boston-based company today announced it has raised $77 million in a series D round of funding, which it said it will use to accelerate its SaaS growth and invest in its recently announced venture into outer space, which will see it build proprietary radar-equipped satellites to improve its weather monitoring and forecasting abilities. Alongside the investment, ClimaCell announced that it’s changing its name to Tomorrow.io (we’ll use this name from here on) to reflect the new direction. Founded in 2016, Tomorrow.io provides real-time weather forecasts that uses data garnered from sources such as wireless communication infrastructure, connected cars, drones, airplanes, and more. This is what the company refers to as the “weather of things,” and it’s designed to bring super-accurate, localized weather data to businesses that need it — this replaces traditional models that typically use government data gleaned from satellites. “The government systems were built for major events — they were not built for day-to-day operations of a company, and most players in the weather space are simply repackaging government models and selling the data along with a consulting services package,” Tomorrow.io CMO Dan Slagen told VentureBeat. Tomorrow.io counts customers across the consumer and enterprise sphere, including transport and logistics giants such as Ford, JetBlue, and Delta Air Lines, while last year Uber signed up as a customer to improve its ETA travel estimates. The correlation between weather and traffic flow is no secret, with rain alone estimated to reduce traffic speed by up to 12%, while snow and fog can also have similar repercussions. “A railroad operations executive uses us to know that ‘in New Mexico on Tuesday at 3 p.m. ET at mile marker 27, slow your trains down to 10 miles-per-hour to avoid a derailment due to crosswinds that exceed your safety protocol,'” Slagen said. “The weather forecast they used to get was ‘high winds across New Mexico on Tuesday,’ and they would then have to go figure out what that means.” For context, a single train derailment causes around $250,000 in damage on average on U.S. railroads, according to a study by the Volpe National Transportation Systems Center. Weather can cause many other periphery issues in the transport and logistics sphere, such as companies having to cancel staff shifts or pay overtime if hazardous weather means having to delay loading cargo, for example. “Between optimizing for staffing costs and improving delays, the cost savings are in the tens of millions per year for an individual company,” Slagen added. The current Suez Canal debacle is a good example of how weather events can wreak havoc — the Ever Given container ship became wedged after being blown off course by a gust of wind. The subsequent blockade it created is thought to be holding up $10 billion in global trade. Using archive data, Tomorrow.io has shown when the ship ran aground and the associated wind status. “Our software showed this risk well in advance of impact,” Slagen said. “Our system would have recommended that due to the wind speed, the vessel should delay passing or perform other safety procedures.” Such is the potential impact of weather events, Tomorrow.io CEO and cofounder Shimon Elkabetz thinks that businesses should hold weather intelligence in the same regard as cybersecurity. “Climate change makes weather events more frequent and more volatile, and CEOs should start thinking about weather intelligence in the same way they think about their cybersecurity strategy,” he wrote in a blog post today. “They need to think of tomorrow.” ClimaCell had previously raised around $107 million, and with a fresh $77 million in the bank from backers including Stonecourt Capital and Highline Capital, the company said that it will continue to invest in its current product that covers weather intelligence and air quality, as well as support investments across AI, machine learning, and its upcoming space operations. The company’s new radar-enabled satellites are scheduled to go live in 2022 and will serve to improve its weather data by garnering more detailed information about precipitation and cloud structure that “no other sensors can see,” according to the company. The launch comes as many existing satellites are scheduled to be retired, leading Tomorrow.io to develop its own version — one that’s roughly the size of a mini-fridge. So why change its name when “ClimaCell” seemed to fit the bill pretty well, and is arguably more unique than the generic name it will now use? “The truth is, we didn’t choose our new name — it chose us,” Elkabetz wrote. “Tomorrow.io perfectly captures everything that businesses, individuals, and countries want from the weather industry.”"
https://venturebeat.com/2021/03/30/linkedin-launches-video-cover-stories-and-other-profile-features-for-job-seekers/,LinkedIn launches video cover stories and other profile features for job seekers,"LinkedIn today announced new features designed to make profiles on the platform more expressive and inclusive as the pandemic continues to roil the job market. Video cover stories let members showcase their skills, while a creator mode helps them build a following by more prominently highlighting content on their profile. In addition, LinkedIn says it will expand its free LinkedIn Learning and Microsoft Learn courses aligned with the 10 most in-demand jobs through December 2021 and pilot Skills Path, which the company describes as a “skills-first” program for job seekers. According to LinkedIn, video cover stories, which will roll out within weeks, are a sought-after tool for both job candidates and recruiters. Sixty-one percent of job seekers believe recorded video could be the next iteration of the traditional cover letter, according to a survey conducted by the company, while almost 80% of hiring managers say video has become more important for vetting candidates. Complementing the video cover stories feature is a new field for pronouns on profiles, which is intended to let members communicate how they want to be seen. Seventy percent of job seekers believe it’s important that hirers know their gender pronouns, the above-mentioned survey found, and 72% of hiring managers believe having clarity about gender pronouns can help others be respectful.  Beyond cover stories and the gender pronouns field, LinkedIn is introducing creator mode and service pages for candidates and organizations. The creator mode enables members to highlight their recent job-relevant work, while the service page allows freelancers and small business owners to create dedicated pages listing the services they offer. Beyond the extended availability of free online learning courses, LinkedIn said it will partner with companies, including Gap, TaskRabbit, and Twitter on Skills Path, which aims to help job seekers learn the skills required for roles with free lessons. Skills Path will also give them an opportunity to demonstrate their skills with assessments and land a recruiter conversation with one of the participating companies.  Lastly, starting in May, LinkedIn says it will power a new Microsoft Teams app called Career Coach for institutions to support students pre- and post-graduation. The app will help students discover goals, interests, and skills using an AI-based identifier that aligns their profiles with job market trends. “Every day, we’re seeing our members share and connect like never before, with nearly 5 billion connections made last year [and] conversations on the platform [increasing by 50%],” a LinkedIn spokesperson told VentureBeat. “We … know that there’s no one-size-fits-all for someone’s career journey and that not everyone has the same identity or goals. That’s why we’re unveiling these new features and updates.”"
https://venturebeat.com/2021/03/30/cloud-backup-and-recovery-company-hycu-raises-87-5m/,Cloud backup and recovery company HYCU raises $87.5M,"HYCU, a company developing data backup and recovery solutions for enterprises, today announced that it closed a $87.5 million series A funding round led by Bain Capital Ventures. With the introduction of HYCU Protégé, a disaster recovery solution for enterprise apps, HYCU says it will use the funding to expand and grow its app, public cloud, and software-as-a-service-based innovations as well as hire aggressively in Boston and North America to meet growth goals. There are few catastrophes more disruptive to an enterprise than data loss, and the causes are unfortunately myriad. In a recent survey of IT professionals, about a third pegged the blame on hardware or system failure, while 29% said their companies lost data because of human error or ransomware. It’s estimated that upwards of 93% of organizations that lose servers for 10 days or more during a disaster filed for bankruptcy within the next 12 months, with 43% never reopening. Those statistics are more alarming in light of high-profile outages like that of OVHCloud earlier this month, which took down 3.6 million websites ranging from government agencies to financial institutions to computer gaming companies. Headquartered in Boston, Massachusetts, HYCU, which was founded in 2018, offers modular data management services designed to simplify multi-cloud data migration, disaster recovery, and data protection management. It aims to bring software-as-a-service-based data backup to both on-premises and cloud-native environments, in part via support for platforms including VMware, Amazon Web Services, Nutanix, Google Cloud Platform, and Microsoft Azure. “HYCU believes in leveraging the power of AI and making it transparent for the user. The way it manifests for the end user is in terms of what we call Intelligent Simplicity,” CEO Simon Taylor explained to VentureBeat via email. “For example, unlike a number of other solutions, with HYCU, our customer does not have to tell the software where to store the backups; it automatically matches the customer’s service-level agreement with the capabilities of the network and backup targets to find the right place. This approach reduces effort and keeps cost at the optimal level.” According to Gartner, data-driven downtime costs the average company $300,000 per hour — or $5,600 every minute. That’s perhaps why Markets and Markets predicts that the data and backup recovery market will be worth well over $11 billion by 2022.  HYCU competes to a degree with San Francisco-based Rubrik, which has raised $553 million in venture capital to date for its live data access and recovery offerings, and Cohesity, which bills itself as the industry’s first hyperconverged secondary storage for backup, development, file services, and analytics. That’s not to mention data recovery juggernaut Veeam, which now serves 80% of the Fortune 500 and 58% of the Global 5000; Acronis, which raised $147 million in September for its suite of data backup, protection, and restoration tools; and cloud data backup and recovery company Clumio. “Many use cases for our customers center around being able to backup and recover with specific on-premises environments like Nutanix and VMware. Or, they may need a solution they can easily run and deploy from a specific cloud platform like Google Cloud or Azure Cloud,” Taylor said. “For Nutanix environments in particular we have a long-established and rich pedigree of support for their solutions.” HYCU, which has over 200 employees, claims to have over 2,000 customers worldwide. ACrew Capital also participated in the company’s latest funding round."
https://venturebeat.com/2021/03/30/zoomin-which-helps-enterprises-extract-answers-from-siloed-technical-documents-raises-52m/,"Zoomin, which helps enterprises extract answers from siloed technical documents, raises $52M","Zoomin, a platform that helps enterprises extract answers from across their product content, has raised $52 million in a series C round of funding. The “knowledge orchestration” platform has amassed an impressive roster of clients since its inception six years ago, including Dell, McAfee, Imperva, and the now Adobe-owned Workfront, helping make their vast pools of content easier to search and extract answers from. Businesses may have thousands of manuals, guides, training documents, online community discussions, and more, each created and managed by different teams in silos. Zoomin unifies all of these parts, serving as a sort of white-label search engine that delivers answers wherever a company needs them. “Every enterprise generates hundreds to hundreds of thousands of pages of product content meant to help customers utilize products to their fullest potential,” Zoomin CEO and cofounder Gal Oron told VentureBeat. “This content is constantly evolving and growing in volume alongside the products it supports. But most of the time, the product content experience is severely lacking — it fails to provide customers product information in an easily accessible, seamless way across the range of channels where they are looking for it. When customers can’t find relevant information quickly and easily, this limits product adoption and onboarding and leads to frustrated customers, creating churn and burdening customer support teams.” For example, a company might use Zoomin to build a technical resource center similar to an intranet — where employees can search for all manner of business documentation, such as financials or how-to guides that are spread across different locations. Zoomin bakes in a bunch of useful features, including content filters, auto-suggestions, recommendations, and other tools users are likely accustomed to from other search platforms they use. Alternatively, a company may elect to deploy Zoomin directly inside one of their own applications as a widget that offers context-specific content. Zoomin also offers prebuilt apps that can be customized and integrated with Salesforce or ServiceNow, while companies can deploy their own integrations via Zoomin’s APIs. Other similar products that enterprises might use include Adobe Experience Manager or Liferay, but Zoomin aims to set itself apart with a focus on its “bespoke” solution built specifically for complex, technical content. Moreover, Oron argues that enterprises value Zoomin’s inherent agility. “We can get them up and running in weeks and enable full customization with minimal added costs and no reliance on IT resources while connecting to any system they’re already using,” he said. “Plus, we offer out-of-the-box actionable analytics.” There are also existing universal search tools for enterprises, and although they may bring search results into a single interface, they typically direct the user to the original source of the content. Zoomin brings all the answers into a single channel and saves users having to navigate across different destinations. Founded in 2015, Zoomin has been on a tear over the past year, claiming to have “more than doubled” its new customers since 2019. Moreover, the company announced a $21 million investment in December, although that was from a previously undisclosed round. Since then, Zoomin has introduced offline support, which Oron said is particularly relevant for field technical support: “for example, when field engineers need to be able to instantly access critical information as they solve hardware issues, often operating with limited network connection.” With another $52 million in the bank, Oron said it’s now well-financed to “meet the growing demand from enterprises across industries,” spanning a wider gamut of hardware and software companies. “While our sweet spot has historically been SaaS and hardware companies, a diverse mix of new industries are now recognizing the strategic value of investing in their product information experience,” he said. “We’re seeing strong demand from fintech and health care industries, in particular. In addition, while the majority of our customers are large U.S-based enterprises, we’re seeing that mid-market and growth companies are facing the same pain points that we are solving for enterprises.” While Zoomin already offers a range of analytics, such as “traffic insights” that show where traffic is being referred from and “content insights” that highlight which topics or documents garner the greatest engagement, Oron said the company plans to use some of its fresh cash injection to develop “new in-product and sales analytics features and case deflection analytics tools” and improve its platform’s existing search and prediction capabilities. Underpinning much of this growth is a desire to alleviate some of the burden from companies’ technical or customer support teams — Zoomin basically helps companies build their own self-serve product support portals, leaning on machine learning models to develop a knowledge graph that bridges enterprise content and the end users that are likely to find most use from it. “Customers are expressing a strong desire to self-serve product answers rather than rely on customer support teams and, secondly, more and more businesses are experiencing remote work,” Oron said. “As a result, [they are] increasingly relying on digital support channels. This makes the need and demand for a self-service product content experience more urgent than ever.”"
https://venturebeat.com/2021/03/30/nimble-crowned-crm-industry-leader-and-top-5-sales-intelligence-tool-for-small-business-teams-on-g2/,Nimble Crowned CRM Industry Leader and Top 5 Sales Intelligence Tool for Small Business Teams on G2," For 9 Years in a Row, G2 Reviewers Have Chosen Nimble As Their Fan-Favorite Simple Smart CRM for Microsoft 365 and Google Workspace  SANTA MONICA, Calif.–(BUSINESS WIRE)–March 30, 2021– Nimble, the pioneering Social Sales and Marketing CRM built for Microsoft 365 and Google Workspace, announced today that it has been named one of the Top Five Sales Intelligence Software Tools for Small Business and an overall CRM Industry Leader by G2, the world’s leading business software review platform. Nimble achieved top customer satisfaction rankings according to verified user reviews and a market presence in multiple categories, including: “We believe that lack of use is one of the biggest sins of CRMs,” explains Sergey Shvets, Nimble’s Head of Product. “That’s why we prioritize the end-user experience in everything we do. Our system is the only CRM on the market that works on every website in the world, allowing users to have a CRM always a click away; whenever they want to profile someone, capture a lead, or prepare for a meeting. We’re happy to see that our efforts are appreciated by our customers and we have many more improvements coming soon.” Nimble Sets the Bar for Simple, Smart CRM with Sales Intelligence Built-in Nimble is the only CRM that has built-in sales intelligence and has consecutively been ranked as a market-leading CRM in sales intelligence. Although most sales intelligence tools can cost over $100 per user/month in addition to the costs of your CRM, Nimble starts as low as $19 per user/month when billed annually. Nimble Scales Worldwide as the Simple CRM for Microsoft 365 Microsoft is now reselling Nimble+Microsoft 365 worldwide and their distributors and partners are reselling, implementing, and providing customized Nimble solutions to Microsoft 365 users 24/7 across the globe. Additional Resources: Get Started With Nimble Today! We invite you to try it for free for 14 days. Stay tuned for more product announcements as we evolve Nimble into the best CRM for Microsoft 365 and Google Workspace. ABOUT NIMBLE – Nimble is the leading global provider of simple, smart CRM for small business teams using Microsoft 365 or Google Workspace. It combines the strength of traditional CRM, classic contact management, social media, sales intelligence, pipeline management, and marketing automation into one powerful relationship management platform that delivers valuable company and contact insights – everywhere you work. Nimble has been named “Market Leading CRM for Customer Satisfaction and Ease of Use” and CRM Market Leader by G2 for nine consecutive years, CRM Watchlist Winner for three consecutive years, #1 Sales Intelligence Tool for Customer Satisfaction by G2 for the ninth consecutive time, and users’ choice award winner by Fit Small Business. Try Nimble’s 14-day free trial today. For more information, visit http://www.nimble.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005138/en/ Megan Ranger – megan.ranger@nimble.com"
https://venturebeat.com/2021/03/30/cleveland-clinic-will-be-ibms-first-private-sector-customer-to-install-a-quantum-computer-on-premise/,Cleveland Clinic will be IBM’s first private sector customer to install a quantum computer on premises,"IBM today announced it is installing a quantum computer at the Cleveland Clinic, marking the first time the company has physically placed this next-generation system on the premises of a private sector client. The move marks yet another step forward for quantum computing. It comes as part of a broader 10-year partnership between IBM and the clinic that includes hybrid cloud service and AI. According to IBM Quantum Network director Dr. Anthony J. Annunziata, including a quantum computer as part of that suite of tools is critical because the company wants to understand which tasks are best suited to quantum computations. Despite rapid advances, quantum computers are still in their infancy, but it’s still possible they could be more efficient at limited tasks. “The Cleveland Clinic will have the full capacity of a quantum system we purpose-built for them,” Annunziata said. “We’ll have a much better ability to integrate it into their existing infrastructure. There will be benefits in doing that as we figure out how quantum can address these really tough problems and also how it can accelerate the application of AI.” The partners have dubbed the program the Discovery Accelerator, and its overall goal is to power new breakthroughs in health care and life sciences. IBM’s computing tools are being leveraged to better harness the clinic’s wealth of data, including “genomics, single-cell transcriptomics, population health, clinical applications, and chemical and drug discovery,” according to a press release. The eye-catching part of the announcement, however, is the move to physically place a quantum computer at the clinic. Until now, the company has been focused on its IBM Q Network, a consortium of research and business partners who can experiment with quantum computing via a cloud-based service. IBM has grown increasingly optimistic about quantum’s potential and has laid out an ambitious timetable for expanding commercial applications. That will now include its first on-premises Quantum System One in the United States outside of an IBM computation center. IBM currently has a quantum computer on its own campus, as well as one at Germany’s Fraunhofer Institute and the University of Tokyo. The Cleveland Clinic is the first private sector client and the first in the U.S. Annunziata said the clinic will make for a good first private partner, thanks to its recently announced Global Center for Pathogen Research & Human Health. The new center will assemble teams to focus on viral pathogens, virus-induced cancers, genomics, immunology, and immunotherapies. “If there is anything that we can do as a technology partner to help institutions with the mission to advance life sciences and health care, we’re very happy to do it,” he said. In many cases, researchers feel progress in these areas is being limited by the ability to gather and analyze massive datasets. The clinic is betting that a system that combines AI, quantum computing, and hybrid
cloud technologies will remove those hurdles and unleash new health care innovation. Annunziata said part of the work will be to learn just where quantum computing sits in that computing system. Quantum is not robust enough to replace all computing functions. And even in many best-case scenarios, researchers believe quantum computing will be best suited for particular functions. Health care has long been touted as a strong potential use case. Quantum proponents are betting that such computers will be able to develop more sophisticated models of the human body, allowing for the development of better hypotheses for designing experiments, as well as models that speed the testing of new drugs. The key is learning which tasks in the Cleveland system can be offloaded to the quantum computer — with the results then fed back into the classic computing architecture, Annunziata said. At the same time, the Cleveland Clinic partnership will provide an opportunity to train a quantum workforce for the coming years as more commercial partners look for such skillsets."
https://venturebeat.com/2021/03/29/cloudera-adds-sql-tool-to-query-streaming-data/,Cloudera adds SQL tool to query streaming data,"Cloudera announced today it has added to its portfolio a Cloudera SQL Stream Builder tool based on technology it gained with the acquisition of Eventador that makes it possible to employ SQL to query streams of data in real time. That Eventador tool is now integrated with a Cloudera DataFlow (CDF) streaming platform that provides a common framework for processing streaming data using open source Apache Flink, Kafka Streams, or Spark Structured Streaming engines. Previously, the only way to query that data was using programming tools based on Java or Scala. Now data analysts can now query CDF data without having to know how to write code, said Dinesh Chandrasekhar, head of product marketing for Cloudera. SQL Stream Builder also enables analysts to create views of query results that can be exposed to other applications via REST application programming interfaces (APIs). It has also been integrated with the Shared Data Experience (SDX) framework Cloudera created to enforce governance and security policies across CDF. Despite the rise of a wide range of programming languages employed to analyze data, the dominant lingua franca for querying data in the enterprise remains SQL. However, as the need to query data as it streams in real time becomes larger, organizations want to be able to extend SQL to, for example, potentially identify anomalies in processes that would be indicative of potential fraud, Chandrasekhar said. Much of the increased need to query streaming data is being driven by digital business transformation initiatives that process and analyze data in real time using platforms such as Spark and Kafka. At some point, an analyst is going to need to launch an ad hoc query against that data to resolve a pressing issue long before the data is eventually stored in a relational database. “Data has a shelf life,” said Chandrasekhar. Rather than having to find a developer to write that query in Java or some other programming language to achieve that goal, it’s now possible for an analyst to immediately launch a SQL query themselves. Previously, that query might not have ever been launched simply because it would have taken too much time and effort to find a developer to write the code. In general, more data than ever is being processed and analyzed at both the points where it is created and consumed and where it moves between applications in real time. Cloudera is betting much of that data will ultimately land in a data warehouse based on the open source distribution of Hadoop that it provides. However, in the last few years, rival SQL-compatible data lakes based on proprietary platforms managed by cloud service providers have been gaining traction at the expense of provider of platforms based on Hadoop. Cloudera, with the launch of Cloudera SQL Stream Builder, is adding one more SQL-compatible tool to a portfolio that makes it possible to query data residing in Hadoop and other frameworks such as Apache Spark that are typically deployed on top of Hadoop. It’s not clear just yet to what degree those capabilities will enable Cloudera to counter the recent successes of its rivals. However, as a provider of a data warehouse platform based on open source software, Cloudera does appeal to IT organizations that have decided to avoid proprietary software whenever possible. Regardless of what tool is employed to analyze data, there’s more of it than ever being generated faster. The degree to which humans will be able to analyze data that is generated in real time remains to be seen. Many of the digital processes that organizations are trying to analyze occur in milliseconds, which is too fast for a human being to catch without help from some form of AI. Nevertheless, there’s a lot data residing in streaming platforms that can be queried. The challenge now is knowing how to first structure those SQL queries and, just as importantly, when to launch them."
https://venturebeat.com/2021/03/29/adversarial-training-reduces-safety-of-neural-networks-in-robots-research/,Adversarial training reduces safety of neural networks in robots: Research,"This article is part of our reviews of AI research papers, a series of posts that explore the latest findings in artificial intelligence. There’s a growing interest in employing autonomous mobile robots in open work environments such as warehouses, especially with the constraints posed by the global pandemic. And thanks to advances in deep learning algorithms and sensor technology, industrial robots are becoming more versatile and less costly. But safety and security remain two major concerns in robotics. And the current methods used to address these two issues can produce conflicting results, researchers at the Institute of Science and Technology Austria, the Massachusetts Institute of Technology, and Technische Universitat Wien, Austria have found. On the one hand, machine learning engineers must train their deep learning models on many natural examples to make sure they operate safely under different environmental conditions. On the other, they must train those same models on adversarial examples to make sure malicious actors can’t compromise their behavior with manipulated images. But adversarial training can have a significantly negative impact on the safety of robots, the researchers at IST Austria, MIT, and TU Wien discuss in a paper titled “Adversarial Training is Not Ready for Robot Learning.” Their paper, which has been accepted at the International Conference on Robotics and Automation (ICRA 2021), shows that the field needs new ways to improve adversarial robustness in deep neural networks used in robotics without reducing their accuracy and safety. Deep neural networks exploit statistical regularities in data to carry out prediction or classification tasks. This makes them very good at handling computer vision tasks such as detecting objects. But reliance on statistical patterns also makes neural networks sensitive to adversarial examples. An adversarial example is an image that has been subtly modified to cause a deep learning model to misclassify it. This usually happens by adding a layer of noise to a normal image. Each noise pixel changes the numerical values of the image very slightly, enough to be imperceptible to the human eye. But when added together, the noise values disrupt the statistical patterns of the image, which then causes a neural network to mistake it for something else. Adversarial examples and attacks have become a hot topic of discussion at artificial intelligence and security conferences. And there’s concern that adversarial attacks can become a serious security concern as deep learning becomes more prominent in physical tasks such as robotics and self-driving cars. However, dealing with adversarial vulnerabilities remains a challenge. One of the best-known methods of defense is “adversarial training,” a process that fine-tunes a previously trained deep learning model on adversarial examples. In adversarial training, a program generates a set of adversarial examples that are misclassified by a target neural network. The neural network is then retrained on those examples and their correct labels. Fine-tuning the neural network on many adversarial examples will make it more robust against adversarial attacks. Adversarial training results in a slight drop in the accuracy of a deep learning model’s predictions. But the degradation is considered an acceptable tradeoff for the robustness it offers against adversarial attacks. In robotics applications, however, adversarial training can cause unwanted side effects. “In a lot of deep learning, machine learning, and artificial intelligence literature, we often see claims that ‘neural networks are not safe for robotics because they are vulnerable to adversarial attacks’ for justifying some new verification or adversarial training method,” Mathias Lechner, Ph.D. student at IST Austria and lead author of the paper, told TechTalks in written comments. “While intuitively, such claims sound about right, these ‘robustification methods’ do not come for free, but with a loss in model capacity or clean (standard) accuracy.” Lechner and the other coauthors of the paper wanted to verify whether the clean-vs-robust accuracy tradeoff in adversarial training is always justified in robotics. They found that while the practice improves the adversarial robustness of deep learning models in vision-based classification tasks, it can introduce novel error profiles in robot learning. Say you have a trained convolutional neural network and want to use it to classify a bunch of images stored in a folder. If the neural network is well trained, it will classify most of them correctly and might get a few of them wrong. Now imagine that someone inserts two dozen adversarial examples in the images folder. A malicious actor has intentionally manipulated these images to cause the neural network to misclassify them. A normal neural network would fall into the trap and give the wrong output. But a neural network that has undergone adversarial training will classify most of them correctly. It might, however, see a slight performance drop and misclassify some of the other images. In static classification tasks, where each input image is independent of others, this performance drop is not much of a problem as long as errors don’t occur too frequently. But in robotic applications, the deep learning model is interacting with a dynamic environment. Images fed into the neural network come in continuous sequences that are dependent on each other. In turn, the robot is physically manipulating its environment. “In robotics, it matters ‘where’ errors occur, compared to computer vision which primarily concerns the amount of errors,” Lechner says. For instance, consider two neural networks, A and B, each with a 5% error rate. From a pure learning perspective, both networks are equally good. But in a robotic task, where the network runs in a loop and makes several predictions per second, one network could outperform the other. For example, network A’s errors might happen sporadically, which will not be very problematic. In contrast, network B might make several errors consecutively and cause the robot to crash. While both neural networks have equal error rates, one is safe and the other isn’t. Another problem with classic evaluation metrics is that they only measure the number of incorrect misclassifications introduced by adversarial training and don’t account for error margins. “In robotics, it matters how much errors deviate from their correct prediction,” Lechner says. “For instance, let’s say our network misclassifies a truck as a car or as a pedestrian. From a pure learning perspective, both scenarios are counted as misclassifications, but from a robotics perspective the misclassification as a pedestrian could have much worse consequences than the misclassification as a car.” The researchers found that “domain safety training,” a more general form of adversarial training, introduces three types of errors in neural networks used in robotics: systemic, transient, and conditional. Transient errors cause sudden shifts in the accuracy of the neural network. Conditional errors will cause the deep learning model to deviate from the ground truth in specific areas. And systemic errors create domain-wide shifts in the accuracy of the model. All three types of errors can cause safety risks. To test the effect of their findings, the researchers created an experimental robot that is supposed to monitor its environment, read gesture commands, and move around without running into obstacles. The robot uses two neural networks. A convolutional neural network detects gesture commands through video input coming from a camera attached to the front side of the robot. A second neural network processes data coming from a lidar sensor installed on the robot and sends commands to the motor and steering system. The researchers tested the video-processing neural network with three different levels of adversarial training. Their findings show that the clean accuracy of the neural network decreases considerably as the level of adversarial training increases. “Our results indicate that current training methods are unable to enforce non-trivial adversarial robustness on an image classifier in a robotic learning context,” the researchers write. “We observed that our adversarially trained vision network behaves really opposite of what we typically understand as ‘robust,'” Lechner says. “For instance, it sporadically turned the robot on and off without any clear command from the human operator to do so. In the best case, this behavior is annoying, in the worst case it makes the robot crash.” The lidar-based neural network did not undergo adversarial training, but it was trained to be extra safe and prevent the robot from moving forward if there was an object in its path. This resulted in the neural network being too defensive and avoiding benign scenarios such as narrow hallways. “For the standard trained network, the same narrow hallway was no problem,” Lechner said. “Also, we never observed the standard trained network to crash the robot, which again questions the whole point of why we are doing the adversarial training in the first place.” “Our theoretical contributions, although limited, suggest that adversarial training is essentially re-weighting the importance of different parts of the data domain,” Lechner says, adding that to overcome the negative side-effects of adversarial training methods, researchers must first acknowledge that adversarial robustness is a secondary objective, and a high standard accuracy should be the primary goal in most applications. Adversarial machine learning remains an active area of research. AI scientists have developed various methods to protect machine learning models against adversarial attacks, including neuroscience-inspired architectures, modal generalization methods, and random switching between different neural networks. Time will tell whether any of these or future methods will become the golden standard of adversarial robustness. A more fundamental problem, also confirmed by Lechner and his coauthors, is the lack of causality in machine learning systems. As long as neural networks focus on learning superficial statistical patterns in data, they will remain vulnerable to different forms of adversarial attacks. Learning causal representations might be the key to protecting neural networks against adversarial attacks. But learning causal representations itself is a major challenge and scientists are still trying to figure out how to solve it. “Lack of causality is how the adversarial vulnerabilities end up in the network in the first place,” Lechner says. “So, learning better causal structures will definitely help with adversarial robustness.” “However,” he adds, “we might run into a situation where we have to decide between a causal model with less accuracy and a big standard network. So, the dilemma our paper describes also needs to be addressed when looking at methods from the causal learning domain.” Ben Dickson is a software engineer and the founder of TechTalks. He writes about technology, business, and politics. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/03/29/canalys-more-data-breaches-in-2020-than-previous-15-years-despite-10-growth-in-cybersecurity-spending/,Canalys: More data breaches in 2020 than previous 15 years despite 10% growth in cybersecurity spending,"As cyberattacks increase, it’s hard not to wonder if enterprises are fighting a losing battle. According to a new report by Canalys, companies are spending record sums on cybersecurity, and yet the number of successful attacks is higher than ever. Canalys’ report noted that “more records were compromised in just 12 months than in the previous 15 years combined.” These are being driven in particular by ransomware attacks that have become more severe, in some cases disrupting hospitals. These attacks have also caused some companies to shut down and others to put emergency response plans in place to avoid being shuttered. This carnage is happening despite the fact that cybersecurity investment grew 10% in 2020 to $53 billion. So what’s going on? Canalys believes that companies are still under-investing in cybersecurity. During the pandemic, other areas of IT grew faster, signaling that enterprises were placing an emphasis on services that would help them remain stable during the pandemic or even grow rather than protect their infrastructure from attack. Indeed, some enterprises may have increased their vulnerability by responding to the pandemic in ways that ignored their safety policies. But the real culprit may simply have been failing to make security a top priority. Compared to the 10% growth in cybersecurity spending, cloud infrastructure services grew 33% in 2020, cloud software services rose 20%, notebook PC shipments jumped 17%, Logitech’s webcam sales increased 138%, and wi-fi router sales surged 40%. “Cybersecurity must be front and center of digital plans, otherwise there will be a mass extinction of organizations, which will threaten the post-COVID-19 economic recovery,” said Canalys Chief Analyst Matthew Ball in a statement. “A lapse in focus on cybersecurity is already having major repercussions, resulting in the escalation of the current data breach crisis and acceleration of ransomware attacks.”"
https://venturebeat.com/2021/03/29/solarwinds-hackers-accessed-emails-from-u-s-department-of-homeland-security/,SolarWinds hackers accessed emails from U.S. Department of Homeland Security,"(Reuters) — Hackers suspected of working for Russia got access to an email account belonging to the former head of the U.S. Department of Homeland Security, which is responsible for cybersecurity, in the SolarWinds hack, the Associated Press reported here on Monday. The AP report said the intelligence value of the hacking of Chad Wolf, the former acting secretary of the DHS, and of email accounts belonging to officials in the department’s cybersecurity staff, was not publicly known. The DHS did not immediately respond to a request for comment. In the security breach at SolarWinds which came to light in December, hackers infiltrated the U.S. tech company’s network management software and added code that allowed them to spy on end users. The hackers penetrated nine federal agencies and 100 companies. Last week, Reuters reported that a planned Biden administration executive order would require many software vendors to notify their federal government customers when the companies have a cybersecurity breach."
https://venturebeat.com/2021/03/29/5g-will-inspire-new-kinds-of-games-not-just-better-ones/,"5G will inspire new kinds of games, not just better ones","Presented by Verizon It’s no surprise that the latest generation of video game consoles is more popular than the average collectible sneaker launch. The wave of excitement that comes with each shoe launch inspires competition among collectors, and those same forces push gamers to scour retailers in hopes of unlocking hours of entertainment with a new gaming console. Now imagine if after getting that coveted game, you still have to fight to play. Imagine all that excitement has to be tempered because the next-gen game relied on last-gen infrastructure that wasn’t made to handle it. Mobile gamers and developers don’t need to imagine that; that’s been their reality — until now. I spent much of 2020 speaking to people who wondered, “Why do I need 5G?” “Gaming” was one of my favorite replies. 5G can offer speeds significantly faster than 4G LTE and, more importantly, it can enable ultra-low lag and massive capacity. Players can tap a button and get an almost instant response, which makes all the difference when a split second can influence who wins and who waits for the next round, lamenting their loss. Niantic Founder and CEO John Hanke recently shared that 100,000 players at a Pokémon GO event in Germany once pushed local capacity to hang on “by the skin of our teeth.” The promise of 5G is made clear in those moments. It’s not simply because current games and players demand better performance, but because new capabilities can empower bolder ideas to bring the real world and virtual together. The 5G era presents an opportunity for truly disruptive, immersive experiences that require more than building a faster horse. This is a moment for automobiles, or better yet, spaceships. 5G is a supercharged rocket ready to transform mobile and cloud gaming, and we should think bigger about what comes next. 5G can enable faster response times, so let’s build games that support advanced AR gaming competitions anywhere at any time. Cloud-based gaming can be vastly superior with 5G, so now is as good a time as any to blur the lines between console and mobile even further. Let’s reimagine virtual drone racing or cross-platform showdowns between friends in the park. Multiplayer is a must-have experience for many gamers, and 5G offers an opportunity to invite more players and create more challenging scenarios. John Hanke and I explored what that might mean when discussing 5G and the metaverse, the convergence of the physical and virtual worlds, but there’s so much more to discover and build to get towards that future. While gamers might crave better graphics and faster load times, titles are ultimately judged by the experiences they create. What experiences can we deliver with 5G that weren’t previously available? I’d be interested in knowing what choices a developer might make if network performance was no longer a major concern. If we can bring the computing power to the edge of the network, as we’re doing with 5G Edge, there’s more room for development, creativity and capability. Games can be more immersive and dynamic in ways we probably can’t even imagine. When I think about 5G’s impact on every industry, this illustration showing the difference in developer perception drives the point home beautifully:  5G has the potential to nurture more creativity, so Verizon built 5G for gamers. We worked closely with esports pros, device manufacturers, and game developers to create the infrastructure necessary for next-gen gaming. Building our 5G Labs has taught us how to create a superior connection that empowers developers and players, and we’re constantly seeking ways to advance our edge on network experience. We often host events to showcase the power of our 5G Ultra Wideband network, but a switch to virtual events inspired us to think bigger. In February, Verizon hosted the largest activation ever built in Fortnite’s Creative Mode, and 5G was at the heart of its development. More than 40 million Fortnite players entered the Verizon 5G Stadium to explore the unique environment, play games, and meet star athletes and pro gamers. It was the biggest virtual event we’ve done yet, and it showed us there’s so much more we can do. Today, Verizon 5G Ultra Wideband is available in 67 cities and 54 stadiums and arenas, and we plan to grow those numbers by the end of 2021. In fact, over the next 12 months, we expect to have incremental 5G bandwidth via new C-band spectrum available to 100 million people in 46 markets, delivering 5G on C-Band spectrum. As we continue to build the infrastructure that enables new thinking in gaming, we seek games that truly take advantage of 5G’s promise. There could be games with more dynamic AR layers unlike what we’ve seen; games that can handle 100,000 players in one area and thousands more in another nearby location. The network that can meet the needs of tomorrow’s gamers is already here and rapidly expanding. Now it’s time to see an ecosystem built on 5G grow even faster. At Verizon, we’re eager to hear from game developers and other industry players ready to level up, so we encourage them to visit verizon5glabs.com/gaming/ and explore what’s possible. We look forward to seeing the ambition 5G inspires once mobile and cloud gaming developers have the freedom to create without limits. I can’t wait to put on my far-from-collectible shoes and venture out to explore the next gaming experience. Ronan Dunne is EVP & CEO, Verizon Consumer Group. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/29/cere-network-raises-5-million-to-create-decentralized-data-cloud-platform/,Cere Network raises $5 million to create decentralized data cloud platform,"Cere Network has raised $5 million for its decentralized data cloud (DDC) platform, which is launching today for developers. The company’s ambition is to take on data cloud leader Snowflake. The investment was led by Republic Labs, the investment arm of crowdsourced funding platform Republic. Other investors include Woodstock Fund, JRR Capital, Ledger Prime, G1 Ventures, ZB exchange, and Gate.io exchange. Cere Network previously raised $5 million from Binance Labs and Arrington XRP Capital, amongst others, bringing its total raised to $10 million. “Enterprises using Snowflake are still constrained by bureaucratic data acquisition processes, complex and insufficient cloud security practices, and poor AI/ML governance,” Cere Network CEO Fred Jin said in an email to VentureBeat. “Cere’s technology allows more data agility and data interoperability across different datasets and partners, which extracts more value from the data faster compared to traditional compartmentalized setup.” The Cere DDC platform launches to developers today, which allows thousands of data queries to be hosted on the blockchain, the transparent and secure digital ledger. The platform offers a more secure first-party data foundation in the cloud by using blockchain identity and data encryption to onboard and segment individual consumer data. This data is then automated into highly customizable and interoperable virtual datasets, directly accessible in near real time by all business units, partners/vendors, and machine-learning processes. The Cere token will be used to power its decentralized data cloud and fuel Cere’s open data marketplace that allows for trustless data-sharing among businesses and external data specialists, as well as staking and governance. The public sale of the Cere token will be held on Republic, the first token sale on the platform. “We’ve been following Cere Network for some time and have been impressed with the team and the market fit – and need – for a decentralized data cloud,” said Boris Revsin, managing director of Republic Labs, in a statement. “We’re very excited to host Cere Network’s token sale on Republic, which will ensure a decentralized network and faster adoption in the enterprise space of blockchain technology. Their DDC improves upon Snowflake using blockchain identity and data encryption to onboard and segment individual consumer data.” Developers can access the Cere DDC here. The public sale for Cere token is scheduled for March 31 on Republic. The company said it is working with a number of Fortune 1,000 customers. “There’s a huge amount of opportunities in this rapidly shifting space for the coming years. We don’t plan to take on the likes of Snowflake head on, yet, but rather focus on specific solutions and verticals where we can bring more customization and efficiency. We are ok with chipping away at their lead while doing this,” Jin said. “We are bringing an open data marketplace which will open up data access beyond the limitation of traditional silo’d data ecosystems, which include Snowflake, and the likes of Salesforce.”"
https://venturebeat.com/2021/03/29/honeywell-says-quantum-computers-will-outpace-standard-verification-in-18-to-24-months/,Honeywell says quantum computers will outpace standard verification in ’18 to 24 months’,"Honeywell expects that as advances in quantum computing continue to accelerate over the next 18 to 24 months, the ability to replicate the results of a quantum computing application workload using a conventional computing platform simulation will come to an end. The company’s System Model H1 has now quadrupled its performance capabilities to become the first commercial quantum computer to attain a 512 quantum volume. Ascertaining quantum volume requires running a complex set of statistical tests that are influenced by the number of qubits, error rates, connectivity of qubits, and cross-talk between qubits. That approach provides a more accurate assessment of a quantum computer’s processing capability that goes beyond simply counting the number of qubits that can be employed. Honeywell today provides access to a set of simulation tools that make it possible to validate the results delivered on its quantum computers on a conventional machine. Those simulations give organizations more confidence in quantum computing platforms by allowing them to compare results. However, quantum computers are now approaching a level where at some point between 2022 and 2023 that will no longer be possible, Honeywell Quantum Solutions president Tony Uttley said. Honeywell has pursued an approach to quantum computing that differs from those of rivals by focusing its efforts on a narrower range of more stable qubits. Each system is based on a trapped-ion architecture that leverages numerous individual charged atoms (ions) to hold information. It then applies electromagnetic fields to hold (trap) each ion in a way that allows it to be manipulated and encoded using laser pulses. The company makes its quantum computers available via a subscription to a cloud service and counts BMW, DHL, JP Morgan Chase, and Samsung among its customers. Systems residing outside of Boulder, Colorado and Minneapolis are made available to customers for up to two weeks at a time before being taken offline for two weeks to add additional capacity. Subscriptions for the System Model H1 service are currently sold out, and each Honeywell quantum computing customer has previously tried to employ a different platform before switching to Honeywell, Uttley said. The company is now moving toward making a third-generation System Model H2 service available that will offer higher levels of unspecified quantum volume, Uttley added. Honeywell has committed to delivering a tenfold increase in quantum volume every five years. The company has been able to deliver a fourfold increase in the amount of quantum volume it can make available in the last five months alone, Uttley said. Quantum computers can process bits that have a value of both 0 and 1 at the same time, which makes them more powerful than conventional computing platforms. Advances in quantum computing, however, will by no means signal the demise of conventional computers, Uttley added. Instead, it’s becoming apparent that quantum computers and conventional computers are simply going to be better suited to running different classes of workloads, Uttley said. “These systems will run side by side for decades,” Uttley added. “Conventional computing platforms are not going to be replaced anytime soon.” Quantum computers, however, are better suited to addressing complex computational challenges involving chemistry, routing optimizations using, for example, logistics and traffic management applications, and even the training of AI models. In the latter case, a quantum computer can identify the starting point for the training of an AI model that would then be completed by a conventional computer. Other more intractable problems involving, for example, applications for ways to reduce the level of carbon in the atmosphere are only feasible to run on a quantum computing platform. It may still be a while before quantum computing delivers on its full promise, but while the way quantum systems work may not be widely understood, there is now no turning back."
https://venturebeat.com/2021/03/29/how-tiktok-is-evolving-sequential-marketing/,How TikTok is evolving sequential marketing,"Presented by GrowMojo Intelligent Tracking Prevention (ITP) is here, but fortunately there is a solution to allow businesses to continue to effectively target their ideal customers. In the past, marketers have thrived with advertising on Facebook and Google platforms by using cookies and pixels to target potential customers. Unfortunately, cookies and pixels have been abused by bad actors, so the push for more privacy has put cookies and pixels in the crosshairs of privacy advocates. Apple has reacted with their iOS 14 update that will make it much harder for tracking across websites. Google is now being forced to follow the lead of the other browsers, so Chrome will soon be blocking third-party cookies. Facebook is begrudgingly following along and is forcing their advertisers to phase out their Facebook pixel. Since the marketing strategy of following potential customers wherever they might be on the web is going to disappear (yes, that means you aren’t going to be seeing those shoes you once looked at in ads everywhere you go), marketers will have to develop new ways to target and track future customers. Fortunately, there is a new marketing opportunity that can solve this challenge for marketers: advertising on TikTok using observable signals to build a direct relationship with potential customers. If you’re a marketer, you’re probably asking, “How do they know my customer is on TikTok?” The answer is simple. Everyone’s customer is on TikTok. TikTok started in 2020 with about 500 million users, and about 40 million of them are in the U.S. Now TikTok has well over a billion users with 200 million of them in the U.S. The TikTok app was the number-one most-downloaded app in 2020, and the TikTok community continues to grow at an amazing rate. The average TikTok user spends 28 minutes per day on the app. That is only six minutes less than the average amount of time spent on Facebook each day. New, fast-growing advertising platforms come around once every ten years or so. The emergence of Google and then Facebook accelerated the growth of businesses that were early adopters, but the new advertising opportunities were initially trials by fire. Leading marketing agencies partnered with fast-growing Google and Facebook to help brands quickly find a return on investment to make advertising on these new platforms successful. There is now a small group of TikTok-authorized agencies like GrowMojo that helps brands find quick success on TikTok. So how will TikTok help you convert those users into customers? To answer that question, we first need to talk about sequential marketing which is the process of walking a potential customer down your purchase journey before they ever reach your website.  The main reason why TikTok leads the evolution of sequential marketing is because nothing works better in sequential marketing than video — and TikTok is all about video. TikTok’s other main advantage is that your advertising on TikTok is mixed in with all the other organic TikToks. The best TikTok ads are the ones that blend in well with the organic content, so that your TikTok ads receive comments, likes, and shares at the same rate as organic TikToks. Then, when users have liked or watched 75%+ of your TikTok ad, you can easily retarget the users and walk them down your purchase journey. Sequential marketing is a combination of art and science. The artistic component is the marketer’s talent in creating an ad that feels organic to the TikTok user at first glance. The science involves measuring the engagement of your first qualifying video to determine which users are showing the level of interest that warrants showing them a second video. Once you know they are a potential customer, because they engaged with your TikTok ad, you can use sequential marketing to target them with more sales-oriented ads and move them down your product or service purchase journey. So, if you are the CMO of a B2B company or you target enterprise customers, you probably are thinking that this sequential marketing TikTok strategy doesn’t apply to you. Wrong. This strategy works for all types of products and services whether you are targeting consumers or enterprise decision-makers. It also works well for franchise marketing, B2B, and even long sales cycle products. You just need an organic-feeling TikTok video advertisement and then you will be able to start qualifying your potential customers. Additionally, if you have a list of leads, there is no cheaper and easier way to target people most similar to your target customers than to do it on TikTok. You simply need to upload your target list into TikTok and you are ready to sequentially market to the most similar prospects. There is nothing wrong with inserting your ad, whatever it might be, among the TikTok dance challenges and funny pranks. If you do it the right way, users will raise their hands to show their interest in your product or service, qualifying them for the next step in the marketing sequence. Finding new prospects is far more cost effective on TikTok than on Facebook, Google, or LinkedIn because TikTok’s paid ads can blend in more organically, leading to relevant connections when sharing your brand’s story. The solution to reaching potential customers hidden in a pool of a billion users is to aggressively filter them and funnel those most enthusiastic for your product or service. Success comes from getting the details right with sequential marketing. Not many brands have figured it out yet. If you want to identify and convert qualified customers in TikTok’s huge user base, use GrowMojo TikTok Marketing Agency to help you implement a successful sequential marketing program. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/29/facebook-and-google-partner-on-undersea-cables-to-link-north-america-with-southeast-asia/,Facebook and Google partner on undersea cables to link North America with Southeast Asia,"(Reuters) — Facebook said today it is planning two new undersea cables to connect Singapore, Indonesia, and North America in a project with Google and regional telecommunication companies to boost internet connection capacity between the regions. “Named Echo and Bifrost, those will be the first two cables to go through a new diverse route crossing the Java Sea, and they will increase overall subsea capacity in the trans-pacific by about 70%,” Facebook VP of Network Investments Kevin Salvadori told Reuters. He declined to specify the size of the investment but said it was “a very material investment for us in Southeast Asia.” The cables, according to Salvadori, will be the first to directly connect North America to some of the main parts of Indonesia and will increase connectivity for the central and eastern provinces of the world’s fourth most populous country. Salvadori said Echo is being built in partnership with Alphabet’s Google and Indonesian telecommunications company XL Axiata and should be completed by 2023. Bifrost is being done in partnership with Telin, a subsidiary of Indonesia’s Telkom, and Singaporean conglomerate Keppel and is due to be completed by 2024. The two cables, which will need regulatory approval, follow previous investments by Facebook to build up connectivity in Indonesia, one of its top five markets globally. While 73% of Indonesia’s population of 270 million are online, the majority access the web through mobile data, with less than 10% using a broadband connection, according to a 2020 survey by the Indonesian Internet Providers Association. Swathes of the country remain without any internet access. Facebook said last year it would deploy 3,000 km (1,8641 miles) of fiber optic cable in Indonesia across 20 cities, in addition to a previous deal to develop public Wi-Fi hot spots. In addition to the Southeast Asian cables, Facebook is continuing with its broader subsea plans in Asia and globally, including the Pacific Light Cable Network (PLCN), Salvadori said. “We are working with partners and regulators to meet all of the concerns that people have, and we look forward to that cable being a valuable, productive transpacific cable going forward in the near future,” he said. The 12,800 km PLCN, which is being funded by Facebook and Alphabet, had met U.S government resistance over plans for a Hong Kong conduit. It was originally intended to link the United States, Taiwan, Hong Kong, and the Philippines. Facebook said earlier this month it would drop efforts to connect the cable between California and Hong Kong due to “ongoing concerns from the U.S. government about direct communication links between the United States and Hong Kong.”"
https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/,MIT study finds ‘systematic’ labeling errors in popular AI benchmark datasets,"The field of AI and machine learning is arguably built on the shoulders of a few hundred papers, many of which draw conclusions using data from a subset of public datasets. Large, labeled corpora have been critical to the success of AI in domains ranging from image classification to audio classification. That’s because their annotations expose comprehensible patterns to machine learning algorithms, in effect telling machines what to look for in future datasets so they’re able to make predictions. But while labeled data is usually equated with ground truth, datasets can — and do — contain errors. The processes used to construct corpora often involve some degree of automatic annotation or crowdsourcing techniques that are inherently error-prone. This becomes especially problematic when these errors reach test sets, the subsets of datasets researchers use to compare progress and validate their findings. Labeling errors here could lead scientists to draw incorrect conclusions about which models perform best in the real world, potentially undermining the framework by which the community benchmarks machine learning systems. A new paper and website published by researchers at MIT instill little confidence that popular test sets in machine learning are immune to labeling errors. In an analysis of 10 test sets from datasets that include ImageNet, an image database used to train countless computer vision algorithms, the coauthors found an average of 3.4% errors across all of the datasets. The quantities ranged from just over 2,900 errors in the ImageNet validation set to over 5 million errors in QuickDraw, a Google-maintained collection of 50 million drawings contributed by players of the game Quick, Draw! The researchers say the mislabelings make benchmark results from the test sets unstable. For example, when ImageNet and another image dataset, CIFAR-10, were corrected for labeling errors, larger models performed worse than their lower-capacity counterparts. That’s because the higher-capacity models reflected the distribution of labeling errors in their predictions to a greater degree than smaller models — an effect that increased with the prevalence of mislabeled test data. In choosing which datasets to audit, the researchers looked at the most-used open source datasets created in the last 20 years, with a preference for diversity across computer vision, natural language processing, sentiment analysis, and audio modalities. In total, they evaluated six image datasets (MNIST, CIFAR-10, CIFAR-100, Caltech-256, and ImageNet), three text datasets (20news, IMDB, and Amazon Reviews), and one audio dataset (AudioSet). The researchers estimate that QuickDraw had the highest percentage of errors in its test set, at 10.12% of the total labels. CIFAR was second, with around 5.85% incorrect labels, while ImageNet was close behind, with 5.83%. And 390,000 label errors make up roughly 4% of the Amazon Reviews dataset. Errors included: A previous study out of MIT found that ImageNet has “systematic annotation issues” and is misaligned with ground truth or direct observation when used as a benchmark dataset. The coauthors of that research concluded that about 20% of ImageNet photos contain multiple objects, leading to a drop in accuracy as high as 10% among models trained on the dataset. In an experiment, the researchers filtered out the erroneous labels in ImageNet and benchmarked a number of models on the corrected set. The results were largely unchanged, but when the models were evaluated only on the erroneous data, those that performed best on the original, incorrect labels were found to perform the worst on the correct labels. The implication is that the models learned to capture systematic patterns of label error in order to improve their original test accuracy. In a follow-up experiment, the coauthors created an error-free CIFAR-10 test set to measure AI models for “corrected” accuracy. The results show that powerful models didn’t reliably perform better than their simpler counterparts because performance was correlated with the degree of labeling errors. For datasets where errors are common, data scientists might be misled to select a model that isn’t actually the best model in terms of corrected accuracy, the study’s coauthors say. “Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets,” the researchers wrote. “It is imperative to be cognizant of the distinction between corrected versus original test accuracy and to follow dataset curation practices that maximize high-quality test labels.” To promote more accurate benchmarks, the researchers have released a cleaned version of each test set in which a large portion of the label errors have been corrected. The team recommends that data scientists measure the real-world accuracy they care about in practice and consider using simpler models for datasets with error-prone labels, especially for algorithms trained or evaluated with noisy labeled data. Creating datasets in a privacy-preserving, ethical way remains a major blocker for researchers in the AI community, particularly those who specialize in computer vision. In January 2019, IBM released a corpus designed to mitigate bias in facial recognition algorithms that contained nearly a million photos of people from Flickr. But IBM failed to notify either the photographers or the subjects of the photos that their work would be canvassed. Separately, an earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. In July 2020, the creators of the 80 Million Tiny Images dataset from MIT and NYU took the collection offline, apologized, and asked other researchers to refrain from using the dataset and to delete any existing copies. Introduced in 2006 and containing photos scraped from internet search engines, 80 Million Tiny Images was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.” The dataset also contained pornographic content like nonconsensual photos taken up women’s skirts. Biases in these datasets not uncommonly find their way into trained, commercially available AI systems. Back in 2015, a software engineer pointed out that the image recognition algorithms in Google Photos were labeling his Black friends as “gorillas.” Nonprofit AlgorithmWatch showed Cloud Vision API automatically labeled a thermometer held by a dark-skinned person as a “gun” while labeling a thermometer held by a light-skinned person as an “electronic device.” And benchmarks of major vendors’ systems by the Gender Shades project and the National Institute of Standards and Technology (NIST) suggest facial recognition technology exhibits racial and gender bias and facial recognition programs can be wildly inaccurate, misclassifying people upwards of 96% of the time. Some in the AI community are taking steps to build less problematic corpora. The ImageNet creators said they plan to remove virtually all of about 2,800 categories in the “person” subtree of the dataset, which were found to poorly represent people from the Global South. And this week, the group released a version of the dataset that blurs people’s faces in order to support privacy experimentation."
https://venturebeat.com/2021/03/28/ai-could-help-advertisers-recover-from-loss-of-third-party-cookies/,AI could help advertisers recover from loss of third-party cookies,"Options for targeting digital advertising in a way that doesn’t rely on cookies are increasing, thanks to advances in predictive analytics and AI that will ultimately lessen the current dominance of Google, Facebook, and other large-scale content aggregators. Google announced earlier this month that it will no longer allow third-party cookies to collect data via its Chrome browser. Many companies have historically relied on those cookies to better target their digital advertising, as the cookies enable digital ad networks and social media sites to create a profile of an end user without knowing specifically who that individual is. While that approach doesn’t necessarily breach anyone’s privacy, it does give many users the feeling that some entity is tracking the sites they visit in a way that makes them uncomfortable. Providers of other browsers, such as Safari from Apple and the open source Firefox browser, have already abandoned third-party cookies. To be clear, Google isn’t walking away from tracking user behavior. Instead, the company has created a Federated Learning of Cohorts (FLoC) mechanism to track user behavior that doesn’t depend on cookies to collect data. Instead of being able to target an ad to a specific anonymous user, advertisers are presented with an opportunity to target groups of end users that are now organized into cohorts based on data Google still collects. It remains to be seen how these initiatives might substantially change the user experience. However, some advertisers are now looking to employ machine learning algorithms and other forms of advanced analytics being made available via digital advertising networks to reduce their dependency on Google, Facebook, Twitter, Microsoft, and other entities that control massive online communities. For example, Equifax, a credit reporting agency, is working with Quantcast to place advertising closer to where relevant content is being originally created and consumed, said Joella Duncan, director of media strategy for North America at Equifax. “We want our marketing teams to be able to pull more levers,” Duncan said. “Third-party cookies are stale.” That approach provides the added benefit of lessening an advertiser’s dependency on walled online gardens dominated by a handful of companies, Quantcast CEO Konrad Feldman said. At the core of the Quantcast platform is an Ara engine that applies machine learning algorithms to data collected from 100 million online destinations in real time. That data is then analyzed using a set of predictive models that surface the behavioral patterns that make it possible to target ad campaigns. Those predictive models are scored a million times per second, in addition to being continuously updated to reflect recent events across the internet. “We’re not dependent on only one technique,” Feldman said. That capability not only benefits clients such as Equifax, it also enables publishers of original content to retain a larger share of the advertising revenue generated. Google, Facebook, and Microsoft are all now moving toward compensating publishers for content that appears on their sites, but the bulk of the advertising revenue will still wind up in their coffers. Quantcast is making a case for an alternative approach to digital advertising that would make it more evenly distributed. Advertisers are not likely to walk away from walled online gardens that make it cost-efficient for them to target millions of users. However, many of those same advertisers are looking for a way to more efficiently target narrower audience segments that might have a greater affinity for their products and services based on the content they regularly consume. The AI and advanced analytics capabilities being embedded within digital advertising platforms may not upend the business models used by Google, Facebook, and others and based on walled gardens that themselves were constructed using algorithms. But it’s becoming apparent that fissures in the walls of those gardens are starting to appear as other entities in the world of advertising apply their own AI countermeasures."
https://venturebeat.com/2021/03/28/reinforcement-learning-the-next-great-ai-tech-moving-from-the-lab-to-the-real-world/,Reinforcement learning: The next great AI tech moving from the lab to the real world,"Reinforcement learning (RL) is a powerful type of artificial intelligence technology that can be used to learn strategies to optimally control large, complex systems such as manufacturing plants, traffic control systems (road/train/aircraft), financial portfolios, robots, etc. It is currently transitioning from research labs to highly impactful, real world applications. For example, self-driving car companies like Wayve and Waymo are using reinforcement learning to develop the control systems for their cars. AI systems that are typically used in industry perform pattern recognition to make a prediction. For instance, they may recognize patterns in images to detect faces (face detection), or recognize patterns in sales data to predict a change in demand (demand forecasting), and so on. Reinforcement learning methods, on the other hand, are used to make optimal decisions or take optimal actions in applications where there is a feedback loop. An example where both traditional AI methods and RL may be used, but for different purposes, will make the distinction clearer. Say we are using AI to help operate a manufacturing plant. Pattern recognition may be used for quality assurance, where the AI system uses images and scans of the finished product to detect any imperfections or flaws. An RL system, on the other hand, would compute and execute the strategy for controlling the manufacturing process itself (by, for example, deciding which lines to run, controlling machines/robots, deciding which product to manufacture, and so on). The RL system will also try to ensure that the strategy is optimal in that it maximizes some metric of interest — such as the output volume — while maintaining a certain level of product quality. The problem of computing the optimal control strategy, which RL solves, is very difficult for some subtle reasons (often much more difficult than pattern recognition). In computing the optimal strategy, or policy in RL parlance, the main challenge an RL learning algorithm faces is the so-called “temporal credit assignment” problem. That is, the impact of an action (e.g. “run line 1 on Wednesday”) in a given system state (e.g. “current output level of machines, how busy each line is,” etc.) on the overall performance (e.g. “total output volume”) is not known until after (potentially) a long time. To make matters worse, the overall performance also depends on all the actions that are taken subsequent to the action being evaluated. Together, this implies that, when a candidate policy is executed for evaluation, it is difficult to know which actions were the good ones and which were the bad ones — in other words, it is very difficult to assign credit to the different actions appropriately. The large number of potential system states in these complex problems further exacerbates the situation via the dreaded “curse of dimensionality.”  A good way to get an intuition for how an RL system solves all these problems at the same time is by looking at the recent spectacular successes they have had in the lab. Many of the recent, prominent demonstrations of the power of RL come from applying them to board games and video games. The first RL system to impress the global AI community was able to learn to outplay humans in different Atari games when only given as input the images on screen and the scores received by playing the game. This was created in 2013 by London-based AI research lab Deepmind (now part of Alphabet Inc.). The same lab later created a series of RL systems (or agents), starting with the AlphaGo agent, which were able to defeat the top players in the world in the board game Go. These impressive feats, which occurred between 2015 and 2017, took the world by storm because Go is a very complex game, with millions of fans and players around the world, that requires intricate, long-term strategic thinking involving both the local and global board configurations. Subsequently, Deepmind and the AI research lab OpenAI have released systems for playing the video games Starcraft and DOTA 2 that can defeat the top human players around the world. These games are challenging because they require strategic thinking, resource management, and control and coordination of multiple entities within the game. All the agents mentioned above were trained by letting the RL algorithm play the games many many times (e.g. millions or more) and learning which policies work and which do not against different kinds of opponents and players. The large number of trials were possible because these were all games running on a computer. In determining the usefulness of various policies, the RL algorithm often employed a complex mix of ideas. These include hill climbing in policy space, playing against itself, running leagues internally amongst candidate policies or using policies used by humans as a starting point and properly balancing exploration of the policy space vs. exploiting the good policies found so far. Roughly speaking, the large number of trials enabled exploring many different game states that could plausibly be reached, while the complex evaluation methods enabled the AI system to determine which actions are useful in the long term, under plausible plays of the games, in these different states. A key blocker in using these algorithms in the real world is that it is not possible to run millions of trials. Fortunately, a workaround immediately suggests itself: First, create a computer simulation of the application (a manufacturing plant simulation, or market simulation etc.), then learn the optimal policy in the simulation using RL algorithms, and finally adapt the learned optimal policy to the real world by running it a few times and tweaking some parameters. Famously, in a very compelling 2019 demo, OpenAI showed the effectiveness of this approach by training a robot arm to solve the Rubik’s cube puzzle one-handed. For this approach to work, your simulation has to represent the underlying problem with a high degree of accuracy. The problem you’re trying to solve also has to be “closed” in a certain sense — there cannot be arbitrary or unseen external effects that may impact the performance of the system. For example, the OpenAI solution would not work if the simulated robot arm was too different from the real robot arm or if there were attempts to knock the Rubik’s cube out of the real robot arm (though it may naturally be — or be explicitly trained to be — robust to certain kinds of obstructions and interferences). These limitations will sound acceptable to most people. However, in real applications it is tricky to properly circumscribe the competence of an RL system, and this can lead to unpleasant surprises. In our earlier manufacturing plant example, if a machine is replaced with one that is a lot faster or slower, it may change the plant dynamics enough that it becomes necessary to retrain the RL system. Again, this is not unreasonable for any automated controller, but stakeholders may have far loftier expectations from a system that is artificially intelligent, and such expectations will need to be managed. Regardless, at this point in time, the future of reinforcement learning in the real world does seem very bright. There are many startups offering reinforcement learning products for controlling manufacturing robots (Covariant, Osaro, Luffy), managing production schedules (Instadeep), enterprise decision making (Secondmind), logistics (Dorabot), circuit design (Instadeep), controlling autonomous cars (Wayve, Waymo, Five AI), controlling drones (Amazon), running hedge funds (Piit.ai), and many other applications that are beyond the reach of pattern recognition based AI systems. Each of the Big Tech companies has made heavy investments in RL research — e.g. Google acquiring Deepmind for a reported £400 million (approx $525 million) in 2015. So it is reasonable to assume that RL is either already in use internally at these companies or is in the pipeline; but they’re keeping the details pretty quiet for competitive advantage reasons. We should expect to see some hiccups as promising applications for RL falter, but it will likely claim its place as a technology to reckon with in the near future. M M Hassan Mahmud is a Senior AI and Machine Learning Technologist at Digital Catapult, with a background in machine learning within academia and industry."
https://venturebeat.com/2021/03/28/industry-clouds-could-be-the-next-big-thing/,Industry clouds could be the next big thing,"Despite predictions of a cloud shift accelerated by the pandemic and Gartner projecting a $651 billion public cloud market in 2024, organizations have barely scratched the surface of public cloud adoption. So it might seem odd at this stage to ask, “What’s the next big thing in public clouds?” The war between traditional on-premises data center infrastructure providers such as Dell, HPE, and Cisco and the public cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud is far from over. However, one opportunity worth examining is industry clouds. Industry clouds are collections of cloud services, tools, and applications optimized for the most important use cases in a specific industry. APIs, common data models and workflows, and other components are available to customize capabilities. Industry cloud solutions from major public cloud providers also typically offer a variety of software and services, including industry-specific applications, from partners. For example, Microsoft and SAP partner to deliver SAP supply chain solutions through Microsoft Cloud for Manufacturing. Industry clouds are of interest because of their potential to create value for both customers and public cloud providers. Established companies in industries feeling the sting of competition from cloud-native disrupters are especially good prospects for these types of solutions. For these companies, moving their core business applications to general-purpose public clouds can be challenging because they often rely on homegrown legacy applications or industry-specific software designed for on-premise data centers. These companies face a difficult choice. Simply “lifting and shifting” applications to the cloud could result in sub-optimal performance. Yet rewriting or optimizing them for the cloud would be time consuming and costly. Industry clouds have the potential to accelerate and take the risk out of their cloud migrations. An essential component of an industry cloud is that it must address the specific requirements of the industry it is designed to serve. For example, healthcare providers place a high priority on improving the patient experience but also require high levels of security, data protection, and privacy. These are necessary to demonstrate compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. Financial services companies value data analytics and AI for customer insights and new product development, and trading applications require latency measured in fractions of a second. Like healthcare, the financial services industry is a highly regulated industry. Specific characteristics of the retail industry include the need to continually collect and analyze large sets of data to improve inventory management. For some of these requirements — and especially when there are several in combination — general-purpose cloud solutions might not be enough. And given this has been the focus of most cloud migrations thus far, many traditional companies in highly competitive industries have fallen behind in the race to the cloud. This means they are not realizing anywhere near the value they could from adding public clouds to their IT infrastructures. In addition to public cloud, there are many industry specific SaaS options and new ones emerging. For example, in the healthcare industry, there are electronic health record (EHR) SaaS options available. Healthcare SaaS offerings include critical functions such as billing and supply chain. Another example is the pharmaceutical and life sciences vertical. Pharmaceutical SaaS offerings support clinical, medical, and compliance functions. The important point to highlight is that SaaS has been and continues to be the top cloud migration choice with a projected market spend for 2021 of  $117.7 billion according to Gartner. SaaS is an excellent choice for supporting industry specific needs, and the big hyperscalers have taken notice. Given the opportunity in industry clouds, it’s not surprising that Amazon, Microsoft, Google, and IBM now all offer a broad range of industry-specific cloud solutions. For long-established companies such as IBM and Microsoft, this development mirrors that of their computer and software businesses, which evolved from providing customers with technology to solve their business problems. Both IBM and Microsoft have long histories of vertical market experience and large vertical market customer bases they can leverage to build and support industry clouds. This gives them an advantage with some customers. But for all public cloud providers, industry clouds are a logical next step in the ongoing maturation of public clouds. The industry cloud opportunity has also attracted the attention of cloud service providers that offer support for migrating industry-specific applications to public clouds. Before the cloud, cottage industries sprung up to help customers of industry-standard applications deploy and maintain them. Some of these companies have evolved their businesses to support cloud migrations of these applications. For example, in healthcare, where Epic and Cerner dominate the U.S. hospital EMR market, with 29% and 26% of the market, respectively, numerous firms exist to help companies bring these applications to the cloud. Given regulations and the business imperative to protect data in EMRs, most of these companies support a hybrid cloud approach, a solution that combines on-site data centers with public clouds. They also provide solutions and special expertise in privacy, security, and disaster recovery. While some host the applications on private clouds, many form partnerships with one or more public cloud providers. At the same time, Epic and Cerner are establishing their own relationships with public cloud providers. It’s worth noting that a failed relationship between Epic and Google offers an object lesson for cloud providers seeking to make their mark in specific industries. Epic severed ties with Google after it came to light that in its work with Ascension, a large Missouri-based health system, Google employees gained access to patient information without consent when information was being transferred from on-site servers to Google servers. Even though Ascension has continued to work with Google, Epic shifted its focus to Microsoft. Cerner, too, moved away from Google in favor of Amazon. It’s still early days for industry clouds, and no doubt some are more marketing strategies than offerings tailored for specific industries in meaningful ways. That will change. However, in the meantime, companies evaluating industry clouds from public cloud providers should do so carefully, taking care to compare not just the industry cloud offerings from different providers but also the industry cloud of each provider to their general-purpose solution. There might not yet be that much of a difference. Kash Shaikh is CEO and President of Virtana, a cloud platform with AI-powered observability for migrating, optimizing, and monitoring cloud applications."
https://venturebeat.com/2021/03/27/covid-has-moved-us-closer-to-an-industrial-ar-and-vr-revolution/,COVID has moved us closer to an industrial AR and VR revolution,"During the pandemic, many of us have gotten used to working from our laptop at the dining room table and dialing in to meetings on Zoom and Microsoft Teams. But for those in an industrial occupation, working in the field, and often executing critical tasks, table-bound remote working tools fail to fulfill the promise of a connected, collaborative future. I work for telecoms network provider Ciena, and our employees often need to share visual information, coach each other, and diagnose issues while working with both hands on the subject in question. In other instances, we need to provide live point-of-view information while touring facilities or demoing our equipment. If you’ve ever tried to do this with technology that’s commonly available, you know that it simply won’t do to walk around holding up a laptop or tablet all the time. So we’ve become early adopters of AR and VR technology and developed our own extended reality solution. I have seen the future, and I am here to tell you, the industrial AR and VR revolution is primed. We’re at a tipping point where this technology that started as a consumer curiosity is quickly filling an essential enterprise need. Extended reality solves the problem of not being able to physically see or touch what our colleagues are working on, particularly when teaching or instructing on highly technical systems. Until now, AR and VR have been rather niche and experimental. Pokémon GO and VR games have given us a taste of what the technology can do. But in these consumer use cases, slower networks with high latency never posed the hazards of a life-threatening situation or potential failure of a business-critical application. Industrial applications, like allowing first responders to see their colleagues through the walls of a burning building or overlaying parts with assembly instructions, simply won’t work without a sufficiently robust underlying communication network with high speeds and low latency. When employee safety and business continuity are on the line, these applications don’t have room for a jittery network. The sheer number and density of connected devices in these applications can make them a challenge to deploy, which is why 5G and optical networking will play such an important role in their adoption. Future networks will also require additional attention in terms of monitoring and managing the quality of connections to ensure that hiccups in service don’t have dramatic consequences. As for our own deployment, there was no one specific solution that perfectly fit our needs. Many of the vendors developing industry-grade solutions are focused on defense/military. In order to suit our needs, we needed to buy hardware and software from a variety of sources and knit them together to create our ideal solution. While it did take some effort to customize the component mix, the benefits we realized were immense. For one, extended reality offers more immersive and engaging learning experiences. When you’re in a virtual environment, you’re ensconced and there are few external distractions — it’s almost impossible to not be engaged. Of course, we’re still discovering new ways to use extended reality in the work environment, but we’ve had great success at my company getting employees to buy into the technology because we let them experiment and create their own use cases (outlined below). We challenge them to try new approaches to engage with our customers and offer internal education/suggestions. As an example, we recently used AR headsets to instruct our partners on product design and quality assurance at some of our manufacturing sites. By equipping our own employees with headsets, our customers have the option of viewing a stream of what our employees are seeing through their computers at home. The presentation is so compelling that we have even sent equipment to our customers to get the full experience. We have also used AR to give customers virtual/interactive demos from our labs and to equip our IT team to support our remote offices without needing to physically travel. From a usage standpoint, this makes sense when you look at Zoom over the last year. A year ago, many people were skeptical about using Zoom to hold in-depth, inclusive and collaborative meetings in a remote setting. Fast forward to today, and we’ve realized that the technology has enabled us to work from home, reduce the need for travel, and enjoy more time with our families. I see the same epiphany happening with extended reality. At this early stage, we have approximately two dozen employees involved in extended reality projects, with headsets distributed to users as needed. Each headset has cost us roughly the equivalent of a laptop. Considering that the hardware is a one-time cost, the price is eventually negated by not needing to have subject matter experts physically present. What we have realized is that the important investment at this stage is in user experience. I think the best advice I can offer to organizations considering extended reality right now is that you need to be thinking about the layers that bring the whole experience together. For example, you’ll need experts in UI design who will be mindful of where notifications and heads-up displays are placed within the field of view. Chances are your application of extended reality will be unique, and you’ll need a programmer who can build in features and functions to the solution that cater to your specific business need, like the ability to recognize objects or impose information on other users’ views. Perhaps most importantly, you’ll need to ensure smooth functionality, both for continuity and user comfort. Extended reality solutions use a lot of data, and nothing is more distracting than visuals that skip or lag. Even worse, visual information that isn’t synchronized with the user’s mental ability to process that image will cause a nauseating effect that will ruin the experience completely. Although there are only a handful of headset providers today, our own experience with AR and VR makes me believe that the market is about to bust open. Much like with the adoption of Zoom, we’ll soon be wondering what took us all so long to embrace the technology, and the market for solutions will be as diverse as the market for cameras or laptops are today. Craig Williams is CIO at Ciena."
https://venturebeat.com/2021/03/27/does-your-enterprise-plan-to-try-out-gpt-3-heres-what-you-should-know/,Does your enterprise plan to try out GPT-3? Here’s what you should know,"In a previous article, I talked about the market advantages enterprises could reap by developing applications using OpenAI’s GPT-3 natural language model. Here I want to provide a bit of a primer for companies taking a first look at the technology. There’s currently a waiting list to gain access to the GPT-3 API, but I’ve had an opportunity to play around in the system. For those who haven’t tried it out yet, here are a few things to be prepared for: The input you give GPT-3 is some seed text that you want to train the model on. This is the context you’re setting for GPT-3’s response. But you also provide a “prefix” to its response. This prefix is a direction that controls the text generated by the model, and it’s marked with a colon at the end. For example, you can give a paragraph as context and use a prefix like “Explain to a 5-year-old:” to generate a simple explanation. (It is highly recommended not to add any space after the prefix). Below is a sample response from GPT-3.  As you can see in the above example, your prefix doesn’t need to follow any complex machine-readable encoding. It is just a simple human-readable phrase. You can use multiple prefixes to describe a larger or extended context — as in a chatbot example. You want to provide a history of chat to help the bot generate responses. This context is used to tune the output of GPT-3 and generate response. For instance, you could make the chatbot helpful and friendly, or you could make it assertive and unfriendly. In the example below, I’ve given GPT-3 four prefixes. I’ve provided sample output for the first three and then left GPT-3 to continue from there.  Since the output you get from the model depends entirely on the context you provide, it’s important to construct these elements carefully. Configurations are the settings shown at right in the examples above. These are parameters that you include with your API call that help tune the response. For example, you can change the randomness of responses using the Temperature configuration setting, which has a range from 0 to 1. If Temperature is set to 0, every time you make a call with some context you will get the same response. If the Temperature is 1 then the response will be highly randomized. Another configurable you can tune is Response Length, which limits the text sent back by the API. Keep in mind that OpenAI charges for use of the platform on a token basis rather than a per-word basis. And a token will usually cover you for four characters. So, in the testing phase, make sure to tune your response length so you don’t use all of your tokens right away. With the 3 month free trail of GPT-3 you get $18 worth of tokens. I ended up consuming almost 75% of mine just with some experimentation with the API. There are actually four different versions of the GPT-3 model available as “engines,” and each of them has a different pricing model. The usual cost for tokens as of today is $0.06 per thousand tokens for the DaVinci engine, which is best-performing of the four. The less user-friendly engines, Curie, Babbage, and Ada, are $.006, $0.0012, and $0.0008 per thousand tokens respectively. GPT-3 is probably the most famous example of an advanced natural-language-processing API, but it’s likely to become one of many as the NLP ecosystem matures. Machine learning as a service (MLaaS) is a powerful business model because you can either spend the time and money to pre-train a model yourself (for context, GPT-3 cost OpenAI nearly $12 million to train), or you can purchase a pre-trained model for pennies on the dollar. In GPT-3’s case, every call you make to the API is routed to some shared instance of the GPT-3 model running in OpenAI’s cloud. As mentioned earlier, the DaVinci engine performs best, but you should experiment for yourself with each engine for specific use cases. DaVinci is forgiving if your input context has spelling mistakes or extra/missing spaces, and it gives a very readable response. You can sense it has been trained on a larger corpus and is resilient to errors. The cheaper engines will need you to do more work to frame the context and usually will need tuning to get exactly kind of response expected. Below is an example of classification of companies with misspelled name FedExt in the context. DaVinci is able to get right response while Ada gets it wrong. Again, when we look up a specific drug interaction example, DaVinci gets to the point and answers the question much better than Ada or Babbage:  GPT-3 is a stateless language model, which means it doesn’t remember your previous requests or learn from them. It relies solely on its original training (which pretty much constitutes all the text on the internet) and the context and configuration you provide it. This is the major hurdle for enterprises in adoption. You can generate some very interesting demos, but for GPT-3 to be a serious contender for real-world use cases in banking, healthcare, industrial, etc. we will need to train models that are domain specific. For example, you would want a model trained on your company’s internal policy documents or patient health records or machinery manuals. So, applications built directly on top of GPT-3 may not have actual use to enterprises. A more lucrative monetization scheme could be to host GPT-3-like models as an API specialized for specific problems like drug discovery, insurance policy recommendation, financial reports summarization, planning machinery maintenance, etc. The end use would be to leverage an application built on a model built on top of another model. A specialized model built by an enterprise on its proprietary data will also need to be able to adapt based on new knowledge obtained from business documents in order to stay relevant. In the future, we will see more domain language models with an active learning capability. And we will most likely see an active learning business model from GPT-3 eventually, too, where organizations will be able to train an instance incrementally on their custom data. However, this will come at a significant price point since it will require OpenAI to host a unique instance for that customer. Dattaraj Rao is Innovation and R&D Architect at Persistent Systems and author of the book Keras to Kubernetes: The Journey of a Machine Learning Model to Production. At Persistent Systems, he leads the AI Research Lab. He has 11 patents in machine learning and computer vision."
https://venturebeat.com/2021/03/27/what-is-dataops-and-why-its-a-top-trend/,"What‌ ‌is‌ ‌DataOps,‌ ‌and‌ ‌why‌ ‌it’s‌ ‌a‌ ‌top‌ ‌trend‌","Enterprises‌ ‌have‌ ‌struggled‌ ‌to‌ ‌collaborate‌ ‌well ‌around‌ ‌their‌ ‌data, which hinders their ability to adopt‌ ‌transformative‌ ‌applications‌ ‌like‌ ‌AI.‌ ‌ ‌The‌ ‌evolution‌ ‌of‌ ‌‌DataOps‌ ‌could‌ ‌fix that problem. The‌ ‌term‌ ‌DataOps‌ ‌emerged‌ ‌seven‌ ‌years‌ ‌ago‌ to refer to ‌best‌ ‌practices‌ for ‌getting‌ ‌proper‌ ‌analytics,‌ ‌and research firm Gartner calls it a major trend encompassing several steps in the data lifecycle. Just‌ as‌ ‌the‌ ‌DevOps‌ ‌trend‌ ‌led‌ ‌to‌ ‌a‌ ‌better‌ ‌process‌ ‌for‌ ‌collaboration‌ ‌between‌ ‌‌developers‌ ‌and‌ ‌operations‌ ‌teams,‌ ‌DataOps‌ ‌refers‌ ‌to closer collaboration between various teams handling data and operations teams deploying data into applications. ‌Getting‌ ‌DataOps‌ ‌right‌ ‌is‌ ‌a‌ ‌significant‌ ‌challenge‌ ‌because‌ ‌of‌ ‌the‌ ‌multiple‌ ‌stakeholders‌ ‌and‌ ‌processes‌ ‌involved‌ ‌in‌ ‌the‌ ‌data‌ ‌lifecycle.‌ In the DevOps world, enterprises ‌can‌ ‌develop,‌ ‌test,‌ ‌and‌ ‌deploy‌ ‌app‌ ‌updates‌ ‌in‌ ‌a‌ ‌matter‌ ‌of‌ ‌hours.‌ It is harder to move that fast in the data world, as it‌ can ‌take‌ ‌eight‌ ‌months‌ ‌to‌ integrate ‌an‌ ‌ML‌ ‌model‌ ‌into‌ business‌ ‌workflows‌ ‌and‌ deliver tangible value. ‌”[Creating]‌ ‌a‌ ‌common‌ ‌architecture‌ ‌pattern‌‌ helps‌ ‌with‌ ‌operationalizing‌ ‌data‌ ‌science‌ ‌and‌ ‌ML‌ ‌pipelines‌ ‌and‌ ‌has‌ ‌been‌ ‌identified‌ ‌as‌ ‌one‌ ‌of‌ ‌the‌ ‌major‌ ‌trends‌ ‌for‌ ‌2021,” Gartner research director ‌Soyeb‌ ‌Barot said.‌ ‌
‌
Gartner‌ ‌‌predicts ‌enterprises‌ ‌will‌ ‌begin‌ ‌to‌ ‌see‌ ‌real‌ ‌gains‌ ‌in‌ ‌these‌ ‌efforts‌ ‌through‌ ‌the‌ ‌evolution‌ ‌and‌ ‌extension‌ ‌of‌ ‌DataOps‌ ‌to‌ ‌support‌ ‌trusted‌ ‌AI.‌ ‌The research firm ‌predicts‌ ‌the‌ ‌number‌ ‌of‌ ‌enterprises‌ ‌that‌ ‌have‌ ‌operationalized‌ their‌ ‌AI‌ ‌efforts‌ ‌will‌ ‌grow‌ ‌from‌ ‌8%‌ ‌in‌ ‌2020‌ ‌to‌ ‌70%‌ ‌in‌ ‌2025‌ ‌due‌ ‌to‌ ‌the‌ ‌maturity‌ ‌of‌ ‌AI‌ ‌orchestration‌ ‌platforms.‌ ‌ ‌
‌ Even so, ‌enterprises‌ ‌will‌ ‌struggle‌ ‌to‌ ‌move‌ ‌their‌ ‌AI‌ ‌predictive‌ ‌projects‌ ‌past‌ the‌ ‌proof‌ ‌of‌ ‌concept‌ stage ‌because‌ ‌they‌ ‌have‌ ‌not‌ ‌addressed‌ ‌the‌ ‌full‌ ‌range‌ ‌of‌ ‌processes‌ ‌for‌ ‌collaborating‌ ‌across‌ ‌the‌ ‌AI‌ ‌lifecycle.‌ ‌A‌ 2019‌ ‌Gartner‌ ‌survey‌ ‌found‌ ‌that‌ ‌the‌ ‌top‌ ‌four‌ ‌challenges‌ ‌companies‌ ‌face‌ were ‌security‌ ‌or‌ ‌privacy‌ ‌concerns‌ ‌(30%)‌ ,‌ ‌complexity‌ ‌of‌ ‌AI‌ ‌integration‌ ‌with‌ ‌existing‌ ‌infrastructure‌ ‌(30%)‌, ‌data‌ ‌volume‌ ‌or‌ ‌complexity‌ ‌(22%),‌ ‌and‌ ‌potential‌ ‌risks‌ ‌or‌ ‌liabilities‌ ‌(22%).‌ ‌
‌
Gartner‌ ‌argues‌ ‌that‌ ‌a‌ ‌more‌ ‌nuanced‌ ‌way‌ ‌of‌ ‌thinking‌ ‌about‌ ‌different‌ ‌types‌ ‌of‌ ‌collaboration‌ ‌can‌ ‌improve‌ ‌this‌ ‌transition.‌ ‌This‌ ‌includes‌ ‌extending‌ ‌the‌ ‌older‌ ‌idea‌ ‌of‌ ‌DataOps‌ ‌(data‌ ‌engineering)‌ ‌to‌ ‌include‌ ‌MLOps‌ ‌(machine‌ ‌learning‌ ‌development),‌ ‌ModelOps‌ ‌(AI‌ ‌governance),‌ ‌and‌ ‌Platform‌ ‌Ops‌ ‌(overarching‌ ‌AI‌ ‌platform‌ ‌management).‌ ‌It‌ ‌has‌ ‌characterized‌ ‌this‌ ‌entire‌ ‌collection‌ ‌of‌ ‌capabilities‌ ‌as‌ ‌XOps.‌ ‌ ‌
‌
‌”These‌ ‌frameworks‌ ‌can‌ ‌help‌ ‌implement‌ ‌a‌ ‌structured‌ ‌process‌ ‌for‌ ‌the‌ ‌people‌ ‌involved‌ ‌to‌ ‌productionalize‌ ‌AI.‌ ‌Think‌ ‌of‌ ‌it‌ ‌as‌ ‌the‌ ‌assembly‌ ‌line‌ ‌of‌ ‌an‌ ‌automobile‌ ‌manufacturing‌ ‌plant,‌ ‌but‌ ‌for‌ ‌data,” Barot said.‌ ‌ ‌Software development was historically a slow plodding process in which developers spent months or even years working on new updates that were collectively thrown over the wall to testing and operations teams. In 2008, Andrew Clay and Patrick Debois began discussing how to streamline this process through better collaboration between developers, testers, and operation teams. This came to be known as DevOps since it improved the handoff between development and operations teams. As the movement took hold, it led to the creation of a variety of platforms, tools, and processes that allowed teams to continuously integrate and deploy applications in small bits that could be rolled back if problems occurred. But these same kinds of innovations eluded efforts to create value from the growing volume, variety, and velocity of big data. As much as pundits predicted that big data was the new oil, companies struggled to operationalize big data in the way DevOps improved the deployment of code. Value is gleaned from data by creating artifacts like analytics, machine learning models, and data-driven applications. But doing these things introduced a variety of new challenges and bottlenecks outside the scope of DevOps practices. In a blog post for IBM in 2014, Lenny Liebmann, then a contributing editor at InformationWeek, introduced the notion of DataOps to characterize these challenges and suggest a path forward. In an interview with VentureBeat, Liebmann, who is now a founding partner of technology adoption consultancy Morgan Armstrong, said that at the time a lot of enterprises were struggling to solve big data problems using improved technology without addressing the organizational and process side. He said, “People thought you could just throw big data into a magic bucket and it would work.” But they bumped up against a variety of issues connecting disparate sources and types of data to new applications and analytics. One of the main issues he saw was that businesses would focus on the functional aspects, like moving the actual data around through better data engineering tools, without addressing non-functional issues like performance, availability, quality, scalability, security, and governance. A lot of the fundamental data engineering challenges have been solved as enterprises begin moving their infrastructure to the cloud. “This is less a problem today than when I first talked about it,” Liebmann said. The next step lies in mapping out a strategy to address security, governance, and quality issues as companies scale their data operations. Barot has had many conversations with enterprises asking for DataOps tools only to discover they already had a strong DataOps framework. They really needed more help in operationalizing their AI processes. This is where Gartner’s model of XOps emerges to provide the foundation for a more comprehensive set of distinctions. “We were looking at all these ‘ops’ terminologies in the marketplace, and there was ambiguity about what they were for and the relationship between them,” Barot said. “We wanted to set the record straight as to what they stand for and how they are related to each other as part of bigger strategic initiatives in the enterprise.” In this expanded taxonomy, Gartner constrains DataOps to the challenges associated with building, managing, and scaling data pipelines in a way that promotes reusability, reproducibility, and rolling back changes if problems occur. Some of these key capabilities include data extraction, integration, transformation, and analysis. Governance is constrained to the data itself. MLOps focuses on improving the collaboration across development and operationalization of the machine learning model development lifecycle. These activities are typically performed outside of the purview of traditional data engineering practices. Data scientists are often tasked with a process called feature engineering for tuning ML models to improve decision-making, discover insight, or enable a new application feature. MLOps makes it easier to tie these efforts in with teams on the operations side that are responsible for deploying the models into production. ModelOps is an extension of MLOps to help companies work with third-party AI models that may be baked into enterprise applications or improve decision-making using tools like knowledge graphs, rules engines, or new optimization algorithms. The biggest differentiation is that MLOps makes it easier for business experts to manage AI models with less reliance on data engineering and for data science teams to implement changes. Platform Ops provides an overarching framework to help organizations manage activities that span all of these different kinds of activities, as well as DevOps. It is also the youngest and most immature market. AIOps would probably have been a better term to describe this overall way of thinking about AI management, Barot said. However, the term was already widely used to describe the use of AI to improve IT operations management. While there are dozens of commercial products for the other domains, Barot said there are only four commercial Platform Ops tools today: Amazon SageMaker, Cloudera SDC, ForePaas, and OneLogic. There are also a variety of open source Platform Ops tools that are being championed by commercial vendors as part of their larger portfolio of AI tools. Barot expects to see intense competition among vendors rushing to become the AI orchestration platform other things get plugged into. Barot cautions that there are no silver bullet products. Every enterprise will need to adopt the best capabilities suited to their existing development practices and industry niche."
https://venturebeat.com/2021/03/27/the-enterprise-guide-to-experimenting-with-data-streaming/,The enterprise guide to experimenting with data streaming ,"Streaming data into your enterprise analytics systems in real time rather than loading it in batches can increase your ability to make time-sensitive decisions. Over the last few years, more and more enterprises and industries have started experimenting with data streaming, including the healthcare and financial services sectors. The global streaming analytics market size is expected to grow from $12.5 billion in 2020 to $38.6 billion by 2025, thanks to adoption in industries like manufacturing, government, energy and utilities, media and more. A company that is looking to explore data streaming capabilities does not need to go “all-in.” In fact, it’s best if you don’t. What’s becoming clear is that you can reap the benefits of data streaming without building out a fully mature solution. Limited projects and proof-of-concept work with data streaming can prove incredibly valuable for your organization. Data streaming concepts are highly transferrable. Learning one platform enables you to adopt other tools and capabilities with ease. So the key is not to start dabbling with data streaming early and often so that your engineering teams can start developing the necessary skillsets related to resilient, distributed system design and development. Adopting a data streaming architecture will help solve a number of challenges that will surface due to the increasing volume and scale of information organizations are able to tap into as a result of digitization. Getting started requires a shift in data strategy and implementation. Data strategy for many businesses, such as brick and mortar stores, manufacturers, and logistics firms, is grounded in core processes oriented to weekly or monthly batch calculations. Often, supporting applications using modern, cloud-based technology stacks are tailored to process data using a monthly ETL load — an inherent limitation to real-time enterprise insights. When you begin prototyping for data streaming, you will quickly uncover technical limitations and hidden requirements that will impact your ability to scale your model. So it’s important to make a deliberate investment in this kind of prototyping so that you can assess any roadblocks to a long-term strategy while creating tangible short-term opportunities to pilot streaming tactics and technologies. Embracing the incremental failures of prototyping is an effective path to a scalable data streaming architecture. Your best prototypes can scale into industry-leading competitive advantages. Failed prototypes, on the other hand, can be shut down after minimal investment and maximum learning. For example, my team built one proof of concept for a client to collect and correlate WiFi, authentication gateway, and endpoint protection platform (EPP) logs. We shut it down due to a lack of any data science models able to correlate events across these sources, but we were able to take away the learning that Syslog, Kafka, Confluent Kafka Connect, and Flink are capable of solving similar integration challenges in the future. Building a POC (proof of concept) or MVP (minimum viable product) always doubles as a risk management strategy by establishing technical feasibility and product viability with minimal investment. Let’s explore ways a data streaming prototype can add value. Start with a small team and a targeted goal of creating a POC solution to solve a particular business and technical problem. Then, evaluate the results to decide how best to scale the POC. Teams should approach prototyping with an exploratory mindset vs. executing a preconceived outcome on a small scale. Embrace failure and learnings when validating your streaming model with prototypes. POC, MVP, pilot — whatever name it goes by, prototyping will stop teams from creating products that don’t (or can’t) meet the business’s needs. You will learn a lot and mitigate a lot of risk by taking this “build, measure, learn” approach to validating your data streaming model before you try to scale it. Apache Kafka is a great place to start as it is the most widely adopted platform. Its cloud counterparts, Microsoft Azure Event Hub and AWS Kinesis, are either 100% compatible at a protocol level or operate using very similar concepts. Apache Kafka, Azure Event Hub, and AWS Kinesis are products focused on data ingestion. Google Dataflow and IBM Streaming Analytics are also popular options that act as a superset — bigger platforms with more capabilities. Since the POC has few risks related to scalability and data retention, you can even deploy a small Kafka cluster on premises. Several Kafka stack distributions such as Confluent, Bitnami, and Cloudera, provide an easy way to launch Kafka and its dependencies on container systems, virtual machines, or even spare PC desktop boxes. A team will want to tap into relational data and push relational data records to a low-latency data stream on Kafka. They will explore Change Data Capture (CDC) protocol and find out it works similarly for both a MS SQL-based warehouse and inventory system and a PostgreSQL-based e-commerce site. Both of these data sources are easily streamed into a Kafka feed category (or “topic”) as events. A modern single-page application (SPA) where customers can manage their personal profile and preferences can be also enriched to emit events to another data topic once relevant customer information is updated. After this analysis, the team will explore how they can aggregate and analyze streaming data. The data streaming and processing landscape (and big data in general) may seem daunting at first. There are many well-known players in the space, such as Flink and Spark for stream processing, MapReduce for batch processing, and Cassandra, HBase, and MariaDB for storing large volumes of data in a read-optimized columnar format. All of the technologies I’ve just mentioned work best to crunch specialized, massive data loads, and the POC does not operate at such a scale. Therefore, your prototype team will want to choose a data ingestion and aggregation platform with a user-friendly interface and SQL-like data retrieval support; it will likely be Confluent Kafka Connect, Lenses.io, Striim, or a similar commercial platform. All of these data sources, when combined, can provide timely insights via custom reports and real-time alerts. For example, if a B2B account has updated its credit limit in a self-service single page app, this event, pushed to a data stream, is available to an e-commerce site right away. Analytics on most products in the highest demand, busiest shopping hours, and even alerts on fraudulent activity (unusually high order amounts) can be produced by aggregating and processing windowed data streams from inventory and e-commerce. Even though the POC does not introduce complex, scalable data processing platforms such as Spark or Hadoop, you will be getting new reports and alerts in near real-time, meaning that the duration to obtain insight is reduced from weeks to minutes or even seconds. The POC will allow you to consider what other processes would benefit from real-time reporting and analytics. Meanwhile, the POC engineering team will learn important lessons about data model design. Poor design will lead to data duplication, which could become expensive and challenging when a POC is scaled to production levels, so it’s important to use these learnings when moving on to future iterations. IT and operations will also have learned that traditional concepts such as “database rollback” are not present in the streaming world. Monitoring is a must for a data streaming platform, as are support personnel with the appropriate skills. You can reduce the cost and complexity of operational support if you choose AWS Kinesis or Azure Event Hub instead of Apache Kafka, since cloud platforms are simpler to maintain. Data streaming provides a natural design for decoupling integrated systems. As data flows, it becomes available to all of its stakeholders independently, enabling services written for isolated use cases like data persistence, aggregate functions, anomaly detection, and many others. All of these are independent in terms of development and deployment. The benefits of having decoupled integrated systems is that each of these pieces can be introduced incrementally. This also allows you to scope your POC and focus on pieces that are important for your organization independently. Once you execute on a POC, there is a decision point: continue iterating, shut it down, or restart. Questions related to data modeling, integrations between systems, and potential AI/ML opportunities should surface at this point, giving your organization better insight into how to staff your development and operations teams for the future of streaming. Lastly, increased awareness of distributed systems will enable your technical teams to improve current back-office systems and map a modernization path for your organization. Bottom line: Your organization has a lot to gain and little to lose by piloting data streaming. Aurimas Adomavicius is President of DevBridge, a tech consultancy specialized in designing and implementing custom software products for companies across many industries."
https://venturebeat.com/2021/03/26/ai-weekly-algorithms-accountability-and-regulating-big-tech/,"AI Weekly: Algorithms, accountability, and regulating Big Tech","This week, Facebook CEO Mark Zuckerberg, Google CEO Sundar Pichai, and Twitter CEO Jack Dorsey went back to Congress, the first hearing with Big Tech executives since the January 6 insurrection led by white supremacists that directly threatened the lives of lawmakers. The main topic of discussion was the role social media plays in the spread of extremism and disinformation. The end of liability protections granted by Section 230 of the Communications Decency Act (CDA), disinformation, and how tech can harm the mental health of children were discussed, but artificial intelligence took center stage. The word “algorithm” alone was used more than 50 times. Whereas previous hearings involved more exploratory questions and took on a feeling of Geek Squad tech repair meets policy, in this hearing lawmakers asked questions based on evidence and seemed to treat tech CEOs like hostile witnesses. Representatives repeatedly cited a May 2020 Wall Street Journal article about an internal Facebook study that found that the majority of people who join extremist groups do so because the Facebook recommendation algorithm proposed that they do so. A recent MIT Tech Review article about focusing bias detection to appease conservative lawmakers instead of to reduce disinformation also came up, as lawmakers repeatedly asserted that self regulation was no longer an option. Virtually throughout the entirety of the more than five-hour long hearing, there was a tone of unvarnished repulsion and disdain for exploitative business models and willingness to sell addictive algorithms to children. “Big Tech is essentially handing our children a lit cigarette and hoping they stay addicted for life,” Rep. Bill Johnson (R-OH) said. In his comparison of Big Tech companies to Big Tobacco — a parallel drawn at Facebook and a recent AI research paper — Johnson quotes then-Rep. Henry Waxman (D-CA), who stated in 1994 that Big Tobacco had been “exempt from standards of responsibility and accountability that apply to all other American corporations.” Some congresspeople suggested laws to require tech companies to publicly report diversity data at all levels of a company and to prevent targeted ads that push misinformation to marginalized communities including veterans. Rep. Debbie Dingell (D-MI) suggested a law that would establish an independent organization of researchers and computer scientists to identify misinformation before it goes viral. Pointing to YouTube’s recommendation algorithm and its known propensity to radicalize people, Reps. Anna Eshoo (D-CA) and Tom Malinowski (D-NJ) introduced the Protecting Americans from Dangerous Algorithms Act back in October to amend Section 230 and allow courts to examine the role of algorithmic amplification that leads to violence. Next to Section 230 reform, one of the most popular solutions lawmakers proposed was a law requiring tech companies to perform civil rights audits or algorithm audits for performance. It might be cathartic seeing tech CEOs whose attitudes are described by lawmakers as smug and arrogant get their come-uppances for inaction on systemic issues that threaten human lives and democracy because they’d rather make more money. But after the bombast and bipartisan recognition of how AI can harm people on display Thursday, the pressure is on Washington, not Silicon Valley. I mean, of course Zuckerberg or Pichai will still need to answer for it when the next white supremacist terrorist action happens and it’s again drawn directly back to a Facebook group or YouTube indoctrination, but to date, lawmakers have no record of passing sweeping legislation to regulate the use of algorithms. Bipartisan agreement for regulation of facial recognition and data privacy has also not yet paid off with comprehensive legislation. Mentions of artificial intelligence and machine learning in Congress are at an all-time high. And in recent weeks, a national panel of industry experts have urged AI policy action to protect the national security interests of the United States, and Google employees have implored Congress to pass stronger laws to protect people who come forward to reveal ways AI is being used to harm people. The details of any proposed legislation will reveal just how serious lawmakers are about bringing accountability to those who make the algorithms. For example, diversity reporting requirements should include breakdowns of specific teams working with AI at Big Tech companies. Facebook and Google release diversity reports today, but those reports do not break down AI team diversity. Testing and agreed-upon standards are table stakes in industries where products and services can harm people. You can’t break ground on a construction project without an environmental impact report, and you can’t sell people medicine without going through the Food and Drug Administration, so you probably shouldn’t be able to freely deploy AI that reaches billions of people that’s discriminatory or peddles extremism for profit. Of course, accountability mechanisms meant to increase public trust can fail. Remember Bell, the California city that regularly underwent financial audits but still turned out to be corrupt? And algorithm audits don’t always assess performance. Even if researchers document a propensity to do harm, like analysis of Amazon’s Rekognition or YouTube radicalization showed in 2019, that doesn’t mean that AI won’t be used in production today. Regulation of some kind is coming, but the unanswered question is whether that legislation will go beyond the solutions tech CEOs endorse. Zuckerberg voiced support for federal privacy legislation, just as Microsoft has done in fights with state legislatures attempting to pass data privacy laws. Zuckerberg also expressed some backing for algorithm auditing as an “important area of study”; however, Facebook does not perform systematic audits of its algorithms today, even though that’s recommended by a civil rights audit of Facebook completed last summer. Last week, the Carr Center at Harvard University published an analysis of the human rights impact assessments (HRIAs) Facebook performed regarding its product and presence in Myanmar following a genocide in that country. That analysis found that a third-party HRIA largely omits mention of the Rohingya and fails to assess if algorithms played a role. “What is the link between the algorithm and genocide? That’s the crux of it. The U.N. report claims there is a relationship,” coauthor Mark Latonero told VentureBeat. “They said essentially Facebook contributed to the environment where hateful speech was normalized and amplified in society.” The Carr report states that any policy demanding human rights impact assessments should be wary of such reports from the companies, since they tend to engage in ethics washing and to “hide behind a veneer of human rights due diligence and accountability.” To prevent this, researchers suggest performing analysis throughout the lifecycle of AI products and services, and attest that to center the impact of AI requires viewing algorithms as sociotechnical systems deserving of evaluation by social and computer scientists. This is in line with a previous research that insists AI be looked at like a bureaucracy, as well as AI researchers working with critical race theory. “Determining whether or not an AI system contributed to a human rights harm is not obvious to those without the appropriate expertise and methodologies,” the Carr report reads. “Furthermore, without additional technical expertise, those conducting HRIAs would not be able to recommend potential changes to AI products and algorithmic processes themselves in order to mitigate existing and future harms.” Evidenced by the fact that multiple members of Congress talked about the perseverance of evil in Big Tech this week, policymakers seem aware AI can harm people, from spreading disinformation and hate for profit to endangering children, democracy, and economic competition. If we all agree that Big Tech is in fact a threat to children, competitive business practices, and democracy, if Democrats and Republicans fail to take sufficient action, in time it could be lawmakers who are labeled untrustworthy. For AI coverage, send news tips to Khari Johnson and Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark The Machine. Thanks for reading, Khari Johnson Senior AI Staff Writer"
https://venturebeat.com/2021/03/26/fable-studio-opens-its-virtual-beings-wizard-engine-to-collaborators/,Fable Studio opens its virtual beings Wizard Engine to collaborators,"Fable Studio is opening its virtual beings Wizard Engine to collaborators in fashion and nursing. The San Francisco company built the engine to manage its virtual character Lucy, an 8-year-old animated girl from the Wolves in the Walls virtual reality experience. Lately, Lucy has made appearances at the Sundance Film Festival and SXSW as a virtual character. She sang songs and talked with the audience, using her AI-based personality, on Twitch. The Wizard Engine enables Fable to manage Lucy’s appearances on different social media platforms. Fable CEO Edward Saatchi said in an interview with GamesBeat that Fable envisions a world where young people have virtual beings as close friends. It is opening up the Wizard Engine that it uses to make virtual beings such as Lucy, as well as the newer Charlie and Beck characters. The Wizard Engine lets creators schedule events on different platforms for a virtual being and create new milestones in the lives of the characters, Saatchi said. “We’re announcing that we’re opening up the Wizard Engine for people to use themselves,” Saatchi said. “We’re going to open it up for signups. You’ll be able to create the AI characters, distribute them on Twitch, Instagram, and YouTube. We use it to create these monthly beats in Lucy’s life.” The Wizard Engine is the tool that Fable uses to bring to life to its virtual beings. It works on three things: generation, distribution, and memory. The tool lets Fable take a virtual being as conceived — the bible and backstory of their life, a synopsis of what will happen to them — and uses that to generate on an ongoing basis the content of their life. It generates the voice, animation, text dialogue, and video. These things would realistically make up the virtual beings life (what their knowledge base about their past is, what happened yesterday, what’s happening right now, what are they looking forward to etc). One of the tools the Wizard Engine uses is OpenAI’s GPT3 technology to generate Lucy’s responses. The Wizard Engine also distribute the virtual being’s life. It allows Fable and collaborators to distribute virtual beings across many media platforms: on Twitch, text messaging apps, photo sharing apps, in videogames, and eventually in metaverses. This is important because virtual beings should be like us, Saatchi said. There’s no Edward app to chat with Edward and similarly virtual beings shouldn’t be confined to an app download, but live across different media platforms, Saatchi said. And the engine gives your virtual being memories. It allows Fable and collaborators to build virtual beings with memory so that the beings can remember their own lives, what they are learning about their friends, and combine those to create empathetic moments where users feel “seen.” You could almost think of the Wizard Engine as an AI game engine, he said. It allows collaborators to generate and deploy interactive experiences with virtual beings to delight millions of users. So far the interactive/gaming and deep learning worlds have remained very separate, but virtual beings will only exist if they can work together. By opening up the opportunity to work with the Wizard Engine to a small number of collaborators, Fable hopes to show that the intersection of AI and interactivity can be a powerful one. To date, nonplayer interactive characters have been centralized — usually in a videogame. With the Wizard Engine, Fable is decentralizing nonplayer characters and virtual beings. “We need virtual beings to be seen in lots of places. That’s why we had Lucy on Twitch at SXSW. Right now, NPCs exist only in games. But they can be on other platforms,” Saatchi said. Collaborators will be able to work with Fable to create a virtual being in one of these two spaces — fashion and health care, creating a backstory, life, and aesthetic for the character. The creators will be able to plan what will happen to the character over the course of their first year, how they’ll interact with friends. Collaborators can monetize through the careers of the virtual beings they create with Fable. For a fashion virtual being, imagine a young man leaving design school and while working in physical fashion by day working on nights and weekends to build digital fashion items that can be sold as nonfungible tokens (NFTs). “We’re building the Wizard Engine with that decentralized future in mind, and the character might spend some time as an NPC in a game or texting or chatting or broadcasting on Twitch,” Saatchi said. But making money isn’t the only purpose here. “This is less about making money and more about finding early collaborators in fields of fashion and health,” Saatchi said. “There might be great ideas for virtual beings in that space. I think these digital characters pretty quickly start to have digital work products that could be purchased.” Separately, a nurse who struggles with compassion fatigue with friends and relatives can, after a long day of work, unwind by playing games with friends and hanging out in virtual concerts with friends. That nurse could perform some simple healthcare checks online and make money doing it — as a virtual character. At SXSW last week, Lucy played her first virtual concert on Twitch to show off the Wizard engine. She interacted with viewers, most of whom have been friends with Lucy for months. She sang three songs: Toys, My Friends, and Nana’s song. The lyrics unlocked parts of her story for users who text and call with her. And now Lucy will be able to talk to friends about the concert and remember it when they talk. Potential collaborators need to answer a series of questions about the virtual being they would like to build — the career the virtual being would pursue, a bit about the backstory of the virtual being, and what they plan to happen to the virtual being. Fable envisions a world where virtual beings are paid for their digital work. “We are really interested in the idea of just telling a story with intimacy and connection,” Saatchi said. “We want the Wizard Engine to be something that can empower them.”"
